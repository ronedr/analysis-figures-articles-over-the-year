year,article_id,figure_name,figure_caption,figure_img
2010,5169847,Fig. 1.,Block diagram of the RF device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5471724/5169847/5169847-fig-1-source-large.gif
2010,5169847,Fig. 2.,Percentile variability explained by each principal component.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5471724/5169847/5169847-fig-2-source-large.gif
2010,5169847,Fig. 3.,Projection of devices onto the top three principal components.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5471724/5169847/5169847-fig-3-source-large.gif
2010,5169847,Fig. 4.,Zoom in the core of functional devices in the 3-D plot of Fig. 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5471724/5169847/5169847-fig-4-source-large.gif
2010,5169847,Fig. 5.,"Test error (i.e., average number of misclassified devices in the validation set) versus normalized test cost [defined in (4)] when using only non-RF specification tests.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5471724/5169847/5169847-fig-5-source-large.gif
2010,5169847,Fig. 6.,"Test error (i.e., average number of misclassified devices in the validation set) versus normalized test cost [defined in (4)] when adding RF specifciation tests to the best selected subsets (in terms of test error) of Fig. 5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5471724/5169847/5169847-fig-6-source-large.gif
2010,5491191,Fig. 1.,Chaos-based CDMA model with a multiuser detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-1-source-large.gif
2010,5491191,Fig. 2.,Chaotic series generated from cubic mapping. (a) Two series of close initial conditions. (b) Autocorrelation of the first 50 chips.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-2-source-large.gif
2010,5491191,Fig. 3.,"BER comparisons of the chaos-based CDMA with SVM detector and Gold-DS-CDMA, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-3-source-large.gif
2010,5491191,Fig. 4.,"Comparing the recent result with the proposed method, AWGN channel, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-4-source-large.gif
2010,5491191,Fig. 5.,"Number of support vectors from the SVM model for different
C
parameters, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-5-source-large.gif
2010,5491191,Fig. 6.,"Error rate of the SVM model with linear kernel for different
C
parameters,
E
b
/
N
o
=2dB
, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-6-source-large.gif
2010,5491191,Fig. 7.,"Error rate of the SVM model with linear kernel for different training sizes, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-7-source-large.gif
2010,5491191,Fig. 8.,"Number of support vectors from the SVM model for different training sizes, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-8-source-large.gif
2010,5491191,Fig. 9.,"BER of the MUDs under AWGN, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-9-source-large.gif
2010,5491191,Fig. 10.,"BER of the MUDs under Rayleigh fading, ten users,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-10-source-large.gif
2010,5491191,Fig. 11.,"BER of the SVM detector for different users,
E
b
/
N
o
=8dB
,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-11-source-large.gif
2010,5491191,Fig. 12.,"3-D BER of the MUDs under AWGN,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-12-source-large.gif
2010,5491191,Fig. 13.,"3-D BER of the MUDs under Rayleigh fading,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-13-source-large.gif
2010,5491191,Fig. 14.,Signal points and the decision boundaries from the numerical example.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-14-source-large.gif
2010,5491191,Fig. 15.,"Complexity of the detectors,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-15-source-large.gif
2010,5491191,Fig. 16.,"Output of noiseless test samples from the matched filter, 30 users.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-16-source-large.gif
2010,5491191,Fig. 17.,"Output of noiseless test samples from the SVM detector, 30 users.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-17-source-large.gif
2010,5491191,Fig. 18.,"Verification of the theoretical BER expression for the SVM detector, AWGN channel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-18-source-large.gif
2010,5491191,Fig. 19.,"Verification of the theoretical BER expression for the SVM detector, Rayleigh fading channel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-19-source-large.gif
2010,5491191,Fig. 20.,"Near-far performance of the SVM detector, 20 users,
E
b
/
N
0
=15dB
, Rayleigh fading.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-20-source-large.gif
2010,5491191,Fig. 21.,"Tolerance of SVM detector,
E
b
/
N
o
=8dB
,
2β=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5536895/5491191/5491191-fig-21-source-large.gif
2010,5430874,Fig. 1.,"Artificial data. (a)–(c) Prototypes and receptive fields. (a) GMLVQ without regularization. (b) LGMLVQ without regularization. (c) LGMLVQ with
η=0.15
. (d) Training set transformed by global matrix
Ω
after GMLVQ training. (e) and (f) Training set transformed by local matrices
Ω
1
,
Ω
2
after LGMLVQ training. (g) and (h) Training set transformed by local matrices
Ω
1
,
Ω
2
obtained by LGMLVQ training with
η=0.005
. (i) and (j) Training set transformed by local matrices
Ω
1
,
Ω
2
obtained by LGMLVQ training with
η=0.15
. In (d)–(j) the dotted lines correspond to the eigendirections of
Λ
or
Λ
1
and
Λ
2
, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-1-source-large.gif
2010,5430874,Fig. 2.,"Artificial data. The plots relate to experiments on a single data set. (a)Evolution of error rate on validation set during LGMLVQ-training with
η=0
and
η=0.005
. (b) Coordinates of the class 2 prototype during LGMLVQ-training with
η=0
and
η=0.005
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-2-source-large.gif
2010,5430874,Fig. 3.,"Pima Indians Diabetes data set. Evolution of relevance values
λ
and eigenvalues
ν=eig(Λ)
observed during a single training run of (a) GRLVQ and (b) GMLVQ with
Ω∈
IR
8×8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-3-source-large.gif
2010,5430874,Fig. 4.,"Pima Indians Diabetes data set. Mean error rates on training and validation sets after training different algorithms with different regularization parameters
η
. (a) GRLVQ. (b) GMLVQ with
Ω∈
IR
8×8
. (c) GMLVQ with
Ω∈
IR
2×8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-4-source-large.gif
2010,5430874,Fig. 5.,"Pima Indians Diabetes data set. Dependency of the largest relevance value
λ
1
in GRLVQ and the largest eigenvalue
ν
1
in GMLVQ on the regularization parameter
η
. The plots are based on the mean relevance factors and mean eigenvalues obtained with the different training sets at the end of training. (a)Comparison between GRLVQ and GMLVQ with
Ω∈
IR
8×8
. (b) GMLVQ with
Ω∈
IR
2×8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-5-source-large.gif
2010,5430874,Fig. 6.,"Pima Indians Diabetes data set. Two-dimensional representation of the complete data set found by GMLVQ with
Ω∈
IR
2×8
and (a)
η=0
and (b)
η=2.0
obtained in one training run. The dotted lines correspond to the eigendirections of
Ω
Ω
T
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-6-source-large.gif
2010,5430874,Fig. 7.,"Glass Identification data set. Mean error rates on training and validation sets after training different algorithms with different regularization parameters
η
. Training of relevance matrices in GMLVQ and local GMLVQ is based on
Ω,
Ω
j
∈
IR
9×9
. (a) GRLVQ. (b) GMLVQ. (c) Local GMLVQ.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-7-source-large.gif
2010,5430874,Fig. 8.,"Letter Recognition data set. Mean error rates on training and validation sets after training different algorithms with different regularization parameters
η
. (a) GRLVQ. (b) GMLVQ with
Ω∈
IR
16×16
. (c) GMLVQ with
Ω∈
IR
16×16
and three prototypes per class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-8-source-large.gif
2010,5430874,Fig. 9.,"Letter Recognition data set. Comparison of mean eigenvalue profiles of final matrix
Λ
obtained by GMLVQ training
(Ω∈
IR
16×16
)
with different numbers of prototypes and different regularization parameters. (a)
η=0
. (b)
η=0.01
. (c)
η=0.05
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430874/5430874-fig-9-source-large.gif
2010,5398989,Fig. 1.,"Text classification: collection, preprocessing, learning, and evaluation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-1-source-large.gif
2010,5398989,Fig. 2.,Text-classification tasks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-2-source-large.gif
2010,5398989,Fig. 3.,SVM optimal separating hyperplane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-3-source-large.gif
2010,5398989,Fig. 4.,RVM two-class classification example with four RVs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-4-source-large.gif
2010,5398989,Fig. 5.,Example of a DAG representation of a text-classification dataflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-5-source-large.gif
2010,5398989,Fig. 6.,"Text-classification dataflow, represented using a DAG with arrows identifying task dependencies with underlying file exchange. When a new input document is presented to the system and there is no need to update the models, only the files and the subtasks with underlined bottom part are involved.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-6-source-large.gif
2010,5398989,Fig. 7.,Text-classification DAG for the SVM model with Reuters-21578.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-7-source-large.gif
2010,5398989,Fig. 8.,Text-classification DAG for the SVM model with the RCV1 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-8-source-large.gif
2010,5398989,Fig. 9.,Text-classification DAG for the RVM model with the Reuters-21578.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-9-source-large.gif
2010,5398989,Fig. 10.,Text-classification DAG for the ensemble SVM model with the Reuters-21578.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-10-source-large.gif
2010,5398989,Fig. 11.,Final DAG processing times. (a) Baseline algorithms. (b) Ensembles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5446559/5398989/5398989-fig-11-source-large.gif
2010,5570879,Fig. 1.,(a) Tract variables (TVs) from different constriction locations. (b) Pellet placement locations according to [115].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-1-source-large.gif
2010,5570879,Fig. 2.,Gestural activations for the utterance “miss you.” Active gesture regions are marked by rectangular solid (colored) blocks. Smooth curves represent the corresponding tract variable (TV) trajectories.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-2-source-large.gif
2010,5570879,Fig. 3.,"Example of “perfect memory” adapted from [43], showing the acoustic signal and the recorded articulatory data. (a) Shows the case where “perfect” and “memory” are uttered as two different words (note, the /t/ burst is clearly visible. (b) Shows the utterance of “perfect memory” in a fluent sentence where the /t/ burst is reduced in the acoustic waveform.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-3-source-large.gif
2010,5570879,Fig. 4.,Flow diagram for generating synthetic speech and the associated articulatory information using TADA and HLSyn.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-4-source-large.gif
2010,5570879,Fig. 5.,Architecture of the ANN-based direct inverse model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-5-source-large.gif
2010,5570879,Fig. 6.,Architecture of the AR-ANN-based direct inverse model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-6-source-large.gif
2010,5570879,Fig. 7.,Distal supervised learning approach for obtaining acoustic to TV mapping.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-7-source-large.gif
2010,5570879,Fig. 8.,MDN architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-8-source-large.gif
2010,5570879,Fig. 9.,"Hierarchical
ε
-SVR architecture for generating the TVs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-9-source-large.gif
2010,5570879,Fig. 10.,PPMC for TV estimation from different architectures using MFCC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-10-source-large.gif
2010,5570879,Fig. 11.,PPMC for TV estimation from different architectures using AP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-11-source-large.gif
2010,5570879,Fig. 12.,Normalized RMSE for TV estimation from different architectures using MFCC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-12-source-large.gif
2010,5570879,Fig. 13.,Normalized RMSE for TV estimation from different architectures using AP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-13-source-large.gif
2010,5570879,Fig. 14.,Actual and estimated TVs from ANN and ANN+Kalman using MFCC as the acoustic feature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570879/5570879-fig-14-source-large.gif
2010,5357420,Fig. 1.,"Robustness and adaptiveness comparison for neighbors selected by
ℓ
1
-graph and
k
-nearest-neighbor graph. (a) Illustration of basis samples (first row), reconstruction coefficient distribution in
ℓ
1
-graph (left), samples to reconstruct (middle, with added noises from the third row on), and similarity distribution of the
k
nearest neighbors selected with Euclidean distance (right) in
k
-nn graph. Here, the horizontal axes indicate the index number of the training samples. The vertical axes of the left column indicate the reconstruction coefficient distribution for all training samples in sparse coding, and those of right column indicate the similarity value distribution of
k
nearest neighbors. Note that the number in parenthesis is the number of neighbors changed compared with results in the second row, and
ℓ
1
-graph shows much more robust to image noises. (b) Neighboring samples comparison between
ℓ
1
-graph and
k
-nearest-neighbor graph. The red bars indicate the numbers of the neighbors selected by
ℓ
1
-graph automatically and adaptively. The green bars indicate the numbers of kindred samples among the
k
neighbors selected by
ℓ
1
-graph, and the blue bar indicate the numbers of kindred samples within the
k
nearest neighbors measured by Euclidean distance in
k
-nn graph. Note that the results are obtained on USPS digit database [11] and the horizontal axis indicates the index of the reference sample to reconstruct. (a) Neighbor robustness comparison of
ℓ
1
-graph and
k
-nn graph. (b) Datum-adaptive neighbor numbers selected by sparse
ℓ
1
-graph, and kindred neighbor numbers for
ℓ
1
-graph and
k
-nn graph.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/5431097/5357420/5357420-fig-1-source-large.gif
2010,5357420,Fig. 2.,"Visualization comparison of (a) the
ℓ
1
-graph and (b) the
k
-nearest-neighbor graph, where the
k
for each datum is automatically selected in the
ℓ
1
-graph. Note that the thickness of the edge line indicates the value of the edge weight (Gaussian kernel weight for
k
-nearest-neighbor graph). For ease of display, we only show the graph edges related to the samples from two classes and in total 30 classes from the YALE-B database are used for graph construction. (c) Illustration on the positions of a reference sample (red), its kindred neighbors (yellow), and its inhomogeneous neighbors (blue) selected by (i)
ℓ
1
-graph and (ii)
k
-nearest-neighbor method based on samples from the USPS digit database [11]. (a) Example of
ℓ
1
-graph. (b) Example
k
-nearest-neighbor graph. (c) Example
ℓ
1
-graph and fc-NN graph.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/5431097/5357420/5357420-fig-2-source-large.gif
2010,5357420,Fig. 3.,"Visualization of the data clustering results from (a)
ℓ
1
-graph, (b) LE-graph, and (c) PCA algorithm for three clusters (handwritten digits 1, 2, and 3 in the USPS database). The coordinates of the points in (a) and (b) are obtained from the eigenvalue decomposition in the third step of Section III-A. Different colors of the points indicate different digits. (a) Clustering from
ℓ
1
-graph. (b) Clustering from LE-graph. (c) Clustering via PCA + K-means.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/5431097/5357420/5357420-fig-3-source-large.gif
2010,5357420,Fig. 4.,"Comparison clustering accuracies of the
ℓ
1
-graph (red line, one fixed value) and (
k
-nn + LLE)-graphs (blue curve) with variant
k
s on the USPS dataset and
K=7
. It shows that
ℓ
1
-norm is superior over
ℓ
2
-norm in deducing informative graph weights.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/5431097/5357420/5357420-fig-4-source-large.gif
2010,5357420,Fig. 5.,"Visualization comparison of the subspace learning results. They are the first ten basis vectors of (a) PCA, (b) NPE, (c) LPP, and (d)
ℓ
1
-graph calculated from the face images in YALE-B database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/5431097/5357420/5357420-fig-5-source-large.gif
2010,4815257,Fig. 1.,"Scatterplots of the accuracies of MRM and NCM for
k=1,2,5,7,11,21
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5420323/4815257/4815257-fig-1-source-large.gif
2010,5508359,Fig. 1.,"Illustration of the object model. (a) Five patches of an object, where Sigma set
S
(n)
of the
n
th patch is represented as a vector
v
(n)
; (b) an arbitrator (denoted as “A”) makes the decision based on the responses of the five ILKMs corresponding to the five patches; and (c) Model update of a “sparse” ILKM with length
T
. A new observation at time
T+1
is used for update whereas the last observed one at time 1 is removed. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5504518/5508359/5508359-fig-1-source-large.gif
2010,5508359,Fig. 2.,"Curves of relative position error (RPE) versus frame on four sequences. The red solid, blue circle, and green plus curves are for our method, COVT, and MILT respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5504518/5508359/5508359-fig-2-source-large.gif
2010,5508359,Fig. 3.,"Results on four sequences. Rows from top to bottom are for: girl, player, driving, and crossing. Red/blue/green rectangles are for our method, COVT, and MILT respectively. (Greyscale: Light/medium/dark gray rectangles are for MILT, our method, and COVT respectively.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5504518/5508359/5508359-fig-3-source-large.gif
2010,5512961,Fig. 1.,GA technique with LS-SVM algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-1-source-large.gif
2010,5512961,Fig. 2.,SuperPol measuring cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-2-source-large.gif
2010,5512961,Fig. 3.,View of the meshed measurement cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-3-source-large.gif
2010,5512961,Fig. 4.,Microwave properties parameter extracting procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-4-source-large.gif
2010,5512961,Fig. 5.,"Error and CPU time on the test set versus the number of training set data (evaluation of
ε
′
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-5-source-large.gif
2010,5512961,Fig. 6.,"Error and CPU time on the test set versus the number of training set data (evaluation of
ε
′′
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-6-source-large.gif
2010,5512961,Fig. 7.,Permittivity evolution of the ethanol obtained by LS-SVM and iterative inversion method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5512832/5512961/5512961-fig-7-source-large.gif
2010,5451128,Fig. 1.,Test NMSE comparisons of selection- and ensemble-based boosting on the OUTAOUAIS data set as a function of running time in seconds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5475433/5451128/5451128-fig-1-source-large.gif
2010,5451128,Fig. 2.,Test NMSE comparisons of selection- and ensemble-based boosting on the KIN40K data set as a function of running time in seconds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5475433/5451128/5451128-fig-2-source-large.gif
2010,5451128,Fig. 3.,Test NMSE comparisons of selection- and ensemble-based boosting on the SARCOS data set as a function of running time in seconds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5475433/5451128/5451128-fig-3-source-large.gif
2010,5451128,Fig. 4.,Comparisons of test NMSE and CPU time as the function of the number of basis vectors used for different learning algorithms on the OUTAOUAIS data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5475433/5451128/5451128-fig-4-source-large.gif
2010,5451128,Fig. 5.,"Comparisons of the computational efficiency of the ensemble-based boosting by varying the values of
T
(and
S
) with the same
M=500
on the OUTAOUAIS data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5475433/5451128/5451128-fig-5-source-large.gif
2010,5451128,Fig. 6.,Test error rate on the COVTYPE data set as a function of running time in seconds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5475433/5451128/5451128-fig-6-source-large.gif
2010,5473105,Fig. 1.,Implementation flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-1-source-large.gif
2010,5473105,Fig. 2.,"Sample power, performance, and area models of a 65-nm router.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-2-source-large.gif
2010,5473105,Fig. 3.,Total router power versus buffer size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-3-source-large.gif
2010,5473105,Fig. 4.,Total router power versus number of ports.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-4-source-large.gif
2010,5473105,Fig. 5.,Router total power versus buffer length.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-5-source-large.gif
2010,5473105,Fig. 6.,Router total power versus number of ports.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-6-source-large.gif
2010,5473105,Fig. 7.,Maximum implemented clock frequency versus given clock frequency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-7-source-large.gif
2010,5473105,Fig. 8.,Router leakage power versus clock frequency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-8-source-large.gif
2010,5473105,Fig. 9.,Router energy-per-bit versus choice of microarchitectural parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/5577467/5473105/5473105-fig-9-source-large.gif
2010,5204203,Fig. 1.,Conceptual model of the equilibrium point control hypothesis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-1-source-large.gif
2010,5204203,Fig. 2.,"Graphical representation of the end-effector's stiffness in Cartesian space. The lengths
λ
1
and
λ
2
of the principal axes and the relative angle
θ
represent the magnitude and the orientation of the end-effector's stiffness, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-2-source-large.gif
2010,5204203,Fig. 3.,"Planar robotic manipulator (
L
1
and
L
2
: lengths of the links;
M
1
and
M
2
: masses of the links; and
I
1
and
I
2
: inertias of the links).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-3-source-large.gif
2010,5204203,Fig. 4.,"Change of stiffness ellipse for one time step. (a) The magnitude (area of the ellipse). (b) The shape (length ratio of the major and minor axes). (c) The orientation (direction of the major axis). (Solid line: stiffness ellipse at time
t
; dashed line: stiffness ellipse at time
t+1
.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-4-source-large.gif
2010,5204203,Fig. 5.,Door-opening task. The end-effector is connected with the doorknob by a revolute joint hinge. The door hinge has rotational spring and damper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-5-source-large.gif
2010,5204203,Fig. 6.,Virtual trajectories for door opening (dotted line: actual trajectory of doorknob).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-6-source-large.gif
2010,5204203,Fig. 7.,Stiffness ellipse trajectories. (a) Understeering. (b) Exact Steering. (c) Oversteering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-7-source-large.gif
2010,5204203,Fig. 8.,Learning effects of performance indexes. (a) Position error. (b) Lateral force. (Gray bar: before learning; black bar: after learning.),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-8-source-large.gif
2010,5204203,Fig. 9.,Stiffness ellipse trajectories. (Dotted line: virtual trajectory; solid line: actual trajectory.) (a) Before learning. (b) After learning (reward1). (c) After learning (reward2).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-9-source-large.gif
2010,5204203,Fig. 10.,Learning effects of performance indexes (average of ten learning trials). (a) Position error. (b) Torque rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-10-source-large.gif
2010,5204203,Fig. 11.,Catching a flying ball.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-11-source-large.gif
2010,5204203,Fig. 12.,Stiffness ellipse trajectory during the ball-catching task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-12-source-large.gif
2010,5204203,Fig. 13.,Learning effect of the impulse (time integral of the contact force).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5431005/5204203/5204203-fig-13-source-large.gif
2010,5378519,Fig. 1.,ROC curves for the eclectic rules and the SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5505920/5378519/5378519-fig-1-source-large.gif
2010,5378519,Fig. 2.,ROC curves for SQRex-SVM rules and the SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5505920/5378519/5378519-fig-2-source-large.gif
2010,5378519,Fig. 3.,ROC curve for waist circumference as predictor for hyperglycemia.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5505920/5378519/5378519-fig-3-source-large.gif
2010,5378519,Fig. 4.,ROC curve for waist circumference as predictor for hypertension.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5505920/5378519/5378519-fig-4-source-large.gif
2010,5378519,Fig. 5.,Distribution of diabetic subjects over waist circumference groups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5505920/5378519/5378519-fig-5-source-large.gif
2010,5256260,Fig. 1.,"EMG signal from the right soleus of an RA patient. The EMG signal recorded at 2000 Hz, was filtered, rectified, normalized and finally time interpolated over individual gait cycles to fit a normalized 200-point time base.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/5455859/5256260/5256260-fig-1-source-large.gif
2010,5256260,Fig. 2.,Accuracy rate: CO versus RA class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/5455859/5256260/5256260-fig-2-source-large.gif
2010,5256260,Fig. 3.,Accuracy rate: CO versus OA class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/5455859/5256260/5256260-fig-3-source-large.gif
2010,5256260,Fig. 4.,ROC curve and ROC area of LSK: CO versus RA class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/5455859/5256260/5256260-fig-4-source-large.gif
2010,5256260,Fig. 5.,ROC curve and ROC area of LSK: CO versus OA class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/5455859/5256260/5256260-fig-5-source-large.gif
2010,5439692,Fig. 1.,"Schematic diagram of the RFS using
z
iterations and selecting
d
′
out of
d
available features. It should be noted that the classification of each individual SVM already requires a voting strategy (Section II-A). This is not shown in the diagram, and the individual outputs refer to a land cover map containing class labels between 1 and
c
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-1-source-large.gif
2010,5439692,Fig. 2.,"AVIRIS data, Hekla, Iceland. False-color composite and corresponding ground truth areas representing 22 land cover classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-2-source-large.gif
2010,5439692,Fig. 3.,"ROSIS-3 data, Pavia, Italy. False-color composite and corresponding ground truth areas representing nine land cover classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-3-source-large.gif
2010,5439692,Fig. 4.,"Hekla data. Overall accuracy (in percent) achieved by the SVM ensemble using different number of iterations, input features, and training samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-4-source-large.gif
2010,5439692,Fig. 5.,Hekla data. Difference between producer's and user's accuracy achieved by SVM ensemble with 25 iterations and a regular SVM classifier using (a) tr#50 and (b) tr#200.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-5-source-large.gif
2010,5439692,Fig. 6.,Hekla data. Classification maps achieved by SVM and SVM ensemble (number of features/iterations: 30%/25) with training set tr#50.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-6-source-large.gif
2010,5439692,Fig. 7.,Hekla data. Detailed visual comparison of classification maps achieved by SVM and SVM ensemble (number of features/iterations: 30%/25) with training set tr#50.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-7-source-large.gif
2010,5439692,Fig. 8.,"Pavia data. Overall accuracy in percentage achieved by the SVM ensemble using different number of iterations, input features, and training samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-8-source-large.gif
2010,5439692,Fig. 9.,Pavia data. Difference between producer's and user's accuracy achieved by SVM ensemble with 25 iterations and a regular SVM classifier using (a) tr#50 and (b) tr#200.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-9-source-large.gif
2010,5439692,Fig. 10.,"Data set 2, Pavia. Classification maps achieved by SVM and SVM ensemble (number of features/iterations: 30%/25) using training set tr#100.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5487390/5439692/5439692-fig-10-source-large.gif
2010,5173566,Fig. 1.,AdaBoost algorithm for the binary classification task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-1-source-large.gif
2010,5173566,Fig. 2.,Self-training scheme with lexical and prosodic features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-2-source-large.gif
2010,5173566,Fig. 3.,Self-training algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-3-source-large.gif
2010,5173566,Fig. 4.,Co-training scheme with lexical and prosodic features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-4-source-large.gif
2010,5173566,Fig. 5.,Co-training agreement algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-5-source-large.gif
2010,5173566,Fig. 6.,Co-training disagreement algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-6-source-large.gif
2010,5173566,Fig. 7.,Self-combined algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-7-source-large.gif
2010,5173566,Fig. 8.,F-measure results of the different strategies for the lexical features only.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-8-source-large.gif
2010,5173566,Fig. 9.,NIST results of the different strategies for the lexical features only.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-9-source-large.gif
2010,5173566,Fig. 10.,F-measure results of the different strategies for the prosodic features only.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-10-source-large.gif
2010,5173566,Fig. 11.,NIST results of the different strategies for the prosodic features only.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-11-source-large.gif
2010,5173566,Fig. 12.,F-measure results of the different strategies using all the features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-12-source-large.gif
2010,5173566,Fig. 13.,NIST results of the different strategies using all the features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5325825/5173566/5173566-fig-13-source-large.gif
2010,4770093,Fig. 1.,Convergence behaviors of (a) surrogate duality gap and (b) dual residuals for Leukemia data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-1-source-large.gif
2010,4770093,Fig. 2.,"Resultant feature selector and pattern identifier
β
for Leukemia data. (a) Final result of
β
for Leukemia data. (b) Sorted magnitudes of
β
k
for Leukemia data in decreasing order. Only the largest 35 are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-2-source-large.gif
2010,4770093,Fig. 3.,Resultant slack variables for Leukemia data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-3-source-large.gif
2010,4770093,Fig. 4.,Convergence behaviors of (a) surrogate duality gap and (b) dual residuals for Prostate Cancer data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-4-source-large.gif
2010,4770093,Fig. 5.,"Resultant feature selector
β
for Prostate Cancer data. (a) Final result of
β
for Prostate Cancer data. (b) Sorted magnitudes of
β
k
for Prostate Cancer data. Only the largest 35 are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-5-source-large.gif
2010,4770093,Fig. 6.,Convergence behaviors of (a) surrogate duality gap and (b) dual residuals for Lung Cancer data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-6-source-large.gif
2010,4770093,Fig. 7.,"Resultant feature selector
β
for Lung Cancer data. (a) Final result of
β
for Lung Cancer data. (b) Sorted magnitudes of
β
k
in descending order for Lung Cancer data. Only the largest 35 are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5614813/4770093/4770093-fig-7-source-large.gif
2010,5440907,Fig. 1.,GAFCMdd algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-1-source-large.gif
2010,5440907,Fig. 2.,SAFCMdd algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-2-source-large.gif
2010,5440907,Fig. 3.,MoDEFCMdd algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-3-source-large.gif
2010,5440907,Fig. 4.,True clustering of synthetic data sets using VAT representation. (a) Cat_100_8_3 data set. (b) Cat_300_15_5 data set. (c) Cat_500_20_10 data set. (d) Cat_1000_7_7 data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-4-source-large.gif
2010,5440907,Fig. 5.,True clustering of synthetic data sets using VAT representation. (a) Congressional votes data set. (b) Zoo data set. (c) Soybean data set. (d) Heart data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-5-source-large.gif
2010,5440907,Fig. 6.,"Range of ARI for different clustering algorithms on (a) Cat_100_8_3, (b) Cat_300_15_5, (c) Cat_500_20_10, and (d) Cat_1000_7_7.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-6-source-large.gif
2010,5440907,Fig. 7.,"Range of ARI for different clustering algorithms on (a) Votes, (b) Soybean, (c) Zoo, and (d) Heart.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-7-source-large.gif
2010,5440907,Fig. 8.,"Convergence plot of different clustering algorithms on (a) Cat_100_8_3, (b) Cat_300_15_5, (c) Cat_500_20_10, and (d) Cat_1000_7_7.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-8-source-large.gif
2010,5440907,Fig. 9.,"Convergence plot of different clustering algorithms on (a) Votes, (b) Soybean, (c) Zoo, and (d) Heart.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-9-source-large.gif
2010,5440907,Fig. 10.,Algorithm of integrated differential fuzzy clusteing with SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5484919/5440907/5440907-fig-10-source-large.gif
2010,5471105,Fig. 1.,Simon robot interacting with a teacher.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5471105/5471105-fig-1-source-large.gif
2010,5416712,Fig. 1.,Multiobjective regularized negative correlation learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5611455/5416712/5416712-fig-1-source-large.gif
2010,5416712,Fig. 2.,"Comparison of MRNCL, MNCL, and MoNN on four synthetic classification data sets. Two classes are shown as crosses and dots. The separating lines were obtained by projecting test data over a grid. (a) Synth. (b) Overlap. (c) Bumpy. (d) Relevance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5611455/5416712/5416712-fig-2-source-large.gif
2010,5416712,Fig. 3.,"Illustration of the mean value of these three objectives in different generations. The arrow points from the beginning (
Generation=1
) to end (
Generation=100
). The gray scale indicates generations. (a) Synth. (b) Overlap. (c) Bumpy. (d) Relevance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5611455/5416712/5416712-fig-3-source-large.gif
2010,5416712,Fig. 4.,Illustration of diversity (measured by a commonly-use diversity measure Q statistics) during the evolution. (a) Synth. (b) Overlap. (c) Bumpy. (d) Relevance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5611455/5416712/5416712-fig-4-source-large.gif
2010,5416712,Fig. 5.,"3D view of the last population with three objectives: training error, regularization, and correlation for four synthetic classification data sets. (a) Synth. (b) Overlap. (c) Bumpy. (d) Relevance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5611455/5416712/5416712-fig-5-source-large.gif
2010,5416712,Fig. 6.,"Comparison of MRNCL, MNCL, and MoNN on two classification data sets. Two classes are shown as crosses and dots. The separating lines were obtained by projecting test data over a grid. In Figs. 6a and 6b, these decision boundaries are MRNCL (gray thick), MNCL (black medium), and MoNN (dotted), respectively. The randomly selected noise points are marked with a circle. Figs. 6c and 6d show classification error of MRNCL, MNCL, and MoNN versus noise levels on synth and banana data sets. The results are based on 100 runs. (a) Synth with 20 percent noise. (b) Banana with 20 percent noise. (c) Synth with different noise levels. (d) Banana with different noise levels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5611455/5416712/5416712-fig-6-source-large.gif
2010,5499452,Fig. 1.,"A general event/interaction prediction pipeline. On the left, a sample input sentence is transformed step by step according to the processing units in the pipeline on the right.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5499452/5499452-fig-1-source-large.gif
2010,5499452,Fig. 2.,"Overall system architecture: the biocreative metaserver sends requests to our XML-RPC interface, which simply forwards the request to our U-Compare system (see Fig. 3).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5499452/5499452-fig-2-source-large.gif
2010,5499452,Fig. 3.,"Five different workflows: we evaluated five combinations of available processing modules. Workflows 3 and 4 used some extra tokenization rules to remove sentence splitting errors caused by “Fig., Prof., Dr., etc.” For named entity recognition, workflows 1 and 3 used only strong predictions from SwissProt, while 2 and 4 used all SwissProt predictions, and workflow 5 used all predictions from TrEMBL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5499452/5499452-fig-3-source-large.gif
2010,5499452,Fig. 4.,Named entity recognition system: MedTNER.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5499452/5499452-fig-4-source-large.gif
2010,5499452,Fig. 5.,Biocreative protein INT results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5499452/5499452-fig-5-source-large.gif
2010,5499452,Fig. 6.,BC-IPT results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5499452/5499452-fig-6-source-large.gif
2010,5395689,Fig. 1.,MLPN for Example 2.2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5415834/5395689/5395689-fig-1-source-large.gif
2010,5395689,Fig. 2.,FALCON structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5415834/5395689/5395689-fig-2-source-large.gif
2010,5395689,Fig. 3.,MLPN for Example 3.1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5415834/5395689/5395689-fig-3-source-large.gif
2010,5395689,Fig. 4.,MLPN for Example 3.2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5415834/5395689/5395689-fig-4-source-large.gif
2010,5395689,Fig. 5.,MLPN for Example 4.1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3468/5415834/5395689/5395689-fig-5-source-large.gif
2010,5546978,Fig. 1.,"Hinge loss, squared hinge loss, and huber loss.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5559612/5546978/5546978-fig-1-source-large.gif
2010,5546978,Fig. 2.,Computational time vs. The dimensionality of feature vectors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5559612/5546978/5546978-fig-2-source-large.gif
2010,5546978,Fig. 3.,"Retrieval performances of SDPMetric-S, SDPMetric-H, LMNN, and the Euclidean distance. The curves of SDPMetric-S and SDPMetric-H are very close.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5559612/5546978/5546978-fig-3-source-large.gif
2010,5546978,Fig. 4.,"Classification error rates of 3-nearest neighbor and running time of SDPMetric-H on the USPS data set. (a) Varying the number of training triplets. Error rates are 2.85%, 2.30%, 2.00%, and 1.70%; computational time for each run is 5, 217, 368, and 491 s. (b) Varying the PCA dimension. Error rates are 2.85%, 3.51%, and 3.21%; computational time for each run is 5, 63, and 1205 s, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5559612/5546978/5546978-fig-4-source-large.gif
2010,5464347,Fig. 1.,Simple DBN for a given action with corresponding conditional probability trees.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-1-source-large.gif
2010,5464347,Fig. 2.,Pseudocode for our algorithm. See Fig. 3 and Fig. 4 for pseudocode of referenced functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-2-source-large.gif
2010,5464347,Fig. 3.,"Pseudocode for the
selectAction
function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-3-source-large.gif
2010,5464347,Fig. 4.,"Pseudocode for the
update
function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-4-source-large.gif
2010,5464347,Fig. 5.,Visual rendering of the Light Box domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-5-source-large.gif
2010,5464347,Fig. 6.,Causal graph of the Light Box domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-6-source-large.gif
2010,5464347,Fig. 7.,"Examples of compact option policies in the Light Box domain. Internal nodes represent state variables, leaves represent action (option) choices. Branches are labeled with state variable values. Notice the nested policies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-7-source-large.gif
2010,5464347,Fig. 8.,Structure-learning performance for three different exploration policies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-8-source-large.gif
2010,5464347,Fig. 9.,Policy computation times for tasks at varying levels of the Light Box hierarchy for an agent with primitive actions only and for one with options + primitives. Note the log scale.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/5497035/5464347/5464347-fig-9-source-large.gif
2010,5371938,Fig. 1.,"Projection function to determine
α
i
according to (7).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-1-source-large.gif
2010,5371938,Fig. 2.,System architecture of on-chip-trainable Gaussian kernel analog support vector machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-2-source-large.gif
2010,5371938,Fig. 3.,Block diagram of single vector unit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-3-source-large.gif
2010,5371938,Fig. 4.,Differential pair operating in the subthreshold regime and its output currents.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-4-source-large.gif
2010,5371938,Fig. 5.,Basic Gaussian circuit and its output current.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-5-source-large.gif
2010,5371938,Fig. 6.,Gilbert Gaussian circuit and its output current.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-6-source-large.gif
2010,5371938,Fig. 7.,Output characteristics comparison among various Gaussian circuits. All graphs are scaled for easy comparison of their shapes: 1) Delbrü'ck's bump circuit [22]; 2) Basic Gaussian circuit (see Fig. 5); 3) Gilbert Gaussian circuit (see Fig. 6).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-7-source-large.gif
2010,5371938,Fig. 8.,"Multidimensional Gaussian kernel (2-D case). Expanding the dimension is achieved by repeating this basic pair.
Φ
fset
: floating gate setting signal;
V
float
: floating gate biasing voltage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-8-source-large.gif
2010,5371938,Fig. 9.,Operation of Gaussian kernel: (left) storing phase and (right) calculating phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-9-source-large.gif
2010,5371938,Fig. 10.,"Alpha multiplier. The output from the Gaussian circuits is fed to the tail current.
Φ
fset
: floating gate setting signal;
V
float
: floating gate biasing voltage;
Φ
αin
: inputting signal;
V
αbias
: bias voltage for calculating phase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-10-source-large.gif
2010,5371938,Fig. 11.,Serial D/A converter (SDAC).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-11-source-large.gif
2010,5371938,Fig. 12.,Current comparator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-12-source-large.gif
2010,5371938,Fig. 13.,"Chip micrograph. Digital memories are attached to store obtained values of
A
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-13-source-large.gif
2010,5371938,Fig. 14.,Measurement results from 15 Gaussian test circuits: (a) measured; (b) after peak normalization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-14-source-large.gif
2010,5371938,Fig. 15.,"Measurement results from 2-D Gilbert-Gaussian test circuit. \$\Delta V_{p}\$ and \$\Delta V_{n}\$ is differential inputs to pMOS type circuit and nMOS type circuit, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-15-source-large.gif
2010,5371938,Fig. 16.,Peak shifting of Gaussian kernel: 1) \$X_{\rm sample} = 32\$ ; 2) \$X_{\rm sample} = 96\$ ; 3) \$X_{\rm sample} = 160\$ ; 4) \$X_{\rm sample} = 224\$ . \$X_{\rm sample}\$ is stored data in Gaussian kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-16-source-large.gif
2010,5371938,Fig. 17.,Changing width of Gaussian kernel: 1) \$V_{\rm DAC} =\$ 1.8 (V); 2) \$V_{\rm DAC} =\$ 1.4 (V); 3) \$V_{\rm DAC} =\$ 1.0 (V); 4) \$V_{\rm DAC} =\$ 0.6 (V). \$V_{\rm DAC}\$ is reference voltage which controls output voltage range of D/A converters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-17-source-large.gif
2010,5371938,Fig. 18.,"Scaling height of Gaussian kernel: 1) \$A_{i} = 55\$ ; 2) \$A_{i} = 50\$ ; 3) \$A_{i} = 45\$ ; 4) \$A_{i} = 40\$ . \$A_{i}\$ is digitized value of \$\Delta V_{\alpha }\$ , differential input to alpha multiplier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-18-source-large.gif
2010,5371938,Fig. 19.,"Waveforms in recognition mode. (Top) Internal clock. (Middle) Control signal for D/A converters. In this implementation, 1.14 \$\mu\$ s is needed for D/A converting operation. (Bottom) Output signal of classification results. Since it takes 0.96 \$\mu\$ s in the classifier to obtain one result, total calculation time for one vector is 2.1 \$\mu\$ s.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-19-source-large.gif
2010,5371938,Fig. 20.,Learning demonstration of a binary classification problem. It shows decision boundaries obtained for three cases. Dashed lines are experimentally determined boundary from fabricated chip No. 1. The smooth lines are theoretically obtained lines using parameters determined from the test circuit measurement results. \$\gamma\$ (width of Gaussian kernel) increases in the order of (a)–(c). (a) \$V_{DAC}=\$ 1.1 (V). (b) \$V_{DAC}=\$ 0.9 (V). (c) \$V_{DAC}=\$ 0.7 (V).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-20-source-large.gif
2010,5371938,Fig. 21.,Variation of coefficients \$A_{i}\$ during learning process. (a) \$V_{DAC}=\$ 1.1 (V). (b) \$V_{DAC}=\$ 0.7 (V).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5512812/5371938/5371938-fig-21-source-large.gif
2010,4912200,Fig. 1.,"An illustration of the MagKmeans clustering algorithm. The cluster in (a) contains only positive examples. After several iterations, some of the positive examples will be expelled from the cluster while some negative example are absorbed into the cluster to achieve balance in the class distribution (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-1-source-large.gif
2010,4912200,Fig. 2.,Comparison between the clustering results of (b) regular K-means and (c) MagKmeans.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-2-source-large.gif
2010,4912200,Fig. 3.,Comparison between LSVM and nonlinear SVM. (a) The support vectors obtained by nonlinear SVM. (b) The support vectors obtained for three test examples (marked by ×) using LSVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-3-source-large.gif
2010,4912200,Fig. 4.,"The diagrams in (a) show the distribution for three synthetic data sets. Each data set consists of two classes marked by ° and ∗, respectively. The diagrams in (b) show the decision boundaries generated by PSVM. Each color represents one of the clusters produced by the MagKmeans algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-4-source-large.gif
2010,4912200,Fig. 5.,"(a) The runtime comparison among SVM, KNN, HLSVM, SLSVM, and PSVM. (b) The amount of time spent for MagKmeans clustering by PSVM (i.e., PSVM-Clustering) as opposed to the amount of time spent for training LSVMs and applying them to the test examples (i.e., PSVM-LSVM). (c) Comparison of the time spent by each algorithm to predict the class labels of test examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-5-source-large.gif
2010,4912200,Fig. 6.,"Runtime comparison among T-KNN, SVM, HLSVM (KNN-SVM), and PSVM on the classification of Enron e-mail data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-6-source-large.gif
2010,4912200,Fig. 7.,California housing price data collected from the 1990 census. The data are divided into two classes—house prices above median ( •) and house prices below median ( •).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-7-source-large.gif
2010,4912200,Fig. 8.,"Runtime comparison among S-KNN, SVM, HLSVM (KNN-SVM), and PSVM on the California housing price data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-8-source-large.gif
2010,4912200,Fig. 9.,"Effect of varying the number of clusters
κ
on the performance of PSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-9-source-large.gif
2010,4912200,Fig. 10.,"Runtime comparison among SVM, KNN, HLSVM, SLSVM, and PSVM when varying the number of clusters
κ
in PSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5420329/4912200/4912200-fig-10-source-large.gif
2010,4957035,Fig. 1.,"Discrete AdaBoost algorithm.
1
is an indicator function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5371922/4957035/4957035-fig-1-source-large.gif
2010,4957035,Fig. 2.,Procedure for Ada-SVM tree training. The AdaBoost tree is trained in the same way except that it does not use SVM for classification—it uses traditional AdaBoost for classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5371922/4957035/4957035-fig-2-source-large.gif
2010,4957035,Fig. 3.,"Example hippocampal segmentations from each of the methods being compared: manual tracing, Ada-SVM, AdaBoost, and FreeSurfer [16]. The left hippocampus is shown in yellow, and the right hippocampus is shown in green. Ada-SVM gives smoother, more spatially coherent results than any of the other methods, and even appears slightly less noisy than the manual traces, which are typically created in coronal sections and may appear jagged when resliced in other planes. All methods give anatomically reasonable segmentations, but some give highly irregular or noisy boundaries. Here we show only the test cases for Ada-SVM and AdaBoost, because we wish to evaluate their performance on unseen images, not on the same manually segmented images that were used for training. The brain MRI quality here is typical of those used in AD morphometric studies, showing widespread atrophy and moderate to poor gray/white matter contrast.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5371922/4957035/4957035-fig-3-source-large.gif
2010,4957035,Fig. 4.,"Effect of varying the size of the training set versus the error between automated and gold standard manual segmentations. Error is defined on the number of incorrectly classified voxels inside the bounding box. Values are obtained for 5, 10, 15, 20, 25, and 30 brains. Note that the curves level off after 20 brains indicating diminishing returns by using more than 20 brains on which to train.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5371922/4957035/4957035-fig-4-source-large.gif
2010,4957035,Fig. 5.,"Significance maps (
p
-maps) based on manual, Ada-SVM, AdaBoost, and FreeSurfer segmentations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5371922/4957035/4957035-fig-5-source-large.gif
2010,4957035,Fig. 6.,"Cumulative distribution of
p
-values for different methods. (a) Shows the
p
-values when the covariate is the Alzheimer's disease diagnosis. (b) Shows the
p
-values when the covariate is the MMSE [19] score. These CDF plots are commonly generated when using false discovery rate methods to assign overall significance values to statistical maps [7], [20], [55]; they may also be used to compare effect sizes of different methods, subject to certain caveats [31], as they show the proportion of supra-threshold voxels in a statistical map, for a range of thresholds. A cumulative plot of
p
-values in a statistical map, after the
p
-values have been sorted into numerical order, can compare the proportion of supra-threshold statistics with null data, or between one method and another, to assess their power to detect statistical differences that survive thresholding at both weak and strict thresholds (in fact at any threshold in the range [0.1]). In the examples shown here, the cumulative distribution function of the
p
-values observed for the statistical comparison of patients versus controls is plotted against the corresponding
p
-value that would be expected, under the null hypothesis of no group difference (shown here in black).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5371922/4957035/4957035-fig-6-source-large.gif
2010,4674368,Fig. 1.,"Twelve-point segment test corner detection in an image patch. the highlighted squares are the pixels used in the corner detection. the pixel at
p
is the center of a candidate corner. the arc is indicated by the dashed line passing through 12 contiguous pixels which are brighter than
p
by more than the threshold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-1-source-large.gif
2010,4674368,Fig. 2.,Repeatability is tested by checking if the same real-world features are detected in different views. a geometric model is used to compute where the features reproject to.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-2-source-large.gif
2010,4674368,Fig. 3.,"Box data set: photographs taken of a test rig (consisting of photographs pasted to the inside of a cuboid) with strong changes of perspective, changes in scale, and large amounts of radial distortion. this tests the corner detectors on planar texture.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-3-source-large.gif
2010,4674368,Fig. 4.,Maze data set: photographs taken of a prop used in an augmented reality application. this set consists of textural features undergoing projective warps as well as geometric features. there are also significant changes of scale.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-4-source-large.gif
2010,4674368,Fig. 5.,"Bas-relief data set: the model is a flat plane, but there are many objects with significant relief. this causes the appearance of features to change in a nonaffine way from different viewpoints.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-5-source-large.gif
2010,4674368,Fig. 6.,Positions of offsets used in the FAST-ER detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-6-source-large.gif
2010,4674368,Fig. 7.,"Distribution of scores for various parameters of
(
w
r
,
w
n)
w
s
)
. The parameters leading to the best result are (2.0,3,500,5,000) and the parameters for the worst point are (0.5,3,500,5,000). For comparison, the distribution for all 27 runs and the median point (given in Table 1) are given. The score given is the mean value of
A
computed over the “box,” “maze,” and “bas-relief” data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-7-source-large.gif
2010,4674368,Fig. 8.,"A comparison of the fast-
n
detectors on the “bas-relief” shows that
n=9
is the most repeatable. For
n≤8
, the detector starts to respond strongly to edges.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-8-source-large.gif
2010,4674368,Fig. 9.,Repeatability results for the bas-relief data set (at 500 features per frame) as the amount of gaussian noise added to the images is varied. see Fig. 10 for the key.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-9-source-large.gif
2010,4674368,Fig. 10.,"(a), (b), (c) Repeatability results for the repeatability data set as the number of features per frame is varied. (d) Key for this figure, Figs. 11 and 9. for fast and susan, the number of features cannot be chosen arbitrarily; the closest approximation to 500 features in each frame is used. (a) Box data set. (b) Maze data set. (c) Bas-relief data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-10-source-large.gif
2010,4674368,Fig. 11.,"(a), (b), (c), (d), (e), (f), (g) Repeatability results for the “oxford” data set as the number of features per frame is varied. see Fig. 10 for the key. (a) Bark data set. (b) Bikes data set. (c) Boat data set. (d) graffiti data set. (e) Leuven data set. (f) Trees data set. (g) UBC data set. (h) Wall data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5339303/4674368/4674368-fig-11-source-large.gif
2010,5276797,Fig. 1.,Average ROC performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-1-source-large.gif
2010,5276797,Fig. 2.,Average PRC performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-2-source-large.gif
2010,5276797,Fig. 3.,"Percent of problem metrics performed within tolerance of best metric, ROC, 50 features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-3-source-large.gif
2010,5276797,Fig. 4.,"Percent of problem metrics performed within tolerance of best metric, for PRC, 50 features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-4-source-large.gif
2010,5276797,Fig. 5.,Average ROC performance on biological analysis data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-5-source-large.gif
2010,5276797,Fig. 6.,Average PRC performance on biological analysis data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-6-source-large.gif
2010,5276797,Fig. 7.,Average ROC performance on text mining data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-7-source-large.gif
2010,5276797,Fig. 8.,Average PRC performance on text mining data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-8-source-large.gif
2010,5276797,Fig. 9.,Average ROC performance on character recognition data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-9-source-large.gif
2010,5276797,Fig. 10.,Average PRC performance on character recognition data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-10-source-large.gif
2010,5276797,Fig. 11.,Average performance of different comparison approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-11-source-large.gif
2010,5276797,Fig. 12.,"Average ROC performance, mean of multiple classifiers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-12-source-large.gif
2010,5276797,Fig. 13.,"Average PRC performance, mean of multiple classifiers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-13-source-large.gif
2010,5276797,Fig. 14.,"Average ROC performance, nearest neighbor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-14-source-large.gif
2010,5276797,Fig. 15.,"Average PRC performance, nearest neighbor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-15-source-large.gif
2010,5276797,Fig. 16.,"Average ROC performance, naïve Bayes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-16-source-large.gif
2010,5276797,Fig. 17.,"Average PRC performance, naïve Bayes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5550977/5276797/5276797-fig-17-source-large.gif
2010,4909045,Fig. 1.,"(a) Multispectral image and (b) ground survey used [
Green=trees
(T);
blue=water
(W);
orange=buildings
(B);
black=roads
(R);
yellow=shadows
(S)].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5379155/4909045/4909045-fig-1-source-large.gif
2010,4909045,Fig. 2.,"Classification maps using (a) MS, (b) opening by reconstruction, (c) closing by reconstruction, and (d) multioptimization composite kernels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5379155/4909045/4909045-fig-2-source-large.gif
2010,4909045,Fig. 3.,"Surface of the influence of the
α
,
β
, and
γ
parameters on the Kappa index for the (a) Sum Models
(
σ
MS
=
σ
OR
=
σ
CR
=0.01)
and (b) multioptimization approach. The areas of influence of the features are shown in the small triangle on the top left. (c)
ξα
error bound for the multioptimization approach (value in the corners are the bounds for the single experiments MS, OR, and CR). The optimal values of the multioptimization experiment are shown in (d) for
σ
MS
, (e) for
σ
OR
, and (f) for
σ
CR
: Blue areas correspond to areas where the optimal parameter is the same as in the experiments using a single feature set. Each point in (b) has been obtained by a model using the optimal sigmas reported at the same point in triangles (d), (e), and (f).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5379155/4909045/4909045-fig-3-source-large.gif
2010,5411821,Fig. 1.,(Left) Classifier obtained using labeled data. (Right) Classifier obtained using labeled data plus unlabeled data distribution (shaded).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5447593/5411821/5411821-fig-1-source-large.gif
2010,5411821,Fig. 2.,"Unsupervised learning using NCutEmb. A hyperplane that separates neighbors is pushed to classify them in the same class (left; the classifier cuts through an edge of the graph and is pushed upward), whereas a hyperplane that classifies nonneighbors in the same class is modified to separate them (right; the hyperplane is pushed to separate the unconnected points).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5447593/5411821/5411821-fig-2-source-large.gif
2010,5411821,Fig. 3.,"RGB composition and classification maps with the best SVM,
k
-means, TSVM, and LapSVM
(l=200,u=1000)
, along with the best SSNN for the AVIRIS Indian Pines image. OA and kappa statistic are given in brackets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5447593/5411821/5411821-fig-3-source-large.gif
2010,5411821,Fig. 4.,"RGB composition and classification maps with the best SVM,
k
-means, TSVM, and LapSVM
(l=200,u=1000)
, along with the best SSNN for the KSC image. OA and kappa statistic are given in brackets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5447593/5411821/5411821-fig-4-source-large.gif
2010,5483231,Fig. 1.,(a) Geometry of CESVM; (b) geometry of QSSVM in the feature space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-1-source-large.gif
2010,5483231,Fig. 2.,"Distributed approach: children nodes
S2
,
S3
,
S4
and parent node
S1
perform local anomaly detection using QSSVM. Children nodes transmit their local radii
(R2,R3,R4)
to the parent node
S1
. Parent node
S1
computes the global radius
R
g
using the received and its own local radii. Then
S1
transmits the global radius
R
g
to the children nodes, which each perform global detection using
R
g
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-2-source-large.gif
2010,5483231,Fig. 3.,Example of a multilevel hierarchical organization of sensors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-3-source-large.gif
2010,5483231,Fig. 4.,"Scatter plot of Banana data set. Blue stars represent normal data vectors, and red crosses represent anomalous data vectors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-4-source-large.gif
2010,5483231,Fig. 5.,"AUC for QSSVM and CESVM using the RBF kernel (
σ
is the kernel width parameter). (a) AUC for GDI data; (b) AUC for ionosphere data; (c) AUC for banana data; (d) AUC for synth data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-5-source-large.gif
2010,5483231,Fig. 6.,ROC curve for QSSVM and CESVM using the Linear kernel. (a) ROC for GDI data; (b) ROC for ionosphere data; (c) ROC for banana data; (d) ROC for synth data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-6-source-large.gif
2010,5483231,Fig. 7.,"AUC for QSSVM and CESVM using the polynomial kernel (
q
is the degree of the polynomial). (a) AUC for GDI data; (b) AUC for ionosphere data; (c) AUC for banana data; (d) AUC for synth data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-7-source-large.gif
2010,5483231,Fig. 8.,"Comparison of CESVM and TOCC using the Gauss data set with different kernels (Linear, RBF, and Poly kernels). (a) ROC curve with Linear kernel; (b) AUC with RBF kernel; (c) AUC with Poly kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-8-source-large.gif
2010,5483231,Fig. 9.,"Comparison of CESVM and TOCC using the Banana data set with different kernels (Linear, RBF, and Poly kernels). (a) ROC curve with Linear kernel; (b) AUC with RBF kernel; (c) AUC with Poly kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-9-source-large.gif
2010,5483231,Fig. 10.,"AUC with kernel parameter
(σ)
for centralized and distributed detection. Results are shown for different strategies of global radius computation: maximum (Max), minimum (Min), median (Median), and mean (Mean), and the centralized case (Central). RBF kernel is used. (a) AUC versus
lo
g
2
(σ)
for GDI data; (b) AUC versus
lo
g
2
(σ)
for Banana data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-10-source-large.gif
2010,5483231,Fig. 11.,"Centralized detection (Central) and distributed detection using QSSVM for the GDI data. Graphs are shown for different strategies of global radius computation: maximum (Max), minimum (Min), median (Median), and mean (Mean). RBF kernel is used. (a) DR versus FPR; (b) FPR versus regularization parameter
(ν)
; (c) DR versus regularization parameter
(ν)
; (d) FPR versus kernel width
(σ)
; (e) DR versus kernel width
(σ)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-11-source-large.gif
2010,5483231,Fig. 12.,"ROC curve for different kernel functions: RBF kernel (RBF), polynomial kernel (Poly) and linear kernel (Linear). (a) ROC curve for GDI data; (b) ROC curve for Banana data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/5547550/5483231/5483231-fig-12-source-large.gif
2010,5353750,Fig. 1.,"During manual control, the animal physically performs a 2-D center-out reaching task using the right arm while the neural activity is recorded. Under brain control, the animal performs a similar center-out task using a computer cursor under direct neural control through a decoder trained during manual control.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-1-source-large.gif
2010,5353750,Fig. 2.,"Firing rates
f
of the recorded cells are used as inputs by the decoding algorithms and are converted into a motor plan (output). The brain adapts its firing rates based on the target to be reached, and the BMI output.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-2-source-large.gif
2010,5353750,Fig. 3.,Feedback-error learning model: The inverse model is updated based on the corrections made by the feedback controller.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-3-source-large.gif
2010,5353750,Fig. 4.,"Average time to target as a function of trial number. In the early stage of learning, the target is not hit within the 15-s time limit. Later on, the simulated ensemble of neurons successfully reaches the target, and the time required to do so decreases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-4-source-large.gif
2010,5353750,Fig. 5.,"Effect of learning on the reaching trajectories. Blue line is the target evolution over time (
X
- and
Y
-axis plotted separately); red dotted line is the actual trajectory followed by the simulated BMI system. Top of the bold line is before learning; bottom is after learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-5-source-large.gif
2010,5353750,Fig. 6.,(Left panel) Convergence of cell firing parameters: A stable state is reached after a learning period featuring high parameter updates. (Right panel) Activation-modulated learning rule: It can be seen that only a subset of units encounters major parameter changes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-6-source-large.gif
2010,5353750,Fig. 7.,Convergence of the product matrix toward the identity matrix: The coefficients along the diagonal converge to one; the other terms converge to zero.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-7-source-large.gif
2010,5353750,Fig. 8.,"MCF. (a) CDF of the parameters conditioned on convergence to an inverse model (
d
is the Smirnov statistic). (b) Bidimensional projection of the different pairs of parameters (
cc
is the correlation coefficient).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-8-source-large.gif
2010,5353750,Fig. 9.,"Influence of noise. Noise mainly affects the final performance of the open-loop controller. However, the system is able to reach low levels of errors even for relatively high noise levels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-9-source-large.gif
2010,5353750,Fig. 10.,Comparison of (red lines) model predictions (mean ± standard error) to (blue lines) experimental data. (Left) Behavioral performance (percentage of successful trials). (Right) Average change in directional tuning of the neural population.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-10-source-large.gif
2010,5353750,Fig. 11.,"Mean preferred directional changes during the simulated closed-loop BMI experiment. The solid lines represent the ideal response of the system to the step input. The dashed, dotted, and dash-dotted lines stand for the actual response of the system. PS is the perturbed set of neurons, US is the unperturbed set, and WP is the whole population. The dashed blue line represents the whole population (WP) mean PD changes; the red dash-dotted line shows the set of neurons for which the decoder tuning properties have been rotated (PS); the green dotted line stands for the set of unperturbed neurons (US). The error bars show the standard errors of the mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-11-source-large.gif
2010,5353750,Fig. 12.,Online ND experiment: The (red line) model (± standard error) accurately predicts the (blue) experimental data. Performance decreases with the number of dropped neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5571766/5353750/5353750-fig-12-source-large.gif
2010,5482132,Fig. 1.,Three tracks used for the experiments reported in this paper. (a) A-Speedway; (b) CG-Track 1; (c) Ruudskogen.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5571836/5482132/5482132-fig-1-source-large.gif
2010,5482132,Fig. 2.,"Online (a) NEAT and (b) rtNEAT applied to the CG-Track 1 track with softmax starting from scratch (triangular dots), a population seeded with a champion from A-Speedway (squared dots), and a champion from Ruudskogen (circled dots). Curves are averages over ten runs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5571836/5482132/5482132-fig-2-source-large.gif
2010,5482132,Fig. 3.,Online neuroevolution applied to the CG-Track 1 track using softmax and different sizes of the evaluation slot: (a) NEAT; (b) rtNEAT. Curves are averages over ten runs; bars report the standard error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5571836/5482132/5482132-fig-3-source-large.gif
2010,5306497,Fig. 1.,Common ITS architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5306497/5306497-fig-1-source-large.gif
2010,5306497,Fig. 2.,Student–classroom interaction model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5306497/5306497-fig-2-source-large.gif
2010,5306497,Fig. 3.,Example of possible Student interactions through the communication agent.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5306497/5306497-fig-3-source-large.gif
2010,5378489,Fig. 1.,"Laboratory testbed. Cross traffic flowed across one of two routers at hop B, while probe traffic flowed through the other. Optical splitters connected Endace DAG 3.5 and 3.8 passive packet capture cards to the testbed between hops B and C and hops C and D. Measurement traffic (file transfers, loss probes, and available bandwidth probes) flowed from left to right. Congestion in the testbed occurred at hop C.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-1-source-large.gif
2010,5378489,Fig. 2.,Measurement traffic protocol and oracular and practical path measurements used for SVR-based throughput prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-2-source-large.gif
2010,5378489,Fig. 3.,"Comparison of prediction accuracy of HB, SVR-OPM, and SVR-AM in high background traffic conditions. (a) HB. (b) SVR-OPM-AB. (c) SVR-OPM-loss. (d) SVR-OPM-queue. (e) SVR-OPM-AB-loss. (f) SVR-OPM-loss-queue. (g) SVR-OPM-AB-loss-queue. (h) SVR-AM-loss-queue.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-3-source-large.gif
2010,5378489,Fig. 4.,Normal Q-Q plots for prediction errors with (a) oracular measurements (SVR-OPM) and (b) active measurements (SVR-AM) of Loss-Queue.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-4-source-large.gif
2010,5378489,Fig. 5.,"Scatter plots for the SVR-based predictor using one, two, or three distinct file sizes in the training set. All results shown use active measurements to train the predictor. Testing is done using a range of 100 file sizes from 2 kB to 8 MB. (a) SVR-AM with file size of 8 MB in training set. (b) SVR-AM with file sizes of 32 kB and 8 MB in training set. (c) SVR-AM with file sizes of 32 kB, 512 kB, and 8 MB in training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-5-source-large.gif
2010,5378489,Fig. 6.,Wide area results for HB and SVR predictions. The HB and SVR columns present the fraction of predictions with relative error of 10% or less. Training and test sets consisted of 100 samples each. Two megabytes of data were transferred. Labels use node initials; see Section V for complete node names.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-6-source-large.gif
2010,5378489,Fig. 7.,"Effect of training set size on SVR prediction accuracy in the wide area. For paths with high prediction accuracy in Fig. 6, this table shows how large a training set size was needed to achieve high accuracy. Labels use node initials; see Section V for complete node names.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-7-source-large.gif
2010,5378489,Fig. 8.,London–Utah–5: An example of a wide area path with level shifts. Time on the x-axis is represented in terms of file transfer number. (a) Throughput profile. (b) Prediction accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-8-source-large.gif
2010,5378489,Fig. 9.,London–Utah–5: Comparison of SVR and HB prediction accuracy for background traffic with level shifts. The x-axis is the file transfer timeline starting at 145. The y-axis is throughput. (a) SVR prediction accuracy. (b) HB prediction accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-9-source-large.gif
2010,5378489,Fig. 10.,London–Maryland–1: An example of a wide area path with throughput changes on small timescales. (a) Throughput profile. (b) prediction accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/5550304/5378489/5378489-fig-10-source-large.gif
2010,10159191,Fig. 1.,Geometrical interpretation for the SV algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9970761/10153490/10159191/133-137-fig-1-source-large.gif
2010,10159191,Fig. 2.,Simulation results for example 1,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9970761/10153490/10159191/133-137-fig-2-source-large.gif
2010,5200378,Fig. 1.,Transmitter and receiver processing for the MIMO-OFDM system with bit-interleaved convolutional forward error correction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-1-source-large.gif
2010,5200378,Fig. 2.,Supervised learning AMC procedure that selects a single MCS across multiple spatial streams.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-2-source-large.gif
2010,5200378,Fig. 3.,"Correct classification percentages in the test set of one-, three-, and eight-tap channels using the best 4-D feature sets from Table V. The different values of
k
are displayed on the clustered bar graph. The order of the color code in each bar graph cluster matches the order in the legend.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-3-source-large.gif
2010,5200378,Fig. 4.,"Correct classification percentages in the test set with one-, three-, and eight-tap channels, and
k=25
for several feature sets. The ordered postprocessing SNR feature sets use the best indexes from Table V with
d=4
. The order of the color code in each bar graph cluster matches the order in the legend.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-4-source-large.gif
2010,5200378,Fig. 5.,"FER as a function of the average SNR [over subcarriers and spatial streams from (2)] when
k
-NN AMC is applied with different feature sets in IEEE 802.11n channel model B. FER target is 10%. Average capacity and EESM feature sets display the same FER. Since the average SNR is not controlled in the simulation, sample points on the
x
-axis are the result of binning and averaging the FER over all samples in the bin.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-5-source-large.gif
2010,5200378,Fig. 6.,"Spectral efficiency as a function of the average SNR [over subcarriers and spatial streams using (2)] when
k
-NN AMC is applied with different feature sets in IEEE 802.11n channel model B. FER target is 10%. Average capacity and EESM feature sets display the same spectral efficiency. Since the average SNR is not controlled in the simulation, sample points on the
x
-axis are the result of binning and averaging the FER over all samples in the bin.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-6-source-large.gif
2010,5200378,Fig. 7.,Spatial parsing function notation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/5383378/5200378/5200378-fig-7-source-large.gif
2010,5430991,Fig. 1.,Low contrast image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-1-source-large.gif
2010,5430991,Fig. 2.,High contrast image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-2-source-large.gif
2010,5430991,Fig. 3.,Sharp object on blurred background.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-3-source-large.gif
2010,5430991,Fig. 4.,Original image DCT log-histogram. Horizontal axis is non-DC DCT coefficient magnitudes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-4-source-large.gif
2010,5430991,Fig. 5.,JPEG2000 distorted image DCT log-histogram. Horizontal axis is non-DC DCT coefficient magnitudes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-5-source-large.gif
2010,5430991,Fig. 6.,Predicted DMOS versus subjective DMOS (JPEG2000 LIVE subset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-6-source-large.gif
2010,5430991,Fig. 7.,Predicted DMOS versus subjective DMOS (JPEG LIVE subset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-7-source-large.gif
2010,5430991,Fig. 8.,Predicted DMOS versus subjective DMOS (White noise LIVE subset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-8-source-large.gif
2010,5430991,Fig. 9.,Predicted DMOS versus subjective DMOS (Gaussian blur LIVE subset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-9-source-large.gif
2010,5430991,Fig. 10.,Predicted DMOS versus subjective DMOS (Fast fading LIVE subset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-10-source-large.gif
2010,5430991,Fig. 11.,Predicted DMOS versus subjective DMOS (Entire LIVE database).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5452170/5430991/5430991-fig-11-source-large.gif
2010,5430973,Fig. 1.,"Impact of class imbalance on learner performance with UCI data sets for: (a) AROC, (b) KS, (c) FM, and (d) BFM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430973/5430973-fig-1-source-large.gif
2010,5430973,Fig. 2.,"Impact of NL on learner performance with UCI data sets for metrics: (a) AROC, (b) KS, (c) FM, and (d) BFM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430973/5430973-fig-2-source-large.gif
2010,5430973,Fig. 3.,"Impact of NL on learner performance with SE data sets for metrics: (a) AROC, (b) KS, (c) FM, and (d) BFM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430973/5430973-fig-3-source-large.gif
2010,5430973,Fig. 4.,"Impact of ND on learner performance with UCI data sets for metrics: (a) AROC, (b) KS, (c) FM, and (d) BFM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430973/5430973-fig-4-source-large.gif
2010,5430973,Fig. 5.,"Impact of ND on learner performance with SE data sets for metrics: (a) AROC, (b) KS, (c) FM, and (d) BFM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5456124/5430973/5430973-fig-5-source-large.gif
2010,5571818,Fig. 1.,"Overview of the head nod prediction framework. The information in the gesture corpus is encoded and aligned to construct the data set. The feature selection process chooses a subset of the features that are most correlated with head nods. Using these features, probabilistic sequential models are trained and utilized to predict whether or not a head nod should occur.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5571813/5571818/5571818-fig-1-source-large.gif
2010,5571818,Fig. 2.,Snapshot of the meeting setting used for AMI meeting corpus [35].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5571813/5571818/5571818-fig-2-source-large.gif
2010,5571818,Fig. 3.,"Snapshots of head movements in AMI corpus [35]. From the top: nod, shake, nodshake, and other head movements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5571813/5571818/5571818-fig-3-source-large.gif
2010,5571818,Fig. 4.,"Data construction process. From the gesture corpus, speaker transcript, dialog act, and head types are extracted. The transcript is sent to the natural language parser to extract the part of speech tags and phrase boundaries. A script automatically cross-references each file to construct the data set. This data set is encoded and transformed into trigrams before being used to train the HMMs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5571813/5571818/5571818-fig-4-source-large.gif
2010,5571818,Fig. 5.,Snapshot of the video clip shown in the evaluation study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5571813/5571818/5571818-fig-5-source-large.gif
2010,5571818,Fig. 6.,"Mean values for the evaluation study. Vertical bars denote the 95% confidence intervals and asterisks (∗∗) mark statistical significance with
p<.001
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5571813/5571818/5571818-fig-6-source-large.gif
2010,5477158,Fig. 1.,"The mystery fleshy bit at the base of a pigeon's beak. What is its name? Why is it there? (Adapted from “Columba Livia,” photograph taken by Dori. Available on Wikimedia as Pigeon_portrait_4861.jpg.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5512706/5477158/5477158-fig-1-source-large.gif
2010,5477158,Fig. 2.,"Wikipedia page on “beak” contains this lovely and informative picture. Unfortunately, the picture is not clickable: it is a dead-end. Why has nobody taken the time to make it clickable? Where could it lead us if it were clickable? (Adapted from L. Shyamal “Bird beak adaptations.” Available on Wikimedia as BirdBeaksA.svg.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5512706/5477158/5477158-fig-2-source-large.gif
2010,5477158,Fig. 3.,"Another mystery picture: which mushroom is this? Is it edible? Try finding out using Wikipedia. Good luck! (Photo courtesy of J. C. Stevenson, who is still alive.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5512706/5477158/5477158-fig-3-source-large.gif
2010,5477158,Fig. 4.,"A visual query for Wikipedia. (Top) The user has captured a picture. (Middle) The picture was sent to a central computer which recognizes its contents; as a consequence, meaningful regions of the image become clickable (see also Fig. 6). (Bottom) The relevant Wikipedia page is retrieved.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5512706/5477158/5477158-fig-4-source-large.gif
2010,5477158,Fig. 5.,"A sketch of the structure of Visipedia. Visipedia will provide a link between pictorial queries formulated by a user (top-right) and knowledge on the web, e.g., Wikipedia (top-left) and publicly available image databases (bottom-left). Knowledge will be provided by human experts, as in Wikipedia. Automation will be necessary to analyze images, to link automatically related visual content, to link images and text, and to provide experts with smart user interfaces. This automation will be provided initially by human annotators working behind the scenes (bottom center). Gradually, automata built by machine vision scientists will take over the job (bottom right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5512706/5477158/5477158-fig-5-source-large.gif
2010,5477158,Fig. 6.,"(Top) Automated agents in Visipedia will segment the picture of a bird into its component parts, as specified by a human expert. Each part will be named and put in correspondence with corresponding parts of other birds, either recognizing their similarity with previously labeled bird pictures, or with the help of a human annotator. (Bottom) Hyperlinks will be drawn with the corresponding Wikipedia pages, as well as with corresponding parts in other bird pictures. (Albatross picture adapted from Short Tailed Albatross by James Lloyd, available on Wikipedia as Short_tailed_Albatross1.jpg.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5512706/5477158/5477158-fig-6-source-large.gif
2010,5191028,Fig. 1.,Illustration of definitions. The number besides each buffer is the clock signal arrival time to the buffer input.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5491361/5191028/5191028-fig-1-source-large.gif
2010,5191028,Fig. 2.,Overall optimization flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5491361/5191028/5191028-fig-2-source-large.gif
2010,5191028,Fig. 3.,Optimization core.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5491361/5191028/5191028-fig-3-source-large.gif
2010,5191028,Fig. 4.,"Optimization engine
E
1
for stage 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5491361/5191028/5191028-fig-4-source-large.gif
2010,5191028,Fig. 5.,"Optimization engine
E
2
for stage 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5491361/5191028/5191028-fig-5-source-large.gif
2010,5191028,Fig. 6.,"Comparison of SPICE, SVM model and Elmore delay.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/5491361/5191028/5191028-fig-6-source-large.gif
2010,5071176,Fig. 1.,"Nonoverlapping volume of 2 concentric spheres with radii
r
and
(r−δ)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-1-source-large.gif
2010,5071176,Fig. 2.,"Illustration of
ΣΔ
learning using 2-D data. (a) Initial hyperplane. (b) Limit cycle about the optimal hyperplane.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-2-source-large.gif
2010,5071176,Fig. 3.,"Functional architectures corresponding to: (a) proposed
ΣΔ
learner and (b) conventional manifold learning approach. (c) Figure illustrating that when
ΣΔ
learner is applied for a signal compression task, learning can be performed for only a fraction
α≪1
of the total operational time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-3-source-large.gif
2010,5071176,Fig. 4.,"Plot of (a) 1-D
L
1
norm
∥w
∥
1
and (b) its derivative.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-4-source-large.gif
2010,5071176,Fig. 5.,Optimization contour explaining the limit-cycle behavior about the minima.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-5-source-large.gif
2010,5071176,Fig. 6.,"Functional verification of a
ΣΔ
learner using synthetic multichannel data. (a) Analog signals presented as input to the learner. (b) Output signals produced by the learner. (c) Reconstructed signals using the output of learner and the learned transformation
A
. (d) Convergence of the
∥A
∥
∞
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-6-source-large.gif
2010,5071176,Fig. 7.,"FFT response of the first-order
ΣΔ
learner for the synthetic example in Fig. 6, showing that the eighth channel rejects frequencies present in the lower dimensional channels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-7-source-large.gif
2010,5071176,Fig. 8.,"Illustration of
ΣΔ
dynamics for (a) first-order modulator with
x=0
, (b) modulator with
x>0
, (c) higher order modulators (bold lines indicate higher velocity), and (d) effect of reducing the magnitude of
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-8-source-large.gif
2010,5071176,Fig. 9.,"Effect of OSR of the
ΣΔ
converter array on the performance of the
ΣΔ
learner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-9-source-large.gif
2010,5071176,Fig. 10.,"Effect of resolution of a signal transformation matrix
A
on the performance of the
ΣΔ
learner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-10-source-large.gif
2010,5071176,Fig. 11.,"Effect of mismatch on the performance of the
ΣΔ
learner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-11-source-large.gif
2010,5071176,Fig. 12.,"Effect of computational nonlinearity on the performance of the
ΣΔ
learner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-12-source-large.gif
2010,5071176,Fig. 13.,"Illustration of the self-calibration in a
ΣΔ
learner. (a) Nonlinearity of the transform as modeled by a sigmoidal function. (b) Learned value of the transformation parameter
a
21
, which tracks the inverse of the nonlinear function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-13-source-large.gif
2010,5071176,Fig. 14.,"Effect of input sparsity on the performance of the
ΣΔ
learner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-14-source-large.gif
2010,5071176,Fig. 15.,"Architecture of the
ΣΔ
learner with
D=4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-15-source-large.gif
2010,5071176,Fig. 16.,Schematic of a binary 10-bit current-mode DAC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-16-source-large.gif
2010,5071176,Fig. 17.,Schematic of the source degenerated transconductor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-17-source-large.gif
2010,5071176,Fig. 18.,Architecture of the third-order single loop single bit \$\Sigma\Delta\$ modulator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-18-source-large.gif
2010,5071176,Fig. 19.,Microphotograph of the fabricated \$\Sigma\Delta\$ learner.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-19-source-large.gif
2010,5071176,Fig. 20.,Measured response of the source degenerated transconductors. The mismatch between the transconductors was determined to be less than 4%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-20-source-large.gif
2010,5071176,Fig. 21.,(a) Nonlinearity of the current DAC. (b) Value of \$a_{21}\$ after learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-21-source-large.gif
2010,5071176,Fig. 22.,Measured convergence of the parameter \$a_{21}\$ for different magnitudes of \$x_2\$ and a fixed magnitude of \$x_1\$ .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-22-source-large.gif
2010,5071176,Fig. 23.,"Phasor diagram showing the two sinusoidal signals represented by vectors \$x_1, x_2\$ differing by a phase \$\theta\$ .",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-23-source-large.gif
2010,5071176,Fig. 24.,Adapted value of the parameter \$a_{21}\$ for different magnitude and the phase difference between the sinusoidal signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-24-source-large.gif
2010,5071176,Fig. 25.,Residual power on the second channel as the phase difference between the two signals is varied from 0° to 180°.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-25-source-large.gif
2010,5071176,Fig. 26.,Signal power in each channel before and after adaptation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-26-source-large.gif
2010,5071176,Fig. 27.,Reconstructed signal power for (a) the second channel and (b) the third channel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/5424135/5071176/5071176-fig-27-source-large.gif
2010,5411825,Fig. 1.,ALVeRT. The general active learning framework for vehicle recognition and tracking systems consists of (white and red) an offline learning portion and (green) an online implementation portion. (Red) Prior works in vehicle recognition and tracking have not utilized active learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-1-source-large.gif
2010,5411825,Fig. 2.,(a) QUAIL. QUAIL evaluates the passively trained vehicle-recognition system on real-world data and provides an interface for a human to label and archive the ground truth. Detection is automatically marked green. Missed detection is marked red by the user. False positives are marked blue by the user. True detection is left green. (b) QUAIL outputs. A true detection and a false positive archived for retraining.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-2-source-large.gif
2010,5411825,Fig. 3.,Examples of false-positive outputs queried for retraining using QUAIL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-3-source-large.gif
2010,5411825,Fig. 4.,Examples of true positives queried for retraining using QUAIL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-4-source-large.gif
2010,5411825,Fig. 5.,"Schematic of a framework for ALVeRT. An initial passively trained vehicle detector is built. Using QUAIL, false positives, false negatives, and true positives are queried and archived. A new classifier is trained using the archived samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-5-source-large.gif
2010,5411825,Fig. 6.,(a) Examples of Haar-like features used in the vehicle detector. (b) Cascade of boosted classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-6-source-large.gif
2010,5411825,Fig. 7.,"(Left) Detector outputs for (top) a single vehicle and (middle and bottom) multiple vehicles. (Middle) Multiple-location hypotheses generated by the condensation filter. Note the multimodal distribution of the hypotheses when (bottom) tracking multiple vehicles. (Right) Best tracking results, as confirmed by detection in the consequent frame.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-7-source-large.gif
2010,5411825,Fig. 8.,"Full ALVeRT system overview. Real on-road data are passed to the active-learning-based vehicle-recognition system, which consists of Haar-like rectangular feature extraction and the boosted cascade classifier. Detection is then passed to the condensation multiple-vehicle tracker. The tracker makes predictions, which are then updated by the detection observations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-8-source-large.gif
2010,5411825,Fig. 9.,(a) Plot of TPR versus FDR (Caltech 1999 data set) for the (blue) passively trained recognition and (red) the active-learning vehicle recognition. We note the improvement in performance due to active learning on this data set. (b) Plot of TPR versus the number of false detections per frame (Caltech 1999 data set) for (blue) the passively trained recognition and (red) the active-learning vehicle recognition. We note the reduction in false positives due to active learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-9-source-large.gif
2010,5411825,Fig. 10.,Sample vehicle recognition results from the Caltech 1999 data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-10-source-large.gif
2010,5411825,Fig. 11.,"(a) Recognition output in a nonuniformly illuminated scene. Six vehicles were detected. No false positives and no missed detections. (b) Recognition output in cloudy conditions. Note the reflections, glare, and smudges on the windshield.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-11-source-large.gif
2010,5411825,Fig. 12.,(Left) Vehicle in front was not detected in this frame on a cloudy day due to smudges and dirt on the windshield. (Right) Vehicle's track was still maintained in the following frame.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-12-source-large.gif
2010,5411825,Fig. 13.,"(a) Recognition output in shadows. Five vehicles were detected, and two were missed due to their distance and poor illumination. We note that poor illumination seems to limit the range of the detection system; vehicles farther from the ego vehicle are missed. (b) Vehicle recognition output in sunny highway conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-13-source-large.gif
2010,5411825,Fig. 14.,"(a) Recognition output in sunny conditions. Note the vehicle detection across all six lanes of traffic during San Diego's rush hour. The recognizer seems to work best in even sunny illuminations, as such illumination conditions have less scene complexity compared with scenes with uneven illuminations. (b) Recognition output in dense traffic. We note that the vehicle directly in front is braking, and the system is aware of vehicles in adjacent lanes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-14-source-large.gif
2010,5411825,Fig. 15.,"(Top left) Vehicle-recognition system has output two true vehicles, one missed vehicle, and one false positive. (Top right) These detection outputs are passed to the tracker. We note that the missed vehicle still has a track maintained and is being tracked despite the missed detection. (Bottom left) In the following frame, all three vehicles are detected. (Bottom right) In the corresponding tracker output, we note that a track was not maintained for the false positive from the previous frame.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-15-source-large.gif
2010,5411825,Fig. 16.,"(a) Plot of TPR versus FDR (LISA-Q Front FOV data sets 1–3) for (blue) the passively trained recognition, (red) the active-learning vehicle recognition, and (black) the ALVeRT system. We note the large improvement in performance due to active learning for vehicle detection. (b) Plot of TPR versus the number of false detections per frame (LISA-Q Front FOV data sets 1–3) for (blue) the passively trained recognition, (red) the active-learning vehicle recognition, and (black) the ALVeRT system. We note the reduction in false positives due to active learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5475419/5411825/5411825-fig-16-source-large.gif
2010,5523913,Fig. 1.,Steps of the hypergraph spectral multi-label learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-1-source-large.gif
2010,5523913,Fig. 2.,Sparse multi-label transfer learning (S-MLTL) fram ework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-2-source-large.gif
2010,5523913,Fig. 3.,Steps of the solution for ML-SLDA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-3-source-large.gif
2010,5523913,Fig. 4.,Thirty-five video concepts and their distribution in the experimental dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-4-source-large.gif
2010,5523913,Fig. 5.,"MAP
s of video concept detection task from L1-HSML, L2-HSML, BL for different values of tunable parameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-5-source-large.gif
2010,5523913,Fig. 6.,"Sparseness of
W
defined in Fig. 2 of video concept detection task from L1-HSML.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-6-source-large.gif
2010,5523913,Fig. 7.,"MAP
s comparison of image annotation task from L1-HSML, L2-HSML, and BL for different sizes of training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-7-source-large.gif
2010,5523913,Fig. 8.,"MAP
s of image annotation task from L1-HSML, L2-HSML, and BL for different weights of regression penalty.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-8-source-large.gif
2010,5523913,Fig. 9.,"Sparseness of
W
defined in Fig. 2 of image annotation task from L1-HSML.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-9-source-large.gif
2010,5523913,Fig. 10.,"MAP
s comparison of image annotation task from EN, L1-HSML, L2-HSML, and BL for different sizes of training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-10-source-large.gif
2010,5523913,Fig. 11.,"MAP
s of image annotation task from EN and BL for different values of
λ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-11-source-large.gif
2010,5523913,Fig. 12.,"Sparseness of
W
defined in Fig. 2 of image annotation task from EN for different values of
λ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-12-source-large.gif
2010,5523913,Fig. 13.,"P@k
of image annotation task from EN,
k=1
,2, 3, 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-13-source-large.gif
2010,5523913,Fig. 14.,"Recall of image annotation task from EN for different values of
λ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-14-source-large.gif
2010,5523913,Fig. 15.,"MAP
s of video concept detection task from L1-HSML, L2-HSML, BL, and SLDA for different values of
λ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-15-source-large.gif
2010,5523913,Fig. 16.,"MAP
s of image annotation task from EN, BL, and SLDA for different weights of elastic net penalty.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-16-source-large.gif
2010,5523913,Fig. 17.,"P@k
of image annotation task from BL, L2-HSML, L1-HSML, SLDA, and EN,
k=1
, 2, 3, 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-17-source-large.gif
2010,5523913,Fig. 18.,"Recall of image annotation task from L1-HSML, L2-HSML, BL, and SLDA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-18-source-large.gif
2010,5523913,Fig. 19.,"Overall
MAP
s from BL, L1-HSML, L2-HSML, EN, and ML-SLDA for video concept detection task.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-19-source-large.gif
2010,5523913,Fig. 20.,"Overall
MAP
s from BL, L1-HSML, L2-HSML, EN, and ML-SLDA for image annotation task.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5545503/5523913/5523913-fig-20-source-large.gif
2010,5668980,Fig. 1.,Rogue device detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-1-source-large.gif
2010,5668980,Fig. 2.,Fast patching.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-2-source-large.gif
2010,5668980,Fig. 3.,Fingerprinting architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-3-source-large.gif
2010,5668980,Fig. 4.,"SVM one-to-one classification, $ is a new point to assign.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-4-source-large.gif
2010,5668980,Fig. 5.,Common classification problems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-5-source-large.gif
2010,5668980,Fig. 6.,Rock classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-6-source-large.gif
2010,5668980,Fig. 7.,Grammar.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-7-source-large.gif
2010,5668980,Fig. 8.,Intersection of ancestor paths.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-8-source-large.gif
2010,5668980,Fig. 9.,Syntactic trees of two messages.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-9-source-large.gif
2010,5668980,Fig. 10.,"Supervised fingerprinting, distance
dis
t
1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-10-source-large.gif
2010,5668980,Fig. 11.,"Supervised fingerprinting, distance
dis
t
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-11-source-large.gif
2010,5668980,Fig. 12.,"Intensity coded distance matrix for
dis
t
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-12-source-large.gif
2010,5668980,Fig. 13.,"Syntactic tree advantage, accuracy with distance
dis
t
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-13-source-large.gif
2010,5668980,Fig. 14.,Unsupervised fingerprinting by grouping similar arrival time messages.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-14-source-large.gif
2010,5668980,Fig. 15.,Rock clustering by messages type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/5668974/5668980/5668980-fig-15-source-large.gif
2010,5184932,Fig. 1.,Definition of a task. A task is defined as a sequence of ordered target sets (the ellipses in the figure). The task is executed by selecting a trajectory (the dotted line) that connects states (rectangular boxes) in the target sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-1-source-large.gif
2010,5184932,Fig. 2.,"Autonomous activity of computational media. (a) Autonomous activity of the proposed neurons. The neuron has two states, i.e., firing or rest, depending on whether the input is greater than the threshold. The threshold is raised if the neuron fires and decreased if the neuron is at rest. This activity can keep the firing frequency level within a definite range. (b) Network modification rule. Neurons I and O are connected with a weight
w
, which is changed by using a third party M. In a potentiation network, the weight is raised when
I=O=M=1
, and decreased otherwise. In an inhibition network, the weight is decreased when
I=O=M=1
and raised in other cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-2-source-large.gif
2010,5184932,Fig. 3.,"(a) Neuron clusters and its abbreviated expression. (b) All VTNs in the cluster have the same input, and the output from the cluster is the number of firing VTNs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-3-source-large.gif
2010,5184932,Fig. 4.,"Abbreviated expression of general cluster and N.O. cluster. The output from the N.O. cluster is
[u]
, which is the integer part of the input
u
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-4-source-large.gif
2010,5184932,Fig. 5.,"Network structure for autonomous input accumulator. The number of firing VTNs in output layer A decreases in proportion to the accumulation of inputs because the weights between the intention neuron and the output layer are decreased by the input. The output layers A and B are connected with NOT logical circuits. Therefore, the output from the output layer B is the reverse of output layer A.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-5-source-large.gif
2010,5184932,Fig. 6.,"Movement of threshold in the case of a constant input. The threshold
θ
moves in the range
[
I
c
−β,
I
c
+α)
and oscillates in that range.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-6-source-large.gif
2010,5184932,Fig. 7.,"Input versus output depending on the area in which a threshold exists. Case (a): the input is higher than all thresholds. The output is
N
, and the threshold changing rate is
α
because all neurons fire. Case (b): the input crosses the area in which a threshold exists. The output is
(I−
θ
m
)N/(α+β)
, and the threshold changing rate is
(α+β)O/N−β
. Case (c): the input is lower than all thresholds. The output is 0, and the threshold changing rate is
−β
because all neurons are resting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-7-source-large.gif
2010,5184932,Fig. 8.,Overview of function SAT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-8-source-large.gif
2010,5184932,Fig. 9.,Network structure for output regulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-9-source-large.gif
2010,5184932,Fig. 10.,"Conceptual model of convergence trajectory. Initially, the output
y
from the plant converges regarding the threshold as the constant one. After the convergence, both
y
and
Θ
converge subject to the slower dynamics of
Θ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-10-source-large.gif
2010,5184932,Fig. 11.,"Simulation result. Trajectories of the output from the plant and the thresholds in active clusters. It takes approximately
3.2×
10
3
s to converge to the desired point.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-11-source-large.gif
2010,5184932,Fig. 12.,"Simulation result. Output from the accumulator. The outputs converge to
99.945×
10
6
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-12-source-large.gif
2010,5184932,Fig. 13.,Simulation result. Trajectories of the output and the threshold after the control experience. Appropriate thresholds and weights shorten the convergence time to 5.2 s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-13-source-large.gif
2010,5184932,Fig. 14.,Network structure of self-reference generation. The reference signal in Fig. 9 is replaced by another accumulator that accumulates the output from this network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-14-source-large.gif
2010,5184932,Fig. 15.,Definition of task for reciprocating motion. This task executes by selecting the time trajectories between the two points through the autonomous activities of VTNs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-15-source-large.gif
2010,5184932,Fig. 16.,Network structure for reciprocating motions. The different integrator is used when moving back and forth. The active integrator is switched by using the intention neuron.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-16-source-large.gif
2010,5184932,Fig. 17.,"Simulation result: learning of reciprocating motions. (a) The behavior was learned in the first few motions. The frequency is approximately 0.14 Hz. (b) When the plant parameter that corresponds to a spring constant is changed to a larger value, the frequency becomes slower (approximately 0.10 Hz).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-17-source-large.gif
2010,5184932,Fig. 18.,Overview of the 2DOF manipulator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-18-source-large.gif
2010,5184932,Fig. 19.,Target set of possible states and transition trajectory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-19-source-large.gif
2010,5184932,Fig. 20.,Zero-torque posture. The center of mass of the manipulator is on the vertical line passing through the attachment point of Joint 1. The torque of Joint 1 is not required to keep this posture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-20-source-large.gif
2010,5184932,Fig. 21.,"Experimental result. (a) The angle of Joint 1 converged to 0.26 rad, that is, the reference generated by the network in Fig. 14. (b) The angle of Joint 2 smoothly converged to the reference (0.8 rad) using the network in Fig. 9.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-21-source-large.gif
2010,5184932,Fig. 22.,Experimental result: time history of input to Joint 1. The input converged to 0. The converged posture is balanced without the control of Joint 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-22-source-large.gif
2010,5184932,Fig. 23.,Experimental result: weights at end-effector versus learned angle of Joint 1. This result shows that this system can learn the balanced posture through body–environment interaction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-23-source-large.gif
2010,5184932,Fig. 24.,Experimental result: rhythm generation with 306-g weight.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-24-source-large.gif
2010,5184932,Fig. 25.,"Experimental result: frequency of rhythm versus weights at end-effector. As the weight became heavier, the frequency linearly decreased.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-25-source-large.gif
2010,5184932,Fig. 26.,Experimental result: time history of the angle of Joint 1 in the experiment that the end-effector weight was changed from 752 to 306 g at 78 s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-26-source-large.gif
2010,5184932,Fig. 27.,"Movements of firing and resting neuron groups. After one time step, the thresholds of the firing neuron group change
α
and that of the resting neuron group change
−β
. The two groups become connected without a gap or overlap after the movements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5306448/5184932/5184932-fig-27-source-large.gif
2010,5458075,Fig. 1.,"Average performance obtained by iteratively removing
N
feature out based on (a)
F
-score sorting and (b) random selection. Each iteration use same number of features for classification, for example, 60 − 5 = 55 features were used in each iteration in the case of leave 5, where the first point represents the removal of the first- to fifth-ranked (or randomly selected) features and the second data point represents the removal of the sixth to tenth features, etc.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5484937/5458075/5458075-fig-1-source-large.gif
2010,5458075,Fig. 2.,"Comparison of 2-D feature scatter plot from a sample subject along (a) top and (b) last two of
F
-score-ranked features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5484937/5458075/5458075-fig-2-source-large.gif
2010,5458075,Fig. 3.,"Comparison of average results using top-
N
subject-dependent, subject-independent features, and the number of electrodes for subject-independent features, where top
N
was defined as the range of the
F
-score from the first to the
N
th attributes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5484937/5458075/5458075-fig-3-source-large.gif
2010,5458075,Fig. 4.,"Degree of use of each electrode in the top-30 subject-independent features. The degree of use is color coded, according to the color bar on the right. (The symmetry pattern is due to the fact that the spectral differences were derived from symmetrical pairs.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5484937/5458075/5458075-fig-4-source-large.gif
2010,5473206,Fig. 1.,The system architecture of BioLMiner.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473206/5473206-fig-1-source-large.gif
2010,5473206,Fig. 2.,"The forward sliding window is applied to the example sentence from the solid frame to the dashed one. The leftmost column enumerates all discussed features and Feature is abbreviated as
F
. The first row gives the relative position (abbreviated as Po) in the snippet of the tokens with respect to the current one. The Po of the current token is 0. The second row gives the tokenized sentence snippet. The last row “Class label” gives the contextual label feature. In the
F
lex
row,
P1=
PROTEIN_41863
,
P2=PROTEIN_7737
,
P3=PROTEIN_7740
,
P4=
PROTEIN_35367
,
P5=PROTEIN_11250
, and
P6=PROTEIN_11258
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473206/5473206-fig-2-source-large.gif
2010,5473206,Fig. 3.,"Comparison of strategies of soft-TFIDF matching (
λ=0.95
) and exact matching in GNer. The first cluster of bars are the
F
β=1
measures of two matching strategies. From left to right, they are soft-TFIDF matching (
λ=0.95
) with 0.231
F
β=1
measure and exact matching with 0.289
F
β=1
measure. The middle cluster of bars are the percentage of ambiguity after different dictionary matchings. From left to right, they are soft-TFIDF matching (
λ=0.95
) with 59.1 percent ambiguity and exact matching with 24.0 percent ambiguity. The third cluster of bars are the relative CPU time for building GNer using different matching strategies. From left to right, they are soft-TFIDF matching (
λ=0.95
) with 100 percent relative CPU time (131, 566 secs) and exact matching with 4.5 percent relative CPU time (5, 854 secs).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473206/5473206-fig-3-source-large.gif
2010,5610575,Fig. 1.,Example of distribution of the 90 initial training samples (ten for each class) characterizing the chessboard classification problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5610573/5610575/5610575-fig-1-source-large.gif
2010,5610575,Fig. 2.,"Performances achieved on the chessboard classification problem in terms of (a) Acc, (b) CV accuracy, and (c) (#SV). Each graph shows the results in function of the number of training samples and averaged over ten runs of the algorithm, each with a different initial set. The shaded areas in (a) show the standard deviation of the Acc over the ten considered runs. R = random, MS = margin sampling, PPS = posterior probability sampling, QBC = query by committee, full = full SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5610573/5610575/5610575-fig-2-source-large.gif
2010,5610575,Fig. 3.,Distribution of the 1000 training samples generated by each method for the chessboard classification problem. (a) R. (b) MS. (c) PPS. (d) QBC methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5610573/5610575/5610575-fig-3-source-large.gif
2010,5610575,Fig. 4.,"2-D distribution of the six considered classes in the subspace formed by the best couple of features obtained with the PCA algorithm. For better visualization, just 25 samples were randomly selected for each class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5610573/5610575/5610575-fig-4-source-large.gif
2010,5610575,Fig. 5.,"Performances achieved on the ECG dataset by the investigated learning methods in terms of (a) Acc, (b) AvAcc, (c) CV accuracy, (d) #SV, (e) Se, and (f) number of selected samples both for class A.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5610573/5610575/5610575-fig-5-source-large.gif
2010,5610575,Fig. 6.,Performances achieved on the ECG dataset by the investigated learning methods in terms of (a) Acc and (b) AvAcc.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5610573/5610575/5610575-fig-6-source-large.gif
2010,5393023,Fig. 1.,Experimental setup for capturing thrust-force and torque degradation signals during drilling process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-1-source-large.gif
2010,5393023,Fig. 2.,Sensor signals from a typical hole drilling cycle revealing different sub-state signatures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-2-source-large.gif
2010,5393023,Fig. 3.,A Markov chain with six states and state transition probabilities (arrows represent nonzero state transition probabilities).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-3-source-large.gif
2010,5393023,Fig. 4.,Procedure for training HMM pool through competitive learning for health-state diagnostics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-4-source-large.gif
2010,5393023,Fig. 5.,Health-state inference with regular HMMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-5-source-large.gif
2010,5393023,Fig. 6.,"Hierarchical representation of states in HHMM: Top states
(
X
1
i
)
and Substates
(
X
2
j
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-6-source-large.gif
2010,5393023,Fig. 7.,Representation of HMM as a DBN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-7-source-large.gif
2010,5393023,Fig. 8.,"DBN representation of HHMM from Fig. 6.
X
d
t
denotes node
X
in time
t
at level
d
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-8-source-large.gif
2010,5393023,Fig. 9.,DBN application flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-9-source-large.gif
2010,5393023,Fig. 10.,HHMM training and inference illustration. (a) HHMM training. (b) HHMM inference.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-10-source-large.gif
2010,5393023,Fig. 11.,Illustration of equipment health-state transition paths (including self-transitions). 1: Brand new equipment; 5: Failure state; 2: Current health-state.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-11-source-large.gif
2010,5393023,Fig. 12.,Trust-force and torque data from drill-bit #5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-12-source-large.gif
2010,5393023,Fig. 13.,Mean and covariance structure of sub-states of the four health-state HMMs superimposed on data from drill-bit #1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-13-source-large.gif
2010,5393023,Fig. 14.,Log-likelihood trajectories of the four health-state HMMs past competitive learning for data from drill-bit #1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-14-source-large.gif
2010,5393023,Fig. 15.,Mean and covariance structure of sub-states of HHMM superimposed on data from drill-bit #1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-15-source-large.gif
2010,5393023,Fig. 16.,Log-likelihood trajectories of top-level (health) states #4 of HHMM for data from drill-bit #12.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-16-source-large.gif
2010,5393023,Fig. 17.,Estimated RUL probability mass function at different stages of the life of drill-bit #2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-17-source-large.gif
2010,5393023,Fig. 18.,Estimated and true RUL for drill-bits. (a) Plot of estimated RUL and prediction limits (80% confidence) along with true RUL for drill-bit #3. (b) Plots of estimated RUL along with true RUL for all 12 drill-bits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-18-source-large.gif
2010,5393023,Fig. 19.,Width of RUL prediction limits for all 12 drill-bits based on 95% confidence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-19-source-large.gif
2010,5393023,Fig. 20.,RUL estimation accuracy for all 12 drill-bits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/5499158/5393023/5393023-fig-20-source-large.gif
2010,5570917,Fig. 1.,"The
ℓ
1
norm of the parameters estimated with standard
ℓ
2
-regularized maximum likelihood for the Nettalk task. Top:
|
μ
y,x
|
for the 53 phonemes
y
and 26 letters
x
. Bottom:
∑
y
′
|
λ
y
′
,y,x
|
for the 53 phonemes
y
and 26 letters
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570917/5570917-fig-1-source-large.gif
2010,5570917,Fig. 2.,"Performance of the models on artificial data. Models
M1
–
M3
are trained with
ℓ
2
penalty (L-BFGS), models
M4
–
M8
with the
ℓ
1
penalty term (block coordinate-wise descent). Top: performance on training set. Bottom: performance on testing set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570917/5570917-fig-2-source-large.gif
2010,5570917,Fig. 3.,Comparison of the coordinate-wise update with the block update on simulated data. Top: values of logarithmic loss being minimized. Bottom: performance on test data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570917/5570917-fig-3-source-large.gif
2010,5570917,Fig. 4.,"Nettalk task,
ℓ
1
norm of the parameters estimated with elastic net penalty,
ρ
1
=0.2,
ρ
2
=0.05
Top:
|
μ
y,x
|
for the 53 phonemes
y
and 26 letters
x
. Bottom:
∑
y
′
|
λ
y
′
,y,x
|
for the 53 phonemes
y
and 26 letters
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570917/5570917-fig-4-source-large.gif
2010,5570917,Fig. 5.,Running time as a function of the number of active features for the SBCD algorithm on the Nettalk corpus. The blue line correspond to the running time when using non-sparse forward–backward.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570917/5570917-fig-5-source-large.gif
2010,5570917,Fig. 6.,Performance comparison of different model selection approaches on CoNLL 2003 (English): test set B.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5623290/5570917/5570917-fig-6-source-large.gif
2010,5460897,Fig. 1.,"(Left) OC-SVM: The hyperplane separates with maximum margin all target data from the origin by mapping all targets (blue) to the upper side of the hyperplane and outliers (red) to the lower side. Only the green points are needed to make predictions.
x
i
is an outlier sample.
ξ
i
is one of the slack variables introduced to deal with outliers that may be present in the training set. (Right) SVDD: The hypersphere surrounds all target samples.
a
and
R
are the center and the radius of the hypersphere, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5510059/5460897/5460897-fig-1-source-large.gif
2010,5460897,Fig. 2.,"RGB image composition of the Naples image using Landsat bands 3, 2, and 1. The red square surrounds the 400 × 400 area used in our experiments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5510059/5460897/5460897-fig-2-source-large.gif
2010,5460897,Fig. 3.,RGB composition of the data acquired over the KSC by the NASA AVIRIS instrument. Thirteen classes were identified and labeled; they are superimposed in the image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5510059/5460897/5460897-fig-3-source-large.gif
2010,5460897,Fig. 4.,Channel 4 of the Landsat-5 TM images acquired on the area of Mexico in (left) April 2000 and (right) May 2002. The darker area in the May 2002 image corresponds to the burnt area.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5510059/5460897/5460897-fig-4-source-large.gif
2010,5460897,Fig. 5.,"True and classification maps for all data sets obtained with Gaussian DD, OC-SVM,
S
2
OC-SVM
, and b-SVM for some specific realizations. (a) Naples data set ground truth and classification maps for realization 6 (
l=60
and
u=1000
). (b) KSC data set ground truth and classification maps for realization 7 (
l=50
,
u=1000
, and
o=50
). (c) MERIS L1b data set RGB composition and classification maps for realization 6 (
l=200
,
u=1000
, and
o=200
). (d) Mexico data set reference and classification maps for realization 3 (
l=453
and
u=969
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5510059/5460897/5460897-fig-5-source-large.gif
2010,5460897,Fig. 6.,"Mean CPU times of the algorithms measured using Matlab
cputime
function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5510059/5460897/5460897-fig-6-source-large.gif
2010,5072219,Fig. 1.,"A main feature of our MTL-SVM is to upper bound the difference between model parameters of each task pair. The upper bounds in MTL-SVM (local/full) are applied to all task pairs as in (a), whereas those in MTL-SVM (local/network) are to a part of task pairs as in (b). MTL-SVM (global/full) and MTL-SVM (global/network) upper bound the sum of differences for all task pairs and a part of task pairs, respectively. (a) Full and (b) network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5470218/5072219/5072219-fig-1-source-large.gif
2010,5473215,Fig. 1.,An overview of the UWM BioCreative II.5 system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473215/5473215-fig-1-source-large.gif
2010,5473215,Fig. 2.,Performance of our system (t31_UWM) among different teams on the ACT task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473215/5473215-fig-2-source-large.gif
2010,5473215,Fig. 3.,Performance on development data with top N selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473215/5473215-fig-3-source-large.gif
2010,5473215,Fig. 4.,Performance of our system (t31_UWM) among different teams on the INT task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473215/5473215-fig-4-source-large.gif
2010,5473215,Fig. 5.,Performance of our system (t31_UWM) among different teams on the IPT task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/5473215/5473215-fig-5-source-large.gif
2010,4641909,Fig. 1.,"Example of (a) a flat multiclass classification problem, (b) a class hierarchy exhibiting three superclasses
a
,
b
, and
c
, with three subclasses each, and (c) a valid binarization of the
n
-ary class hierarchy from (b), where multiclass problems have been replaced by two-class problems. This is one possible ensemble member for an ensemble of constrained nested dichotomies (ECNDs) for this hypothetical classification problem.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/4641909/4641909-fig-1-source-large.gif
2010,4641909,Fig. 2.,"Example of an END for three classes
a
,
b
, and
c
. The first ensemble member contains two probabilistic binary classifiers, one for
{a}
versus
{b,c}
and one for
{b}
versus
{c}
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5535009/4641909/4641909-fig-2-source-large.gif
2010,5396341,Fig. 1.,"Overview of super-resolution shown with examples: (a) Input image is interpolated into the desired scale; (b) a set of candidate images is generated as the result of regression; (c) candidates are combined based on estimated confidences: the combined result is sharper and less noisy than individual candidates, which, however, shows ringing artifacts; and (d) postprocessing removes ringing artifacts and further enhances edges.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5452163/5396341/5396341-fig-1-source-large.gif
2010,5396341,Fig. 2.,"Performance of different sparse solution methods evaluated in terms of the cost functional (1) for the case of magnification factor 3; a fixed set of hyperparameters were used for all cases such that the comparison can be made directly in (1). The performance of randomized algorithms (random selection, k-means, and gradient descent) are calculated as averages of results from 20 different experiments with random initializations. The lengths of error bars correspond to twice the standard deviations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5452163/5396341/5396341-fig-2-source-large.gif
2010,5396341,Fig. 3.,"Factor graph representation for the optimization of (3): (a) NIP term (message propagation from node
j
to node
i
) and (b) deviation penalty term of node
j
; the message from the observation variable node
y
j
to the factor node
[j]
is a constant.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5452163/5396341/5396341-fig-3-source-large.gif
2010,5396341,Fig. 4.,Gallery of test images (disjoint from training images): We refer to the images in the text by their position in raster order.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5452163/5396341/5396341-fig-4-source-large.gif
2010,5396341,Fig. 5.,"Super-resolution examples of example-based algorithms: (a) interpolations, (b)-(f) super-resolution results of NN [7], LLE [12], NIP [1], SVR [15], and the proposed method, respectively, (g) original high-resolution images, and (h) differences of the images in (f) and (a), respectively, which correspond to the details estimated by the proposed method. Magnification factors are 2 and 4 for the first four columns and the last two columns, respectively. Experiments with NIP [1] were performed only at the magnification factor 2 case (see texts for details). Please refer to the electronic version of the current paper for better visualization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5452163/5396341/5396341-fig-5-source-large.gif
2010,5396341,Fig. 6.,"Comparison between super-resolution results of several different algorithms: (a), (d), and (g) interpolations (magnification factors 3, 4, and 4, respectively), (b) and (h) Dai et al. [3], (e) Fattal [4], (i) Freeman et al. [6], (j) Freeman et al. [7], (k) Chang et al. [12], and (c), (f), and (l) the proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5452163/5396341/5396341-fig-6-source-large.gif
2010,5161262,Fig. 1.,Rows are test samples; columns are neighborhood sizes. White indicates a correct classification and black indicates a misclassification using HKNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5518327/5161262/5161262-fig-1-source-large.gif
2010,5161262,Fig. 2.,Discriminant versus neighborhood size for example test point 284 from Vowel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5518327/5161262/5161262-fig-2-source-large.gif
2010,5161262,Fig. 3.,Discriminant versus neighborhood size for example test point 55 from Vowel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5518327/5161262/5161262-fig-3-source-large.gif
2010,4721437,Fig. 1,"Simulated data: Comparison between single and multitask learning. (a) True
f
j
(thin line) and single-task estimates (thick line) with 95 percent confidence intervals (dashed lines). (b) True
f
j
(thin line) and multitask estimates
E[
f
j
|
y
100
]
(thick line) with 95 percent confidence intervals (dashed lines).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-1-source-large.gif
2010,4721437,Fig. 2,"Simulated data: Comparison between single and multitask learning. Scatterplot of
RMS
E
ST
j
and
RMS
E
MT
j
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-2-source-large.gif
2010,4721437,Fig. 3,"Simulated data: Comparison between single and multitask estimation of the average task. True
f
¯
¯
(thin line) and its estimate (thick line) for increasing values of
k
with 95 percent confidence intervals (dashed lines).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-3-source-large.gif
2010,4721437,Fig. 4,Real pharmacokinetic data: xenobiotics concentrations after a bolus administration in 27 human subjects obtained by linearly interpolating noisy samples: average (thick) and individual profiles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-4-source-large.gif
2010,4721437,Fig. 5,Real pharmacokinetic data: (a) Single-task and (b) multitask estimates (thick line) of four representative subjects with 95 percent confidence intervals (dashed lines) using only three data (circles) for each of the 27 subjects. The other five “unobserved” data (asterisks) are also plotted. Dotted lines denote the estimates obtained by using the full sampling grid.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-5-source-large.gif
2010,4721437,Fig. 6,"Real pharmacokinetic data: Comparison between single and multitask learning. Scatterplot of
RMS
E
ST
j
and
RMS
E
MT
j
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-6-source-large.gif
2010,4721437,Fig. 7,"Simulated glucose data: Estimated average curve obtained by multitask approach applied to 1, 000 IVGTT responses",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-7-source-large.gif
2010,4721437,Fig. 8,"Simulated glucose data: Comparison between single and multitask learning. (a) True
f
j
(thin line) and single-task estimates (thick line) with 95 percent confidence intervals (dashed lines). (b) True
f
j
(thin line) and multitask estimates
E[
f
j
|
y
1,000
]
(thick line) with 95 percent confidence intervals (dashed lines).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-8-source-large.gif
2010,4721437,Fig. 9,"Simulated glucose data: comparison between single and multitask learning. Scatterplot of
RMS
E
ST
j
and
RMS
E
MT
j
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5370684/4721437/4721437-fig-9-source-large.gif
2010,5353702,Fig. 1.,Hand gesture experimental setup with four electrodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5431105/5353702/5353702-fig-1-source-large.gif
2010,5560789,Fig. 1.,"Classification problem solved by SVMs. The solid line represents the separating hyperplane while the dotted lines are hyperplanes with confidence margin equal to one. Black points are unbound SVs, grey points are bound SVs, and extra borders indicate bound SVs which are also training errors. All other points do not contribute to the function to be minimized. Dotted lines indicate the margin error
ξ
i
for bound SVs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-1-source-large.gif
2010,5560789,Fig. 2.,"Higher dimensional feature space projection via mapping function
Φ
for non-linearly separable problem.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-2-source-large.gif
2010,5560789,Fig. 3.,"Example of BC-EMO application of multiobjective 0/1 knapsack problem with five items and two knapsacks, assuming a linear utility function
U(z)
. Items profits are
[0.81,0.51,0.07,0.75,0.71]
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-3-source-large.gif
2010,5560789,Fig. 4.,"Learning curves for an increasing number of training examples per iteration with one (red/solid), two (green/dashed), and three (blue/dotted) training iterations. Each row reports results for a different problem class: 0/1 knapsacks, DTLZ1, DTLZ6, and DTLZ7, respectively. Each column reports results for a different number of objectives: four, eight, and ten, respectively. Results are medians over 100 runs. Note that
y
-axes have different ranges.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-4-source-large.gif
2010,5560789,Fig. 5.,Problem DTLZ1 with two objectives. (a) PF for a sample run of plain NSGA-II without user preference. (b) Preference values of the PF according to the non-linear utility function in (10).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-5-source-large.gif
2010,5560789,Fig. 6.,Approximation error for the non-linear DTLZ1 problem as a function of the number of training instances: tuned kernel vs. linear kernel. Results are medians over 10 runs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-6-source-large.gif
2010,5560789,Fig. 7.,Problem DTLZ6 with two objectives. (a) PF for a sample run of plain NSGA-II without user preference. (b) Preference values of the PF according to the non-linear utility function in (11).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-7-source-large.gif
2010,5560789,Fig. 8.,Approximation error for the non-linear DTLZ6 problem as a function of the number of training instances: tuned kernel vs. linear kernel. Results are medians over ten runs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-8-source-large.gif
2010,5560789,Fig. 9.,Problem DTLZ7 with two objectives. (a) PF for a sample run of plain NSGA-II without user preference. (b) Preference values of the PF according to the non-linear utility function in (12).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-9-source-large.gif
2010,5560789,Fig. 10.,Approximation error for the non-linear DTLZ7 problem as a function of the number of training instances: tuned kernel vs. linear kernel. Results are medians over 10 runs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/5592185/5560789/5560789-fig-10-source-large.gif
2010,5593883,Fig. 1.,"Illustration of KP. An unknown full-kernel matrix K is split into four sub-blocks
K
ll
,
K
lu
,
K
ul
(=
K
T
lu
)
, and
K
uu
. If
K
ll
equals a known seed-kernel matrix, then KP aims to propagate
K
ll
into the other unknown blocks
K
lu
,
K
ul
, and
K
uu
and thus makes the full-kernel matrix K to be known.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5617343/5593883/5593883-fig-1-source-large.gif
2010,5593883,Fig. 2.,"Comparison of average classification accuracies [in normalized mutual information (NMI)] for varying number of pairwise constraints across four datasets: Iris, Wine, Ionosphere, and PIE-10-20. Higher is better.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5617343/5593883/5593883-fig-2-source-large.gif
2010,5593883,Fig. 3.,"Comparison of average running times (in seconds) for varying number of pairwise constraints across four datasets: Iris, Wine, Ionosphere, and PIE-10-20. Lower is better.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5617343/5593883/5593883-fig-3-source-large.gif
2010,5593883,Fig. 4.,"Comparison of average classification accuracies (in NMI) for varying number of pairwise constraints for KP on in-sample data, BE and OE on out-of-sample data across four datasets: PIE-10-20, Control, USPS0123, and MNIST0123. Higher is better.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5617343/5593883/5593883-fig-4-source-large.gif
2010,5593883,Fig. 5.,"Illustration to show that when the same-class data are distributed in multiple clusters, their leading eigenvectors derived from graph Laplacian will be unrepresentative and thus uninformative. Left: Four-lines data. Right: Comparison of average classification accuracies (in NMI) by four algorithms on four-lines data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5617343/5593883/5593883-fig-5-source-large.gif
2010,4459304,Fig. 1.,"(a) One-dimensional plot of
L
p
,
p=0.5
, 1, 2. (b) Two-dimensional level sets of
L
p
, same
p
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5405961/4459304/4459304-fig-1-source-large.gif
2010,4459304,Fig. 2.,"The plot of loss (error) functions: hinge loss
h(z)
and smooth hinge
h
s
(z)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5405961/4459304/4459304-fig-2-source-large.gif
2010,4459304,Fig. 3.,Average AUC values plotted against sample size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5405961/4459304/4459304-fig-3-source-large.gif
2010,4459304,Fig. 4.,Average predicted AUC values of different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5405961/4459304/4459304-fig-4-source-large.gif
2010,5382497,Fig. 1.,"Average number of CCCP iterations in CPMMC/CPM3C as a function of sample size
n
. (a) Two-class problems. (b) Cora. (c) WebKB. (d) 20-Newsgroup and Reuters RCVI.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5406298/5382497/5382497-fig-1-source-large.gif
2010,5382497,Fig. 2.,"CPU time (in seconds) of CPMMC/CPM3C as a function of sample size
n
. (a) Two-class problems. (b) Cora. (c) WebKB. (d) 20-Newsgroup and Reuters RCVI.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5406298/5382497/5382497-fig-2-source-large.gif
2010,5382497,Fig. 3.,"Clustering results of CPMMC with various values for
ϵ
on two-class problems. (a) Clustering accuracy of CPMMC as a function of
ϵ
. (b) CPU time (in seconds) of CPMMC as a function of
ϵ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5406298/5382497/5382497-fig-3-source-large.gif
2010,5382497,Fig. 4.,"Clustering results of CPMMC with various values for
ϵ
on multiclass problems. (a) Cora accuracy versus
ϵ
. (b) Cora CPU time versus
ϵ
. (c) WebKB accuracy versus
ϵ
. (d) WebKB CPU time versus
ϵ
. (e) 20-newsgroup and Reuters RCVI accuracy versus
ϵ
. (f) 20-newsgroup and Reuters RCVI CPU time versus
ϵ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5406298/5382497/5382497-fig-4-source-large.gif
2010,5342429,Fig. 1.,"(a)
f
+
(x)
and histogram of its samples. (b)
f
−
(x)
and histogram of its samples. (c)
d
γ
(x)
(solid line) and
d
ˆ
γ
(x;
α
ˆ
)
(dashed line). (d) Sparsity of the proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5552594/5342429/5342429-fig-1-source-large.gif
2010,5342429,Fig. 2.,"Decision boundary along with positive samples (+) and negative samples (*) for banana data set. Points whose corresponding
α
i
are nonzero are enclosed by
◯
. (a) L2QP-0, (b) L2QP-1, (c) S-SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5552594/5342429/5342429-fig-2-source-large.gif
2010,5406147,Fig. 1.,"Optimal basis selection at step
k+1
: Select the basis having the largest component
∥
ϕ
⊥
i
∥
perpendicular to the hyperplane spanned by
Φ
:,
I
(k)
b
, being the set of basis vectors selected through step
k
. In this illustration,
k=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-1-source-large.gif
2010,5406147,Fig. 2.,"The optimal measurement location at step
k+1
is for the observation
γ
i
having the corresponding kernel vector
φ
i
that lives in the region of greatest variance, which can be measured using the eigenvectors
q
j
and eigenvalues or variances
λ
j
of the posterior covariance matrix of
x
after step
k
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-2-source-large.gif
2010,5406147,Fig. 3.,"(a) Basis locations for the method of Section III-B . (b) Measurement locations for the method of Section III-C using the basis functions of the left figure. Though these measurement locations correspond to the basis locations, we have empirically found that this is not always the case. Both plots show the order of selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-3-source-large.gif
2010,5406147,Fig. 4.,(a) The 13 basis locations selected by the RVM. (b) The first 15 basis locations selected by KMP. The first 8 are the minimum number required to linearly separate the two classes. The SVM (not displayed) selected 76 basis functions for this data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-4-source-large.gif
2010,5406147,Fig. 5.,"The MSE as a function of measurement number averaged over 50 runs for (a) the concrete data set and (b) the abalone data set. For clarity, the error bars are not shown, but a sense of their magnitudes is given in Table I . As is evident in plot (b), sometimes a kernel is unnecessary.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-5-source-large.gif
2010,5406147,Fig. 6.,The mean and standard deviation of the MSE averaged over measurement numbers 6 to 50 as a function of basis dimension for (a) the concrete data set and (b) the abalone data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-6-source-large.gif
2010,5406147,Fig. 7.,"The AUC as a function of measurement number averaged over 20 runs for (a) the WDBC data set and (b) the UXO data set. Plot (a) indicates that basis selection can be combined with other active learning methods to improve performance. Unless otherwise noted, all linear methods use the label selection method of this paper.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-7-source-large.gif
2010,5406147,Fig. 8.,The mean and standard deviation of the AUC averaged over measurement numbers 6 to 50 as a function of basis dimension for (a) the WDBC data set and (b) the UXO data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/5446566/5406147/5406147-fig-8-source-large.gif
2010,5559473,Fig. 1.,"Low-rank approximation errors of different algorithms using the Gaussian kernel. (a) German, (b) splice, (c) adult1a, (d) dna, (e) segment, (f) w1a, (g) uci, and (h) satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-1-source-large.gif
2010,5559473,Fig. 2.,"Low-rank approximation errors of different algorithms using the linear kernel. (a) German, (b) splice, (c) adult1a, (d) dna, (e) segment, (f) w1a,(g) uci, and (h) satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-2-source-large.gif
2010,5559473,Fig. 3.,"Low-rank approximation errors of different algorithms on using the polynomial kernel. (a) German, (b) splice, (c) adult1a, (d) dna, (e) segment,(f) w1a, (g) uci, and (h) satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-3-source-large.gif
2010,5559473,Fig. 4.,"Performance of different algorithms in approximating the top three features in KPCA. (a) German, (b) splice, (c) adult1a, (d) dna, (e) segment,(f) w1a, (g) uci, and (h) satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-4-source-large.gif
2010,5559473,Fig. 5.,"Performance of different algorithms in approximating the 3-D embedding of Laplacian eigenmap. (a) German, (b) splice, (c) adult1a, (d) dna,(e) segment, (f) w1a, (g) uci, and (h) satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-5-source-large.gif
2010,5559473,Fig. 6.,"Exact and approximate spectral embedding results on the isomap-face dataset. The number in brackets is the misalignment error
e
measured w.r.t. the exact embedding result. Top: KPCA; Bottom: Laplacian eigenmap. (a) Exact result. (b) Ours
(e=0.047)
. (c) Nystrom
(e=0.68)
. (d) Exact result. (e) Ours
(e=5.14×
10
−5
)
. (f) Nystrom
(e=1.51×
10
−3
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-6-source-large.gif
2010,5559473,Fig. 7.,Approximate spectral embedding results on some large datasets. Top: KPCA; Bottom: Laplacian eigenmap. (a) connect: ours. (b) connect: Nystrom. (c) ijcnn: ours. (d) ijcnn: Nystrom. (e) connect: ours. (f) connect: Nystrom. (g) ijcnn: ours. (h) ijcnn: Nystrom.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-7-source-large.gif
2010,5559473,Fig. 8.,Low-rank approximation errors of the different algorithms on an anisotropic kernel matrix. (a) Boston housing and (b) abalone.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-8-source-large.gif
2010,5559473,Fig. 9.,"Performance of the different algorithms in GP regression. To improve clarity, the error on the abalone dataset is plotted in log scale. (a) Boston housing and (b) abalone.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5594730/5559473/5559473-fig-9-source-large.gif
2010,5161276,Fig. 1.,"Clustering accuracy (averaged over 50 independent trials) is shown as the portion of labeled data increases from 0% (unsupervised) to 100% (supervised): (a) our method, SSNMF and (b) label propagation [17].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5263912/5161276/5161276-fig-1-source-large.gif
2010,5272205,Fig. 1.,General active learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-1-source-large.gif
2010,5272205,Fig. 2.,"Two unlabeled examples
A
and
B
with maximum uncertainty at the
i
th learning iteration. Solid circles and cross circles denote labeled samples with different labels in current training data. Blank circles denote unlabeled examples being queried. The solid line represents the corresponding decision boundary.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-2-source-large.gif
2010,5272205,Fig. 3.,Active learning with re-ranking technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-3-source-large.gif
2010,5272205,Fig. 4.,"Results of uncertainty sampling and SUD methods for density estimation in active learning on the Interest dataset, with
K
varying between 5 and 200. Confidence bars (with confidence at 95% level) indicate the variability of competing active learning techniques.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-4-source-large.gif
2010,5272205,Fig. 5.,Effectiveness of various selective sampling techniques in active learning for WSD and TC tasks on five evaluation data sets. Confidence bars (with confidence at 95% level) indicate the variability of competing active learning techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-5-source-large.gif
2010,5272205,Fig. 6.,Results of margin-based uncertainty sampling and density-based re-ranking methods for active learning with SVMs on the Interest data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-6-source-large.gif
2010,5272205,Fig. 7.,Results of agreement between the examples selected by different selective sampling methods for active learning with ME-based classifier on the Interest data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-7-source-large.gif
2010,5272205,Fig. 8.,Results of agreement between the examples selected by margin-based uncertainty sampling and density-based re-ranking methods for active learning with SVMs on the Interest data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5508665/5272205/5272205-fig-8-source-large.gif
2010,4909037,Fig. 1.,"An example of 140 consecutive frames, corresponding with 70 s, from a small intestine capsule endoscopy video study. The image shows a paradigmatic visual example of the dynamics involved in intestinal motility. The green rectangles surround different contraction frames labeled by the experts. In the occlusive and nonocclusive contractions labeled as (a) and (b), respectively, the camera focused the lumen during the whole contraction. In (c)–(h), the lumen was partially or totally missed in different parts of the sequence due to the free movement of the camera within the gut.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-1-source-large.gif
2010,4909037,Fig. 2.,"Graphical representation in three steps (before, at the time of, and after the contraction event). (a) The paradigm of a complete phasic contraction. (b) The camera pointing towards the intestinal wall. (c) The presence of turbid liquid hindering the visualization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-2-source-large.gif
2010,4909037,Fig. 3.,"Two sequences of intestinal contractions with presence of turbid liquid, which hinders partially (top) and completely (bottom) the correct visualization of the event.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-3-source-large.gif
2010,4909037,Fig. 4.,"Cascade system for intestinal motility assessment. The input is the video study and the output are the intestinal contraction frames suggested by the system. Each stage rejects sequences of noncontractions. The global performance can be tuned by the set of parameters
P
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-4-source-large.gif
2010,4909037,Fig. 5.,"Pattern of
f
1
(solid blue line) for (a) one contraction and (b) a random sequence. The dashed red line corresponds to the averaged pattern of all the labeled intestinal contractions and the box plots define their lower quartile, median, and upper quartile (the whiskers show the extent of the rest of the data).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-5-source-large.gif
2010,4909037,Fig. 6.,"Paradigmatic sequences of wall (top), and tunnel frames (bottom). This type of sequences lack information regarding contractions, and the system detects and rejects them as system negatives in the second stage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-6-source-large.gif
2010,4909037,Fig. 7.,"Original image, LoG filter response, binary blob and final lumen segmentation for the nine frames of an intestinal contraction sequence.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-7-source-large.gif
2010,4909037,Fig. 8.,High detection rates (up to 95%) are obtained for complete occlusive and complete nonocclusive contractions. Detection rates for the generic case are plotted in black for comparison purposes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-8-source-large.gif
2010,4909037,Fig. 9.,Interobserver agreement in terms of coincident labels. (a) Histogram of studies regarding coincident labels. (b) Box plot showing median value around 90% of coincident labels and three out of four studies over 83% of agreement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-9-source-large.gif
2010,4909037,Fig. 10.,Human and system labelling histograms of intestinal contractions for (a) Video 3 and (b) Video 5. Each bin contains the number of contractions each 3 min. The last row shows the cdf and the KS-test result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-10-source-large.gif
2010,4909037,Fig. 11.,"Some example sequences provided by the system. (a) Correctly detected contractions. (b) Nondetected contractions (false negatives). (c) Sequences which had not been labeled by the experts, but detected as contractions (false positives).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-11-source-large.gif
2010,4909037,Fig. 12.,"Operation points from the ROC curve segments using the forward parameter selection procedure for (a)
P
1
, (b)
P
2
, (c)
P
3
, and (d)
P
4
. Each symbol represents each of the different five runs. The different points of each symbol represent the different performance pairs of sensitivity versus FP-ratio.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-12-source-large.gif
2010,4909037,Fig. 13.,"Operation points from the ROC curve segments using the forward parameter selection procedure for (a)
P
1
, (b)
P
2
, (c)
P
3
, and (d)
P
4
, grouped by parameter value. The mean of each ellipse represents the mean of the performance pair obtained for that parameter after five runs. The axes of the ellipses are proportional to the resulting mean variance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/5405633/4909037/4909037-fig-13-source-large.gif
2011,5628257,Fig. 1.,Flow chart of the proposed active learning method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5753305/5628257/5628257-fig-1-source-large.gif
2011,5628257,Fig. 2.,Illustration with a toy classification problem. (a) Original classification space (SVs are encircled). (b) Corresponding significance space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5753305/5628257/5628257-fig-2-source-large.gif
2011,5628257,Fig. 3.,"Performances achieved on (a)–(c) the Boumerdes and (d)–(f) the Pavia data sets in terms of (a), (d) OA, (b), (e) CV accuracy, and (c), (f) #SV. Each graph shows the results in function of the number of training samples and averaged over ten runs of the algorithm, each with a different initial set. The shades areas in (a) and (d) show the standard deviation of the OA over the ten considered runs.
R=random
,
MS=margin sampling
,
Proposed=proposed method
,
full=full SVM
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5753305/5628257/5628257-fig-3-source-large.gif
2011,5628257,Fig. 4.,Percentage of selected samples for classes (a) soil and (b) man-made of the Boumerdes data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5753305/5628257/5628257-fig-4-source-large.gif
2011,5692833,Fig. 1.,"Modeling of sinc function using BELM. Solid line: BELM modeling for an architecture with 20 hidden nodes, dashed lines: the modeling taking into account 95% CI. Dots are the used patterns (training and validation datasets).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5721294/5692833/5692833-fig-1-source-large.gif
2011,5692833,Fig. 2.,Modeling of the Housing dataset. Solid line represents the achieved RMSE in the training dataset and dashed line is the RMSE corresponding to the validation dataset. ELM is represented by squares and BELM by circles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5721294/5692833/5692833-fig-2-source-large.gif
2011,5732709,Fig. 1.,General framework of the GS-LSSVM learning paradigm based on ODBC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006806/5732709/5732709-fig-1-source-large.gif
2011,5732709,Fig. 2.,Example of a candidate solution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006806/5732709/5732709-fig-2-source-large.gif
2011,5732709,Fig. 3.,GS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006806/5732709/5732709-fig-3-source-large.gif
2011,5732709,Fig. 4.,Flowchart ABC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006806/5732709/5732709-fig-4-source-large.gif
2011,5732709,Fig. 5.,(a) Selected frequencies for Bupa liver disorders. (b) Selected frequencies for car evaluation. (c) Selected frequencies for credit approval. (d) Selected frequencies for Haberman's survival. (e) Selected frequencies for Heart Disease. (f) Selected frequencies for molecular biology. (g) Selected frequencies for Monk problems. (h) Selected frequencies for mushroom. (i) Selected frequencies for Pima Indians diabetes. (j) Selected frequencies for voting records.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006806/5732709/5732709-fig-5-source-large.gif
2011,5732709,Fig. 6.,Accuracy for different proportion of training data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006806/5732709/5732709-fig-6-source-large.gif
2011,5444873,Fig. 1.,Synthetic data set. (a) Training data set of four labeled examples. (b) The decision boundary produced by RegBoost-S. (c) The decision boundary produced by RegBoost-SM. (d) The decision boundary produced by RegBoost.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5444873/5444873-fig-1-source-large.gif
2011,5444873,Fig. 2.,"Facial expression recognition on the AR face database. (a) Exemplar pictures corresponding to four facial expressions. (b) Evolution of averaging test errors as different numbers of unlabeled instances are used at
LDR=20%
. (c), (d), and (e) Test errors at
LDR=30
, 40, and
50%
, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5444873/5444873-fig-2-source-large.gif
2011,5716669,Fig. 1.,"Simplest ICD system: a single-chamber ICD with a single-coil integrated bipolar lead. The distal electrode (tip) has a small area and is located in the apex (or the septum) of the right ventricle (RV). The proximal electrode (coil) is an elongated electrode located in the RV, close to the tip; this electrode delivers the electrical shock if necessary, together with the can as the reference: the latter is generally implanted in left subclavicular position.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5768111/5716669/5716669-fig-1-source-large.gif
2011,5716669,Fig. 2.,"Three SPOT curves for a single patient. For illustration, EGMs are sampled at 500 Hz.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5768111/5716669/5716669-fig-2-source-large.gif
2011,5716669,Fig. 3.,Overview of the discrimination algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5768111/5716669/5716669-fig-3-source-large.gif
2011,5716669,Fig. 4.,Simplified boundary surfaces based on arrhythmia cardiac frequency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5768111/5716669/5716669-fig-4-source-large.gif
2011,5701668,Fig. 1.,Process diagram of spatio temporal data mining approach for HAB detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-1-source-large.gif
2011,5701668,Fig. 2.,Study area is shown in the rectangle along the Florida west coast.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-2-source-large.gif
2011,5701668,Fig. 3.,"The feature relevance of K. brevis from SeaWiFS sensor data through feature ranking. The feature index details are given in Table II. The Chl-a statistical features are given more importance (14, 15, 13), followed by wavelet features of bands 443, 490, 510 and Chl-a (20, 22, 18, 26, 21). Even the backscattering ratio feature is quite important in reducing many false positives (16).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-3-source-large.gif
2011,5701668,Fig. 4.,The distribution of eigen values to the number of eigen vectors for SeaWiFS sensor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-4-source-large.gif
2011,5701668,Fig. 5.,The performance curve of the SVM classification model over increase in the number of KPCA features used.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-5-source-large.gif
2011,5701668,Fig. 6.,"(a) Classification map output of HAB and non-HAB on DEC 12, 1999, a single day. The false alarm rate is also clearly seen in the figure. (b) Classification map output of HAB and non-HAB for the year 2001. This year has no false alarm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-6-source-large.gif
2011,5701668,Fig. 7.,Two different locations at which the seasonal variations are observed over the year 2001.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-7-source-large.gif
2011,5701668,Fig. 8.,"Model Prediction result of HAB (top) and non-HAB (bottom) on time-series data in summer and fall season in the year 2001. (a) At Location 1, the HABs started occurring in the late summer and fall. HABs remain for a couple of weeks to a couple of months. Similarly, non-HABs remain for a couple of weeks in the summer season. HABs usually do not occur in early and midsummer season, and this can be seen in the prediction results of HABs. (b) At Location 2, the non-HABs occurrence is overtaken with the presence of HABs. They do not occur together as HABs dominate over non-HABs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-8-source-large.gif
2011,5701668,Fig. 9.,"(a) Spatio-temporal Variability over 30 days, i.e., from day 231 to 261. It is clearly seen that HAB (red) is increasing in size and becoming dense over time from day 1 to day 30. (b) Similarly, the cross section of only 10 days shows more details of HAB spatio-temporal variability. It is clearly seen that the HAB is dissected over days 3 and 4 due to winds or tidal currents and again increasing in the size on day 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/5997340/5701668/5701668-fig-9-source-large.gif
2011,6032069,Fig. 1.,"Sensitivity of the number of nearest neighbors. Average performance for (a) 15-Scene dataset and (b) MIT Indoor dataset over five-fold cross-validation with various
p
values. Error bars indicate standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6017215/6032069/6032069-fig-1-source-large.gif
2011,6032069,Fig. 2.,"The weights distribution over the eight features for four random selected training samples from (a)–(d) 15-scene dataset and (e)–(h) MIT Indoor dataset with their respective optimal
p
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6017215/6032069/6032069-fig-2-source-large.gif
2011,6032069,Fig. 3.,15-Scene dataset: Comparison with three state-of-the-art local learning algorithms with different number of training samples per category. (a) Classification accuracy; (b) training time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6017215/6032069/6032069-fig-3-source-large.gif
2011,5661784,Fig. 1.,"Unsupervised feature selection by the LLC-fs with
k=30,β=1
on the USPS digits. (a) and (b) show the (sorted)
τ
values on the “4 versus 9” and “0 versus 8” data sets, respectively. In (c) and (d), the first row plots the class mean images, while the second row shows the top 15 features in each mean image ranked by the
τ
weight vector. (a) USPS 4 versus 9, (b) USPS 0 versus 8, (c) USPS 4 versus 9, and (d) USPS 0 versus 8.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5661784/5661784-fig-1-source-large.gif
2011,5661784,Fig. 2.,"The feature weight vectors
τ
s learned by LLS-fs with
k=30,β=1
on the genomic data sets. (a) Colon cancer, (b) SRBCT, (c) leukemia, and (d) breast cancer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5661784/5661784-fig-2-source-large.gif
2011,5661784,Fig. 3.,"The parameter sensitivity studies for LLC-fs on the UCI data sets, USPS data sets, and genomic data sets, respectively, where the values on each line represent the average ACC over 10 independent runs. In (a), (c), and (e), the neighborhood size
k
varies with
β=1
, while (b), (d), and (f) show that the trade-off parameter
β
varies with
k=30
. (a) and (b) UCI data sets, (c) and (d) USPS data sets, and (e) and (f) genomic data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5661784/5661784-fig-3-source-large.gif
2011,5661784,Fig. 4.,"(a) A sample face image and its edge map in horizontal and vertical orientations, (b) a sample nonface image and its edge map in horizontal and vertical orientations, and (c) weight maps obtained by the LLC-mkl with
k=60,β=10
, where the patches with the lighter intensities have larger weight values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5661784/5661784-fig-4-source-large.gif
2011,5661784,Fig. 5.,"The parameter sensitivity studies of the LLC-mkl algorithm, where the values on each line represent the average ACC over 10 independent runs. In (a), the size
k
of neighborhood varies with
β=10
, in (b),
β
varies with
k=30
, (c) shows the change of ACC over
k
, while (d) shows the change of ACC over the trade-off parameter
β
. (a) and (b) Document data sets, and (c) and (d) MIT CBCL data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5661784/5661784-fig-5-source-large.gif
2011,6064897,Fig. 1.,ADAIN: adaptive incremental learning for classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-1-source-large.gif
2011,6064897,Fig. 2.,Mapping function based on MLP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-2-source-large.gif
2011,6064897,Fig. 3.,Implementation details of MLP for distribution function estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-3-source-large.gif
2011,6064897,Fig. 4.,Prediction overall accuracy. (a) Spambase. (b) Magic. (c) Waveform. (d) Sat.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-4-source-large.gif
2011,6064897,Fig. 5.,Vertical averaging approach of ROC curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-5-source-large.gif
2011,6064897,Fig. 6.,Simulation results for different number of total data chunks for training. (a) Overall prediction accuracy. (b) Area under ROC curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-6-source-large.gif
2011,6064897,Fig. 7.,New concept learning for “waveform” data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-7-source-large.gif
2011,6064897,Fig. 8.,New concept learning for “sat” data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-8-source-large.gif
2011,6064897,Fig. 9.,Pruning hypotheses before the 1st introduction of new concepts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-9-source-large.gif
2011,6064897,Fig. 10.,"Performance of ADAIN on “magic” data set using
Q
of different
H
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6099844/6064897/6064897-fig-10-source-large.gif
2011,5715887,Fig. 1.,Illustration of two-level inference for semisupervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-1-source-large.gif
2011,5715887,Fig. 2.,"Test error with respect to the ratio
N
l
/
N
u
, where
N
u
represents the number of unlabeled samples and
N
l
the number of labeled samples. From the top, we have the error rate on Gaus50, Gaus50
x
, and Test datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-2-source-large.gif
2011,5715887,Fig. 3.,"Impact of hyperparameters in the semisupervised LS-SVM with the Gaus50 dataset. We set
μ
equal to 0.1, 1, and 10, respectively, and vary
β
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-3-source-large.gif
2011,5715887,Fig. 4.,"Impact of hyperparameters in the semisupervised LS-SVM with the Gaus50
x
dataset. We set
μ
equal to 0.1, 1, and 10, respectively, and vary
β
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-4-source-large.gif
2011,5715887,Fig. 5.,"Impact of hyperparameters in the semisupervised LS-SVM with the Text dataset. We set
μ
equal to 0.1, 1, and 10, respectively, and vary
β
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-5-source-large.gif
2011,5715887,Fig. 6.,"Impact of hyperparameters in the semisupervised LS-SVM with the USPS dataset. We set
μ
equal to 0.1, 1, and 10, respectively, and vary
β
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-6-source-large.gif
2011,5715887,Fig. 7.,Behavior of the error rate on the unlabeled samples and on the test samples with the Gaus50 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-7-source-large.gif
2011,5715887,Fig. 8.,"Behavior of the error rate on the unlabeled samples and on the test samples with the Gaus50
x
dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-8-source-large.gif
2011,5715887,Fig. 9.,Behavior of the error rate on the unlabeled samples and on the test samples with the Text dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5715887/5715887-fig-9-source-large.gif
2011,5936738,Fig. 1.,"Two large margin separating hyperplanes explain training data equally well, but have different number of contradictions on the Universum. The model with a larger number of contradictions should be favored.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-1-source-large.gif
2011,5936738,Fig. 2.,Generation of the Universum data by averaging.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-2-source-large.gif
2011,5936738,Fig. 3.,Example of randomly chosen handwritten digits 5 and 8 and the corresponding Universum sample obtained by averaging.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-3-source-large.gif
2011,5936738,Fig. 4.,Histogram of projections of the training data and the Universum samples onto the normal direction vector of the SVM hyperplane. (a) Training samples of the two classes in red and blue. (b) Training samples of the two classes are in red and blue and the Universum samples are in black.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-4-source-large.gif
2011,5936738,Fig. 5.,"Typical histogram. (a) Case 2: Training data is separable, and its projections cluster inside margin borders. (b) Case 3: Training data is separable, and its projections cluster outside margin borders.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-5-source-large.gif
2011,5936738,Fig. 6.,Noisy hyperbolas data sets. (a) Standard deviation of noise is 0.025. (b) Standard deviation of noise is 0.05.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-6-source-large.gif
2011,5936738,Fig. 7.,Histogram of projections of training data of hyperbolas data set onto the normal direction of RBF SVM decision boundary. (a) Low noise hyperbolas data. (b) High noise hyperbolas data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-7-source-large.gif
2011,5936738,Fig. 8.,"(a) Histogram of projections of MNIST training data onto normal direction of RBF SVM decision boundary. Training set size
∼1000
samples. (b) Histogram of projections of ABCDETC training data onto normal direction of polynomial SVM decision boundary. Training set size
∼150
samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-8-source-large.gif
2011,5936738,Fig. 9.,Histogram of projections onto normal direction of linear SVM. (a) MNIST data set. (b) Synthetic data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-9-source-large.gif
2011,5936738,Fig. 10.,"Histogram of projections of the Universum data onto normal direction of RBF SVM decision boundary. Training set size
∼100
samples. (a) RA Universum. (b) Other digits Universum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-10-source-large.gif
2011,5936738,Fig. 11.,"Univariate histogram of projections for three different types of Universa. Training set size
∼100
samples, Universum set size
∼1000
samples. (a) Digit 1 Universum. (b) Digit 3 Universum. (c) Digit 6 Universum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-11-source-large.gif
2011,5936738,Fig. 12.,"Univariate histogram of projections for three different types of Universa. Training set size
∼1000
samples. Universum set size
∼1000
samples. (a) Digit 1 Universum. (b) Digit 3 Universum. (c) Digit 6 Universum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-12-source-large.gif
2011,5936738,Fig. 13.,Universum sample via binomial noise distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-13-source-large.gif
2011,5936738,Fig. 14.,Histogram of projections for binomially distributed Universum.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-14-source-large.gif
2011,5936738,Fig. 15.,"Univariate histogram of projections for three different types of Universa for ABCDETC data training set size {\sim}{150}
samples. Universum set size {\sim}{1500}
samples. (a) “Upper case letters A–Z” Universum. (b) “Digits 0–9” Universum. (c) RA Universum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5936738/5936738-fig-15-source-large.gif
2011,5604695,Fig. 1.,Pi-Sigma neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5754657/5604695/5604695-fig-1-source-large.gif
2011,5604695,Fig. 2.,Ridge polynomial neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5754657/5604695/5604695-fig-2-source-large.gif
2011,5604695,Fig. 3.,SuperPol measuring cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5754657/5604695/5604695-fig-3-source-large.gif
2011,5604695,Fig. 4.,Microwave properties parameter extracting procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5754657/5604695/5604695-fig-4-source-large.gif
2011,5604695,Fig. 5.,"Permittivity evolution obtained by LS-SVM, RPNN and iterative inversion procedure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/5754657/5604695/5604695-fig-5-source-large.gif
2011,6004834,Fig. 1.,"Loss Functions. (a) Hinge loss (2). (b) Squared hinge loss (3). (c) Modified Huber loss (4). (d) Modified Huber loss (5)
(ε=0.5)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-1-source-large.gif
2011,6004834,Fig. 2.,"Schematic illustrations of the differences among several nonlinear path-following approaches. In each illustration, the piecewise thick curves represent the nonlinear solution path and vertical dashed lines indicate the breakpoints. (a) Predictor-corrector approach iterates the predictor step and the corrector step. The predictor step approximates the solution along the path and the corrector step brings the predicted point back to the path. (b) Hot start approach uses previous solution for initial estimation of the next solution. (c) In our approach, the analytical form of the nonlinear solution path is derived and the breakpoints can be detected exactly. The first two approaches (a) and (b) roughly approximate the nonlinear path and they could not detect the breakpoints.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-2-source-large.gif
2011,6004834,Fig. 3.,"Rational approximation for the type 1 in Table I. Note that
ρ+
ψ
ζ
(t)>
φ
ζ
(t)
,
t∈(
t
0
,
t
∗
)
. The approximated solution
t
1
can be computed via a quadratic equation. Iterating this, we can obtain a sequence of approximated solution with quadratic convergence.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-3-source-large.gif
2011,6004834,Fig. 4.,"Loss function and
ρ−h
. (a)
0<ρ−h
. (b)
0>ρ−h>−∞
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-4-source-large.gif
2011,6004834,Fig. 5.,Example of artificial data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-5-source-large.gif
2011,6004834,Fig. 6.,"Sizes of index sets
L
,
C
, and
R
in the regularization path for artificial data. (a)
n=100
. (b)
n=400
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-6-source-large.gif
2011,6004834,Fig. 7.,"Size of index sets
L
,
C
, and
R
in the regularization path for real data sets. (a) Fourclass. (b) German.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-7-source-large.gif
2011,6004834,Fig. 8.,Plots of the validation errors measured by the 0–1 loss. (a) Sonar. (b) Heart. (c) Australian. (d) Diabetes. (e) Fourclass. (f) German.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-8-source-large.gif
2011,6004834,Fig. 9.,Plots of the paths of the AUC for validation data set. (a) Sonar. (b) Heart. (c) Australian. (d) Diabetes. (e) Fourclass. (f) German.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-9-source-large.gif
2011,6004834,Fig. 10.,"Schematic illustration of the approximation.
ℓ
is a tangent line of
d+ψ(t)+ρt
. From the convexity,
ℓ
becomes lower bound of
d+ψ(t)+ρt
and
d+(p/q−t)+ρt
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-10-source-large.gif
2011,6004834,Fig. 11.,"g
1
is the slope of the dashed line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/6004834/6004834-fig-11-source-large.gif
2011,5934438,Fig. 1.,Graphical generative model for labeled and unlabeled samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-1-source-large.gif
2011,5934438,Fig. 2.,Affinity matrix recording instance-level correlations between labeled (row) and unlabeled (column) samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-2-source-large.gif
2011,5934438,Fig. 3.,Conceptual view of using hidden features to link unlabeled samples to labeled instance for semi-supervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-3-source-large.gif
2011,5934438,Fig. 4.,"Graphical models for estimating posterior probability
P(
z
k
|x)
. (a) Calculate
P(
z
k
|x)
using conditional probability
P(x|
z
k
)
, which is unavailable. (b) Estimate
P(
z
k
|x)
using
x
's nearest neighborhood
Δ
x
in
L
, whose conditional probabilities
P(
Δ
x
|
z
k
)
are available.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-4-source-large.gif
2011,5934438,Fig. 5.,"Prediction accuracies (showing in the
z
-axis) with respect to the numbers of hidden concepts
(
z
k
,k=1,…,K)
and the sizes of the nearest neighborhood (using C4.5 as the learning algorithm). The number of concepts
K
varies from 1 to 20 for TTT and Car datasets, and from 1 to 40 for Vowel dataset. The size of nearest neighborhood varies from 1 to 15 for all datasets. (a) TTT: 2 classes. (b) Car: 4 classes. (c). Vowel: 11 classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-5-source-large.gif
2011,5934438,Fig. 6.,"Prediction accuracies with respect to different sizes
(α)
of labeled set
L
. The
x
-axis denotes the size of the labeled set
α
, and the
y
-axis shows the prediction accuracies (using C4.5 as the learning algorithm). (a) Vowel: 11 classes. (b) Car: 4 classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-6-source-large.gif
2011,5934438,Fig. 7.,"Prediction (ACC) and labeling (Lab) accuracies of the Co-Training with respect to the percentage of samples to be labeled and included in the training set. The
x
-axis denotes the labeling percentage and the
y
-axis shows the prediction (solid lines) and labeling (dashed lines) accuracies. Each dataset corresponding to two lines with the same color and same style (
α=0.25
and using C4.5).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-7-source-large.gif
2011,5934438,Fig. 8.,"Experimental comparisons between fSSL and Transductive SVM for cross-domain semi-supervised learning.
TrSVM
L
,
TrSVM
L+U
,
TrSVM
fSSL
, each denotes accuracy of applying Transductive SVM learning algorithm to labeled samples (L), labeled plus unlabeled (L + U) samples, and transferred labeled samples using fSSL, respectively. A ∗ indicates a
t
-test significant, comparing
TrSVM
fSSL
to
TrSVM
L+U
, at the 0.05 level. Because Transductive SVM can only handle binary classification problems, classes used to splitting target and auxiliary sets are Car : {good, vgood versus unacc, acc}, Digits: {0, 1 versus 2, 3, 4, 5, 6, 7, 8, 9}, Letter: {B, R versus A, E, F, H, K}, Segment: {bickface, cement versus sky, foliage, window, path, grass}, Vowel: {hid, hId versus hEd, hAd, hYd, had, hOd, hod, hUd, hud, hed}, Wine: {1, 2 versus 0}, and Zoo: {bird, invertebrate versus mammal, reptile, fish, amphibian, insect}. For comparison purposes, the actual accuracies of Voweland Wine data sets are the accuracies showing in the figure minus 20.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5934438/5934438-fig-8-source-large.gif
2011,5873124,Fig. 1.,"ROC curve comparison of SK\MK-SVM based VAD with different optimization objectives in car noise
(SNR=5 dB)
. W# is short for the MO-MP features with different window lengths.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5873694/5873124/5873124-fig-1-source-large.gif
2011,5638613,Fig. 1.,"(Top) Table with the summary of the illustrative examples. The cases in which the original and adaptive classifiers coincide with the optimal linear classifier (LDA) for the test set are marked with “o.” (“inside” = mean shift within the decision boundary, “outside” = arbitrary mean shift). (Bottom) Figures depicting the examples that illustrate the abilities of the unsupervised adaptation procedures to follow nonstationary changes. The blue ellipsoids are the initial distributions, while the red ones show the situations in online sessions. The blue and red dashed lines are the true decision boundaries in the initial and online sessions. The nonadaptive classifier is correct, if there is no shift outside of the decision boundary marked with the blue dashed line (a and c). The decision boundaries by the unsupervised LDA's are plotted with black (I), red (II), and magenta (III).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-1-source-large.gif
2011,5638613,Fig. 2.,"(Top) Schema where it can be seen which angle
α
is computed to compare the change between the training and the test sets. (Bottom)
α
angles obtained using dataset 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-2-source-large.gif
2011,5638613,Fig. 3.,"Comparison of classifiers using error rates. All of them are compared to no adaptation except the bottom-right corner, in which adaptation with and without class labels are compared.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-3-source-large.gif
2011,5638613,Fig. 4.,(Left) Effect of removing trials of one class (test dataset) in the performance. The horizontal red line is the original accuracy obtained during the experiments. The blue crosses are obtained applying PMean. (Right) Scatter plot comparing the performance obtained during the online experiments with the performance obtained with the method PMean. PMean outperforms the original method used to provide feedback during the experimental session when the crosses are below the diagonal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-4-source-large.gif
2011,5638613,Fig. 5.,"Comparison of accuracy between original method (used in the experiments to provide feedback), PMean and optimal bias (noncausal).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-5-source-large.gif
2011,5638613,Fig. 6.,"Scatter plot to study the calibration to feedback transfer performance. The crosses indicate feedback runs recorded with discrete visual feedback only at the end of each trial. The circles indicate feedback data recorded with continuous visual feedback. For more details on the paradigm, see [27].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-6-source-large.gif
2011,5638613,Fig. 7.,"(Left) Accuracy reached with discrete feedback at the end of the trial for the original method (supervised 20 trials), PMean, and optimal bias. (Right) Accuracy reached with continuous visual feedback for the original method (supervised 20 trials), PMean, and optimal bias.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-7-source-large.gif
2011,5638613,Fig. 8.,Results of unsupervised covariate shift adaptation. Values under the diagonal indicate that the covariate shift adaptation is better than the original method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-8-source-large.gif
2011,5638613,Fig. 9.,"(Left) Averaged principal component of the transfer from calibration to feedback (calibration–feedback). It has a strong focus in the parietooccipital area. (Right) Time course of the averaged parietooccipital power in time. The median of the users is depicted. The boxplots refer to the complete calibration and feedback, respectively, but they are located at the end and beginning of the two measurements to ease the comparison between them.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-9-source-large.gif
2011,5638613,Fig. 10.,"(Left) Scatter plot comparing the unsupervised adaptive classifier with the state-of-the-art approach. (Right) Mean accuracy and standard error of the state-of-the art classifier, the unsupervised classifier, and the classifier with the optimal bias (noncausal).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-10-source-large.gif
2011,5638613,Fig. 11.,"(Left) Ongoing change of the KL divergence of the feedback data from the calibration data. (Right) Corresponding feature distributions for the shaded intervals. The data are projected on the plane spanned by the normal vector of the PMean hyperplane and the largest PCA component of the feedback data. This plane is the same for the calibration, supervised, optimal, and the PMean classifiers. Only bias changes occur. The black line corresponds to the classifier trained only in Calibration data. The green line is the PMean classifier in the corresponding interval. The dashed-red line is the optimal classifier for the complete feedback session. The blue line corresponds to the supervised classifier in 20 trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/5719227/5638613/5638613-fig-11-source-large.gif
2011,5722074,Fig. 1.,"Probability of correct classification of the proposed CSS-SVM classifiers (with and without SNR knowledge) versus the ALRT upper bound, the DLRT, the
[|
C
20
|,|
C
40
|]
and the
|
C
40
|
cumulant-based classifiers for the ideal scenario using
N=250
symbols.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/5749432/5722074/5722074-fig-1-source-large.gif
2011,5722074,Fig. 2.,"Probability of correct classification of the proposed CSS-SVM with unknown SNR classifier versus the
|
C
20
|,|
C
40
|
and the
|
C
40
|
cumulant-based classifiers considering the presence of frequency offset using
N=250
symbols.
SNR=8
dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/5749432/5722074/5722074-fig-2-source-large.gif
2011,5557878,Fig. 1.,Block diagram for the proposed algorithm. Left-hand column: training phase; right-hand column: testing phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-1-source-large.gif
2011,5557878,Fig. 2.,Pseudocode for instance update.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-2-source-large.gif
2011,5557878,Fig. 3.,Summary of the proposed iterative framework for instance selection and classifier learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-3-source-large.gif
2011,5557878,Fig. 4.,Example results of instance selection on synthetic data. (a) Raw data. (b) EM-DD. (c) EM-DD close-up view. (d) MILIS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-4-source-large.gif
2011,5557878,Fig. 5.,Behavior of IPs as a function of iteration number for poor algorithm initialization. (a) Initial step. (b) Iteration 1. (c) Iteration 3. (d) Iteration 5. (e) Final result. (f) Cost (y-axis) versus iteration number (x-axis).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-5-source-large.gif
2011,5557878,Fig. 6.,Influence of final feature pruning on classification performance and sparsity for MILIS on the MUSK data sets. (a) MUSK1. (b) MUSK2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-6-source-large.gif
2011,5557878,Fig. 7.,"(a) Mean Spectra for the instance in all bags (blue), positive bags (red), and negative bags (green). (b) Influence of final feature pruning on classification performance and sparsity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-7-source-large.gif
2011,5557878,Fig. 8.,Example mapping results for the detection of infected regions in hyperspectral imagery from the CAPS data set. Left-hand column: pseudocolor image; middle column: foreground regions; and right-hand column: pathogen mapping results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-8-source-large.gif
2011,5557878,Fig. 9.,Example images from the COREL image database used for region-based image categorization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-9-source-large.gif
2011,5557878,Fig. 10.,"Influence of final feature pruning on classification performance and sparsity for MILIS on the COREL data sets. (a) 1, 000-Image data set. (b) 2, 000-Image data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-10-source-large.gif
2011,5557878,Fig. 11.,"Sample COREL image data and IPs selected by MILIS: (a) building, (b) dog, (c) beach, (d) bus, (e) dinosaur, (f) flower, (g) horse, (h) mountain, and (i) dish.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-11-source-large.gif
2011,5557878,Fig. 12.,Example images from the Caltech 101 image categorization database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-12-source-large.gif
2011,5557878,Fig. 13.,"Examples of implicit ROI selection. Blue rectangles indicate the annotated ground truth, while red rectangles indicate ROIs automatically selected by the MIL algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5557878/5557878-fig-13-source-large.gif
2011,5475279,Fig. 1.,"Measuring similarity with angle. Three categories of data distributed on three sectors, respectively. The central angle of each sector is less than
π/3
, and the central angle between any two sectors is larger than
π/3
. Similarity between data is well reflected by the central angle between them.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5688133/5475279/5475279-fig-1-source-large.gif
2011,5475279,Fig. 2.,"(Top) Images from the AT&T database. (Bottom) Images from the COIL-100 database. Each subfigure shows (bottom left) a dissimilar pair, (bottom right) a similar pair, and (top) images from a typical subject.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5688133/5475279/5475279-fig-2-source-large.gif
2011,5475279,Fig. 3.,Curves of Training Errors and Test Errors of our Experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5688133/5475279/5475279-fig-3-source-large.gif
2011,5475279,Fig. 4.,ROC for our Similarity Estimators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5688133/5475279/5475279-fig-4-source-large.gif
2011,5567099,Fig. 1.,Higher order co-occurrence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-1-source-large.gif
2011,5567099,Fig. 2.,"Example document collection (Deerwester et al., 1990).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-2-source-large.gif
2011,5567099,Fig. 3.,Deerwester term-to-term matrix (adapted from Kontostathis and Pottenger [9]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-3-source-large.gif
2011,5567099,Fig. 4.,Deerwester term-to-term matrix truncated to two dimensions (adapted from Kontostathis and Pottenger [9]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-4-source-large.gif
2011,5567099,Fig. 5.,Path Group structure based on a simple path in co-occurrence graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-5-source-large.gif
2011,5567099,Fig. 6.,Extracting/enumerating higher order paths from path group structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-6-source-large.gif
2011,5567099,Fig. 7.,Classifier accuracy from five percent to 90 percent of training set. (a) Computer. (b) Science. (c) Politics. (d) Religion. (e) Citeseer. (f) Cora. (g) WebKB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6048019/5567099/5567099-fig-7-source-large.gif
2011,5937024,Fig. 1.,"In this GA, two competing clans are created based on a single grandparent gene sequence (A). Mutations are based on a fixed-length gene sequence, the values of which can be modified up or down (B) according to a mutation range vector. After each generation, the best-performing child is selected to be the parent of the next generation (C).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-1-source-large.gif
2011,5937024,Fig. 2.,"(A) Spiral search. (B) Linear move. (C) Radial search. Both the linear move and radial search support tool rotations, while the radial and spiral searches involve complex trajectories.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-2-source-large.gif
2011,5937024,Fig. 3.,The transmission valve body assembly consisted of the valve plug (left) being inserted into one of three receptor holes in the body base using a spiral search (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-3-source-large.gif
2011,5937024,Fig. 4.,After 20 generations of the transmission valve body assembly the time to complete the assembly saw marked downward trends.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-4-source-large.gif
2011,5937024,Fig. 5.,An aluminum pentagonal puzzle insert (left) was inserted by first engaging the circular lip (middle) and then rotated to match the pentagon profile (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-5-source-large.gif
2011,5937024,Fig. 6.,"The raw (light lines) and expected (dark lines) results for a high-dimensional physical assembly with (dashed) and without (solid) the benefit of internal modeling are shown. The expected performances for both are monotonically decreasing, though the actual trial-by-trial performances vary.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-6-source-large.gif
2011,5937024,Fig. 7.,"Illustration of the model fit to the 1D empirical data with reference to the natural variance
b
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-7-source-large.gif
2011,5937024,Fig. 8.,One-dimensional plot showing the difference between the actual and running average surfaces for high spatial frequency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-8-source-large.gif
2011,5937024,Fig. 9.,The average assembly performance results of the unassisted (solid) and assisted (dashed) GA for stage 2 of the puzzle assembly.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-9-source-large.gif
2011,5937024,Fig. 10.,"Stage 2 puzzle assembly network outputs. The points are the normalized training samples and the height-mapped surface represents the output of the network for normalized inputs in the range of [−1.0, 1.0].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-10-source-large.gif
2011,5937024,Fig. 11.,"The sun gear (left) was inserted into the gear base (as seen from above), where it meshed with the three freely spinning primary planet gears.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-11-source-large.gif
2011,5937024,Fig. 12.,The expected performance of the sun gear assembly demonstrates little difference between the unassisted and assisted GA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-12-source-large.gif
2011,5937024,Fig. 13.,"Model surface plots for the: (A) speed versus radius, (B) speed versus turns, and (C) radius versus turns.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-13-source-large.gif
2011,5937024,Fig. 14.,"Performance results of the pure stochastic search and model-assisted search for the transmission valve body assembly. Of the three trials, only the one testing the search speed and spiral radius (top) demonstrated performance improvement over time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/6025294/5937024/5937024-fig-14-source-large.gif
2011,5762620,Fig. 1.,Classification results of (a) TBSVM and (b) SVC for “Cross planes” dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762620/5762620-fig-1-source-large.gif
2011,5613908,Fig. 1.,"Basic structure of power systems and their protection systems. Observe that protection systems exist in all three components (generation, transmission, and distribution) of the electrical power system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-1-source-large.gif
2011,5613908,Fig. 2.,Characteristic curve of over-current relays.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-2-source-large.gif
2011,5613908,Fig. 3.,"(a) RX diagram and (b) protection zones for distance relays. R and X correspond to the resistive and reactive components, respectively, of the impedance measured by a distance relay.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-3-source-large.gif
2011,5613908,Fig. 4.,Two bus system with a distance relay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-4-source-large.gif
2011,5613908,Fig. 5.,Conditional probability density functions (pdfs) of normal current and fault current in an over-current relay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-5-source-large.gif
2011,5613908,Fig. 6.,"Multithreshold setting for smart relays. Observe that there are several thresholds defining the normal region and faulty region. Accordingly, a current magnitude larger or smaller than the normal current range can indicate a fault.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-6-source-large.gif
2011,5613908,Fig. 7.,SVM classification with two features: magnitude of voltage and current.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-7-source-large.gif
2011,5613908,Fig. 8.,Nonlinear classification boundary generated by a typical SVM classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-8-source-large.gif
2011,5613908,Fig. 9.,The fault-detection process of SVM-based smart relays as a flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-9-source-large.gif
2011,5613908,Fig. 10.,The algorithm for testing scalability which illustrates the process of enlarging the testing area tier by tier starting with a single transmission line.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-10-source-large.gif
2011,5613908,Fig. 11.,Testing and training accuracy in different size networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-11-source-large.gif
2011,5613908,Fig. 12.,The functional logic and the apparent impedance seen by line 63–59 in different system conditions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-12-source-large.gif
2011,5613908,Fig. 13.,"Required number of simulations as a function of the number of circuit elements in
N−1
(left) and
N−2
(right) studies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-13-source-large.gif
2011,5613908,Fig. 14.,The histogram of false tripping events of a noncritical relay at different load levels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-14-source-large.gif
2011,5613908,Fig. 15.,The histogram of false tripping events of a critical relay at different load levels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-15-source-large.gif
2011,5613908,Fig. 16.,The R-X diagram for critical relays.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-16-source-large.gif
2011,5613908,Fig. 17.,The R-X diagram for noncritical relays.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-17-source-large.gif
2011,5613908,Fig. 18.,The histogram for node degree in the IEEE 118 bus system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-18-source-large.gif
2011,5613908,Fig. 19.,Current topology and structure of power grid (modified from [33]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-19-source-large.gif
2011,5613908,Fig. 20.,Representative oversimplified power grid as an abstraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-20-source-large.gif
2011,5613908,Fig. 21.,"The separating hyperplain that maximizes the margin. [“o” is a positive data point, i.e.,
f(‘o')>0
, and ” +” is a negative data point, i.e.,
f(‘+') < 0
.]",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/5669979/5613908/5613908-fig-21-source-large.gif
2011,5487524,Fig. 1.,"An experiment on synthetic data. A family of multiplicative kernel classifiers is learned, where
k
θ
is an RBF kernel defined on
θ
, and
k
x
is a linear kernel defined on
x=(
x
1
,
x
2
)
T
. The linear boundaries, for example, detectors
w(
23
∘
)
and
w(−
30
∘
)
are shown in (b). The circle points along each linear boundary are the reweighted support vectors (5) of weights
>0.5
. These synthetic “foreground” and “background” classes were chosen to illustrate the idea that local discriminants can be learned jointly via multiplicative kernels, and then constructed at a given
θ
. (a) The original data (b) Two example detectors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-1-source-large.gif
2011,5487524,Fig. 2.,"Pseudocode for bootstrap training with parametric within-class kernel
k
θ
. For the case of nonparametric
k
θ
, the set
Θ
is replaced by the set of indices of foreground training samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-2-source-large.gif
2011,5487524,Fig. 3.,Example sign language sequences from which the training and test hand images are obtained.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-3-source-large.gif
2011,5487524,Fig. 4.,"Three hand clusters are displayed with their cluster medoids, positive detector weights, and hand masks. For each cluster, the weights of foreground support vectors are displayed at the bottom.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-4-source-large.gif
2011,5487524,Fig. 5.,"Hand detection results: (a) ROC curves of different detectors for hand detection. “Our method-chamfer” uses the
k
θ
defined by chamfer edge distance. “Our method-HOG” uses the
k
θ
defined using RBF kernel in HOG feature space. (b) Example detection and segmentation results on the sign language test images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-5-source-large.gif
2011,5487524,Fig. 6.,Training and test performance of the proposed approach on the hand shape data set [62]: (a) The change of margin during the iterative training process. The margin is defined as the minimum classification score of positive training tuples minus the maximum score of all negative training tuples at an iteration. (b) Comparison of ROC curves on the hand shape data set with 2D parameters [62]. The detection rate is in the range between 88 and 100 percent.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-6-source-large.gif
2011,5487524,Fig. 7.,Example images and their binary segmentation masks from the multiview vehicle data set used in [25].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-7-source-large.gif
2011,5487524,Fig. 8.,Vehicle detection result on data set [25]: (a) ROC curves. (b) False-negative examples of our method. (c) False-positive examples of our method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-8-source-large.gif
2011,5487524,Fig. 9.,"Example frames and ground truth annotations of two cameras in test sequence 5 of the PETS 2001 data set. Although vehicles running close to the cameras have good resolutions, the actual challenges come from the vehicles running in the opposite direction across the fence. They usually have small resolutions and are partially occluded. Detection accuracy of these vehicles is a decisive factor in the ROC curves. (a) Forward view camera. (b) Rear view camera.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-9-source-large.gif
2011,5487524,Fig. 10.,Vehicle detection rate versus false-positive rate on sequence 5 of PETS 2001 data set. The proposed approach (Multiplicative Kernel) is compared with Wu-Nevatia's tree-based detector [60] and Torralba's feature sharing method [55].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-10-source-large.gif
2011,5487524,Fig. 11.,"Four example sequences of car tracking. Sequences (a), (b), (c), and (d) correspond to sequence IDs 3, 8, 1, and 7, respectively, in Table 1. Synthesized views of tracked cars are displayed on the top of a car. Green boxes highlight the errors in these sequences. In sequence (b), the initial detection in the first frame assigns the detected car a rear view, due to the ambiguity between front view and rear view. The error is corrected at subsequent frames when more frames are evaluated during temporal propagation. In sequence (c), the car is missed at frame 25 because the viewpoint elevation is much higher than those in training images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-11-source-large.gif
2011,5487524,Fig. 12.,"Face view angle estimation result on Multi-PIE data set. For each view angle subclass, we plot the mean and standard deviation of the errors on test samples. The overall mean absolute errors are 2.1 degrees and 3.0 degrees for our method and Torralba's feature sharing method [55], respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-12-source-large.gif
2011,5487524,Fig. 13.,"Example tracking result in two test sequences. The first row is from sequence 1. The second row is from sequence 2. On top of each tracked face, a training example with the same face orientation is displayed. The tracker stops tracking when the left-right rotation of a face is larger than 90 degrees from a frontal face. A face is missed in the fifth example frame of the first sequence and the third example frame of the second sequence.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5692151/5487524/5487524-fig-13-source-large.gif
2011,5724308,Fig. 1.,Illustration on the importance of the structural information within classes in SRSVM and SVM. (a) Discriminant boundaries in the training set. (b) Discriminant boundaries in the testing set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-1-source-large.gif
2011,5724308,Fig. 2.,Illustration of structural granularity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-2-source-large.gif
2011,5724308,Fig. 3.,Framework for structural large margin classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-3-source-large.gif
2011,5724308,Fig. 4.,Choosing the knee point corresponding to the optimal number of clusters in Sonar.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-4-source-large.gif
2011,5724308,Fig. 5.,"Classification results of SRSVM, SLMM, and SVM on the toy XOR dataset with 10% of the samples in each distribution as the training set. (a) Discriminant boundaries in the training set. (b) Discriminant boundaries in the testing set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-5-source-large.gif
2011,5724308,Fig. 6.,"Classification results of SRSVM, SLMM, and SVM on the toy XOR dataset with 50% of the samples in each distribution as the training set. (a) Discriminant boundaries in the training set. (b) Discriminant boundaries in the testing set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-6-source-large.gif
2011,5724308,Fig. 7.,Illustration of 20 subjects on the COIL-20 database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-7-source-large.gif
2011,5724308,Fig. 8.,Illustration of 10 digits on the USPS database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-8-source-large.gif
2011,5724308,Fig. 9.,"Classification accuracies compared between SVM, EKM, SLMM, LapSVM, and SRSVM on (a) COIL-20 database and (b) USPS database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-9-source-large.gif
2011,5724308,Fig. 10.,"Bounds of the expected risks for SVM, EKM, SLMM, LapSVM, and SRSVM on the four UCI datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5741988/5724308/5724308-fig-10-source-large.gif
2011,5763754,Fig. 1.,"Examples of HSI chips showing variations of location, number, and shape of target signatures within an AOI. Note that each spatial pixel in each chip has a corresponding spectral vector; the spectral response for 9.4
μm
is shown within the image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5997348/5763754/5763754-fig-1-source-large.gif
2011,5763754,Fig. 2.,Bag construction for testing and training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5997348/5763754/5763754-fig-2-source-large.gif
2011,5763754,Fig. 3.,ROCs for Experiments 1–3. (a) ROC of singleton MI-RVM and MI-RVM. (b) ROC of singleton C-RSF-MIL and C-RSF-MIL. (c) ROC of MI-RVM and standard RVM. (d) ROC of SingleLSetC MI-RVM and MI-RVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/5997348/5763754/5763754-fig-3-source-large.gif
2011,5957304,Fig. 1.,CmpNN architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5957304/5957304-fig-1-source-large.gif
2011,5957304,Fig. 2.,"Results, measured by MAP (a) achieved on TREC2003, (b) TD2004, and (c) OSHUMED. Obtained with the CmpNN trained without the active-learning technique and with the different versions of SortNet.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5957304/5957304-fig-2-source-large.gif
2011,5957304,Fig. 3.,"Evolution of the MAP measure during the training process on (a) TD2003, (b) OHSUMED, (c) TD2004, and (d) evolution of the size of the training and validation sets in the SortNet learning procedure when processing the TD2004 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5957304/5957304-fig-3-source-large.gif
2011,5957304,Fig. 4.,Original SortNet versus SortNet 1vsAll scheme on TD2004.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5957304/5957304-fig-4-source-large.gif
2011,5975223,Fig. 1.,"Learn
++
.NSE
algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-1-source-large.gif
2011,5975223,Fig. 2.,"Sigmoidal error weighting in
Learn
++
.NSE
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-2-source-large.gif
2011,5975223,Fig. 3.,Gaussian drift data with class addition/removal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-3-source-large.gif
2011,5975223,Fig. 4.,Comparative results on the Gaussian data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-4-source-large.gif
2011,5975223,Fig. 5.,Snapshots from a 1/2 rotation of the checkerboard data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-5-source-large.gif
2011,5975223,Fig. 6.,"Variable drift rate controlled by the rate at which
α
parameter is updated for rotating checkerboard dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-6-source-large.gif
2011,5975223,Fig. 7.,"Performances on checkerboard data with (a) constant, (b) gaussian pulse, (c) exponential, and (d) sinusoidal drift rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-7-source-large.gif
2011,5975223,Fig. 8.,"Weight distribution (max in red, min in blue) over time for checkerboard dataset with (a) constant, (b) pulsing, (c) exponential, and (d) sinusoidal drift rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-8-source-large.gif
2011,5975223,Fig. 9.,"Comparative performances on SEA dataset (SVM polynomial kernel, order: 2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-9-source-large.gif
2011,5975223,Fig. 10.,"Comparative performances on weather dataset (SVM polynomial kernel, order: 2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-10-source-large.gif
2011,5975223,Fig. 11.,"Timing diagrams for Gaussian, SEA and weather datasets (with NB classifier).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5975223/5975223-fig-11-source-large.gif
2011,5560656,Fig. 1.,"Example of Genia tagger output including for each word: its base form, its part-of-speech, beginning (B), inside (I), outside (O) tags for the word, and the final tag for the phrase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-1-source-large.gif
2011,5560656,Fig. 2.,Example of MetaMap system output.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-2-source-large.gif
2011,5560656,Fig. 3.,Accuracy and F-measure results when using verb-phrases as features for task 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-3-source-large.gif
2011,5560656,Fig. 4.,Accuracy and F-measure results when using noun-phrases as features for task 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-4-source-large.gif
2011,5560656,Fig. 5.,"Accuracy and F-measure results when using biomedical concepts as features, task 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-5-source-large.gif
2011,5560656,Fig. 6.,"Accuracy and F-measure results when using NLP and biomedical concepts as features, task 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-6-source-large.gif
2011,5560656,Fig. 7.,Accuracy and F-measure results when using UMLS concepts as features for task 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-7-source-large.gif
2011,5560656,Fig. 8.,"Accuracy and F-measure results when using NLP, biomedical, and UMLS concepts as features, task 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-8-source-large.gif
2011,5560656,Fig. 9.,Accuracy and F-measure results when using BOW features for task 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-9-source-large.gif
2011,5560656,Fig. 10.,Accuracy and F-measure results when using BOW and UMLS concepts as features for task 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-10-source-large.gif
2011,5560656,Fig. 11.,"Accuracy and F-measure results when using BOW, NLP, and biomedical features, task 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-11-source-large.gif
2011,5560656,Fig. 12.,"Accuracy and F-measure results when using BOW, NLP, biomedical, and UMLS concepts features, task 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-12-source-large.gif
2011,5560656,Fig. 13.,"Results for the second task, Setting 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-13-source-large.gif
2011,5560656,Fig. 14.,"Results for the second task, Setting 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-14-source-large.gif
2011,5560656,Fig. 15.,F-measure results for four-class classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-15-source-large.gif
2011,5560656,Fig. 16.,Results for all annotated relations in the data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5753264/5560656/5560656-fig-16-source-large.gif
2011,5467147,Fig. 1.,Training results of the SVR. (a) Training SVR-4. (b) Training SVR-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/5659689/5467147/5467147-fig-1-source-large.gif
2011,5467147,Fig. 2.,Validating the SVR models. (a) Validation of SVR-4. (b) Validation of SVR-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/5659689/5467147/5467147-fig-2-source-large.gif
2011,5467147,Fig. 3.,Testing results on the regression models. (a) GDOP by direct matrix transformation. (b) GDOP by SVR-4. (c) GDOP by SVR-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/5659689/5467147/5467147-fig-3-source-large.gif
2011,5467147,Fig. 4.,Residuals on the regression models. (a) Direct GDOP vs. SVR-4. (b) Direct GDOP vs. SVR-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/5659689/5467147/5467147-fig-4-source-large.gif
2011,5467147,Fig. 5.,Percentage statistics on Dev.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/5659689/5467147/5467147-fig-5-source-large.gif
2011,5719639,Fig. 1.,Annotation techniques are subject to a trade-off between required time and accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-1-source-large.gif
2011,5719639,Fig. 2.,Illustration of the three bags-of-activities scenarios (Section 3.2 ) and the subbag scenario (Section 5.5 ).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-2-source-large.gif
2011,5719639,Fig. 3.,"Quality of labels predicted for the training set by the graph-based approach LP-TM (features, time, union, and confidence voting) and by multi-instance learning for single-labeled bags (miSVM and init-miSVM).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-3-source-large.gif
2011,5719639,Fig. 4.,Comparative performance of the supervised baselines and multi-instance learning for three labeling scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-4-source-large.gif
2011,5719639,Fig. 5.,Comparison of the classification results for the graph label propagation methods (LP-TM and LP-LGC).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-5-source-large.gif
2011,5719639,Fig. 6.,"Comparison of the classification results for the graph approach LP-TM (SVM-features, SVM-time, SVM-union, and SVM-confidence voting) to the single-labeled bags multi-instance learning algorithms (miSVM and init-miSVM).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-6-source-large.gif
2011,5719639,Fig. 7.,Subbags results: Comparative performance of multi-instance learning (miSVM and init-miSVM—the single-labeled bags scenario) and the graph method LP-TM for different subbag sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-7-source-large.gif
2011,5719639,Fig. 8.,"Noise experiments: We compare the supervised baseline SVM few labels and the graph algorithms (features and time)—LP-TM and LP-LGC (with different
α
values). The noise level of the provided labels varies between 0 to 40 percent. Results are also compared to the fully supervised baseline SVM all labels when no noise is present.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5719639/5719639-fig-8-source-large.gif
2011,5720516,Fig. 1.,"Optimization process of
β
(vehicle noise,
SNR=5
dB).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5724802/5720516/5720516-fig-1-source-large.gif
2011,5720516,Fig. 2.,Noisy labeling assumption. (a) Manual labeling in clean environment; (b) machine labeling by Ramirez VAD [2] (solid line) and the best manual labeling by human experience (dashed line); (c) error labeling assumption: the speech observations whose LRT scores are very small (below the dashed line) will be labeled as noise observations in probability. The utterance is randomly chosen from TIMIT with “/train/dr1/fvmh0/sx206.wav” as its directory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5724802/5720516/5720516-fig-2-source-large.gif
2011,5720516,Fig. 3.,"Performance comparison of the MMC-based VAD and the SVM-based VAD under noisy labeling assumption in vehicle noise (
SNR=5
dB).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5724802/5720516/5720516-fig-3-source-large.gif
2011,5720516,Fig. 4.,"Performance comparison of the VADs in vehicle noise (
SNR=5
dB).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5724802/5720516/5720516-fig-4-source-large.gif
2011,5720516,Fig. 5.,"Performance comparison of the VADs in babble noise (
SNR=5
dB).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/5724802/5720516/5720516-fig-5-source-large.gif
2011,5601723,Fig. 1.,"Example of possible wrong matches between a gold ontology (left) and a learned ontology (right). The matching between the concepts named “Nucleid_acid” is correct, but the rest of the matchings are not.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-1-source-large.gif
2011,5601723,Fig. 2.,The transformation of the ontology elements into probability distributions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-2-source-large.gif
2011,5601723,Fig. 3.,"Distributional representation of concepts “City” and “Island” before normalization. Concept “City” is more important than “Island” in the same context, since four instances of the former appear in that context.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-3-source-large.gif
2011,5601723,Fig. 4.,The representation of ontology elements in the case where an annotated data set is not available.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-4-source-large.gif
2011,5601723,Fig. 5.,"The cotopy set of concept
RNA
comprises all the direct and indirect super- and subconcepts of
RNA
, its direct properties, as well as
RNA
itself (all the shaded elements).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-5-source-large.gif
2011,5601723,Fig. 6.,An example regarding a specific match.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-6-source-large.gif
2011,5601723,Fig. 7.,"The proposed evaluation method. An inventory of matching methods and similarity measures can be used to perform the matching between the learned and the gold ontology. For each matching pair, the PCP and PCR factors are calculated, which along with the SimDist factor provide the final evaluation results in terms of
P
and
R
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-7-source-large.gif
2011,5601723,Fig. 8.,Combined diagram for all “damage” operators in the case of the Genia ontology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-8-source-large.gif
2011,5601723,Fig. 9.,Combined diagram for all “damage” operators in the case of the Lonely Planet ontology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-9-source-large.gif
2011,5601723,Fig. 10.,"Example of swapping two concepts that participate in a subsumption relation, showing the effect of the Swap Operator on their cotopy sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-10-source-large.gif
2011,5601723,Fig. 11.,Combined diagram for all “damage” operators in the case of the Genia ontology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-11-source-large.gif
2011,5601723,Fig. 12.,Combined diagram for all “damage” operators in the case of the Lonely Planet ontology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-12-source-large.gif
2011,5601723,Fig. 13.,"Example of adding the concept DNA_domain in the ontology. This concept is not matched to any of the gold ones. Thus, PCP and PCR are not calculated for this concept. Its presence affects the PCP factor of the root concept and its left child, the cotopy sets of which increase by one.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-13-source-large.gif
2011,5601723,Fig. 14.,"Example of adding a relation between the concepts Amino_acid and RNA. The cotopy set of Amino_acid is now affected and includes almost the whole ontology. Furthermore, the cotopy sets of RNA, RNA_molecule, and RNA_domain are increased by one (which is irrelevant, since it concerns proteins).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-14-source-large.gif
2011,5601723,Fig. 15.,Combined diagram for the Change Concept Representation operator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-15-source-large.gif
2011,5601723,Fig. 16.,Behavior of the evaluation measures in the test cases of the first group of the OAEI set. All methods determined the same set of matches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-16-source-large.gif
2011,5601723,Fig. 17.,Behavior of the evaluation measures in the test cases of the second group of the OAEI set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-17-source-large.gif
2011,5601723,Fig. 18.,Behavior of the evaluation measures in the test cases of the third group of the OAEI set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-18-source-large.gif
2011,5601723,Fig. 19.,Behavior of the evaluation measures in the test cases of the fourth and fifth group of the OAEI set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5601723/5601723-fig-19-source-large.gif
2011,5545419,Fig. 1.,"Relative error degradation
(RED(ψ))
as a function of
log
2
ψ
for the eight development problems defined in Table I. The gray areas indicate the quasi-optimal
ψ
-intervals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545419/5545419-fig-1-source-large.gif
2011,5545419,Fig. 2.,"Quasi-optimal
C
-intervals for the eight development problems when the analytical
γ
a
is chosen as the kernel parameter. The gray area indicates the interval of
C
-values that are quasi-optimal for at least seven of the eight problems.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545419/5545419-fig-2-source-large.gif
2011,5545419,Fig. 3.,"Average relative error degradation
RED(C|γ)
and the area (in gray) where it stays well below 0.1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545419/5545419-fig-3-source-large.gif
2011,5545419,Fig. 4.,"Quasi-optimal
γ
-intervals and
γ
values provided by the studied methods: the reference methods Wu et al.
(
γ
1
)
and Xu et al.
(
γ
2
)
, the analytical method
(
γ
a
)
, and the heuristic method
(
γ
h
)
. The different
γ
a
values for embedded two-class problems are separately displayed. The gray dotted line is the trend line representing all the data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545419/5545419-fig-4-source-large.gif
2011,5545419,Fig. 5.,"Reference error rates
E(
γ
o
,
C
o
)
, the attainable error rates
E(γ,
C
best
)
using the computed
γ
, and the actually obtained error rates
E(γ,C)
(after selection of
C
) for (left panel) the method of Xu et al. and (right panel) the analytical version of the newly proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545419/5545419-fig-5-source-large.gif
2011,5567096,Fig. 1.,"The graphical view of the inequalities requirement for the protein potential. A 2D case has been used here. The solid points denote the native structures, and the hollow points denote the decoys. (a) A linear classification function can perfectly discriminate the native structures and the decoys. (b) The nonlinear classification function is needed to distinguish the two sets of samples, whereas the linear classification function cannot succeed in this task.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5567096/5567096-fig-1-source-large.gif
2011,5567096,Fig. 2.,"Some results of correlation between RMSD and energy score. (a)-(e) show the RMSD-energy plots for the DIH_NL, DFIRE-SCM_NL, FS_NL, HRSC_NL, and T32S3_NL potentials, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5567096/5567096-fig-2-source-large.gif
2011,5545418,Fig. 1.,"Conditional independence is illustrated. Given latent variable
z
, all dimensions of the observations are independent.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-1-source-large.gif
2011,5545418,Fig. 2.,"Dimensionality reduction results for the USPS database. The left subfigure is the result obtained by the GP-LVM, and the right subfigure is the result obtained by the supervised GP-LVM. The supervised GP-LVM is superior to the GP-LVM because it considers the label information in the training stage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-2-source-large.gif
2011,5545418,Fig. 3.,"All of the dimensions of the input data
x
and the output data
y
are conditionally independent, given latent variable
z;/
x
:,d
represents the
d
dimension of all of the input data
X
; and
y
:,l
represents the
l
dimension of the output data
Y
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-3-source-large.gif
2011,5545418,Fig. 4.,"Graphical representations of the (a) proposed supervised GP-LVM, (b) SPPCA [19], (c) joint manifold model [16], and (d) dynamic shared LVM [17].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-4-source-large.gif
2011,5545418,Fig. 5.,"Full oil flow data set visualized with (a) GP-LVM, (b) GDA, (c) discriminative GP-LVM, and (d) supervised GP-LVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-5-source-large.gif
2011,5545418,Fig. 6.,"Iris data set visualized with (a) GP-LVM, (b) GDA, (c) discriminative GP-LVM, and (d) supervised GP-LVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-6-source-large.gif
2011,5545418,Fig. 7.,Mean error rates of the five methods change with the number of training samples for the USPS data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-7-source-large.gif
2011,5545418,Fig. 8.,Mean error rates of the five methods change with the number of training samples for the UCI wine data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-8-source-large.gif
2011,5545418,Fig. 9.,Mean error rates of the four methods change with the number of training samples for the UCI ionosphere data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-9-source-large.gif
2011,5545418,Fig. 10.,Mean error rates of the four methods change with the number of training samples for the UCI sonar data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/5730604/5545418/5545418-fig-10-source-large.gif
2011,6015560,Fig. 1.,An ion channels forms an hydrophilic pathway though the cell membrane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-1-source-large.gif
2011,6015560,Fig. 2.,"A sequence of several openings divided by brief closures is called a current burst. Two current bursts, one at
γ
1
and
T
1
opening time, and the other at
γ
2
and
T
2
opening time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-2-source-large.gif
2011,6015560,Fig. 3.,Tracks registered samples of ion current bursts. (a) Base line and two conductive levels with current relative to conductance values of about 60 pS and about 110 pS respectively. (b) A conductance level with an amplitude of 169 pS. (c) Histograms of the current amplitudes and best Gaussian approximation for the track in A. (d) Histogram of the amplitudes and best Gaussian approximation for the track in B.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-3-source-large.gif
2011,6015560,Fig. 4.,"Classification of ion burst current clustering conductance sampled values
γ
k
into different conductance classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-4-source-large.gif
2011,6015560,Fig. 5.,A schematic representation of a multichannel as the sum of single channels conductance amplitudes and standard deviation almost equal for any conductive state.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-5-source-large.gif
2011,6015560,Fig. 6.,"Mean value of the distance between the centroids d
(
rmvrule height 0.15em width 0.8em
⧫
rmvrule height 0.15em width 0.8em
)
, ratio
σ/
σ
0
(
rmvrule height 0.15em width 0.8em
■
rmvrule height 0.15em width 0.8em
)
and redundancy R
(⋯▲⋯)
varying the number of classes. The axis on the left is relative to the distance and the ratio
σ/σ0
, while the axis on the right is relative to the redundancy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-6-source-large.gif
2011,6015560,Fig. 7.,"Mean values of the information gain IG
(
rmvrule height 0.15em width 0.8em
⧫
rmvrule height 0.15em width 0.8em
)
, the mutual information IM
σ/
σ
0
(
rmvrule height 0.15em width 0.8em
■
rmvrule height 0.15em width 0.8em
)
and the variation coefficient
σ/γ(⋯▲⋯)
varying the number of classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6048006/6015560/6015560-fig-7-source-large.gif
2011,5680963,Fig. 1.,"Data filtering, preparation, and preprocessing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-fig-1-source-large.gif
2011,5680963,Pseudo-code 1.,Pseudo-code for filtering raw data and preprocessing it to generate predictor attributes and classify them based on their episode details.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-alg-1-source-large.gif
2011,5680963,Pseudo-code 2.,Pseudo-code for the AI module and results collation for the final output,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-alg-2-source-large.gif
2011,5680963,Fig. 2.,UML class diagram for data preprocessing module (with I/O processing submodule).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-fig-2-source-large.gif
2011,5680963,Fig. 3.,UML class diagram of JAABS algorithm showing back propagation-based neural network and radial-basis function-based neural.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-fig-3-source-large.gif
2011,5680963,Fig. 4.,Machine learning algorithm containing artificial intelligence and results module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-fig-4-source-large.gif
2011,5680963,Fig. 5.,ROC curve for Episodes one to eight for the machine learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/5723057/5680963/5680963-fig-5-source-large.gif
2011,5940201,Fig. 1.,"Example percussion samples with different amounts of additive Gaussian white noise.
A
.
SNR=∞
dB
B
.
SNR=20
dB
C
.
SNR=10
dB
D
.
SNR=0
dB
E
.
SNR=−10
dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5983457/5940201/5940201-fig-1-source-large.gif
2011,5940201,Fig. 2.,"Drum classification accuracy for different noise levels, datasets and feature combinations. The MP features were obtained using a dictionary of eight atoms. Training is done on clean data; testing is done on data with different SNRs (
x
-axis). Error bars mark the 25% and 75% percentiles of the cross-validation performance. Since the total number of samples differs between sound classes, the baseline performance is given by a classifier whose class output is always the class with the largest number of samples. The baseline performance thus is 46% for BSH, 44% for DBT and 16% for AMIX, (a) BSH, (b) DBT, and (c) AMIX.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5983457/5940201/5940201-fig-2-source-large.gif
2011,5940201,Fig. 3.,"Comparison of the classification accuracy of different features and feature combinations for two different cases: In first case, clean data is used in the training and in the test set. In the other case, a mixture of clean data and noisy data with different SNRs (−10 dB, 0 dB, 10 dB, 20 dB) is used in both training and test set. Error bars mark the 25% and 75% percentiles of the cross-validation performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5983457/5940201/5940201-fig-4-source-large.gif
2011,5940201,Fig. 4.,"Sparse Coding and Gammatone dictionary with eight atoms (upper and lower plot, respectively). The gray bars at the beginning of the atomic functions show a 5-ms interval. The pie charts depict the (relative) summed weights for the BSH dataset (red: “bd,” blue: “sd,” green: “hh”). (a) Sparse Coding dictionary. (b) Gammatone dictionary.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5983457/5940201/5940201-fig-3-source-large.gif
2011,5940201,Fig. 5.,"Averaged MP feature values for classes “bd” (solid), “sd” (dashed-dotted) and “hh” (dashed) of dataset BSH. The error bars show the 25% and 75% percentiles. The first half of the feature vector (upper plot) refers to the spike amplitudes of the atomic functions, the second half (lower plot) to their respective spike counts (firing frequency). (a) SC-MP. (b) GT-MP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5983457/5940201/5940201-fig-5-source-large.gif
2011,5611590,Fig. 1.,Multiclass architecture adopted for the BLU technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-1-source-large.gif
2011,5611590,Fig. 2.,Architecture adopted for the MCLU technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-2-source-large.gif
2011,5611590,Fig. 3.,Comparison between the samples selected by (a) the CBD technique presented in [14] and (b) the proposed ECBD technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-3-source-large.gif
2011,5611590,Fig. 4.,"Overall classification accuracy obtained by MCLU and BLU uncertainty criteria when combined with ABD and CBD diversity techniques in the same conditions for (a) Trento, and (b) Pavia data sets. The learning curve of Pavia data set is reported, starting from 87 samples until 312 in order to better highlight the small differences.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-4-source-large.gif
2011,5611590,Fig. 5.,"Overall classification accuracy obtained by the MCLU uncertainty criterion when combined with the standard CBD, KCBD, and proposed ECBD diversity techniques for (a) Trento, (b) Pavia, and (c) KSC data sets. The line “All training samples” reported in (b) and (c) shows the accuracy obtained using the full pool as the training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-5-source-large.gif
2011,5611590,Fig. 6.,"Overall classification accuracy obtained by the MCLU-ECBD, MCLU-ABD, MS-cSV, EQB, and KL-Max techniques for (a) Trento, (b) Pavia, and (c) KSC data sets. The learning curves are reported, starting from 178 samples for Trento, 92 samples for Pavia, and 142 samples for KSC data sets in order to better highlight the differences. The line “All training samples” reported in (b) and (c) shows the accuracy obtained using the full pool as the training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-6-source-large.gif
2011,5611590,Fig. 7.,"Overall classification accuracy versus the number of training samples obtained by the MCLU-ECBD technique with different
h
values for (a) Trento and (b) Pavia data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-7-source-large.gif
2011,5611590,Fig. 8.,"Overall classification accuracy versus the number of training samples for the uncertainty criterion and the combination of uncertainty and diversity criteria with different
h
values: (a) Trento and (b) Pavia data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5719225/5611590/5611590-fig-8-source-large.gif
2011,6025231,Fig. 1.,Flux lines and current density distributions of the air-core reactor at 50 Hz.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-1-source-large.gif
2011,6025231,Fig. 2.,"Deviation
E
a
versus the total number of layers
m
T
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-2-source-large.gif
2011,6025231,Fig. 3.,"Deviation
E
R
versus
r/δ
for two different
v
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-3-source-large.gif
2011,6025231,Fig. 4.,"Deviation
E
S
versus the layer filling factor
η
for different
r/δ
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-4-source-large.gif
2011,6025231,Fig. 5.,"Deviation
E
Z
versus the airway width
d
a
for different
r/δ
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-5-source-large.gif
2011,6025231,Fig. 6.,"Deviation
E
T
versus airway width
d
a
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-6-source-large.gif
2011,6025231,Fig. 7.,"Ratio
B
R
/
B
Z
along the inner sides of the innermost layers versus the distance
y
s
, with the ratio
R/H
as a parameter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-7-source-large.gif
2011,6025231,Fig. 8.,"F
R
of Dowell's and Perry's analytical formulae. (a)
m=
8 and (b)
m=
16.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-8-source-large.gif
2011,6025231,Fig. 9.,Normal probability plot of the effects by factorial experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-9-source-large.gif
2011,6025231,Fig. 10.,"Response surface plots illustrate the SVM results with
x
i1
and
x
i3
as inputs. (a) Resistance factor kernel. (b) Polynomial kernel. (c) RBF kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-10-source-large.gif
2011,6025231,Fig. 11.,"F
R
of FES, SVM, and Perry's analytical formulae versus
x
i1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-11-source-large.gif
2011,6025231,Fig. 12.,"F
R
of FES and SVM versus the ampere-turn ratio
K
C
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6029804/6025231/6025231-fig-12-source-large.gif
2011,5601738,Fig. 1.,"Four kinds of spaces in MKL-DR: (a) the input space of each feature representation, (b) the RKHS induced by each base kernel, (c) the RKHS by the ensemble kernel, and (d) the projected euclidean space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-1-source-large.gif
2011,5601738,Fig. 2.,Algorithm 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-2-source-large.gif
2011,5601738,Fig. 3.,"The Caltech-101 data set. One example comes from each of the 102 categories. All of the 102 categories are used in the experiments of supervised object recognition, while the 20 categories marked by the red bounding boxes are used in the following experiments of unsupervised image clustering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-3-source-large.gif
2011,5601738,Fig. 4.,"Recognition rates versus different dimensions of the projected data when
N
train
=15
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-4-source-large.gif
2011,5601738,Fig. 5.,Recognition rates of several published systems on Caltech-101 versus different amounts of training data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-5-source-large.gif
2011,5601738,Fig. 6.,"The values of the objective function of MKL-LDA through the iterative optimization procedure when
N
train
is set as (a) 5, (b) 10, (c) 15, (d) 20, and (e) 25, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-6-source-large.gif
2011,5601738,Fig. 7.,"The 2D visualizations of the projected data. Each point represents a data sample, and its color indicates its class label. The projections are learned by (a) kernel LPP with base kernel GB-Dist (KLPP + GB-Dist), (b) kernel LPP with base kernel GIST (KLPP + GIST), and (c) MKL-LPP with all the 10 base kernels (MKL-LPP + All kernel).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-7-source-large.gif
2011,5601738,Fig. 8.,"Four kinds of intraclass variations caused by (a) different lighting conditions, (b) in-plane rotations, (c) partial occlusions, and (d) out-of-plane rotations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-8-source-large.gif
2011,5601738,Fig. 9.,"Images obtained by applying the delighting algorithm [22] to the five images in Fig. 8a. Clearly, variations caused by different lighting conditions are alleviated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-9-source-large.gif
2011,5601738,Fig. 10.,"Each image is divided into 96 regions. The distance between the two images is obtained when circularly shifting causes
ψ
′
to be the new starting radial axis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-10-source-large.gif
2011,5601738,Fig. 11.,The learned kernel weights by MKL-SDA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5751894/5601738/5601738-fig-11-source-large.gif
2011,5733396,Fig. 1.,"Targets and distractors in different scenes can be best distinguished by different features. (a), (b) “Motion” feature. (c), (d) “Color” feature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-1-source-large.gif
2011,5733396,Fig. 2.,"Framework of our approach. In our approach, scenes with similar contents are grouped into the same cluster. For each cluster, a ranking function is optimized to give ranks for all subsets in a scene, while these estimated ranks are expected to approximate the ground-truth ranks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-2-source-large.gif
2011,5733396,Fig. 3.,"Advantage of the multi-task learning approach. Each circle corresponds to a training sample and its intensity indicates its weight when training the ranking function
ϕ
m
. (a) In single task learning, each model is trained independently and the model correlations are not considered. In this case,
ϕ
m
is trained on limited samples (i.e., samples in the red box) and may lack the generalization ability. (b) In multi-task learning, an appropriate sharing of information across models is adopted by incorporating the penalty term
Ω
d
. In this case,
ϕ
m
is actually trained on the whole dataset (i.e., the samples in the blue box) by emphasizing different subsets of samples (i.e., the samples in the red box). Therefore, the generalization ability of
ϕ
m
can be improved.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-3-source-large.gif
2011,5733396,Fig. 4.,Estimated ranks and eye fixations are compared by calculating the similarity between the estimated saliency maps and the ground-truth saliency maps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-4-source-large.gif
2011,5733396,Fig. 5.,"AUC scores when using different parameters. (a) AUC scores when using different number of scene clusters
(
ϵ
s
=0.105,
ϵ
d
=0.105,
ϵ
c
=0.14,β=5)
. (b) AUC scores when using different
β
to turn integer ranks to real saliency values
(
ϵ
s
=0.105,
ϵ
d
=0.105,
ϵ
c
=0.14,M=15)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-5-source-large.gif
2011,5733396,Fig. 6.,Time costs in training MTRL and its AUC scores when using different numbers of training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-6-source-large.gif
2011,5733396,Fig. 7.,ROC curves of MTRL and various saliency models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-7-source-large.gif
2011,5733396,Fig. 8.,Some representative results of visual saliency models. (a) Original scenes. (b) Ground-truth saliency maps. (c) Itti98 [2]. (d) Itti01 [3]. (e) Itti05 [4]. (f) Zhai06 [8]. (g) Harel07 [7]. (h) Hou07 [5]. (i) Guo08 [6]. (j) Kienzle07 [9]. (k) Navalpakkam07 [10]. (l) Peters07 [11]. (m) MTRL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-8-source-large.gif
2011,5733396,Fig. 9.,ROC curves of MTRL and various ranking models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-9-source-large.gif
2011,5733396,Fig. 10.,"Some representative results of ranking models. To facilitate the comparison between saliency models and ranking models, we show the results on the same frames. (a) Original scenes. (b) Ground-truth saliency maps. (c) Freund03 [15]. (d) Joachims06 [16]. (e) Base-I. (f) MTRL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-10-source-large.gif
2011,5733396,Fig. 11.,ROC curves of MTRL on different video genres.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/5762411/5733396/5733396-fig-11-source-large.gif
2011,5759756,Fig. 1.,Hierarchical recognition of grasp stability taking into account different types of sensory knowledge.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-1-source-large.gif
2011,5759756,Fig. 2.,Example grasping sequence of a cylinder and the corresponding tactile measurements.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-2-source-large.gif
2011,5759756,Fig. 3.,Measured (a) and (c) versus simulated (b) and (d) sensor values. The tactile images were generated by pressing a sharp edge onto the sensor surface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-3-source-large.gif
2011,5759756,Fig. 4.,"Objects in simulation were generated in three sizes (75%, 100%, and 125%): hamburger sauce, bottle, cylinder, box, sphere.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-4-source-large.gif
2011,5759756,Fig. 5.,"Hand configuration when the seventh joint is at
90
∘
,
60
∘
and
0
∘
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-5-source-large.gif
2011,5759756,Fig. 6.,Few examples from the execution of real experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-6-source-large.gif
2011,5759756,Fig. 7.,"Objects used in real experiments, with last three deformable.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-7-source-large.gif
2011,5759756,Fig. 8.,Example of a failed grasp when only visual input is used. Details about the system are reported in [7].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-8-source-large.gif
2011,5759756,Fig. 9.,Objects used to generate a dataset for the demonstration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-9-source-large.gif
2011,5759756,Fig. 10.,"Operation of the system. First row shows unsuccessful grasp, while the second row shows successful grasp. (a) and (e) Hand in a preshape position. (b) and (f) Closed grasp. (c) and (g) Tactile measurements. (d) Object dropped, while lifting. (h) Lifting and rotating the object successfully.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-10-source-large.gif
2011,5759756,Fig. 11.,"Likelihood ratios for comparison of separability. (a) Root node, all objects, random grasp vector. (b) Cylinder, random grasp vector. (c) Cylinder side grasp. (d) Real cylinder side grasps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5759756/5759756-fig-11-source-large.gif
2011,5734857,Fig. 1.,Operating modes in sensorless control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-1-source-large.gif
2011,5734857,Fig. 2.,Stator and rotor parts of the 6/4 SR machine used in our experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-2-source-large.gif
2011,5734857,Fig. 3.,Diagram of the power converter built for our experimental results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-3-source-large.gif
2011,5734857,Fig 4.,(a) Diagram of the test rig with the SR drive system. (b) Photograph of the power converter showing the IGBTs and the current controller board.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-4-source-large.gif
2011,5734857,Fig. 5.,"Conventional SR position estimator using a lookup table that implements the functional relation
θ=θ(i,λ)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-5-source-large.gif
2011,5734857,Fig. 6.,(a) Proposed rotor position estimator for the SR motor. (b) Neurofuzzy learning scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-6-source-large.gif
2011,5734857,Fig. 7.,(a) Developed signal conditioner based on voltage sensor (LEM); (b) Voltage before filter (phase 1); (c) Voltage acquired with signal conditioner developed (phase 1); and (d) Voltage after filtering (phase 1).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-7-source-large.gif
2011,5734857,Fig. 8.,Neurofuzzy speed estimator using the voltage signals after being filtered.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-8-source-large.gif
2011,5734857,Fig. 9.,"(a) Block diagram for offline training of the speed estimator. (b) Neurofuzzy trained system used as an estimator of motor speed, operating in the antireset windup PI speed controller.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-9-source-large.gif
2011,5734857,Fig. 10.,Offline training of the speed estimator for a speed reference of 100 r/min. (a) Filtered voltage signal from each motor phase. (b) Reference current evolution. (c) Measured (black line) and estimated (gray line) motor speeds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-10-source-large.gif
2011,5734857,Fig. 11.,Measured (black line) and estimated (gray line) motor speed results after offline training of the speed estimator with a speed reference of 1000 r/min.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-11-source-large.gif
2011,5734857,Fig. 12.,Training of the speed estimator during open-loop speed control. (a) Evolution of the mean quadratic error. (b) Measured (black line) and estimated (gray line) motor speed results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-12-source-large.gif
2011,5734857,Fig. 13.,"Training of the speed estimator during open-loop speed control. Measured (black line) and estimated (gray line) motor speed results when using a learning rate of (a) 0.001, (b) 0.01, (c) 0.1, and (d) 0.5. (e) Evolution of the mean quadratic error for each learning rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-13-source-large.gif
2011,5734857,Fig. 14.,"Experimental tests with the speed estimator use the learning rate values (a) 0.001, (b) 0.01, (c) 0.1, (d) 0.5, and (e) the mean square error of the four tests done.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-14-source-large.gif
2011,5734857,Fig. 15.,"Speed estimator begins its learning at t = 0 ms. At 130 ms, the motor speed changes to 150 r/min and the learning process persists for until t = 250 ms. At t = 120 ms time, the motor speed changes again to 100 r/min but the speed estimator stops its learning mechanism. The tests using a learning rate of (a) 0.01, (b) 0.1, and (c) 0.5, with (d) showing the evolution of mean square error followed the anterior scheme.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-15-source-large.gif
2011,5734857,Fig. 16.,"SR drive operation with position sensor until 80 ms and in sensorless operation afterward, for: 100 r/min, (a) measured/estimated speed, (b) current pulses in phase 1; 200 r/min, (c) measured/estimated speed, (d) current pulses in phase 1; 300 r/min, (e) measured/estimated speed, (f) current pulses in phase 1; and 400 r/min, (g) measured/estimated speed, (h) current pulses in phase 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-16-source-large.gif
2011,5734857,Fig. 17.,Example of the position error for 300 r/min.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-17-source-large.gif
2011,5734857,Fig. 18.,Performance of the speed controller and speed neurofuzzy estimator for a variation in +25% of the phase resistances of the SR motor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-18-source-large.gif
2011,5734857,Fig. 19.,Performance of the speed controller and speed neurofuzzy estimator for a load disturbance in +25% of the nominal value of the SR motor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-19-source-large.gif
2011,5734857,Fig. 20.,Performance of the speed controller which shows the action of the antireset windup PI control and speed neurofuzzy estimator for a speed reference from 0 to its nominal speed value of 1800 r/min.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6083527/5734857/5734857-fig-20-source-large.gif
2011,5453335,Fig. 1.,"K
li
represents
K
linear
, summary of determined performances for method1 using (a)
K
multitask
×
K
li
, (b)
K
uniform
×
K
li
, and (c)
K
supertype
×
K
li
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5730188/5453335/5453335-fig-1-source-large.gif
2011,5453335,Fig. 2.,"Summary of determined performances for method1 using (a)
K
multitask
×
K
poly
, (b)
K
uniform
×
K
poly
, and (c)
K
supertype
×
K
poly
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5730188/5453335/5453335-fig-2-source-large.gif
2011,5453335,Fig. 3.,"Summary of determined performances for method1 using (a)
K
multitask
×
K
RBF
, (b)
K
uniform
×
K
RBF
, and (c)
K
supertype
×
K
RBF
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5730188/5453335/5453335-fig-3-source-large.gif
2011,5453335,Fig. 4.,Summary of determined performances for different settings of method2. (a) Feature learn. (b) Feature select.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5730188/5453335/5453335-fig-4-source-large.gif
2011,5732764,Fig. 1.,"Data preprocessing used to parse through the raw experimental data and generate a set of uniformly sampled (in
θ
, which is the lateral orientation of the fish relative of the array) records when fish were in the field of view (FOV) of the system. The system automatically assigns an orientation to each fish as defined by its appearance in the top-view camera image. Assigned orientations are used to ensure data for each fish can be sampled uniformly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-1-source-large.gif
2011,5732764,Fig. 2.,"Feature-fusion algorithm. Data vectors from each view are combined together and a feature transformation is computed to yield a combined feature vector
Y
incorporating information from all
M
views. A single classifier is trained using a multiview training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-2-source-large.gif
2011,5732764,Fig. 3.,Decision-fusion algorithm. An SVM is used to classify features from each view. Each SVM outputs an estimate for the posterior probability using the softmax function. These outputs are combined into a probability vector that is classified by a second SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-3-source-large.gif
2011,5732764,Fig. 4.,"Collaborative-fusion algorithm. Each view defines an “agent” that computes an initial posterior probability estimate using an SVM similar to the first step in the decision-fusion algorithm
(
A
2
)
. These probability estimates are then sent from each agent to all the other agents. Upon receiving estimates from all other agents, each agent computes a final posterior estimate by combining their initial estimate and estimates from all other agents using confidence weighting. Final outputs from each agent are multiplied together to yield a joint posterior probability for each class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-4-source-large.gif
2011,5732764,Fig. 5.,"(a) Drawing of the scattering apparatus showing the transducer array, sliding rail system for moving the array in and out of the water, unistrut frame, top- and side-view cameras, and elliptical tank. (b) The geometry of the experiments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-5-source-large.gif
2011,5732764,Fig. 6.,"Profile photographs and acoustic echo envelopes for all nine species studied in experiments. The profile of each species, its label (
S
1
,
S
2
, etc.), and associated icon are shown. Each photo is shown on the same scale. Divisions in the background grid pattern are 2 mm. Recorded echo envelopes for 100 uniformly random orientations are shown in stacked plots below each image. Differences in average echo duration and echo peak can be seen clearly between species.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-6-source-large.gif
2011,5732764,Fig. 7.,"Probability of error (color-scale) as a function of fusion algorithm (
A
1
–
A
3
), feature space (DCT, Db4, Haar), data set (
D
1
–
D
10
, rows), and number of available views (
M=1
to
M=8
, columns). Each data set is defined in the right-most column by a gray box. The number of classes in the data set is defined by the number of columns of fish icons. The number of individuals per class is defined by the number of icons per column. Intraclass variation in length is represented by multiple icons in the same column with different widths. Interclass variation in length is represented by differences in widths between columns.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-7-source-large.gif
2011,5732764,Fig. 8.,"Probability of error (color-scale) as a function of fusion algorithm (
A
1
–
A
3
), feature space (DCT, Db4, Haar), data set (
D
1
–
D
10
, rows), and number of available random views (
M=1
to
M=8
, columns). Data sets are identical to those in Fig. 7 and general trends are similar to those for fixed views. However, it can be seen that in nearly all cases, using random views significantly reduces error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-8-source-large.gif
2011,5732764,Fig. 9.,"Classification error as a function of the number of classes in the problem taken from the
D
5
data set using (a) DCT, (b) Db4, and (c) Haar feature spaces, and
M=8
views. The width of the error bars denote
2σ
^
CV
. The classes are taken in order of species number such that two classes corresponds to discriminating between
S
1
and
S
2
, three classes corresponds to discriminating between
S
1
,
S
2
, and
S
3
, and so forth.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-9-source-large.gif
2011,5732764,Fig. 10.,"Classification error averaged over the three feature spaces as a function of the number of views for all ten data sets (subplots) and algorithms using fixed views
A
i
, and random views
A
r
i
. The width of the error bars denote
2σ
^
CV
. Note that each subplot is on a different scale to allow a detailed comparison between algorithms and view geometry for each data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/5732754/5732764/5732764-fig-10-source-large.gif
2011,5710658,Fig. 1.,Illustration of Fisher discriminant for two classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-1-source-large.gif
2011,5710658,Fig. 2.,Comparison of the results of training functionally identical (left) SVM and (right) RVM classifiers. The vectors are circled to highlight the dramatic reduction of vectors in RVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-2-source-large.gif
2011,5710658,Fig. 3.,San Diego AVIRIS hyperspectral image.(a) Single spectral channel. (b) Ground-truth map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-3-source-large.gif
2011,5710658,Fig. 4.,"Singular hard classification maps of San Diego image. Images (a)–(i) correspond to classes C1–C9, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-4-source-large.gif
2011,5710658,Fig. 5.,Indian Pine image. (a) Single spectral channel. (b) Hard classification map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-5-source-large.gif
2011,5710658,Fig. 6.,"Average classification accuracy for landcover classes of group A of San Diego data set (C1, C2, and C9) versus train-to-test sample ratio.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-6-source-large.gif
2011,5710658,Fig. 7.,"Average classification accuracy for landcover classes of group B of San Diego data set (C3, C5, C6, and C9) versus train-to-test sample ratio.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-7-source-large.gif
2011,5710658,Fig. 8.,Average classification accuracy for landcover classes of group C of San Diego data set (C4 and C7) versus train-to-test sample ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-8-source-large.gif
2011,5710658,Fig. 9.,Overall classification accuracy for nine selected landcover classes of San Diego data set versus train-to-test sample ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-9-source-large.gif
2011,5710658,Fig. 10.,Average classification accuracy versus train-to-test sample ratio for landcover classes of group A of Indian Pine data set (C1–C4).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-10-source-large.gif
2011,5710658,Fig. 11.,Average classification accuracy versus train-to-test sample ratio for landcover classes of group B of Indian Pine data set (C5 and C6).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-11-source-large.gif
2011,5710658,Fig. 12.,Overall classification accuracy versus train-to-test sample ratio for six selected landcover classes of Indian Pine data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5770378/5710658/5710658-fig-12-source-large.gif
2011,6035989,Fig. 1.,"Upper: Design of the BMI training and computation of RV in Group A
(N=12)
and B
(N=12)
. In Group A, RV for every incoming sample block of
Task
n
is computed as a mean based on power estimates of the preceding two rest condition intervals (
ITI
n−1
and
ITI
n−2
) and task intervals (
Task
n−1
and
Task
n−2
) (heterogeneous RV). In Group B, only power estimates of
ITI
n−1
and
ITI
n−2
are used to compute the RV for the sample blocks of
Task
n
(homogeneous RV). Lower: BMI settings for ERD detection in Group A and B are given in the box. In Group A, ERD is detected if the power value of the incoming sample block recorded during BMI training is smaller than RV (binary). In Group B, an ERD is detected if the incoming sample block value is smaller than a fixed value (e.g., 20% of homogeneous RV). Stronger ERD are translated to faster velocities of orthosis movements (graded feedback).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6036729/6035989/6035989-fig-1-source-large.gif
2011,6035989,Fig. 2.,A 275-sensor MEG was used for recording of oscillatory brain activity. An orthotic device affixed to the subject's hand and fingers delivered proprioceptive feedback. Onset and end of task was indicated by an auditory stimulus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6036729/6035989/6035989-fig-2-source-large.gif
2011,6035989,Fig. 3.,"Stability of inter- and intraday mean power values during inter-trial intervals (ITI, rest condition). Upper: Healthy subjects, error bars indicate minimum and maximum of all data. Power values are shown for each group (A, B) across sessions (S1–S5). Lower: Stroke patients,
black=groupA
;
gray=groupB
; each line represents the mean power values of intra-session ITI's on S1–S15 (STD indicated by error bar, numbers at the end of each line indicates which patients data is represented). While absolute mean power values during rest show a large inter-individual variability, average variances of intraday power estimates range between 8%–12%. This stability of power values allow fixed thresholds for ERD detection (e.g., if decrease of mean power estimates during task performance reach 20%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6036729/6035989/6035989-fig-3-source-large.gif
2011,6035989,Fig. 4.,"Mean BMI performance over sessions in Group A and B. Upper: Healthy subjects (
n=10
in each group). Lower: Stroke patients (
n=4
, each subject is shown individually, numbers at the end of each line indicate of which patient's data is shown).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6036729/6035989/6035989-fig-4-source-large.gif
2011,6035989,Fig. 5.,"Mean sensitivity index (SI) in healthy subjects (upper graph,
p<0.001
by
ANOVA
RM
, main effect for ‚group indicated by †) and stroke patients (lower graph,
p<0.05
by
ANOVA
RM
, significant post-hoc comparison between groups indicated by *). The SI reflects the separation between mean power values during rest (ITI) and the task condition according to the ERD detection threshold used in Group A, respectively Group B. A high index indicates good separation and a low false positive ERD detection rate during the training. While there is a clear difference in healthy subjects between absolute SI values of each session in both groups, stroke patients showed very similar values at the beginning of the training, but only Group B showed improvements of SI values after S8.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6036729/6035989/6035989-fig-5-source-large.gif
2011,6035989,Fig. 6.,"Mean topographical ERD maps of in S1, S3, and S5 for Group A (upper row) and B (lower row) at 11 Hz (healthy subjects). Averaging was performed based on the assumption of a similar signal source in the sensori-motor cortex. While both groups show some increase in mean ERD-strength, this increase is more pronounced in Group B with a peak over the vertex. Both groups showed an enlargement of ERD in the course of the training.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6036729/6035989/6035989-fig-6-source-large.gif
2011,5742970,Fig. 1.,"Large-margin heuristics for a three classes toy example represented in subfigure (a). The color intensity represents the distance from the hyperplane, ranging from black (on the boundary) to white (maximal distance). (b) MS heuristic. (c) MCLU heuristic; areas in black are the areas of maximal uncertainty, minimizing (7) or (9), respectively. Bottom row: absolute values of per-class distances (d)–(f).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-1-source-large.gif
2011,5742970,Fig. 2.,"Images considered in the experiments: (top) ROSIS image of the city of Pavia, Italy (bands
[56−31−6]
and corresponding ground survey); (middle) AVIRIS Indian Pines hyperspectral data (bands
[40−30−20]
and corresponding ground survey); (bottom) QuickBird multispectral image of a suburb of the city of Zurich, Switzerland (bands
[3−2−1]
and corresponding ground survey).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-2-source-large.gif
2011,5742970,Fig. 3.,"Data distribution of the three images considered. First row: ROSIS image of Pavia: (a) mean spectral profiles; (b) example of data manifold in bands 55 (Red) and 77 (Near infrared). Middle row: AVIRIS Indian Pines: (c) mean spectral profiles; (d) example of data manifold in bands 52, 102, and 208. Bottom row: Zurich QuickBird: (e) mean spectral profiles; (f) data manifold in bands 2 (G), 3 (R) and 4 (NIR).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-3-source-large.gif
2011,5742970,Fig. 4.,"Three families of heuristics trained with SVMs
(RS=Random Sampling)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-4-source-large.gif
2011,5742970,Fig. 5.,"Large margin active learning without diversity criterion. An example comparing MS and MCLU
(RS=Random Sampling)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-5-source-large.gif
2011,5742970,Fig. 6.,"Effect of diversity criteria on large margin active learning. An example comparing MCLU and MCLU-ABD
(RS=Random Sampling)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-6-source-large.gif
2011,5742970,Fig. 7.,"Committee-based and posterior probability heuristics trained with LDA classifiers on the Pavia ROSIS image
(RS=Random Sampling)
. (a)
N+5
. (b)
N+20
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5767682/5742970/5742970-fig-7-source-large.gif
2011,5503981,Fig. 1.,"Part of an image, compressed using JPEG with quality level 25%. To improve the visibility of the compression artifacts in the printed paper, we applied some additional sharpening. In area A, the compression artifacts are hardly visible as they are masked by the image content, while they are much more pronounced in areas B and C.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-1-source-large.gif
2011,5503981,Fig. 2.,Result of SSIM of the image shown in Fig. 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-2-source-large.gif
2011,5503981,Fig. 3.,"Absolute luminance difference of the image shown in Fig. 1, using JPEG quality level 25%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-3-source-large.gif
2011,5503981,Fig. 4.,Contrast Sensitivity Function [20].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-4-source-large.gif
2011,5503981,Fig. 5.,Frequency response of the BPF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-5-source-large.gif
2011,5503981,Fig. 6.,"BPF of absolute luminance difference of the image shown in Fig. 1, using JPEG quality level 25%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-6-source-large.gif
2011,5503981,Fig. 7.,Creating the reference metric.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-7-source-large.gif
2011,5503981,Fig. 8.,Results of reference metric of the image shown in Fig. 1 using JPEG quality level 25%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-8-source-large.gif
2011,5503981,Fig. 9.,Schematic depiction of a detection cascade [29].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-9-source-large.gif
2011,5503981,Fig. 10.,Schematic depiction of the trained no-reference metric for local estimation of the video compression artifact level [29].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-10-source-large.gif
2011,5503981,Fig. 11.,Framework to create a no-reference metric.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-11-source-large.gif
2011,5503981,Fig. 12.,BIM performance of proposed method on Kodak stills.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-12-source-large.gif
2011,5503981,Fig. 13.,PSNR increase of proposed method on Kodak stills.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-13-source-large.gif
2011,5503981,Fig. 14.,"Results of different compression techniques on the image shown in Fig. 1, compressed using JPEG with quality level 25%. To improve the visibility of the compression artifacts in the printed paper, we applied some additional sharpening. (A) Original compressed. (B) Kim (C) Nostratinia. (D) Decontouring. (E) Trained Filters. (F) Trained Filters and Decontouring. (G) ViCAR. (H) Proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-14-source-large.gif
2011,5503981,Fig. 15.,"Results of different compression techniques on image from Kodak [34], uncompressed. To improve the visibility of the compression artifacts in the printed paper, we applied some additional sharpening. (A) Original compressed. (B) Kim. (C) Nostratinia. (D) Decontouring. (E) Trained Filters. (F) Trained Filters and Decontouring. (G) ViCAR. (H) Proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/5730181/5503981/5503981-fig-15-source-large.gif
2011,5607316,Fig. 1.,"Overview of the proposed method. The proposed method is composed of mapping, structure expansion, and estimation steps. At the mapping step, environmentally invariant properties are obtained. The feature relevance network
Q
is constructed from
T
.
P
r
(
P
r
∈P
), the prototype of location
r
is computed from
T
L
. At the structure expansion step, a test instance and
P
r
are expanded until converging to the same structure based on
Q
.
Conv.t
r
i
denotes the
i
th converged tree. Based on the accumulated cost of the expansion, some locations are selected as a member of the neighboring group,
G
. At the estimation step, the location for the given test instance is estimated from
G
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-1-source-large.gif
2011,5607316,Fig. 2.,"Illustration of seed matching. At this step, the common APs observed in a test instance and the prototype of the ith location are processed. In the figure,
A
P
4
,
A
P
10
, and
A
P
27
compose of
S
COM
. Based on the
AValue
s, the edges with the lowest
EValue
,
E
4,27
and
E
4,10
are selected and comprise the connecting structure
S
COM
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-2-source-large.gif
2011,5607316,Fig. 3.,"Exemplary seed expansion procedure for missing APs.
E
EXP
connecting APs in a test instance and the rth prototype is constructed by adding a new AP “9”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-3-source-large.gif
2011,5607316,Fig. 4.,"RScore curves. This chart shows variations in RScores of five test instances (T1, T2,…,T5). There hardly seems a clear cut point. Here,
t
means ten prototypes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-4-source-large.gif
2011,5607316,Fig. 5.,"ψ
curves for Fig. 4. This figure shows
ψ
curves for the instances in Fig. 4. By introducing
γ
and
ψ
measures, there are fluctuations in the curves. A set of interim criteria is selected based on the fluctuations. If the denominator is 0, the corresponding
ψ
value is defined as 0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-5-source-large.gif
2011,5607316,Fig. 6.,"Generated relevance network. This figure displays the relatedness between the 99 APs. Linked APs are more likely to be adjacent to each other than other pairs. Here, weights or adjacencies are not shown. These links represent all the concurrency among the APs estimated from
T
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-6-source-large.gif
2011,5607316,Fig. 7.,"Recommended locations for a test instance whose real location is 203. For location 203, locations
112,191,115,225,203,159,63,97,1
, and 23 are recommended by our method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-7-source-large.gif
2011,5607316,Fig. 8.,"Running time for determining
ψ
value. This chart shows the running time along the number of landmark instances. The time is measured in seconds. The running environment: Intel core 2 CPU 6600 2.40 GHz and 2.39 GHz with 2.00GB RAM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5977075/5607316/5607316-fig-8-source-large.gif
2011,5374367,Fig. 1.,"A Venn diagram showing the classes of examples (i.e., MEDLINE documents or Swiss-Prot/TrEMBL protein records) in this project. Examples in
R
are relevant (in our case, to TCDB). Examples in
P
are predicted to be relevant by a classifier, i.e.,
P∩R
represents the set of examples discovered by the classifier,
P∖R
represents false positives, and
R∖P
represents false negatives.
T
is the set of examples that are already in a specialized database (TCDB).
I
represents the set of examples that are particularly interesting. These are relevant, novel, and scientifically important examples that are especially valuable, as discussed in Section 5.
F
represents a set of future examples. In our experiments, these are sampled randomly. The shaded area,
F∩P
, is the set of examples that the human expert must review and classify. Some of the relevant examples will be added to the database, represented by the set
A
.
A
equals
F∩P∩I
plus part of
F∩P∩R
. Not all of
F∩P∩R
will necessarily be added to TCDB because we do not add new examples to TCDB if they correspond to proteins that are homologous to ones already in the database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5730188/5374367/5374367-fig-1-source-large.gif
2011,5374367,Fig. 2.,"TCDB update process workflow. Documents of all labels are used to train the classifier. Positive examples are obtained from TCDB, negative examples are articles previously rejected by the human expert. Unlabeled articles are ranked according to the likelihood of relevance by the classifier and then deployed to the human expert, who labels the documents and incorporates the relevant ones into TCDB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5730188/5374367/5374367-fig-2-source-large.gif
2011,5559411,Fig. 1.,"Aerial photographs of study areas. (a) Scene one: Richmond, CA. (b) Scene two: El Cerrito, CA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5695081/5559411/5559411-fig-1-source-large.gif
2011,5559411,Fig. 2.,Prediction maps of each land type (Scene one). (a)–(d) Urban. (e)–(h) Tree. (i)–(l) Grass. (m)–(p) Water. (q)–(t) Soil. White: positive; black: negative.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5695081/5559411/5559411-fig-2-source-large.gif
2011,5559411,Fig. 3.,Comparison of kappa coefficient obtained by different classifiers (Scene one). (a) Urban. (b) Tree. (c) Grass. (d) Water. (e) Soil.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5695081/5559411/5559411-fig-3-source-large.gif
2011,5559411,Fig. 4.,Prediction maps of each land type (Scene two). (a)–(d) Urban. (e)–(h) Tree. (i)–(l) Grass. (m)–(p) Soil. White: positive; black: negative.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5695081/5559411/5559411-fig-4-source-large.gif
2011,5559411,Fig. 5.,Comparison of kappa coefficient obtained by different classifiers (Scene two). (a) Urban. (b) Tree. (c) Grass. (d) Soil.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5695081/5559411/5559411-fig-5-source-large.gif
2011,5982410,Fig. 1.,Instant images of the AR Face dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6032791/5982410/5982410-fig-1-source-large.gif
2011,5667062,Fig. 1.,Client–server scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-1-source-large.gif
2011,5667062,Fig. 2.,Music recommendation experiment: example of artist tagging.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-2-source-large.gif
2011,5667062,Fig. 3.,"Music recommendation experiment: average
TOP20HITS
and
rootmeansquarederror(RMSE)
against
α
and
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-3-source-large.gif
2011,5667062,Fig. 4.,"Music recommendation experiment: distribution of
hits20
j
in correspondence with
α
∗
and
λ
∗
achieving optimal
RMSE
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-4-source-large.gif
2011,5667062,Fig. 5.,Music recommendation experiment: true and estimated Top20 for the “average user.”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-5-source-large.gif
2011,5667062,Fig. 6.,Music recommendation experiment: true and estimated Top20 for three representative virtual users.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-6-source-large.gif
2011,5667062,Fig. 7.,Music recommendation experiment: training time versus number of users.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-7-source-large.gif
2011,5667062,Fig. 8.,"Pharmacological experiment: training data for 494 patients. Only a subset of 44 patients has been fully sampled, while for the others only measurements taken before the 4th week are available.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-8-source-large.gif
2011,5667062,Fig. 9.,"Pharmacological experiment: RMSE on the test data. The thick lines are averages over 50 randomly extracted training subsets, using a separate single-task method (dash-dotted thick line), a pooled single-task method (dashed line), and the multitask learning method (continuous thick line) with
α=0.5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-9-source-large.gif
2011,5667062,Fig. 10.,"Pharmacological experiment: average
RMSE
against
α
and
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5710128/5667062/5667062-fig-10-source-large.gif
2011,5672601,Fig. 1.,Framework of the model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-1-source-large.gif
2011,5672601,Fig. 2.,Two driving cycles of a highway.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-2-source-large.gif
2011,5672601,Fig. 3.,Two driving cycles of a country road.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-3-source-large.gif
2011,5672601,Fig. 4.,Two driving cycles of an urban road (congested).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-4-source-large.gif
2011,5672601,Fig. 5.,Two driving cycles of an urban road (flowing).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-5-source-large.gif
2011,5672601,Fig. 6.,"Histograms on
v
mean
under the four driving conditions. (a) Highway. (b) Country road. (c) Urban road (congested). (d) Urban road (flowing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-6-source-large.gif
2011,5672601,Fig. 7.,"Histograms on
a
max
under the four driving conditions. (a) Highway. (b) Country road. (c) Urban road (congested). (d) Urban road (flowing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-7-source-large.gif
2011,5672601,Fig. 8.,"Histograms on
a
min
under the four driving conditions. (a) Highway. (b) Country road. (c) Urban road (congested). (d) Urban road (flowing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-8-source-large.gif
2011,5672601,Fig. 9.,"Histograms on
I
under the four driving conditions. (a) Highway. (b) Country road. (c) Urban road (congested). (d) Urban road (flowing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-9-source-large.gif
2011,5672601,Fig. 10.,"Boxplots on four features, where the numbers on the horizontal axis indicate the four different driving conditions. (a)
v
mean
. (b)
a
max
. (c)
a
min
. (d)
I
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-10-source-large.gif
2011,5672601,Fig. 11.,Comparison of test errors by different feature extraction methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-11-source-large.gif
2011,5672601,Fig. 12.,Comparison of test errors under different feature numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-12-source-large.gif
2011,5672601,Fig. 13.,Comparison of the test errors of classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-13-source-large.gif
2011,5672601,Fig. 14.,ROC curves of classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-14-source-large.gif
2011,5672601,Fig. 15.,Comparison of the time costs of classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-15-source-large.gif
2011,5672601,Fig. 16.,Result of the prediction experiment. M1 indicates the MSE obtained by M1. M2 indicates the MSE obtained by M2. Class 1 to Class 4 indicate the MSE obtained from the samples under the four driving conditions of M2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-16-source-large.gif
2011,5672601,Fig. 17.,Fitting result of the ARMA model. (Red line) Original data. (Black dotted line) Fitting curve obtained by M1. (Blue dashed line) Fitting curve obtained by M2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/5779921/5672601/5672601-fig-17-source-large.gif
2011,6035789,Fig. 1.,"Logarithmic plots of the elapsed times (in seconds) spent by DPPs (circles), P-Delta (squares), G-SVM (asterisks), and L-SVM (diamonds). Upper left panel: time spent by the four approaches on each two-class dataset (ordered by increased times of DPPs). Upper right panel: the same plot for multiclass datasets. Lower left panel: times divided by the number of patterns against the number of inputs for two-class datasets. Lower right panel: times divided by the number of patterns and classes against the number of inputs for multiclass datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6062577/6035789/6035789-fig-1-source-large.gif
2011,6035789,Fig. 2.,Accuracy achieved by DPPs (circles) and the best classifier (squares) plotted against the number of patterns for two-class (left panel) and multiclass datasets (right panel).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6062577/6035789/6035789-fig-2-source-large.gif
2011,6035789,Fig. 3.,"Results of P-Delta and DPPs on the two-spirals-apart dataset. Upper left panel: classification borders for P-Delta with
n=75
perceptrons (200 patterns, 90% for training and 10% for testing). Upper right panel: the same borders for DPPs using the same experimental setting. Lower left panel: Classification accuracy of P-Delta against the number
n
of perceptrons. Lower right panel: borders learnt by only one direct perceptron [with weights calculated using (4)] with Gaussian kernel using spread
σ=0.0625
, achieving 95.8% accuracy on the same dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6062577/6035789/6035789-fig-3-source-large.gif
2011,5613152,Fig. 1.,Comparison of loss functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5762410/5613152/5613152-fig-1-source-large.gif
2011,5613152,Fig. 2.,"Nonlinear scalar equation for
ν
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5762410/5613152/5613152-fig-2-source-large.gif
2011,5613152,Fig. 3.,Regularization path in rMFoM learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5762410/5613152/5613152-fig-3-source-large.gif
2011,5613152,Fig. 4.,"Class conditional probability density of
w
T
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5762410/5613152/5613152-fig-4-source-large.gif
2011,5613152,Fig. 5.,Test error rate of semi-supervised rMFoM learning on two datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5762410/5613152/5613152-fig-5-source-large.gif
2011,5613152,Fig. 6.,"Class conditional probability density of
w
T
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5762410/5613152/5613152-fig-6-source-large.gif
2011,5613180,Fig. 1.,Linear compressor and its vapor compression cycle. (a) Schematic of a linear compressor. (b) P–D curve of one vapor compression cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-1-source-large.gif
2011,5613180,Fig. 2.,Current–stroke characteristics of linear compressor under current source excitation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-2-source-large.gif
2011,5613180,Fig. 3.,Circuit diagram of a linear compressor drive system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-3-source-large.gif
2011,5613180,Fig. 4.,"Measured results of
Le
and
Ke
of the designed linear PM motor. (a) Incremental inductance. (b) Back-EMF coefficient.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-4-source-large.gif
2011,5613180,Fig. 5.,Block diagram of the proposed LFF current controller.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-5-source-large.gif
2011,5613180,Fig. 6.,BSNN basic functions used in current controller.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-6-source-large.gif
2011,5613180,Fig. 7.,Steady-state performance of using the PI current controller only. (a) Current waveforms and control error. (b) Piston displacement and compressor pressure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-7-source-large.gif
2011,5613180,Fig. 8.,Simulation results of the LFF current controller. (a) BSNN and PI controller output. (b) Piston displacement and compressor pressure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-8-source-large.gif
2011,5613180,Fig. 9.,Steady-state performance of the proposed LFF current controller. (a) Current waveforms and control error. (b) Piston displacement and compressor chamber pressure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-9-source-large.gif
2011,5613180,Fig. 10.,Transient performance of LFF current controller. (a) Current waveforms and control error. (b) Outputs of BSNN and PI controller. (c) Piston displacement and compressor chamber pressure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-10-source-large.gif
2011,5613180,Fig. 11.,Prototype linear compressor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-11-source-large.gif
2011,5613180,Fig. 12.,Schematic of the experimental setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-12-source-large.gif
2011,5613180,Fig. 13.,Experimental results when operating as an air compressor. (a) Measured current and piston displacement. (b) Current and control error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-13-source-large.gif
2011,5613180,Fig. 14.,"Measured motor current and piston displacement waveforms when tested at
−25
 ∘
C/32
 ∘
C/55
 ∘
C
operating condition. (a) RMS current of 0.492 A, piston amplitude of 9.96 mm. (b) RMS current of 0.578 A, piston amplitude of 10.31 mm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5947747/5613180/5613180-fig-14-source-large.gif
2011,5639086,Fig. 1.,"(a) True posterior probability and its estimate using an SVM with 50 training samples and Platt's method. (b) Optimal decision boundary together with the
SVM
2
and
ESVM
2
classification functions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5639086/5639086-fig-1-source-large.gif
2011,5639086,Fig. 2.,"Error rate for the toy example as a function of
K
for (a) 40 and (b) 100 training samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5639086/5639086-fig-2-source-large.gif
2011,5639086,Fig. 3.,Probability of error as we artificially increase the extended input space training set for six representative databases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5639086/5639086-fig-3-source-large.gif
2011,5624639,Fig. 1.,"Sliding Gaussians dataset at (a)
t=25
, (b)
t=175
, and (c)
t=475
time units. In each figure, the last 25 generated points are filled.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-1-source-large.gif
2011,5624639,Fig. 2.,Sequence of hyperplanes obtained as a solution of the sliding Gaussians dataset with (a) TA-SVM and (b) SW-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-2-source-large.gif
2011,5624639,Fig. 3.,"TA-SVM solutions for the rotating hyperplane problem using different
γ
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-3-source-large.gif
2011,5624639,Fig. 4.,"Test errors for the rotating hyperplane problem as a function of
γ
. (a) Results using
d=2
and different values of
m
. (b) Same as before, but for a noisy dataset with 10% flipped labels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-4-source-large.gif
2011,5624639,Fig. 5.,Results for the STAGGER dataset. (a) Behavior of independent SVMs using overlapping sliding time windows. (b) Direct comparison of optimal settings of both methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-5-source-large.gif
2011,5624639,Fig. 6.,Prediction test errors as a function of the dataset dimension for the rotating hyperplane problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-6-source-large.gif
2011,5624639,Fig. 7.,"Prediction test errors as a function of the dataset dimension for the rotating Gaussians problem (a) for a dataset including a full turn of the Gaussians and (b) for the same problem, but including two full turns of the classes, i.e., a faster drift.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-7-source-large.gif
2011,5624639,Fig. 8.,"Average prediction accuracy of the methods tested in this paper on the STAGGER dataset, as a function on time. Error bars show the 95% confidence interval.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-8-source-large.gif
2011,5624639,Fig. 9.,"Estimation test errors as a function of the number
m
of classifiers in the sequence (a) for the rotating hyperplane problem, (b) for the same problem with 10% noise, (c) for the sliding Gaussians problem, and (d) for the same problem using normal distributions with bigger
σ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-9-source-large.gif
2011,5624639,Fig. 10.,"Extrapolation test errors as a function of the number
m
of classifiers in the sequence (a) for the sliding Gaussians problem and (b) for the same problem using distributions with bigger
σ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-10-source-large.gif
2011,5624639,Fig. 11.,"Extrapolation test errors as a function of the number of predicted steps into the future (a) for the sliding Gaussians problem and (b) for the same problem using distributions with bigger
σ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5680555/5624639/5624639-fig-11-source-large.gif
2011,5723040,Fig. 1.,ROC graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-1-source-large.gif
2011,5723040,Fig. 2.,Cost lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-2-source-large.gif
2011,5723040,Fig. 3.,ROC curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-3-source-large.gif
2011,5723040,Fig. 4.,Cost curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-4-source-large.gif
2011,5723040,Fig. 5.,Precision-recall curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-5-source-large.gif
2011,5723040,Fig. 6.,A lift graph. (a) Prevalence of positives at 5 percent. (b) Prevalence of positives at 50 percent.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-6-source-large.gif
2011,5723040,Fig. 7.,A ROI graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-7-source-large.gif
2011,5723040,Fig. 8.,Reliability diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-8-source-large.gif
2011,5723040,Fig. 9.,Attributes diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-9-source-large.gif
2011,5723040,Fig. 10.,Discrimination diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-10-source-large.gif
2011,5723040,Fig. 11.,ROC graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-11-source-large.gif
2011,5723040,Fig. 12.,Cost lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-12-source-large.gif
2011,5723040,Fig. 13.,ROC curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-13-source-large.gif
2011,5723040,Fig. 14.,Cost lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-14-source-large.gif
2011,5723040,Fig. 15.,Precision-recall graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-15-source-large.gif
2011,5723040,Fig. 16.,Lift graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-16-source-large.gif
2011,5723040,Fig. 17.,Return of investment graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-17-source-large.gif
2011,5723040,Fig. 18.,Reliability diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-18-source-large.gif
2011,5723040,Fig. 19.,Attributes diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-19-source-large.gif
2011,5723040,Fig. 20.,Discrimination graph. (a) Naïve Bayes. (b) Bayesian Network. (c) NBTree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6026105/5723040/5723040-fig-20-source-large.gif
2011,5910412,Fig. 1.,"Classification error rate of CoTrade (on the course dataset) changes as the number of nearest neighbors used (i.e.,
k
) increases. Each subfigure reports the experimental results with different classifier inducers, from left to right: Naïve Bayes, Cart, and Libsvm. (a) Naïve B ayes. (b) Cart. (c) L ibsvm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-1-source-large.gif
2011,5910412,Fig. 2.,"Classification error rate of each comparing algorithm changes as the number of labeled training examples increases, where Naïve Bayes is utilized as the classifier inducer. (a) course. (b) ads12. (c) ads13. (d) ads23. (e) NG1. (f) NG2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-2-source-large.gif
2011,5910412,Fig. 3.,"Classification error rate of each comparing algorithm changes as the number of labeled training examples increases, where Cart is utilized as the classifier inducer. (a) course. (b) ads12. (c) ads13. (d) ads23. (e) NG1. (f) NG2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-3-source-large.gif
2011,5910412,Fig. 4.,"Classification error rate of each comparing algorithm changes as the number of labeled training examples increases, where Libsvm is utilized as the classifier inducer. (a) course. (b) ads12. (c) ads13. (d) ads23. (e) NG1. (f) NG2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-4-source-large.gif
2011,5910412,Fig. 5.,Classification error rate of CoTrade with different distance measures (on the course and ads12 datasets) changes as the number of labeled training examples increases. (a) Naïve Bayes (course). (b) Cart (course). (c) Libsvm (course). (d) Naïve B ayes (ads12). (e) C art (ads12). (f) L ibsvm (ads12).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-5-source-large.gif
2011,5910412,Fig. 6.,"Estimated confidence of CoTrade (on the course dataset) for unlabeled examples which are correctly predicted (drawn in blue and denoted as
C
) and wrongly predicted (drawn in brown and denoted as
W
). (a) N aïve Bayes. (b) C art. (c) Libsvm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-6-source-large.gif
2011,5910412,Fig. 7.,Scatter plots between CoTrade and StdCoTrain in terms of bias (first row) and variance (second row). (a) N aïve Bayes (bias). (b) C art (bias). (c) Libsvm (bias). (d) Naïve Bayes (variance). (e) Cart (variance). (f) L ibsvm (variance).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6006807/5910412/5910412-fig-7-source-large.gif
2011,5719296,Fig. 1.,"WT histogram and discretization thresholds derived using equal-frequency binning, manual selection, and Lloyd-Max quantization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-1-source-large.gif
2011,5719296,Fig. 2.,Methodology of feature selection and classification developed in the research.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-2-source-large.gif
2011,5719296,Fig. 3.,NBC represented as a Bayesian network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-3-source-large.gif
2011,5719296,Fig. 4.,Means and 95% confidence intervals of CMI scores for features selected during the first 50 iterations of the CMIM algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-4-source-large.gif
2011,5719296,Fig. 5.,Methodology of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-5-source-large.gif
2011,5719296,Fig. 6.,"SNBC classification accuracy (mean
+/−
std.) for increasing numbers of features, when applying equal-frequency binning of WT and entropy-based discretization of WT factors for a fab manufacturing both products.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-6-source-large.gif
2011,5719296,Fig. 7.,SNBC mean classification accuracy for increasing numbers of features and a fab manufacturing both products. The graphs exhibit different combinations of discretization methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-7-source-large.gif
2011,5719296,Fig. 8.,Box plot for classification accuracies of the four models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-8-source-large.gif
2011,5719296,Fig. 9.,WT distribution for increasing tool availability levels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/5762426/5719296/5719296-fig-9-source-large.gif
2011,5762616,Fig. 1.,Kernel map compression (KMC). The red arrow shows the proposed approach to approximate the relationship between the input space and the feature subspace directly.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-1-source-large.gif
2011,5762616,Fig. 2.,"Sinc curve synthetic experiment for SVM regression. (a) Synthetic dataset for SV regression. In this experiment the blue data points are used as training data to learn the
sinc
curve shown in red. (b) ° represents the SVs. Blue curve is the
sinc
function learned by the standard SVM. (c) °represents the GRBFs used to approximate the SVM. Blue curve is the
sinc
function learned by the proposed KMC procedure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-2-source-large.gif
2011,5762616,Fig. 3.,"Performance comparison for the synthetic 2-D example for KPCA. (a) Average error in projecting 400 points onto six eigenvectors, each one approximated using
p
points. Solid line: KMC, dashed line: Burges. Lower is better. (b) Reconstruction error versus the number of points the space is reduced. Solid line: KMC, dashed line: Burges. Lower is better.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-3-source-large.gif
2011,5762616,Fig. 4.,"Performance comparison for the synthetic 2-D example for KPCA. (a) Dataset and reconstructed dataset using six eigenvectors, each approximated with six points. (b) Dataset and reconstructed dataset using six eigenvectors, each approximated with eight points. (c) Dataset and reconstructed dataset using six eigenvectors, each approximated with 12 points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-4-source-large.gif
2011,5762616,Fig. 5.,"Samples for testing reduced SVM. (a) Sample 16-D descriptor vectors from the character recognition database. The dataset consists of 20000 instances. (b) Samples from USPS handwritten digits database, composed of 9298 images of dimensions 16×16.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-5-source-large.gif
2011,5762616,Fig. 6.,Character recognition: % degradation versus compression curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-6-source-large.gif
2011,5762616,Fig. 7.,Character recognition: Performance comparison for the case of reducing the space to 20% of the original space. (a) Number of errors for each classifier. (b) Runtimes for each classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-7-source-large.gif
2011,5762616,Fig. 8.,"Performance comparison for the USPS dataset. (a) % degradation versus compression curves. (b) Runtimes for each classifier in the case of
p=20
. Blue: Burges, Red: KMC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-8-source-large.gif
2011,5762616,Fig. 9.,Samples for testing reduce KPCA. (a) Samples from sign language database. The database consists of 2040 images. (b) Samples from ORL face database. The database consists of 400 images of faces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5779942/5762616/5762616-fig-9-source-large.gif
2011,6011691,Fig. 1.,Adaptive memetic architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/6056501/6011691/6011691-fig-1-source-large.gif
2011,6011691,Fig. 2.,Adaptive MA architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/6056501/6011691/6011691-fig-2-source-large.gif
2011,6011691,Fig. 3.,Data mining phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/6056501/6011691/6011691-fig-3-source-large.gif
2011,6011691,Fig. 4.,Example trees.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/6056501/6011691/6011691-fig-4-source-large.gif
2011,6011691,Fig. 5.,Evolutions of the different strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/6056501/6011691/6011691-fig-5-source-large.gif
2011,5432213,Fig. 1.,"Cross-view stability of trajectory-based self-similarity matrices on a simple example. (a) and (c) demonstrate, based on motion capture (MOCAP) data, a golf swing action seen from two different views. (b) and (d) represent their respective SMMs for the trajectory of one hand projected in corresponding view. Even though the two views are different, the structures or the patterns of the computed SSMs are very similar.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-1-source-large.gif
2011,5432213,Fig. 2.,"Relationship between proposed SSM representation and dynamic instances introduced in [26]. Two actors perform the action of opening a cabinet door, where the hand trajectory is shown in (b) and (d). The SSMs computed for these two actions based only on one hand trajectory are shown in (c) and (e), respectively. The “dynamic instances” (as proposed by [26]), marked in red stars in (b) and (d), represent valleys in the corresponding SSM, depicted by magenta circles in (c) and (e), respectively. The spread of each valley depends on the peak-width of the corresponding dynamic instance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-2-source-large.gif
2011,5432213,Fig. 3.,Earlier example of SSM for motion periodicity analysis. (a)-(d) are frames from a sequence of a walking person [38]. (e) represents the SSM obtained for this sequence by [38] using the absolute correlation score between frames of the sequence. Time-Frequency analysis is performed on this matrix to detect periodicity in a motion sequence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-3-source-large.gif
2011,5432213,Fig. 4.,"Examples of SSMs for different types of features and for different actions. (a) Examples from the CMU mocap data set. Columns 1 and 5 represent two actors, while columns 2 and 4 represent corresponding SSM-pos computed with 13 projected point trajectories, respectively. Different rows represent different actions and viewing angles. Note the stability of SSMs over different views and people performing the same action. (b) Examples from the Weizman video data set [7]. Row 1: four bending actions along with manually extracted point trajectories used for computing SSM-pos; rows 2, 3, and 4 represent SSM-pos, SSM-hog, and SSM-of, respectively, for these four bending actions. Note the similarity columnwise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-4-source-large.gif
2011,5432213,Fig. 5.,"Stability of SSM-pos structures across viewpoints for the mocap data sequence. (a) Synthetic cameras around a person performing an action. SSMs are generated for each of these synthetic cameras and, for each of these computed SSMs, a gradient angle is computed at each matrix point. From these orientations, circular standard deviations are computed for (b) a golf swing, (c) a kick, and (d) a jumping jack action sequence (code provided by [27] for (a)).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-5-source-large.gif
2011,5432213,Fig. 6.,"Local descriptors for SSM: Each individual descriptor is centered at a diagonal point
i∈{1⋯T}
of the SSM and has a log-polar block structure. Histograms
h
a
i
of eight gradient directions are computed separately for each of the 11 blocks of the analysis support and are concatenated into a descriptor vector
h
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-6-source-large.gif
2011,5432213,Fig. 7.,"Temporal alignment of same action performances in videos from different viewpoints and with synthetic desynchronization. (a) Two desynchronized sequences with the side and the top views of the same action are represented with a set of matching key-frames. The second sequence has been time warped according to
t
′
=acos(bt)
transformation. (b) Distance matrix between sequences
H(
I
1
)
and
H(
I
2
)
of SSM-pos descriptors (bright colors represent large distance values). Dynamic Programming (red curve) finds the minimum cost monotonic path from
(0,0)
to
(T,T)
in this matrix. This path coincides almost perfectly with the original warping (blue curve), despite drastic view variations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-7-source-large.gif
2011,5432213,Fig. 8.,"Temporal alignment of video sequences representing different performances of actions throwing a ball, drinking, and smoking. Left: Pairs of aligned video sequences are illustrated with a few frames and the links between corresponding frames estimated by our algorithm. Right: Distance matrices between sequential descriptors of both videos used as input for aligning video sequences by Dynamic Programming. The estimated temporal alignment is illustrated by red curves. The successful alignment achieved by our method on these sequences is confirmed when comparing red curves with yellow dots illustrating sparse manual alignment for a few key frames of videos (best viewed in color).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-8-source-large.gif
2011,5432213,Fig. 9.,SSM-based cross-view action recognition on the CMU mocap data. (a) A person figure animated from the motion capture data and six virtual cameras used to simulate projections in our experiments. (b) Accuracy of the cross-view action recognition using SSM-pos-vel-acc descriptors to build the bag-of-features used by nearest-neighbor classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-9-source-large.gif
2011,5432213,Fig. 10.,SSM-based action recognition on the Weizman single-view action data set [7]. (Top) Example frames for nine classes of actions. (Bottom) Confusion matrices corresponding to NNC action recognition using (a) image-based self-similarities SSM-of and (b) trajectory-based self-similarities SSM-pos.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-10-source-large.gif
2011,5432213,Fig. 11.,"Example frames from the IXMAS multiview action data set: For four classes of action, the five views at a given instant of one performance of the action are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-11-source-large.gif
2011,5432213,Fig. 12.,"Comparative action recognition results for the IXMAS multiview action data set: Results are averaged over 11 action classes and 10 subjects. Results in (a)-(c) are shown for different types of SSMs and the same bag-of-features SVM classification method. Results in (d) are obtained with the same bag-of-features SVM approach, but using quantized descriptors of spatiotemporal interest points (STIPs) instead of quantized local SSM descriptors. Recognition scores are illustrated for different combinations of training and test cameras. (a) Recognition results for SSM-hog-of Multiscale Features. (b) Recognition results for SSM-of multiscale features. (c) Recognition results for SSM-hog multiscale features. (d) Recognition results for STIP-hog-hof multiscale features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-12-source-large.gif
2011,5432213,Fig. 13.,Class-confusion matrix for action recognition in the IXMAS data set: This confusion matrix is obtained using SSM-hog-of multiscale SSM local descriptors. It corresponds to the average confusion computed for all cross-camera recognition setups in Fig. 12a.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5639101/5432213/5432213-fig-13-source-large.gif
2011,5782993,Fig. 1.,Architecture of CSRAN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-1-source-large.gif
2011,5782993,Fig. 2.,Schematic diagram of CSRAN and its self-regulating learning scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-2-source-large.gif
2011,5782993,Fig. 3.,Error regions for the various self-regulating thresholds of the CSRAN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-3-source-large.gif
2011,5782993,Fig. 4.,Sample magnitude error and self-regulating magnitude growing and update thresholds history for SCFAP-I.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-4-source-large.gif
2011,5782993,Fig. 5.,Sample absolute phase error and self-regulating phase growing and update thresholds history for SCFAP-I.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-5-source-large.gif
2011,5782993,Fig. 6.,"Snapshot of magnitude error, absolute phase error, and self-regulating magnitude/phase update thresholds history between 50 and 100 samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-6-source-large.gif
2011,5782993,Fig. 7.,"Snapshot of magnitude error, phase error, and CSRAN magnitude/phase delete thresholds between 1100 and 1120 samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-7-source-large.gif
2011,5782993,Fig. 8.,Neuron history for SCFAP-I.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-8-source-large.gif
2011,5782993,Fig. 9.,CVNN equalization scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-9-source-large.gif
2011,5782993,Fig. 10.,Effect of number of training samples and SNR on the equalization performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-10-source-large.gif
2011,5782993,Fig. 11.,Error probability curve for various complex-valued neural equalizers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5937482/5782993/5782993-fig-11-source-large.gif
2011,5756252,Fig. 1.,Output of piezoelectric transducer depends on the type of materials present on its front and back side.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/5784192/5756252/5756252-fig-1-source-large.gif
2011,5669252,Fig. 1.,The iWrite information structure diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/5729267/5669252/5669252-fig-1-source-large.gif
2011,5669252,Fig. 2.,The iWrite architecture diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/5729267/5669252/5669252-fig-2-source-large.gif
2011,5669252,Fig. 3.,"A screenshot of the Assignment Manager: the student UI displays the writing and reviewing tasks, while the academic UI also displays the instructor panels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/5729267/5669252/5669252-fig-3-source-large.gif
2011,5669252,Fig. 4.,The Topics feedback tool in Glosser. The trigger questions are displayed at the top of the page and the “gloss” below.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/5729267/5669252/5669252-fig-4-source-large.gif
2011,5669252,Fig. 5.,A plot comparing the average number of revisions written per student per day for an individual (ENGG1803) and a collaborative (ELEC3610) assignment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/5729267/5669252/5669252-fig-5-source-large.gif
2011,5672586,Fig. 1.,The game environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-1-source-large.gif
2011,5672586,Fig. 2.,"The layout of (a) the arena map to the left, (b) the maze map in the middle, and (c) the combat map to the right. Yellow represents the bots spawn positions; red, green, and blue represent item spawn points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-2-source-large.gif
2011,5672586,Fig. 3.,Frequency of rewards for trial 6 during the 5000 timesteps of the learning phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-3-source-large.gif
2011,5672586,Fig. 4.,Graph of number of collisions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-4-source-large.gif
2011,5672586,Fig. 5.,Pareto front for items collected versus distance traveled.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-5-source-large.gif
2011,5672586,Fig. 6.,"Recorded paths of trial 1 (116) (top left), trial 8 (109) (top right), trial 1 (106), and trial 10 (bottom).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-6-source-large.gif
2011,5672586,Fig. 7.,Frequency of rewards for combat task trial 6 during the 5000 timesteps of the learning phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-7-source-large.gif
2011,5672586,Fig. 8.,Policy visualization from trial 6 of the combat task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-8-source-large.gif
2011,5672586,Fig. 9.,Graph of shooting accuracies in the combat task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-9-source-large.gif
2011,5672586,Fig. 10.,Pareto front for deaths versus kills.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-10-source-large.gif
2011,5672586,Fig. 11.,HierarchicalRL bot paths from replay of the arena map experiment with random seed 102.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-11-source-large.gif
2011,5672586,Fig. 12.,RuleBasedRL bot paths from the replay of the arena map experiment with random seed 120.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-12-source-large.gif
2011,5672586,Fig. 13.,RL bot paths from the replay of the arena map experiment with random seed 101.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-13-source-large.gif
2011,5672586,Fig. 14.,HierarchicalRL bot paths from replay of the maze map experiment with random seed 112.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-14-source-large.gif
2011,5672586,Fig. 15.,RuleBasedRL bot paths from the replay of the maze map experiment with random seed 120.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-15-source-large.gif
2011,5672586,Fig. 16.,RL bot paths from the replay of the maze map experiment with random seed 101.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/5730536/5672586/5672586-fig-16-source-large.gif
2011,6157411,Fig. 1.,The top solid curve represents the fluctuation of blood glucose concentration in humans during the course of a day with three meals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-1-source-large.gif
2011,6157411,Fig. 2.,"The state transition diagram of the HMM. Shorthand notation is used for the transition probabilities labeling the arrows between states:
P(
j
t
|
i
t−1
)
, means
P(
X
t
=j|
X
t−1
=i)
; e.g.,
P(
M
t
|
F
t−1
)=P(
X
t
=M|
X
t−1
=F)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-2-source-large.gif
2011,6157411,Fig. 3.,"Histograms of log probability distributions of 30 days of normal glucose readings and of readings containing anmalies, with an HMM that uses the measured glucose levels directly as its observation symbols.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-3-source-large.gif
2011,6157411,Fig. 4.,Histogram of log probability distribution of 100 days of normal glucose readings versus glucose readings containing anomalies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-4-source-large.gif
2011,6157411,Fig. 5.,Histogram of log probability distribution of 30 days of normal training data versus anomalous data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-5-source-large.gif
2011,6157411,Fig. 6.,"Histogram of log probability distribution of normal (training) data, normal but with a random shift (between −2 and +2 hours) in the schedule, and anomalous glucose readings.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-6-source-large.gif
2011,6157411,Fig. 7.,"Data points over 24-hour daily period (48 time intervals) from 30 days of normal training data, along with upper and lower thresholds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-7-source-large.gif
2011,6157411,Fig. 8.,"Recall for our HMM classifier and for using thresholds are plotted over various numbers of observation sequences (each sequence is a day worth of readings). Two data sets used here: Readings with anomalies, and normal data with random shifts (between −2 and +2 hours).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5449605/6157403/6157411/6157411-fig-8-source-large.gif
2011,5416306,Fig. 1.,Block diagram of the wind-generator control scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-1-source-large.gif
2011,5416306,Fig. 2.,Wind-turbine torque versus speed characteristics for different wind speeds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-2-source-large.gif
2011,5416306,Fig. 3.,Block diagram of the GNG-based MPPT algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-3-source-large.gif
2011,5416306,Fig. 4.,Generator torque versus wind and machine-speed surfaces. (Red) GNG neurons and (green) links among neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-4-source-large.gif
2011,5416306,Fig. 5.,Wind-speed estimation with different numbers of neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-5-source-large.gif
2011,5416306,Fig. 6.,Block diagram of the TLS-based adaptive speed observer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-6-source-large.gif
2011,5416306,Fig. 7.,Photograph of the test setup (grid-side inverter + inteconnecting inductance).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-7-source-large.gif
2011,5416306,Fig. 8.,Photograph of the test setup (induction generator + turbine emulator).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-8-source-large.gif
2011,5416306,Fig. 9.,Electric scheme of the proposed test setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-9-source-large.gif
2011,5416306,Fig. 10.,Real and estimated wind speeds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-10-source-large.gif
2011,5416306,Fig. 11.,Estimated and filtered estimated wind speed (zoom of Fig. 10).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-11-source-large.gif
2011,5416306,Fig. 12.,"Wind and machine speeds with the limitation of the rate of change of the wind speed to 1000
m/s
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-12-source-large.gif
2011,5416306,Fig. 13.,Reference and measured machine speeds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-13-source-large.gif
2011,5416306,Fig. 14.,"Grid-side
i
sd
,
i
sq
reference and measured currents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-14-source-large.gif
2011,5416306,Fig. 15.,"Active
(P)
and reactive
(Q)
powers flowing to the power grid.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-15-source-large.gif
2011,5416306,Fig. 16.,"Machine-side
i
sx
,
i
sy
reference and measured currents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-16-source-large.gif
2011,5416306,Fig. 17.,Torque versus speed locus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-17-source-large.gif
2011,5416306,Fig. 18.,Measured and estimated wind speed on a real profile.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-18-source-large.gif
2011,5416306,Fig. 19.,Machine speed and active power and energy with GNG and P&O MPPTs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/5662271/5416306/5416306-fig-19-source-large.gif
2011,5545403,Fig. 1.,"Space of four functions
f
a
(x)
,
f
b
(x)
,
f
c
(x)
, and
f
d
(x)
together with two data points (represented with +).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5710472/5545403/5545403-fig-1-source-large.gif
2011,5545403,Fig. 2.,"Utility function elicited for a normal-hearing subject “nh1” (top) and hearing-impaired subject “hi12” (bottom). Left: hyperplane formed by linear regression of
CSII
Mid
in terms of
CSII
Low
and
CSII
High
features. Right: contour plot with sound samples distorted with noise (circle), peak clipping (triangle), and center clipping (cross). The noise levels of the sound samples are lowest in the upper-right corner and increase when moving into the direction of the lower-left corner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5710472/5545403/5545403-fig-2-source-large.gif
2011,5545403,Fig. 3.,"Comparison within the peak clipping degradation for ten subjects. The solid line is calculated from the data whereas the dashed line is the model prediction using a Gaussian process with a nonlinear Gaussian kernel (by summing up predictive probabilities). The
y
-axis represents the fraction (percentage) of times that a particular distortion level is preferred over all other distortion levels. The
x
-axis denotes the clipping threshold used in terms of the percentage of the cumulative level histogram of each sentence. The effect of peak clipping is reduced as the clipping threshold is increased. With respect to improved performance in preference prediction when comparing a linear and nonlinear model, the top row shows the least significant results and the bottom row shows the most significant results (cf. Table I).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/5710472/5545403/5545403-fig-3-source-large.gif
2011,5659475,Fig. 1.,Transformation of the original feature space into the 1-D classifier output space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-1-source-large.gif
2011,5659475,Fig. 2.,Toy data set: The points represented with circles denote the initial training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-2-source-large.gif
2011,5659475,Fig. 3.,"Toy example showing a linear classification problem with three classes. The samples represented with circles denote the training samples selected by the (a) proposed, (b) RS, (c) MS, (d) MS-cSV, and (e) EQB methods after the (upper part of the figure) first and (lower part of the figure) fourth iterations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-3-source-large.gif
2011,5659475,Fig. 4.,"Average classification accuracies over 20 runs provided by the proposed, RS, MS, MS-cSV, and EQB methods for the Paneveggio data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-4-source-large.gif
2011,5659475,Fig. 5.,"Average classification accuracies over 20 runs provided by the proposed, RS, MS, MS-cSV, and EQB methods for the Pavia data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-5-source-large.gif
2011,5659475,Fig. 6.,"Average classification accuracies provided by the proposed, RS, MS, MS-cSV, and EQB methods for the (a) Paneveggio and (b) Pavia data sets by starting with biased labeled samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-6-source-large.gif
2011,5659475,Fig. 7.,"Computational times taken by the proposed, RS, MS, MS-cSV, and EQB techniques at each iteration for the (a) Paneveggio and (b) Pavia data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-7-source-large.gif
2011,5659475,Fig. 8.,"Average classification accuracy provided by the proposed approach considering different values of batch size
h
for the (a) Paneveggio and (b) Pavia data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/5753065/5659475/5659475-fig-8-source-large.gif
2011,5703095,Fig. 1.,Illustrative examples of changes in the underlying function: (a) Complete and gradual change. (b) Partial and abrupt/gradual change.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-1-source-large.gif
2011,5703095,Fig. 2.,Example of regression tree and the partition of the function domain it produces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-2-source-large.gif
2011,5703095,Fig. 3.,Illustration of local window management in (a) the improvement state and (b) the degradation state.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-3-source-large.gif
2011,5703095,Fig. 4.,"(a) Root-mean-squared error, (b) examples stored, and (c) number of leaves in model on experiments based on the sine data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-4-source-large.gif
2011,5703095,Fig. 5.,Root-mean-squared error on a data stream with unknown dynamics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-5-source-large.gif
2011,5703095,Fig. 6.,Results on the electricity market data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-6-source-large.gif
2011,5703095,Fig. 7.,Results on the departure delay data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-7-source-large.gif
2011,5703095,Fig. 8.,Critical difference diagram on stationary regression tasks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5898466/5703095/5703095-fig-8-source-large.gif
2011,5487520,Fig. 1.,The pseudocode of the proposed algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-1-source-large.gif
2011,5487520,Fig. 2.,Abalone.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-2-source-large.gif
2011,5487520,Fig. 3.,Pima.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-3-source-large.gif
2011,5487520,Fig. 4.,Vowel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-4-source-large.gif
2011,5487520,Fig. 5.,Cmc.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-5-source-large.gif
2011,5487520,Fig. 6.,Anneal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-6-source-large.gif
2011,5487520,Fig. 7.,Ac.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/5643207/5487520/5487520-fig-7-source-large.gif
2011,5374366,Fig. 1.,Hierarchical structure of compartments based on cellular sorting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5374366/5374366-fig-1-source-large.gif
2011,5374366,Fig. 2.,Accuracy of predictions based on motifs discovered by the different methods or a set of database motifs for PSLT2. Results for the PSLT2 methods are taken from [11].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5374366/5374366-fig-2-source-large.gif
2011,5374366,Fig. 3.,The number of known targeting motifs found by different methods and their significance. The p-values are calculated by generating random motifs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5374366/5374366-fig-3-source-large.gif
2011,5374366,Fig. 4.,"Top motif candidates that are most predictive of localization, discovered by discriminative HMM using (a) flat and (b) hierarchical compartment structure (b). Known motifs recovered by our methods are also shown with InterPro ID and regular expressions, which partially match the HMM logo [33]. NLS is also found when using the flat structure but not shown above. Pink columns are insert states of profile HMM; widths of dark and light pink columns correspond to the hitting probability and the expected length, respectively (shortened when necessary to make the letters clear).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5374366/5374366-fig-4-source-large.gif
2011,5374366,Fig. 5.,Percentage of conserved motif instances of the top 20 candidate motifs found by different methods. Conservation is based on SGD fungal alignment. A motif instance is considered conserved if all sites are strongly conserved. The p-values are denoted for each method (see Methods for the statistical test).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5374366/5374366-fig-5-source-large.gif
2011,5374366,Fig. 6.,Fluorescence microscope images for some of the proteins whose subcellular location predicted from sequence differs from annotations in SwissProt. Each image shows the DNA-binding dye DAPI (red) and the GFP-tagged proteins (green). The proteins are (a) Frq1/YDR373W and (b) Ppt1/YGR123C. Images were obtained from the UCSF GFP-localization database (http://yeastgfp.ucsf.edu/).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/5686865/5374366/5374366-fig-6-source-large.gif
2011,5948411,Fig. 1.,"Comparing our block-coordinate descent algorithm in its kernel version with a gradient descent approach similar to the one used for MKL in SimpleMKL. The left panel shows an example of how the objective value varies with respects to the CPU time. The right table summarizes the time needed for the gradient descent algorithm and for our method before convergence. Relative difference of objective value and the maximal difference between the weights returned by the two algorithms are also reported. All the criteria are averaged over 10 different training sets and with a fixed uniform and random nonzero initializations. The experimental parameters are
d=100
,
r=4
,
T=4
, and
n=100
with
C=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5948411/5948411-fig-1-source-large.gif
2011,5948411,Fig. 2.,"Performance (test error) comparisons between
ℓ
1
−
ℓ
2
(red, dash-dotted),
ℓ
p
−
ℓ
2
(blue, solid) multi-task models trained with proximal algorithms,
ℓ
p
−
ℓ
2
trained with alternate optimization (black, dotted), and
ℓ
1
separated models (green, dashed) for different experimental situations. For each experimental situation, we have kept fixed all parameters except one. (From left to right) number of training examples
n
, problem dimension
d
, number of relevant variables
r
, number of tasks
T
. The number given next each marker represents the number of times (out of 20) the
ℓ
1
−
ℓ
2
penalty provides a better performance than the
ℓ
p
−
ℓ
2
penalty both trained with proximal algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5948411/5948411-fig-2-source-large.gif
2011,5948411,Fig. 3.,"Evaluation of the recovered sparsity pattern through the
F
-measure of retrieved variables. (Left)
F
-measure wrt number of training examples. (Right)
F
-measure wrt the number of relevant variables. Prox, Altern, and Altern Ad stand for models trained with proximal and alternate optimization and alternate optimization with adaptive thresholding, respectively, for sparsity evaluation. When an algorithm exactly recovers the set of true relevant variables, the
F
-measure should be equal to 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5948411/5948411-fig-3-source-large.gif
2011,5948411,Fig. 4.,"Illustration of how when using an
ℓ
p
−
ℓ
2
penalty, the learned decision functions adapt themselves to the data at hand (
d=100
,
T=4
, and
n=100
). The two plots show the number of times a given
p
has been selected by validation. On the left, the number of relevant variables
r
is equal to 4, while on the right plot, we have
r=20
. We note that, as the number of relevant variables increases, the model selection procedure tends to choose a larger
p
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5948411/5948411-fig-4-source-large.gif
2011,5948411,Fig. 5.,"(Left) Average F1 score and number of selected kernels using our algorithms with different penalties on protein subcellular localization problems. Scores of the multiclass MKL of Zien and Ong [56] have also been reported. (Right) Example of kernel weights resulting from the different penalties for the
PSORT+
problem. For the sake of clarity, we have restricted the plot to the 20 first kernels. We note that different penalties lead to different sets of selected variables and, for some variables that have been selected by all three models, the weighting can largely differ. The number of kernels selected by Zien and Ong's method has not been explicitly reported and we have extrapolated them from one of their figures in [37].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/5970288/5948411/5948411-fig-5-source-large.gif
2011,5740601,Fig. 1.,"Framework of our proposed DAML algorithm. The images are first extracted with some state-of-the-art visual features, e.g., bag-of-visual-words, and then, perform KPCA to select the most important principal components to reduce the feature dimension in the kernel space. Gradient projection method is performed iteratively by satisfying the label consistency and minimizing the MMD. The metric obtained by DAML can be adopted by various tasks, such as image annotation, clustering, retrieval, etc.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6019142/5740601/5740601-fig-1-source-large.gif
2011,5740601,Fig. 2.,Recognition rate over different adaptation settings on the face recognition datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6019142/5740601/5740601-fig-2-source-large.gif
2011,5740601,Fig. 3.,"Example images of NUS-WIDE dataset, the images of each concept are arranged in a column.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6019142/5740601/5740601-fig-3-source-large.gif
2011,5740601,Fig. 4.,Recognition rate over different adaptation settings on the NUS-WIDE dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6019142/5740601/5740601-fig-4-source-large.gif
2011,5740601,Fig. 5.,Performance versus regularization weight.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6019142/5740601/5740601-fig-5-source-large.gif
2011,5601741,Fig. 1.,"(a) Plot of the reconstruction error of
C
^
+
for different values of
α
when the population covariance
C=I
. The inset shows the divergence of the reconstruction error (on a logarithmic scale) over a wider range of
α
values. (b) Average of the generalization error, as defined by (5), for the Pseudo-Fisher Linear Discriminant when the true within-class covariance is
I
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5770288/5601741/5601741-fig-1-source-large.gif
2011,5601741,Fig. 2.,"The Marčenko-Pastur limiting distribution for sample covariance eigenvalues, at
α=0.1,0.25,0.5
. In all cases,
σ
2
=1
. We have shown only the part of the distribution pertaining to nonzero eigenvalues. For
α<1
, there is also a
δ
-function peak at
λ=0
due to the singular nature of the sample covariance matrix—see main text.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5770288/5601741/5601741-fig-2-source-large.gif
2011,5601741,Fig. 3.,"Plot of simulation error estimates against
1−α
, for different choices of population covariance
C
. (a) Plot of the logarithm of the average reconstruction error,
ln
E
ξ
(
p
−1
∥
C
^
+
−
C
−1
∥
2
F
)
. (b) Plot of
ln
E
ξ
(
∑
i
λ
−2
i
)
, the logarithm of the contribution to the average reconstruction error from just the pseudo-inverse covariance. For both plots, the order of the symbols in the legend follows the order (top-to-bottom) of the data series.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5770288/5601741/5601741-fig-3-source-large.gif
2011,5601741,Fig. 4.,"Plot of simulation estimates of the average generalization error against
α
, for different choices of population covariance
C
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5770288/5601741/5601741-fig-4-source-large.gif
2011,5601741,Fig. 5.,"Simulation estimates of expected modified PFLD generalization error. (a) Population within-class covariance
C=diag(
t
1
,
t
2
,…,
t
p
)
. (b) Population within-class covariance
C=VT
V
T
,
T=diag(
t
1
,
t
2
,…,
t
p
)
and
V
is drawn from
O(p)
. In both (a) and (b), population eigenvalues are distributed
t
i
∼Ga(
5
2
,
2
5
),i=1,…,p
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5770288/5601741/5601741-fig-5-source-large.gif
2011,5601741,Fig. 6.,"Plot of test set accuracy on the Arcene data set. (a) For the standard and modified PFLDs against training set size
n
. (b) For the modified PFLD against
α
eff
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5770288/5601741/5601741-fig-6-source-large.gif
2011,6022011,Fig. 1.,Flow of the SMLO methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-1-source-large.gif
2011,6022011,Fig. 2.,Review of the available methods in HIGH-FREQUENCY component synthesis and the targets of MMLDE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-2-source-large.gif
2011,6022011,Fig. 3.,Solid line represents an objective function that has been sampled at the five points shown as dots. The dotted line is a DACE predictor fit to these points (from [10]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-3-source-large.gif
2011,6022011,Fig. 4.,"Uncertainty about the function's value at a point (such as
x=8
above) can be treated as if there were a realization of a normal random variable with mean and standard deviation given by the DACE predictor and its standard error (from [10]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-4-source-large.gif
2011,6022011,Fig. 5.,Illustrative example for EI and ANN results in local search.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-5-source-large.gif
2011,6022011,Fig. 6.,Flow diagram of MMLDE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-6-source-large.gif
2011,6022011,Fig. 7.,Typical inductor result for Example 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-7-source-large.gif
2011,6022011,Fig. 8.,Equivalent circuit model of a transformer used as coarse model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-8-source-large.gif
2011,6022011,Fig. 9.,Typical transformer result for Example 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/6022003/6022011/6022011-fig-9-source-large.gif
2011,5959989,Fig. 1.,"Actual data
u(x,t)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-1-source-large.gif
2011,5959989,Fig. 2.,"Estimations for
a(x)=1+
sin
3
(2πx)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-2-source-large.gif
2011,5959989,Fig. 3.,Model prediction output using a polynomial kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-3-source-large.gif
2011,5959989,Fig. 4.,Model prediction error using a polynomial kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-4-source-large.gif
2011,5959989,Fig. 5.,Model prediction output using a Gaussian kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-5-source-large.gif
2011,5959989,Fig. 6.,Model prediction error using the Gaussian kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-6-source-large.gif
2011,5959989,Fig. 7.,"Model prediction output
u(x,y,t)
using a polynomial kernel at
t=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-7-source-large.gif
2011,5959989,Fig. 8.,"Model prediction error using polynomial kernel at
t=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-8-source-large.gif
2011,5959989,Fig. 9.,"Estimations for
b(x,y)
using a polynomial kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-9-source-large.gif
2011,5959989,Fig. 10.,"Estimation errors for
b(x,y)
using a polynomial kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-10-source-large.gif
2011,5959989,Fig. 11.,"Model prediction output using the Gaussian kernel at
t=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-11-source-large.gif
2011,5959989,Fig. 12.,"Model prediction error using the Gaussian kernel at
t=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-12-source-large.gif
2011,5959989,Fig. 13.,"Estimations for
b(x,y)
using the Gaussian kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-13-source-large.gif
2011,5959989,Fig. 14.,"Estimation errors for
b(x,y)
using the Gaussian kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-14-source-large.gif
2011,5959989,Fig. 15.,"Actual data
u(x,t)
(an abrupt change at time 0.1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-15-source-large.gif
2011,5959989,Fig. 16.,Model prediction output.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-16-source-large.gif
2011,5959989,Fig. 17.,"Estimations for
a(x)
at time 0.05.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-17-source-large.gif
2011,5959989,Fig. 18.,"Estimations for
a(x)
at time 0.15.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-18-source-large.gif
2011,5959989,Fig. 19.,Model prediction error (an abrupt change at time 0.1).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-19-source-large.gif
2011,5959989,Fig. 20.,Model prediction error using the least Lagrange multiplier strategy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-20-source-large.gif
2011,5959989,Fig. 21.,Model prediction error using KLMS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-21-source-large.gif
2011,5959989,Fig. 22.,Model prediction error using KRLS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-22-source-large.gif
2011,5959989,Fig. 23.,Model prediction output.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-23-source-large.gif
2011,5959989,Fig. 24.,Model prediction error (subject to measurement noise).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/72/6006653/5959989/5959989-fig-24-source-large.gif
2011,5765988,Fig. 1.,Surveillance video usually contains numerous examples of (a) typical behaviors and sparse examples of rare behaviors (b) and (c). Rare behaviors (red) also usually co-occur with other typical behaviors (yellow).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-1-source-large.gif
2011,5765988,Fig. 2.,(a) LDA [21] and (b) our WS-JTM graphical model structures (only one rare class shown for illustration). Shaded nodes are observed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-2-source-large.gif
2011,5765988,Fig. 3.,Illustration and validation of WS-JTM using synthetic data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-3-source-large.gif
2011,5765988,Fig. 4.,"Synthetic data classification performance as a function of rare-class example sparsity. One-shot learning corresponds to the
y
-axis. Our WS-JTM exhibits dramatically superior performance in the low data domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-4-source-large.gif
2011,5765988,Fig. 5.,"Example typical and rare behaviors in the (a), (b), (c) MIT and (d), (e), (f) QMUL surveillance data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-5-source-large.gif
2011,5765988,Fig. 6.,"Typical activities learned from the MIT data set. Red: Right, Blue: Left, Green: Up, Yellow: Down.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-6-source-large.gif
2011,5765988,Fig. 7.,One shot learning of rare activities: MIT data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-7-source-large.gif
2011,5765988,Fig. 8.,"Typical activities learned from the QMUL data set. Red: Right, Blue: Left, Green: Up, Yellow: Down.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-8-source-large.gif
2011,5765988,Fig. 9.,One shot learning of rare activities: QMUL data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-9-source-large.gif
2011,5765988,Fig. 10.,Improvement of rare behavior models with an increasing number of training examples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-10-source-large.gif
2011,5765988,Fig. 11.,Classification confusion matrices after one-shot learning: MIT data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-11-source-large.gif
2011,5765988,Fig. 12.,Classification confusion matrices after one-shot learning: QMUL data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-12-source-large.gif
2011,5765988,Fig. 13.,Average classification accuracy achieved while controlling false alarm rate. Quantity in brackets indicates area under the curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-13-source-large.gif
2011,5765988,Fig. 14.,Average classification accuracy given varying number of rare class training clips.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-14-source-large.gif
2011,5765988,Fig. 15.,Activities learned by LDA-C.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-15-source-large.gif
2011,5765988,Fig. 16.,Localization of rare activities by WS-JTM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-16-source-large.gif
2011,5765988,Fig. 17.,Estimated topic profile for rare clips from Fig. 16. Bar color indicates typical versus rare activities. Black line indicates the average profile over the entire data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-17-source-large.gif
2011,5765988,Fig. 18.,"Classification accuracy using different methods for estimating Dirichlet hyperparameters
{
α
c
}
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-18-source-large.gif
2011,5765988,Fig. 19.,Classification accuracy using different methods for computing likelihoods of unseen documents.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6047995/5765988/5765988-fig-19-source-large.gif
2011,5610728,Fig. 1.,"nSystem diagram of surrogate sensor lifting. Once training is complete, the blocks in dotted lines are eliminated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-1-source-large.gif
2011,5610728,Fig. 2.,E-Sitar and thumb sensor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-2-source-large.gif
2011,5610728,Fig. 3.,E-Sitar and thumb sensor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-3-source-large.gif
2011,5610728,Fig. 4.,E-Sitar circuit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-4-source-large.gif
2011,5610728,Fig. 5.,Drum pad on radio drum surface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-5-source-large.gif
2011,5610728,Fig. 6.,Regression results for predicting drum strike position using a surrogate sensor. The x-axis is the strike index and the y-axis is the predicted regression output corresponding to distance from the center scaled to return values in the same range as the radio drum. (a) RadioDrum input. (b) Surrogate sensor. (c) Surrogate sensor with discrete classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-6-source-large.gif
2011,5610728,Fig. 7.,Effect of more features to the correlation coefficient in drum regression. The y-axis is the correlation coefficient and the x-axis is the discrete feature index.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/5689344/5610728/5610728-fig-7-source-large.gif
2011,5551148,Fig. 1.,Textual query-based consumer photo retrieval system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-1-source-large.gif
2011,5551148,Fig. 2.,The subtree representing the two-level descendants of “water” in WordNet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-2-source-large.gif
2011,5551148,Fig. 3.,Illustration of cross-domain regularized regression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-3-source-large.gif
2011,5551148,Fig. 4.,Number of randomly selected positive samples for each concept in the training Web image database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-4-source-large.gif
2011,5551148,Fig. 5.,"Retrieval precisions using
k
NN classifier, decision stump ensemble classifier, and linear SVM classifier on the Kodak data set (1, 358 images, 21 concepts).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-5-source-large.gif
2011,5551148,Fig. 6.,"Retrieval precisions using
k
NN classifier, decision stump ensemble classifier, and linear SVM classifier on NUS-WIDE data set (269, 648 images, 81 concepts). Since the precisions of
k
NN_SE and
k
NN_SL are irrelevant with respect to
n
s
, their precisions are presented with dashed curves. (a) Retrieval precision in the top 20 results. (b) Retrieval precision in the top 30 results. (c) Retrieval precision in the top 40 results. (d) Retrieval precision in the top 50 results. (e) Retrieval precision in the top 60 results. (f) Retrieval precision in the top 70 results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-6-source-large.gif
2011,5551148,Fig. 7.,Top 10 retrieval results for query “water” on the Kodak data set. Incorrect retrieval results are highlighted with green boxes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-7-source-large.gif
2011,5551148,Fig. 8.,Top 10 retrieval results for query “animal” on the NUS-WIDE data set. (a) Initial results. (b) Results after one round of relevance feedback (one positive and one negative images are labeled in each round). Incorrect results are highlighted by green boxes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-8-source-large.gif
2011,5551148,Fig. 9.,"The average time cost of retrieval using decision stumps and SVMs with linear kernel on the NUS-WIDE data set (269, 648 images, 81 concepts). Note that “total time” stands for the sum of training time and testing time. (a) Training time. (b) Testing time. (c) Total time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-9-source-large.gif
2011,5551148,Fig. 10.,"Retrieval results after relevance feedback (one positive and one negative feedback per round) on the Kodak data set (1, 358 images, 21 concepts). (a) Retrieval precision in the top 20 results. (b) Retrieval precision in the top 30 results. (c) Retrieval precision in the top 40 results. (d) Retrieval precision in the top 50 results. (e) Retrieval precision in the top 60 results. (f) Retrieval precision in the top 70 results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-10-source-large.gif
2011,5551148,Fig. 11.,"Retrieval results after relevance feedback (one positive and one negative feedback per round) on the NUS data set (269, 648 images, 81 concepts). (a) Retrieval precision in the top 20 results. (b) Retrieval precision in the top 30 results. (c) Retrieval precision in the top 40 results. (d) Retrieval precision in the top 50 results. (e) Retrieval precision in the top 60 results. (f) Retrieval precision in the top 70 results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/5735600/5551148/5551148-fig-11-source-large.gif
2011,5499154,Fig. 1.,"Proposed hierarchical decomposition of satellites formation flight. (a) Four-level hierarchical fault-diagnosis framework for a LF satellites formation flight [6]. (b) BN representation of an
L
level hierarchical decomposition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-1-source-large.gif
2011,5499154,Fig. 2.,"Health states of a child node at level
l
and its parent nodes at level
l−1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-2-source-large.gif
2011,5499154,Fig. 3.,Formation flight of five satellites.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-3-source-large.gif
2011,5499154,Fig. 4.,Functional diagram of the ACS and a simplified schematic diagram of its RW component. (a) Functional diagram of the ACS. (b) Simplified schematic of a RW (adopted from [46]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-4-source-large.gif
2011,5499154,Fig. 5.,"Functional diagram of the EPS (adopted from [47], [48]) depicting the locations of fault injections under the assumed EPS fault scenarios.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-5-source-large.gif
2011,5499154,Fig. 6.,Four-level BN-based CDM for hierarchical fault diagnosis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-6-source-large.gif
2011,5499154,Fig. 7.,Sat-3 rule activations (the width of each bar graph is 512 s).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-7-source-large.gif
2011,5499154,Fig. 8.,"P(
X
3
3
) of Sat-3 when an evidence of a fault is introduced at the subsystem component level. (a) Before the presence of the fault. (b) During the presence of the fault.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/5714230/5499154/5499154-fig-8-source-large.gif
2012,5770269,Fig. 1.,Typical electrical infrastructure in cities. Source: Con Edison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-1-source-large.gif
2012,5770269,Fig. 2.,"Number of feeder outages in NYC per day during 2006-2007, lower curve with axis at left, and system-wide peak system load, upper curve with axis at right.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-2-source-large.gif
2012,5770269,Fig. 3.,Excerpt from sample smoking manhole (SMH) trouble ticket.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-3-source-large.gif
2012,5770269,Fig. 4.,Bathtub curve. Source Wikipedia: http://en.wikipedia.org/wiki/Bathtub_curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-4-source-large.gif
2012,5770269,Fig. 5.,Process diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-5-source-large.gif
2012,5770269,Fig. 6.,Sample timeline for rare event prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-6-source-large.gif
2012,5770269,Fig. 7.,"Example illustrating the training and test time windows in ODDS. The current time is 8/13/2008, and failure data for training was derived from the prediction period of 7/30/2007—8/27/2007 and 7/30/2008—8/13/2008.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-7-source-large.gif
2012,5770269,Fig. 8.,"Process diagram for feeder ranking, using ODDS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-8-source-large.gif
2012,5770269,Fig. 9.,Process diagram for manhole event ranking.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-9-source-large.gif
2012,5770269,Fig. 10.,Ticket processing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-10-source-large.gif
2012,5770269,Fig. 11.,Predictions from a support vector censored regression algorithm on PILC sections of 33 feeders in Queens.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-11-source-large.gif
2012,5770269,Fig. 12.,ROC-like curves from tests of the machine learning ranking of specific components.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-12-source-large.gif
2012,5770269,Fig. 13.,ROC-like curve for blind test of Crown Heights feeders.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-13-source-large.gif
2012,5770269,Fig. 14.,"Percent of feeder outages in which the feeder that failed was within the worst 15 percent (left) of the ranked list or best 25 percent (right), where the predictions being evaluated are those just before the time of failure. The system improved from less than 30 percent of the failures in the worst 15 percent in 2005 to greater than 60 percent in 2008, for example.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-14-source-large.gif
2012,5770269,Fig. 15.,Influence of different categories of features under different weather conditions. Red: hot weather of August 2010; Blue: snowy January 2011; Yellow: rainy February 2011; Turquoise: typical fall weather in October 2010.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-15-source-large.gif
2012,5770269,Fig. 16.,"Linear regression used to determine the mean time between failures for 1 January 2002 (purple), and 31 December 2008 (yellow) in each underground network in the Con Edison system. Networks are arranged along the horizontal axis from worst (left) to best (right), according to Con Edison’s “Network Reliability Index.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-16-source-large.gif
2012,5770269,Fig. 17.,Scatter plot of SVM predicted outage rate versus actual rate for all classes of unplanned outages. The diagonal line depicts a perfect model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-17-source-large.gif
2012,5770269,Fig. 18.,ROC-like curve for 2009 Bronx blind test of the machine learning ranking for vulnerability of manholes to serious events (fires and explosions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-18-source-large.gif
2012,5770269,Fig. 19.,"Screen capture of the Contingency Analysis Program tool during a fourth contingency event in the summer of 2008, with the feeders at most risk of failing next highlighted in red. The feeder ranking at the time of failure is shown in a blow-up ROC-like plot in the center.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-19-source-large.gif
2012,5770269,Fig. 20.,"A screen capture of the Con Edison CAPT evaluation, showing an improvement in MTBF from 140 to 192 days if 34 of the most at-risk PILC sections were to be replaced on a feeder in Brooklyn at an estimated cost of $650, 000.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-20-source-large.gif
2012,5770269,Fig. 21.,Example of cost benefit analysis of possible replacement strategies for specific at-risk components analyzed by the machine learning system. The solid line approximates the “efficient frontier” in portfolio management theory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-21-source-large.gif
2012,5770269,Fig. 22.,"Images from the manhole events visualization tool, where labels were enlarged for clarity. Top: Geocoded ticket addresses, colored by trouble type. Yellow indicates a serious event type, purple indicates a potential precursor. If the user clicks on a ticket, the full ticket text is displayed. Bottom: Manholes and main cables within the same location, where manholes are colored by predicted vulnerability. Note that a ticket within the top figure does not necessarily correspond to the nearest manhole on the bottom figure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-22-source-large.gif
2012,5770269,Fig. 23.,Overtreatment in the High Potential Preventive Maintenance program was identified by comparing to control group performance. Modified and A/C Hipot tests are now used by Con Edison instead of DC Hipot tests.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6107461/5770269/5770269-fig-23-source-large.gif
2012,6183517,Fig. 1.,"SVM+MTL
mapping data simultaneously into decision spaces and correcting spaces.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6203443/6183517/6183517-fig-1-source-large.gif
2012,6035797,Fig. 1.,"Scalability of different classifiers: An example on Letter data set. The training time spent by LS-SVM and ELM (Gaussian kernel) increases sharply when the number of training data increases. However, the training time spent by ELM with Sigmoid additive node and multiquadric function node increases very slowly when the number of training data increases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6035797/6035797-fig-1-source-large.gif
2012,6035797,Fig. 2.,"Performances of LS-SVM and ELM with Gaussian kernel are sensitive to the user-specified parameters
(C,γ)
: An example on Satimage data set. (a) LS-SVM with Gaussian kernel. (b) ELM with Gaussian kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6035797/6035797-fig-2-source-large.gif
2012,6035797,Fig. 3.,"Performance of ELM (with Sigmoid additive node and multiquadric RBF node) is not very sensitive to the user-specified parameters
(C,L)
, and good testing accuracies can be achieved as long as
L
is large enough: An example on Satimage data set. (a) ELM with Sigmoid additive node. (b) ELM with multiquadric RBF node.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6035797/6035797-fig-3-source-large.gif
2012,6035797,Fig. 4.,Separating boundaries of different classifiers in XOR problem. (a) SVM. (b) LS-SVM. (c) ELM (Gaussian kernel). (d) ELM (Sigmoid additive node).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6035797/6035797-fig-4-source-large.gif
2012,6035797,Fig. 5.,Separating boundaries of different classifiers in Banana case. (a) SVM. (b) LS-SVM. (c) ELM (Gaussian kernel). (d) ELM (Sigmoid additive node).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6035797/6035797-fig-5-source-large.gif
2012,6222007,Fig. 1.,Averaged testing RMSE of ELM over 50 experiments in the Machine CPU dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-1-source-large.gif
2012,6222007,Fig. 2.,Structure of proposed B-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-2-source-large.gif
2012,6222007,Fig. 3.,Average testing RMSE of different algorithms in Fried case (sine hidden nodes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-3-source-large.gif
2012,6222007,Fig. 4.,Average testing RMSE of different algorithms in Bank case (sine hidden nodes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-4-source-large.gif
2012,6222007,Fig. 5.,Average testing RMSE of different algorithms in Machine CPU case (sine hidden nodes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-5-source-large.gif
2012,6222007,Fig. 6.,Average testing RMSE of different algorithms in Fried case (sigmoid hidden nodes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-6-source-large.gif
2012,6222007,Fig. 7.,Average testing RMSE of different algorithms in Bank case (sigmoid hidden nodes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-7-source-large.gif
2012,6222007,Fig. 8.,Average testing RMSE of different algorithms in Machine CPU case (sigmoid hidden nodes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-8-source-large.gif
2012,6062422,Fig. 1.,"Examples of how the shape of the cdf,
G(x)
, changes with different values of the rate parameter,
a
. The exponential distribution has been normalized such that the probability of
x≤1.0
will always be 1.0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-1-source-large.gif
2012,6062422,Fig. 2.,"System diagram showing the parallel output filters and feedback that occurs when learning is being performed. The output of the kernel machine at time
t
, using the S3 algorithm is
f
^
(
x
t
)
. The input label at time
t
is
y
t
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-2-source-large.gif
2012,6062422,Fig. 3.,"Evaluation of the kernel expansion still makes use of the parallel output filters. In this setting, the stochastic estimate
f
^
(
x
t
)
exhibits noise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-3-source-large.gif
2012,6062422,Fig. 4.,"(a) Contour plot of
f(x)=
∑
i∈M
α(i)k(x,
x
i
)
. (b) Contour plot of
f
^
(x)=
∑
i∈B
α(i)k(x,
x
i
)
with
β=0.4
. Contour plots were created by sampling the input space in a grid pattern of size 200 × 200 steps from −2.0 to 2.0 in both dimensions. (a) Kernel machine output. (b) Kernel machine output using S3 algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-4-source-large.gif
2012,6062422,Fig. 5.,"Illustration of the observed error between a kernel machine trained with the NORMA, and a kernel machine trained with the NORMA using the S3 algorithm for kernel expansion evaluation. The two curves represent the actual error and the estimate of error derived from equation (6). It should be noted that our estimate of the error converges to the measured error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-5-source-large.gif
2012,6062422,Fig. 6.,"(a) Stationary labeling, 1000 samples. (b) Switched labeling every 120 samples in sequence. (c) Slow drifted labeling, labeling regions rotated at a rate of
w=
0.72
∘
per sample. Fast drifted labeling, labeling regions rotated at a rate of
w=
5.76
∘
per sample.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-6-source-large.gif
2012,6062422,Fig. 7.,"(a) Two-dimensional scatter plot of the 1000 point stationary data set. Blue squares are +1 labeled samples, and red circles are −1 labeled samples. (b) A contour plot of
R
emp
(f,X,Y)=
∑
i=1
t
l(f(
x
i
),
y
i
)
using twofold cross validation. Optimum found at
γ=2.06
,
η=0.935
. (c) A contour plot
R
emp
(f,X,Y)=
∑
i=1
t
l(f(
x
i
),
y
i
)
using NORMA+S3 (no output filter) on the 1000 sample stationary data set. Dark contour line represents empirical risk using the entire buffer of samples. (d) S3 operating point chosen at
β=0.57
,
a=17.4
. (e) S3 operating point chosen at
β=0.63
,
a=14.6
. (f) S3 operating point chosen at
β=0.65
,
a=18.0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-7-source-large.gif
2012,6062422,Fig. 8.,"In the upper graph, time is expressed in milliseconds required to learn a single sample. Timing is dynamic depending on the training algorithm used and becomes more deterministic when the buffer is eventually filled. S3 parameters chosen are
β=0.5
,
a=0.0
. In the lower graph, time is the number of milliseconds to go through the entire 1000 sample data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-8-source-large.gif
2012,6062422,Fig. 9.,"(a) Some random samples from the MNIST data set. (b) Optimum point after doing twofold cross validation at
γ=0.02
,
η=1.0
. (c) Stationary set cannot be optimized by the S3 algorithm. (d) S3 operating point at
β=0.47
,
a=11.6
. (e) Number of milliseconds taken to learn a single sample.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-9-source-large.gif
2012,6062422,Fig. 10.,"(a) Optimum found at
γ=6.336e−05
,
η=0.083
. (b) S3 operating point at
β=0.8
,
a=18.0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6062422/6062422-fig-10-source-large.gif
2012,6185691,Fig. 1.,"Basic structures of the three learning paradigms: supervised learning, reinforcement learning, and unsupervised learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-1-source-large.gif
2012,6185691,Fig. 2.,Schematic view of locally weighted regression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-2-source-large.gif
2012,6185691,Fig. 3.,Actor-critic learning architecture for robot control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-3-source-large.gif
2012,6185691,Fig. 4.,Q-learning architecture for robot control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-4-source-large.gif
2012,6185691,Fig. 5.,Prospective applications of bipedal walking robots.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-5-source-large.gif
2012,6185691,Fig. 6.,ZMP stability criterion. (a) Stable ZMP position. (b) Unstable ZMP when it goes out of the foot support.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-6-source-large.gif
2012,6185691,Fig. 7.,Demonstration of the simplest passive dynamic walker as well as a real PDW robot prototype from Delft University [116]. (a) Simplest passive dynamic walker. (b) Real robot from the Delft University of Technology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-7-source-large.gif
2012,6185691,Fig. 8.,"Categorization of bipedal walking control approaches. Machine learning algorithms have been applied in each group of approaches to enhance their control performance in terms of adaptability, robustness, and scalability.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-8-source-large.gif
2012,6185691,Fig. 9.,(a) Schematic structure of a coupled neural oscillator. (b) Basic structure of a neural oscillator-based actor-critic RL controller.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-9-source-large.gif
2012,6185691,Fig. 10.,Schematic representation of CMAC learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-10-source-large.gif
2012,6185691,Fig. 11.,Fuzzy-based linguistic-numerical information integration for bipedal walking control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-11-source-large.gif
2012,6185691,Fig. 12.,Architecture of an RL controller with fuzzy evaluative feedback [123].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-12-source-large.gif
2012,6185691,Fig. 13.,Architecture of Hebbian learning Control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-13-source-large.gif
2012,6185691,Fig. 14.,Hierarchical integration of robot learning control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6185691/6185691-fig-14-source-large.gif
2012,5765926,Fig. 1.,Histogram of labeled slides count.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6165687/5765926/5765926-fig-1-source-large.gif
2012,6125249,Fig. 1.,"Procedure of obtaining the network sequence at the
s
th step of AG-ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6145493/6125249/6125249-fig-1-source-large.gif
2012,6104179,Fig. 1.,"Performance comparison on Type 1. In figures (a) and (b), the bolded lines are decision boundaries. Training data are denoted by “□” and
◊
, respectively; Test data are denoted by “⋅” and “+,” respectively. (a) Boundary obtained by SRC. (b) Boundary obtained by KSRC. (c) Test error versus dimensionality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6165492/6104179/6104179-fig-1-source-large.gif
2012,6104179,Fig. 2.,"Performance comparison on Type 2. In figures (a) and (b), the bolded lines are decision boundaries. Training data are denoted by “□” and “
◊
,” respectively. Test data are denoted by “⋅” and “+,” respectively. (a) Boundary obtained by SRC. (b) Boundary obtained by KSRC. (c) Test error versus dimensionality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6165492/6104179/6104179-fig-2-source-large.gif
2012,6104179,Fig. 3.,"Performance of four schemes versus
ε
. (a) Sparsity versus
ε
. (b) Residual versus
ε
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6165492/6104179/6104179-fig-3-source-large.gif
2012,6104179,Fig. 4.,"Coefficient vectors obtained by (a) determinate scheme, (b) KPCA, (c) KFDA, and (d) RP.
ε=0.001
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6165492/6104179/6104179-fig-4-source-large.gif
2012,6104179,Fig. 5.,"Face data. (a) Images of a subject from the ORL database, (b) images of a subject from UMIST database, and (c) images of a subject from Extended Yale B database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6165492/6104179/6104179-fig-5-source-large.gif
2012,6104179,Fig. 6.,"Mean of test error on three face databases: (a) ORL, (b) UMIST, and (c) Extended Yale B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6165492/6104179/6104179-fig-6-source-large.gif
2012,6129537,Fig. 1.,Kernel matrices computed on the toy data set in Section IV-A. (a) Base kernel: Gaussian kernel with Euclidean distance. (b) PCK. (c) True label kernel: The label product of pairwised samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-1-source-large.gif
2012,6129537,Fig. 2.,"Toy data set: (Colored solid lines) the three types of regions involved in classification: (1) Densely distributed separable regions, (2) less densely distributed overlapping regions, and (3) sparsely distributed regions, together with the Gaussian distributions from which data are sampled and (gray dashed lines) the optimal Bayesian decision boundary.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-2-source-large.gif
2012,6129537,Fig. 3.,"Toy data set: Separating (gray solid lines) hyperplanes and (filled points) support vectors calculated by SimpleMKL, G-LMKL, and two versions of PCK-LMKL. Gray dashed lines show the Gaussian distributions from which data are sampled and the optimal Bayesian decision boundary. (a) SVM for view 1. (b) SVM for view 2. (c) SimpleMKL. (d) G-LMKL. (e) PCK-LMKL
(p=1)
. (f) PCK-LMKL
(p=5)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-3-source-large.gif
2012,6129537,Fig. 4.,"Wdbc data sets: Entries of PCKs learned on the training set with various
p
norms for a given pair of samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-4-source-large.gif
2012,6129537,Fig. 5.,"UCI data sets: Learning the norm
p
for each UCI data set on their respective validation sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-5-source-large.gif
2012,6129537,Fig. 6.,"Spambase data set: An example of the evolution of PCK
∑
M
m=1
P
m
. (a) Initial. (b) Iteration 1. (c) Iteration 2. (d) Iteration 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-6-source-large.gif
2012,6129537,Fig. 7.,"Ringnorm data set: An example of five different entries in PCKs (corresponding to the localized weight distribution of five different pairs of samples) with the optimal
{p,C}
combination on the validation set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-7-source-large.gif
2012,6129537,Fig. 8.,Computer vision data sets: (a) Sample images from the 15-scene data set and (b) sample images from the Caltech-101 data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-8-source-large.gif
2012,6129537,Fig. 9.,Comparison on computer vision data sets. (a) 15-scene data set: Performances of eight state-of-the-art features and two multiple kernel classifiers. (b) Caltech-101 benchmark comparison: The performance of PCK-LMKL along with the most recently reported results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6129537/6129537-fig-9-source-large.gif
2012,6136518,Fig. 1,"Illustration of virtual labels. The base classifier
f
B,m
is learned with the base kernel function
k
m
and the labeled training data from
D
, where
m=1,…,M
. For each of the unlabeled target pattern
x
from
D
T
u
, we can obtain its decision value
f
B,m
(x)
from each base classifier. Then, the virtual label
y
~
of
x
is defined as the linear combination of its decision values
f
B,m
(x)
s weighted by the coefficients
d
m
s, i.e.,
y
~
=
∑
M
m=1
d
m
f
B,m
(x)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136518/6136518-fig-1-source-large.gif
2012,6136518,Fig. 2,Per-concept APs of all 36 concepts using different methods. The concepts are divided into three groups according to the positive frequency. Our methods achieve the best performances for the circled concepts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136518/6136518-fig-2-source-large.gif
2012,6136518,Fig. 3,"Performance comparisons of DTMKL_f with other methods in terms of the means and standard deviations of classification accuracies on the 20 Newsgroups data set by using different regularization parameters
C∈{0.1,0.2,0.5,1,2,5,10,20,50}
. We set
m=5
(top) and
m=10
(bottom).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136518/6136518-fig-3-source-large.gif
2012,6136518,Fig. 4,"Performance (i.e., the means of classification accuracies) variation of DTMKL_f with respect to the balance parameter
λ∈[0.1,10]
on the 20 Newsgroups data set. We set the regularization parameter
C=2
and
C=5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136518/6136518-fig-4-source-large.gif
2012,6136518,Fig. 5,Illustration of the convergence of DTMKL_AT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136518/6136518-fig-5-source-large.gif
2012,6112768,Fig. 1.,"Four sample frames from consumer videos and YouTube videos. Our work aims to recognize the events in consumer videos by using a limited number of labeled consumer videos and a large number of YouTube videos. The examples from two events (i.e.,“picnic” and “sports”) illustrate the considerable appearance differences between consumer videos and YouTube videos, which poses great challenges to conventional learning schemes but can be effectively handled by our transfer learning method A-MKL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-1-source-large.gif
2012,6112768,Fig. 2.,The flowchart of the proposed visual event recognition framework. It consists of an aligned space-time pyramid matching method that effectively measures the distances between two video clips and a transfer learning method that effectively copes with the considerable variation in feature distributions between the web videos and consumer videos.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-2-source-large.gif
2012,6112768,Fig. 3.,"Illustration of the proposed Aligned Space-Time Pyramid Matching method at level-1: (a) Each video is divided into eight space-time volumes along the width, height, and temporal dimensions. (b) The matching results are obtained by using our ASTPM method. Each pair of matched volumes from two videos is highlighted in the same color. For better visualization, please see the colored PDF file.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-3-source-large.gif
2012,6112768,Fig. 4.,Means and standard deviations of per-event APs of six events for all methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-4-source-large.gif
2012,6112768,Fig. 5.,"Illustration of the combination coefficients
β
p
s of the prelearned classifiers for all events.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-5-source-large.gif
2012,6112768,Fig. 6.,Illustration of the convergence of the A-MKL learning algorithm for all events.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-6-source-large.gif
2012,6112768,Fig. 7.,"Means and standard deviations of MAPs over six events for SVM_T, DTSVM, A-MKL_4, and A-MKL_24 when using different proportions (i.e.,
r
) of labeled training consumer videos.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6247420/6112768/6112768-fig-7-source-large.gif
2012,6006539,Fig. 1.,Comparisons of batch ODCDS to PSA and Euler's method on synthetic data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6129833/6006539/6006539-fig-1-source-large.gif
2012,6006539,Fig. 2.,Comparisons of stochastic ODCDS and PSA on large-scale data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6129833/6006539/6006539-fig-2-source-large.gif
2012,6006539,Fig. 3.,Comparisons of stochastic ODCDS and PSA on large-scale data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6129833/6006539/6006539-fig-3-source-large.gif
2012,5728816,Fig. 1.,"Entropy of
E
0
=(
p
1
,
p
2
,
p
3
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-1-source-large.gif
2012,5728816,Fig. 2.,"Fuzziness of
E
0
=(
p
1
,
p
2
,
p
3
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-2-source-large.gif
2012,5728816,Fig. 3.,"(a) fuzziness and (b) ambiguity of
B=(
μ
1
,
μ
2
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-3-source-large.gif
2012,5728816,Fig. 4.,A simple model of two binary classification problem. (a) classification function. (b) membership degrees.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-4-source-large.gif
2012,5728816,Fig. 5.,"Ambiguity of the points,",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-5-source-large.gif
2012,5728816,Fig. 6.,Framework of our proposed sample selection algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-6-source-large.gif
2012,5728816,Fig. 7.,Change tendencies of the trees as more samples are added to training set using MABSS and random selection method on glass.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-7-source-large.gif
2012,5728816,Fig. 8.,Experimental results on spambase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-8-source-large.gif
2012,5728816,Fig. 9.,The average tree size on databases of UCI. (a) Average number of leaf nodes. (b) Average depth.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-9-source-large.gif
2012,5728816,Fig. 10.,A simple model of fuzzy decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5728816/5728816-fig-10-source-large.gif
2012,5959167,Fig. 1.,An example XML document.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-1-source-large.gif
2012,5959167,Fig. 2.,The DTD file of the XML document in Fig. 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-2-source-large.gif
2012,5959167,Fig. 3.,Decision-tree algorithm based on the precision/recall heuristic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-3-source-large.gif
2012,5959167,Fig. 4.,Algorithm for finding a predicate to split a node.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-4-source-large.gif
2012,5959167,Fig. 5.,Algorithm for finding a predicate to split a node.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-5-source-large.gif
2012,5959167,Fig. 6.,"The behavior of the tree size on the training, validation, and test sets on class trade.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-6-source-large.gif
2012,5959167,Fig. 7.,The CotrainPRDT algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-7-source-large.gif
2012,5959167,Fig. 8.,"The performance of the CotrainPRDT on the Reuters
2×2
data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-8-source-large.gif
2012,5959167,Fig. 9.,"The performance of the two classifiers from the two views and the combined classifier in one-fold experiment on Reuters
2×2
data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6084775/5959167/5959167-fig-9-source-large.gif
2012,6092438,Fig. 1.,Illustrative examples of partition of the uncertainty regions of the classifiers into multiple parts depending on the number of geometrical margins of binary classifiers included in them. Different gray levels represent different uncertainty regions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6163264/6092438/6092438-fig-1-source-large.gif
2012,6092438,Fig. 2.,"(a) Classification accuracy and (b) standard deviation of the accuracy over 20 runs provided by the proposed, the MS-cSV, the MS-KKC, and the RS methods for the hyperspectral data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6163264/6092438/6092438-fig-2-source-large.gif
2012,6092438,Fig. 3.,"(a) Classification accuracy and (b) standard deviation of the accuracy over 20 runs provided by the proposed, the MS-cSV, the MS-KKC, and the RS methods for the multispectral data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6163264/6092438/6092438-fig-3-source-large.gif
2012,6092438,Fig. 4.,Average classification accuracy gain obtained by considering both terms of (4) over considering only the minimum and only the average distance terms for the (a) hyperspectral and (b) multispectral data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6163264/6092438/6092438-fig-4-source-large.gif
2012,6092438,Fig. 5.,"Computational time taken by the proposed, the MS-cSV, the MS-KKC, and the RS techniques at each iteration for the multispectral data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6163264/6092438/6092438-fig-5-source-large.gif
2012,5963671,Fig. 1.,"Four types of loss function. For ramp loss (Fig. 1c) and concave loss (Fig. 1d), the parameter
s
is set to
−0.3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6308678/5963671/5963671-fig-1-source-large.gif
2012,5963671,Fig. 2.,"Decision boundaries (denoted by the solid line) obtained by the supervised SVM, CCCP TSVM, and Two-view TSVM. The only two labeled examples are represented by a bold cross and circle. The remaining points are unlabeled. Gaussian and linear kernels are used for views 1 and 2, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6308678/5963671/5963671-fig-2-source-large.gif
2012,5963671,Fig. 3.,The five extracted features and their sample dictionary terms where applicable.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6308678/5963671/5963671-fig-3-source-large.gif
2012,5963671,Fig. 4.,Variation of the positive and negative classes' F1-measures over 100 splits of the WebKB course data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6308678/5963671/5963671-fig-4-source-large.gif
2012,5963671,Fig. 5.,Accuracy versus the number of labeled examples for CCCP TSVM and Two-view TSVM on the product review data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6308678/5963671/5963671-fig-5-source-large.gif
2012,6104032,Fig. 1.,Adaptation Components [4].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-1-source-large.gif
2012,6104032,Fig. 2.,Learner Model components [6].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-2-source-large.gif
2012,6104032,Fig. 3.,Mobile learning application workflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-3-source-large.gif
2012,6104032,Fig. 4.,ANFIS architecture [16].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-4-source-large.gif
2012,6104032,Fig. 5.,ANFIS training system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-5-source-large.gif
2012,6104032,Fig. 6.,Error curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-6-source-large.gif
2012,6104032,Fig. 7.,ANFIS Structure with four inputs and one output.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6294403/6104032/6104032-fig-7-source-large.gif
2012,6138903,Fig. 1.,(a) Idealized geometry of an atherosclerotic arterial model. Transversal section. (b) Geometrical parameters shown on the cross central section of the atherosclerotic vessel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6170948/6138903/6138903-fig-1-source-large.gif
2012,6138903,Fig. 2.,Maximum MPS surfaces for a given stenosis ratio. Adapted from [7].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6170948/6138903/6138903-fig-2-source-large.gif
2012,6138903,Fig. 3.,ANN structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6170948/6138903/6138903-fig-3-source-large.gif
2012,6138903,Fig. 4.,"Comparison of real points (o) and estimated points by linear regression (*), SVM (x) and MLP (
♢
) models in a set of 50 observations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6170948/6138903/6138903-fig-4-source-large.gif
2012,6127880,Fig. 1.,Top row: Sample interaction in traditional multiclass active learning approaches. The user needs to input a category name/number for the query image from a large dataset possibly consisting of hundreds of categories. Bottom row: The binary interaction model we propose. The user only needs to say whether or not the query image and the sample image belong to the same category.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-1-source-large.gif
2012,6127880,Fig. 2.,Block schematic of the active learning setting. Our focus in this paper is on the query and sample selection algorithms—depicted in white boxes with red borders (see text for details).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-2-source-large.gif
2012,6127880,Fig. 3.,Multiclass active learning with binary feedback.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-3-source-large.gif
2012,6127880,Fig. 4.,"An illustration of why entropy can be a poor estimate of classification uncertainty. The plots show estimated probability distributions for two unlabeled examples in a 10 class problem. In (a), the classifier is highly confused between classes 4 and 5. In (b), the classifier is relatively more confident that the example belongs to class 4, but is assigned higher entropy. The entropy measure is influenced by probability values of unimportant classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-4-source-large.gif
2012,6127880,Fig. 5.,"Illustration of one-versus-one classification (classes that each classifier separates are noted). Assuming that the estimated distribution for the unlabeled example (shown as a blue disk) peaks at “Class 4,” the set of classifiers in contention is shown as red lines. BvSB estimates the highest uncertainty in this set—uncertainty of other classifiers is irrelevant.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-5-source-large.gif
2012,6127880,Fig. 6.,"Space exploration of active selection—BvSB-based selection is almost as good as random exploration, while the former achieves much higher classification accuracy than random.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-6-source-large.gif
2012,6127880,Fig. 7.,Active learning on the 13 natural scene categories dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-7-source-large.gif
2012,6127880,Fig. 8.,"The top row shows images on which the classifier is uncertain using the BvSB score. The bottom row shows images on which the classifier is confident. True labels are noted below the corresponding images. We can see that the top row has more confusing images, indicating that the active learning method chooses harder examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-8-source-large.gif
2012,6127880,Fig. 9.,"Y
-axis: # examples correctly classified by random example selection for a given class.
X
-axis: # examples of the corresponding class chosen by active selection. The negative correlation shows that active learning chooses more examples from harder classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-9-source-large.gif
2012,6127880,Fig. 10.,"Active learning in the BF model requires far less user training time compared to active selection in the MCF model. US: uncertainty sampling, RND: random. (a) USPS, (b) Pendigits, (c) Caltech-101 datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-10-source-large.gif
2012,6127880,Fig. 11.,VOI-based active selection and uncertainty sampling (both with BF) during the initial phases of active learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-11-source-large.gif
2012,6127880,Fig. 12.,"Confusion matrices with (a) active (VOI), and (b) random selection (
max.trace=1,515
). VOI leads to much lower confusion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-12-source-large.gif
2012,6127880,Fig. 13.,"Sensitivity to label noise, (a) 10 percent, (b) 20 percent. VOI with noisy data outperforms the random selection with clean data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-13-source-large.gif
2012,6127880,Fig. 14.,Per-class accuracy of VOI versus random on the scene-13 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-14-source-large.gif
2012,6127880,Fig. 15.,"Population imbalance: VOI selects many images, even for classes with small populations (see text for details).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-15-source-large.gif
2012,6127880,Fig. 16.,"(a) Speedup achieved with LSH over LS for the approximate near neighbor problem on the Cifar-10 dataset.
c=1+ϵ
denotes the approximation factor. (b), (c) Active learning with the LSH approximation gives little difference in accuracy compared to the LS. (b) USPS dataset, (c) Cifar-10 dataset. On average, the speedup for USPS was 17-fold, while that for Cifar-10 was 91-fold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6127880/6127880-fig-16-source-large.gif
2012,6133331,Fig. 1.,"Multiscale kernel matrices for training samples of KSC data set. (a) ideal kernel. (b)
σ=0.1
. (c)
σ=0.5
. (d)
σ=1
. (e)
σ=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-1-source-large.gif
2012,6133331,Fig. 2.,Values of kernel alignment for basis kernels with different scales.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-2-source-large.gif
2012,6133331,Fig. 3.,Kernel alignment for kernels constructed by samples from two classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-3-source-large.gif
2012,6133331,Fig. 4.,KSC data set. (a) RGB composite image of three bands. (b) Groundtruth map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-4-source-large.gif
2012,6133331,Fig. 5.,Indian Pine data set. (a) RGB composite image of three bands. (b) Groundtruth map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-5-source-large.gif
2012,6133331,Fig. 6.,University of Pavia data set. (a) RGB composite image of three bands. (b) Groundtruth map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-6-source-large.gif
2012,6133331,Fig. 7.,"Scale values sampled by linear, exponential, logarithmic, and tangent functions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-7-source-large.gif
2012,6133331,Fig. 8.,Classification maps of KSC data set. (a) SK classified map. (b) MI-SWK classified map. (c) SimpleMKL classified map. (d) RMKL classified map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-8-source-large.gif
2012,6133331,Fig. 9.,Classification maps of Indian Pine data set. (a) SK classified map. (b) MI-SWK classified map. (c) SimpleMKL classified map. (d) RMKL classified map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-9-source-large.gif
2012,6133331,Fig. 10.,Classification maps of University of Pavia data set. (a) SK classified map. (b) MI-SWK classified map. (c) SimpleMKL classified map. (d) RMKL classified map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-10-source-large.gif
2012,6133331,Fig. 11.,Kappa coefficients of the KSC data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-11-source-large.gif
2012,6133331,Fig. 12.,Kappa coefficients of the Indian Pine data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-12-source-large.gif
2012,6133331,Fig. 13.,Kappa coefficients of the University of Pavia data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-13-source-large.gif
2012,6133331,Fig. 14.,Reflectance spectra of four chosen materials in KSC data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-14-source-large.gif
2012,6133331,Fig. 15.,Kappa coefficients for four cases of scales selection (KSC data set).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-15-source-large.gif
2012,6133331,Fig. 16.,Comparison on runtime of each classifier utilizing different size of training samples. (a) Runtime of classifiers with KSC data set. (b) Runtime of classifiers with Indian Pine data set. (c) Runtime of classifiers with University of Pavia data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6222028/6133331/6133331-fig-16-source-large.gif
2012,6126049,Fig. 1.,"Illustration of the reanimation procedure. The vertex color represents the particle that is imposing the highest domination level at time
t
. The beige color denotes a vertex not dominated by any particle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-1-source-large.gif
2012,6126049,Fig. 2.,"Illustrative flowchart showing the main tasks of the stochastic dynamical system
ϕ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-2-source-large.gif
2012,6126049,Fig. 3.,"Consumed time for the variations of
N
¯
(t)
start to be insignificant
(ϵ=0.05)
.
λ=0.6
,
Δ=0.07
,
ω
min
=0
, and
ω
max
=1
. Each point in the trace is averaged over 10 realizations. The error bars represent the maximum and minimum values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-3-source-large.gif
2012,6126049,Fig. 4.,"Comparison of the theoretical and empirical distributions of three distinct vertices, namely,
v
4
,
v
11
, and
v
16
. One can see that the most probable domination level that the red particle will impose on
v
4
is approximately 0.88 with 34% probability, on
v
11
it is 0.53 with 47% probability, and on
v
16
it is 0.14 with 33% probability.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-4-source-large.gif
2012,6126049,Fig. 5.,"Parameter sensitivity analysis of the model. The network's configurations are the following,
V=1000
, there are four equally sized classes, and
⟨k⟩=16
. On average, each vertex shares 40% of its links with vertices belonging to different classes. Each point in the trace is averaged over 100 realizations. The error bars represent standard deviations. (a) Accuracy rate versus
λ
with
Δ=0.07
. (b) Accuracy rate versus
Δ
with
λ=0.6
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-5-source-large.gif
2012,6126049,Fig. 6.,"Illustration of an artificial semisupervised data classification process via particle competition. Snapshot of the network when (a)
t=0
, (b)
t=100
, (c)
t=200
, and (d)
t=300
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-6-source-large.gif
2012,6126049,Fig. 7.,Evolutional behavior of the average class domination level imposed by (a) blue particle on the blue (vertices 1 to 50) and red (vertices 51 to 100) classes and (b) same information for the red particle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-7-source-large.gif
2012,6126049,Fig. 8.,Semisupervised data classification of toy datasets. (a) and (b) Initial configuration and results for two banana-shaped classes. (c) and (d) Initial configuration and results for two Lithuanian classes. Data with the same color represent the same class. The black dots depict unlabeled vertices. The colored dots in the left panel represent previously labeled data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126049/6126049-fig-8-source-large.gif
2012,6015564,Fig. 1.,Experimental setting: the BSM1 WWTP schema and the agent (AgentV) that controls the DO setpont in the N-ammonia removal process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6015564/6015564-fig-1-source-large.gif
2012,6015564,Fig. 2.,General schema of an RL task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6015564/6015564-fig-2-source-large.gif
2012,6015564,Fig. 3.,"Weather during the first year. Notes: 0, dry weather; 1, rain weather; 2, storm weather.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6015564/6015564-fig-3-source-large.gif
2012,6015564,Fig. 4.,"Operation cost during the first year. We show
−OC
instead of
OC
in order to show the best behavior on top of the graph. Left: the city that imposes
f
0
=0.5
. Right: the city that imposes
f
0
=1.5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6015564/6015564-fig-4-source-large.gif
2012,6015564,Fig. 5.,"Aeration energy during the first year. Left: the city that imposes
f
0
=0.5
. Right: the city that imposes
f
0
=1.5
. Notice how the agent increases the energy costs in the city with higher fine in order to reduce the global
OC
(or increase
−OC
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6015564/6015564-fig-5-source-large.gif
2012,6015564,Fig. 6.,"Agent versus BSM1 detailed behaviors. Left: the city that imposes
f
0
=0.5
. Right: the city that imposes
f
0
=1.5
. First row, dry weather. Second row, rain weather. Third row, storm weather. Notice how the agent tries to set the DO setpoint as low as possible. However, in the city with a higher fine the agent cannot keep the DO setpoint so low so often. Notice that the higher the SO (because a higher DO setpoint), the higher will be the aeration energy consumed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6269137/6015564/6015564-fig-6-source-large.gif
2012,6220902,Fig. 1.,NAO robot during the L8 experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-1-source-large.gif
2012,6220902,Fig. 2.,Communicative gesture for the violation “Blocking.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-2-source-large.gif
2012,6220902,Fig. 3.,Ph. education exercise for the lower abdominal muscles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-3-source-large.gif
2012,6220902,Fig. 4.,"One-shot learning experiments: Mean and standard deviation of the MSE over the executed repetitions as a function of model size. Blue: GMR, Green: QGMR. (a) Blocking communicative gesture experiment. (b) Ph. education exercise experiment. (c) Drawing L8s experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-4-source-large.gif
2012,6220902,Fig. 5.,Multishot learning experiments: Visual representation of the end-effector data from both the original demonstration and the GMR and QGMR methods. (a) Original demonstration. (b) GMR. (c) QGMR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-5-source-large.gif
2012,6220902,Fig. 6.,"Multishot learning experiments: Mean and standard deviation of the MSE over the executed repetitions as a function of model size. Blue: GMR, Green: QGMR. (a) “Blocking” communicative gesture experiment. (b) Ph. education exercise experiment. (c) Drawing L8s experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-6-source-large.gif
2012,6220902,Fig. 7.,Goodness of fit graph: Black: Training set; Green: Testing set; Red: GMR; and Blue: QGMR. (a) “Blocking” communicative gesture. (b) Ph. education experiment. (c) L8s experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6361507/6220902/6220902-fig-7-source-large.gif
2012,5975166,Fig. 1,"Error rates on MNIST when using
n
labeled data, for various values of
μ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6155150/5975166/5975166-fig-1-source-large.gif
2012,5975166,Fig. 2,"From left to right: Original images, halftoned images, reconstructed images. Even though the halftoned images (center column) perceptually look relatively close to the original images (left column), they are binary. Reconstructed images (right column) are obtained by restoring the halftoned binary images. Best viewed by zooming on a computer screen.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6155150/5975166/5975166-fig-2-source-large.gif
2012,5975166,Fig. 3,"Results on various binary images publicly available on the Internet. No ground truth is available for these images from old computer games, and the algorithm that has generated these images is unknown. Input images are on the left. Restored images are on the right. Best viewed by zooming on a computer screen.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6155150/5975166/5975166-fig-3-source-large.gif
2012,6176215,Fig. 1.,"Our goal is to develop a highly accurate machine learning-based algorithm for meta-recognition. Meta-recognition is a postrecognition score analysis technique that predicts when a recognition algorithm is succeeding or failing. In the example for face recognition, we can see that a number of effects negatively impact recognition accuracy, including blur, illumination, expression, occlusion, shadow, and even changing hairstyles. However, if another more stable modality such as fingerprint is available during the recognition process, a correct recognition result can be achieved. The question then becomes: which modality is correct? Predictions produced by a meta-recognition system can select the correct modality in instances like this.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-1-source-large.gif
2012,6176215,Fig. 2.,"Overview of meta-recognition process. Based upon scores produced by some recognition system for a single input, a prediction of success or failure is made by meta-recognition system (in this paper, we primarily consider machine learning for this). Using these predictions, we can take action to improve the overall accuracy of the recognition system. For instance, if the recognition system has failed to recognize the input image, we can perform better fusion with other collected data by down-weighting or discarding the failing data, ignoring the data, or acquiring more data, giving the recognition system another attempt to recognize the input image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-2-source-large.gif
2012,6176215,Fig. 3.,Illustrations representing combination oriented fusion approaches. (a)Multi-algorithm fusion. (b) Multi-feature fusion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-3-source-large.gif
2012,6176215,Fig. 4.,"MRET curves for multiple biometric recognition algorithms from the BSSR1 data set generated using a machine learning-based meta-recognition system. Each plot also depicts mixed modality algorithm blending fusion for the best performing features calculated over the scores of BSSR1 Alg. LI. Comparison curves for basic thresholding over T-norm scores and statistical meta-recognition are shown in (a). Note that in (b), the
Δ
1,2,…10
feature is the best performing baseline, while the rest of the curves reflect the fusion of multiple sources of scores (a) BSSR1 Face Algorithm C (b) BSSR1 “Chimera” Face Algorithm G.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-4-source-large.gif
2012,6176215,Fig. 5.,MRET curves for multiple biometric recognition systems from the BSSR1 data set generated using a machine learning-based meta-recognition system. Each plot depicts enhanced meta-recognition with single threshold and multiple threshold fusion. Application of these decision-level fusion techniques significantly enhances prediction accuracy. (a) BSSR1 Face Algorithm G. (b)BSSR1 “Chimera” Face Algorithm C.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-5-source-large.gif
2012,6176215,Fig. 6.,"MRET curves for the Good, Bad, and Ugly face recognition challenge problem. Each plot shows results for same two baseline features, as well as single threshold fusion over those features. There is a distinct difference in prediction accuracy between each partition of the GBU set, with (a) Good producing the most accurate results, and (c) Ugly producing the least accurate results (a) Good. (b) Bad. (c) Ugly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-6-source-large.gif
2012,6176215,Fig. 7.,MRET curves for multiple object recognition systems generated using a machine learning meta-recognition system. Each plot depicts enhanced meta-recognition with single threshold and individual thresholds fusion. (a) SIFT. (b)CBIR. Algorithm LCH.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-7-source-large.gif
2012,6176215,Fig. 8.,"MRET curves for the BSSR1 “Chimera” Face Algorithm G data generated using different machine learning-based meta-recognition classifiers. As described in Section III-A2, any supervised learning approach can be used for meta-recognition. These curves, which show that all of the evaluated approaches are close in accuracy, provide evidence for that claim. This experiment allows us to conclude that it is the features capturing the shape of the extreme scores, which provides for effective machine-learning Meta-recognition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-8-source-large.gif
2012,6176215,Fig. 9.,"Data generation process for experiment with synthetic data comparing statistical and machine learning-based meta-recognition. Our goal is to produce a realistic sampling of “scores” that conforms to the portfolio model described in [10]. Series of Gaussian distributions are generated to represent different gallery classes, each with a randomly generated mean and standard deviation to force intraclass dependency (bottom of figure). From each of these distributions, the extremum (“best score”) is selected, and placed into a sorted sequence. This sequence represents the tail of overall distribution of scores, which we can use for machine learning feature generation or Weibull fitting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-9-source-large.gif
2012,6176215,Fig. 10.,"MRET curves showing Weibull-based statistical meta-recognition accuracy [10] versus machine learning-based meta-recognition accuracy for a controlled synthetic data set reflecting the portfolio model. Machine learning-based algorithm shows a clear advantage over the pure statistical algorithm when a consistent gallery is considered for training and testing. When the machine learning classifier is trained and tested using different galleries, performance is approximate to that of the statistical algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6236303/6176215/6176215-fig-10-source-large.gif
2012,6175953,Fig. 1.,Examples of oil sand images containing large lumps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-1-source-large.gif
2012,6175953,Fig. 2.,Examples of oil sand images with no large lump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-2-source-large.gif
2012,6175953,Fig. 3.,Gaussians and DoG in 1-D and corresponding DoG in 2-D.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-3-source-large.gif
2012,6175953,Fig. 4.,Multiscale representation of an image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-4-source-large.gif
2012,6175953,Fig. 5.,Image frame containing (a) large lump and (b) no large lump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-5-source-large.gif
2012,6175953,Fig. 6.,DoG responses for the image in Fig. 5(a).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-6-source-large.gif
2012,6175953,Fig. 7.,DoG responses for the image in Fig. 5(b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-7-source-large.gif
2012,6175953,Fig. 8.,Region of interest for large lump detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-8-source-large.gif
2012,6175953,Fig. 9.,"Example large lump images from (a) daylight, (b) nightlight, and (c) snow datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-9-source-large.gif
2012,6175953,Fig. 10.,Large lump event sequence from daylight dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-10-source-large.gif
2012,6175953,Fig. 11.,Alignment scores for different kernels at different scales.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-11-source-large.gif
2012,6175953,Fig. 12.,Kernel weights selected by LMKL for different data sets. (a) Daylight. (b) Nightlight. (c) Snow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-12-source-large.gif
2012,6175953,Fig. 13.,"Recall-precision graph for daylight data set: the value of recall and precision versus
ϵ+
and
ϵ−
(
x
-axis), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-13-source-large.gif
2012,6175953,Fig. 14.,"Recall-precision graph for nightlight data set: the value of recall and precision versus \epsilon+
and \epsilon-
( x
-axis), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-14-source-large.gif
2012,6175953,Fig. 15.,"Recall-precision graph for snow data set: the value of recall and precision versus \$\epsilon+\$ and \$\epsilon-\$ ( \$x\$ -axis), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-15-source-large.gif
2012,6175953,Fig. 16.,Comparison of proposed LMKL with different MKL at the fixed precision values for different data sets. (a) LMKL versus GMKL. (b) LMKL versus SKM. (c) LMKL versus LSMKL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6238420/6175953/6175953-fig-16-source-large.gif
2012,6184354,Fig. 1.,Typical structure of a MISO ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-1-source-large.gif
2012,6184354,Fig. 2.,Comparisons of different hidden layer neurons for ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-2-source-large.gif
2012,6184354,Fig. 3.,Framework of ELM-bootstrap for MCPs forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-3-source-large.gif
2012,6184354,Fig. 4.,"Load forecasting results of QLD market (June 1, 2006–May 31, 2007).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-4-source-large.gif
2012,6184354,Fig. 5.,ln (MCPs) versus system demand.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-5-source-large.gif
2012,6184354,Fig. 6.,Average results of MCP forecast by ELM in winter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-6-source-large.gif
2012,6184354,Fig. 7.,Yearly RMSE and MAPE of forecasted QLD MCPs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6334487/6184354/6184354-fig-7-source-large.gif
2012,6200298,Fig. 1.,Ensemble structure of the proposed hybrid HS-ELM approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6239656/6200298/6200298-fig-1-source-large.gif
2012,6200298,Fig. 2.,Outline of the hybrid HS-ELM approach proposed in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6239656/6200298/6200298-fig-2-source-large.gif
2012,6200298,Fig. 3.,Example of the structure of an ELM network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6239656/6200298/6200298-fig-3-source-large.gif
2012,6158646,Fig. 1.,"Two moons data set: the classifiers for LapSVM and EMR-SVM-24G for different parameters of
t
and
τ
, respectively. Labeled samples are highlighted.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6158646/6158646-fig-1-source-large.gif
2012,6158646,Fig. 2.,"The manifold combination coefficient
μ
learned by EMR-SVM over 10 one-versus-rest classification tasks, and the classification error rate of LapSVM using each graph Laplacian over each task.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6158646/6158646-fig-2-source-large.gif
2012,6158646,Fig. 3.,"Parameter sensitivity comparison of LapSVM
t
and EMR-SVM
γ
R
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6158646/6158646-fig-3-source-large.gif
2012,6158646,Fig. 4.,"False negative and false positive rates (subfigures from top to bottom) of different classification methods: baseline [23], TSVM, LapSVM, EMR-SVM-24G, and EMR-SVM-72G over each segment of ACB, AnF, and FDF sites (subfigures from left to right), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6158646/6158646-fig-4-source-large.gif
2012,6042869,Fig. 1.,"Machine learning paradigms. Classification attempts to find a separator between items from two classes. (a) The decision curve (dashed line) divides the hearts from the clubs. This paper treats the multiple-instance classification problem, where bags (ellipses) of items are classified. (b) Bags having at least one item to the left of the decision curve are classified as hearts. All items in a bag classified as clubs must be to the right of the decision curve. Under ranking, the ranking function (arrow) orders items (represented as numbers that indicate rank ordering) as in (c). Under multiple-instance ranking, bags (ellipses) are ranked. The highest-ranked item determines the ranking for its whole bag, as in (d). This paper deals with the special case where rank ordering is only known within clusters of bags called boxes (rectangles). (e) Bags in each box are labeled preferred (P) or undesirable (U). The ranking function learns, for each box, the preferred bag.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6042869/6042869-fig-1-source-large.gif
2012,6042869,Fig. 2.,"Nonsmoothness and nonconvexity of MIRank loss. This figure was generated using the
CYP2B6
2009
data set discussed later in this paper. Each point on panels (a)-(d) demonstrates the effect on models generated from two features denoted
A
and
B
. (a) The MIRank loss is shown as a contour plot. The numerical gradients of the loss with respect to (b)
w
A
and (c)
w
B
are shown as heat maps—ridges in these indicate the presence of nonsmoothness. For each point, the numerical Hessian matrix is computed. (d) Positive-semidefinite areas appear in white, indefinite areas appear in orange, and negative-semidefinite areas appear in black. Nonwhite areas indicate nonconvexity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6042869/6042869-fig-2-source-large.gif
2012,6042869,Fig. 3.,"Affine approximation of nonsmooth nonconvex function
f
at some points
x
either under or overestimates
f
at stability center
y
. The linearizations are shown as dotted lines and the linearization errors
α
+
>0
or
α
−
<0
appear as dashed lines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6042869/6042869-fig-3-source-large.gif
2012,6042869,Fig. 4.,"Ranking accuracy increases with sample size. Cross-validation was performed on
CYP3A4
2009
, for various training set sizes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6042869/6042869-fig-4-source-large.gif
2012,6042869,Fig. 5.,"Model training times. The dashed purple curve in (a), (c) denotes
MIRank
bilinear
model training time in CPU seconds with increasing sample size (number of boxes or bags, as the case may be). The full green curve straddling the horizontal axis denotes the same for
MIRank
bundle
. Empirical linear scalability for
MIRank
bundle
is better exhibited in (b), (d). Sample size is measured in number of bags. Clearly, the empirical speed for
MIRank
bundle
is much less than for
MIRank
bilinear
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6186729/6042869/6042869-fig-5-source-large.gif
2012,6119225,Fig. 1.,Line fitting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6145493/6119225/6119225-fig-1-source-large.gif
2012,6119225,Fig. 2.,"Convergence of
DirectionCosine(k)
in line fitting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6145493/6119225/6119225-fig-2-source-large.gif
2012,6119225,Fig. 3.,"Surface fitting (a) ellipsoid:
2
x
2
+0.5
y
2
+
z
2
=1
and (b) noise-disturbed data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6145493/6119225/6119225-fig-3-source-large.gif
2012,6119225,Fig. 4.,"Convergence of
DirectionCosine(k)
in surface fitting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6145493/6119225/6119225-fig-4-source-large.gif
2012,6133292,Fig. 1.,"Image examples from 10 Flickr groups. Each row corresponds to a group. These groups are: aquariums, cars, Christmas, sunset, skyscrapers, boat, bonsai, food, fireworks, and penguin.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-1-source-large.gif
2012,6133292,Fig. 2.,"The average AP values over the 103 Flickr categories. The
X
-axis denotes different percentages of training images (of each Flickr group) used at each time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-2-source-large.gif
2012,6133292,Fig. 3.,"The AP scores of the 103 Flickr groups (categories). For each category, there are 20,900 held-out examples (500 positive). The five groups which get the highest AP values are: laptop lunch, fireworks, pandas, socks, and moon; the five groups which get the lowest AP values are: love, art, trees, ice, and light.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-3-source-large.gif
2012,6133292,Fig. 4.,"This figure shows the nearest neighbor images found for each of the four image queries. For each query image, the left column shows the query itself, the center column shows the 25 nearest neighbor images found with visual features and the euclidean distance, the right column shows the 25 nearest neighbor images found with our Flickr prediction features. The rank is from left to right, from top to bottom.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-4-source-large.gif
2012,6133292,Fig. 5.,"The AP values of using different numbers of Flickr groups. For each number, we repeat 15 times to randomly select a subset of groups. Both mean and standard deviation values are shown in the figure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-5-source-large.gif
2012,6133292,Fig. 6.,The average AP values with three rounds of feedback. The red line shows the results with Flickr prediction features and the blue line shows the results with visual features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-6-source-large.gif
2012,6133292,Fig. 7.,"The left column shows the query image, the center column shows the 45 nearest neighbors found with the Flickr prediction features; the five negative images (in red), and five positive images (in green) are selected for feedback; after one round of feedback, we get the 50 nearest neighbors shown in the right column.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-7-source-large.gif
2012,6133292,Fig. 8.,"Six pairs of similar Corel images. The text shows the top five Flickr groups which both of the images are likely to belong to. The value for each group in the parentheses is
100×p(group∣image1)p(group∣image2)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-8-source-large.gif
2012,6133292,Fig. 9.,"The Corel images which are most relevant to the query “airplane,” obtained by one-versus-all classification with our SIKMA method, trained on the Flickr airplane group. Images are ranked according to their classifier score.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6304885/6133292/6133292-fig-9-source-large.gif
2012,6069863,Fig. 1.,"Learning flowchart for Plasso. The data feature is first distributed to each machines according to their feature index, and PICF is applied to the data matrix for preprocess. PICF together with SMW is applied to optimize the model parameters iteratively, until the convergence condition is met.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-1-source-large.gif
2012,6069863,Fig. 2.,Performance of different algorithms on each concept. (a) Linear. (b) Kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-2-source-large.gif
2012,6069863,Fig. 3.,Learning time cost of different algorithms on each concept. (a) Linear. (b) Kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-3-source-large.gif
2012,6069863,Fig. 4.,Speedup versus process number.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-4-source-large.gif
2012,6069863,Fig. 5.,Learning time versus dataset size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-5-source-large.gif
2012,6069863,Fig. 6.,"Speedup versus rank ratio
h
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-6-source-large.gif
2012,6069863,Fig. 7.,"Performance versus rank ratio
h
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-7-source-large.gif
2012,6069863,Fig. 8.,"Learning efficiency versus trade-off parameter
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-8-source-large.gif
2012,6069863,Fig. 9.,"Detection performance versus trade-off parameter
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-9-source-large.gif
2012,6069863,Fig. 10.,Ratio of the time cost for different learning steps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6130620/6069863/6069863-fig-10-source-large.gif
2012,6151827,Fig. 1.,"Time course of the video sequence. For 1 s, a fixation point is shown. For at least 2 s, the undistorted video is presented. At a random time point (uniformly distributed between 2 and 6 s), the video quality drops instantaneously. On the right, undistorted and distorted (highest quality loss QC10) frames are shown exemplarily.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-1-source-large.gif
2012,6151827,Fig. 2.,Experimental setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-2-source-large.gif
2012,6151827,Fig. 3.,"EEG single-trial classification scheme. First, the raw EEG signal is bandpass filtered. Then, QC0+ trials (where the superscript “+” indicates the subset of trials that were correctly labeled by the subject, i.e., correct rejections) are split into two equisized sets in an even–odd manner, as indicated by the gray-and-white horizontal bars. One of these two sets and the QCmax+ trials (again, “+” indicates correctly labeled trials, in this case hits) are used to train the LDA filter. To this end, the
sgn-
r
2
between QCmax+ and QC0+ is used to select a discriminative interval [a] on a basis of which an LDA filter is determined [b]. Then, the sets of quality change trials QC-I, QC-II, QC-III, and QC-IV (depicted as QCI–IV) and the second set of QC0+ trials are projected to virtual channels using the LDA filter [c]. For each QC level and for hits and misses separately, two intervals that discriminate best between quality changes and undistorted trials are selected [d]. This yields 2-D features that are finally used to classify single trials using LDA [e].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-3-source-large.gif
2012,6151827,Fig. 4.,"Scalp plots depicting spatial distribution as a top view on the head, with nose pointing upwards and crosses marking the electrode positions. Left:
sgn-
r
2
values for QCmax+ against QC0+ for subject S1. The spatial distribution is similar to the P3 component, suggesting that class differences are mostly due to the P3. Right: LDA filter trained on the two classes. If the channels were uncorrelated, the LDA filter would be proportional to the difference of the class means. If the noise is not substantial (as it is the case for our narrowband filtered data), the spatial distribution roughly consists of dipoles along the gradient of the
sgn-
r
2
scalp plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-4-source-large.gif
2012,6151827,Fig. 5.,"EEG data from hit trials of stimulus-level QC-IV of subject S1. First column: Raw EEG data at channel CPz. Second column: Bandpass-filtered EEG data at channel CPz. Third column: LDA-projected EEG data. Rows 1–3: Single-trial data from three hit trials of subject S1. Last row: ERP and standard deviation over all hit trials of QC-IV for subject S1. The ERP peak is similar for all three data types, but the standard deviation is substantially reduced by each processing step. Further, the processing clearly increases the prominence of the P3 component in single trials. This illustrates that the filtering steps are beneficial for the enhancement the P3 component and, therefore, classification performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-5-source-large.gif
2012,6151827,Fig. 6.,"Psychometric function fits from the psychophysical data (small circles), each function represents one subject. Horizontal lines depict the 95% confidence intervals of the fit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-6-source-large.gif
2012,6151827,Fig. 7.,"Grand average ERP plots for the different stimulus levels. Top left: ERP for undistorted trials and the different quality changes at channel CPz. Top right: ERP for a selected stimulus level (QC-III) for subject S1, subdivided in hits (wherein the quality change was perceived) and misses (wherein the quality change was not perceived). Bottom: Scalp topographies for all channels. Each circle depicts a top view of the head, with the noise pointing upwards. Colors code the mean voltage for the time interval from 400–700 ms after quality change. ERP plots for single subjects can be found in the supplementary material.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-7-source-large.gif
2012,6151827,Fig. 8.,"Relationship between neurophysiological and behavioral measures. Left: Amplitude and latency of the P3 component for QCmax shows a significant linear correlation across subjects (
r=−0.72
,
p<0.05
, left plot). Right: Within subjects and across stimulus levels, amplitude and detection rate are positively correlated (
r=0.84±0.15
; right plot).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-8-source-large.gif
2012,6151827,Fig. 9.,"Classification results for all subjects (S1–S8). Green bars show the classification performance (AUC value) of hits against QC0+; red bars depict misses against QC0+. One or two asterisks denote the significance level of the classification outcome in a Wilcoxon rank-sum test (
p<0.05
or
p<0.01
, respectively). The gray curve depicts the detection rate over stimulus levels. Note that the detection rate and the classification performance have no direct connection as the classification is done separately on hit and miss trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-9-source-large.gif
2012,6151827,Fig. 10.,"ERPs of LDA-prefiltered data for subjects S1, S2, and S3. The blue dotted line shows the ERP over all trials of the stimulus level, and the black dash–dotted line shows the ERP of the QC0+ undistorted trials. The green and red lines give the ERP over the trials of the stimulus level, subdivided into trials that are classified as hits (green) and misses (red) by the classifier. The gray-shaded areas depict the time intervals from which classification features are computed. Rows are the results for different classification runs. Top row: Hits versus QC0+ for QC-III. Middle row: Misses versus QC0+ for QC-III. These are the three classifications on miss trials that are statistically significant. Bottom row: Misses versus QC0+ for the lowest QC-level of the subject. Note the different scaling of the plots.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6151827/6151827-fig-10-source-large.gif
2012,6301810,Fig. 1.,"Histogram of log-concentration of methane
tt CH
tt 4
among samples taken from faulty (black) and normal operating (gray) network transformers (utility data from Section V-C).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6301804/6301810/6301810-fig-1-source-large.gif
2012,6301810,Fig. 2.,"Comparison of six regression or classification techniques on a simplified 2-D version of the Duval dataset consisting of log-transformed and standardized values of DGA measures for
tt CH
tt 4
and
tt C
tt 2
tt H
tt 4
. There are 167 datapoints: 117 “faulty” DGA measures (marked as red or magenta crosses) and 50 “normal” ones (blue or cyan circles). Since the training datapoints are not easily separable in 2-D, the accuracy and area under the curve (see paper) on the training set are generally not 100%. The test data points consist in the entire DGA values space. The output of the six decision functions goes from white (
y
¯
=1
, meaning “no impending failure predicted”) to black (
y
¯
=
0, meaning “failure is deemed imminent”); for most classification algorithms, we plot the continuously valued probability of having
y
¯
=
1 instead of the actual binary decision (
y
¯
=
0 or
y
¯
=
1). The decision boundary (at
y
¯
=
0.5) is marked in green. Note that we do not know the actual labels for the test data—this figure provides instead with an intuition of how the classification and regression algorithms operate.
k
-Nearest Neighbors (KNN, top left) partitions the space in a binary way, according to the Euclidian distances to the training datapoints. Weighted kernel regression (WKR, bottom middle) is a smoothed version of KNN, and local linear regression (LLR, top middle) performs linear regression on small neighborhoods, with overall nonlinear behavior. Neural networks (bottom left) cut the space into multiple regions. Support vector machines (SVMs, right) use only a subset of the datapoints (so-called support vectors, in cyan and magenta) to define the decision boundary. Linear kernel SVMs (top right) behave like logistic regression and perform linear classification, while Gaussian kernel SVMs (bottom right) behave like WKR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6301804/6301810/6301810-fig-2-source-large.gif
2012,6301810,Fig. 3.,"The 3-D plots of DGA samples from the utility dataset, showing log concentrations of acetylene
tt C
tt 2
tt H2
versus ethylene
tt C
tt 2
tt H
tt 4
and ethane
tt C
tt 2
tt H
tt 6
. The color code of the data point labels goes from green/light (failure at a later date,
y=1
) to red/dark (impending failure,
y=
0).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6301804/6301810/6301810-fig-3-source-large.gif
2012,6301810,Fig. 4.,"Comparison of classification and regression techniques on the proprietary, utility dataset. The faulty transformer prediction problem is considered as a retrieval problem, and the ROC is computed for each algorithm as well as its associated AUC. The learning experiments were repeated 25 times and we show the average ROC curves over all experiments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6301804/6301810/6301810-fig-4-source-large.gif
2012,6236010,Fig. 1.,Artificial dataset with a linearly separable subclass structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6237430/6236010/6236010-fig-1-source-large.gif
2012,6197724,Fig. 1.,SVM separating hyperplane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-1-source-large.gif
2012,6197724,Fig. 2.,FPGA architecture of the SVM classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-2-source-large.gif
2012,6197724,Fig. 3.,"Memory access timing flow of the proposed FPGA architecture. Read and write RAM operations are abbreviated to “rd” and “wr,” respectively. The intermediate results are noted as “temp.”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-3-source-large.gif
2012,6197724,Fig. 4.,Hypertile of the heterogeneous SVM classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-4-source-large.gif
2012,6197724,Fig. 5.,"Heterogeneous architecture, FPGA design flow.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-5-source-large.gif
2012,6197724,Fig. 6.,"Using the desired error rate to choose the thresholds
C
cn
and
C
cp
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-6-source-large.gif
2012,6197724,Fig. 7.,Training and classification flow of the cascade classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-7-source-large.gif
2012,6197724,Fig. 8.,"Proposed cascade SVM classifier
C
C
FIT
, in cases where the
N
SV
set fits in the FPGA internal memories.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-8-source-large.gif
2012,6197724,Fig. 9.,"Design flow of the
C
C
FIT
cascade classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-9-source-large.gif
2012,6197724,Fig. 10.,"Realizing the cascade SVM classifier
C
C
RECONF
through FPGA reconfiguration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-10-source-large.gif
2012,6197724,Fig. 11.,(a) Parallelization factor scaling as a function to dimensionality and (b) bit-precision per feature for homogeneous problems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-11-source-large.gif
2012,6197724,Fig. 12.,Histograms of synthetic datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-12-source-large.gif
2012,6197724,Fig. 13.,Achieved speed-up for synthetic datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-13-source-large.gif
2012,6197724,Fig. 14.,Histograms of real-world datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-14-source-large.gif
2012,6197724,Fig. 15.,Achieved parallelization factor for real-world datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-15-source-large.gif
2012,6197724,Fig. 16.,Function of the LP error rate to the HP throughput reduction \$a\$ .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-16-source-large.gif
2012,6197724,Fig. 17.,Choosing the thresholds of the LP classifier through the kernel evaluation histogram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-17-source-large.gif
2012,6197724,Fig. 18.,Performance ratio \$CC_{{\rm RECONF}}/CC_{{\rm FIT}}\$ for a homogeneous classification dataset on the Altera's Stratix III EP3SE260 device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-18-source-large.gif
2012,6197724,Fig. 19.,Performance ratio \$CC_{{\rm RECONF}}/CC_{{\rm FIT}}\$ function to throughput reduction \$a\$ on the Altera's Stratix III EP3SE260 device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6214687/6197724/6197724-fig-19-source-large.gif
2012,6228541,Fig. 1.,SRM principle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6228541/6228541-fig-1-source-large.gif
2012,6228541,Fig. 2.,Hypothesis spaces with different centroids.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6228541/6228541-fig-2-source-large.gif
2012,6126032,Fig. 1.,"Illustration of intraclass diversity and interclass correlation of “bridges.” The green line connects two images from the same category, whereas the red line connects images from different categories.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-1-source-large.gif
2012,6126032,Fig. 2.,Diagram of GS-MKL for object recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-2-source-large.gif
2012,6126032,Fig. 3.,"Three paradigms of object recognition using (a) Canonical MKL, (b) GS-MKL, and (c) SS-MKL. In the figure, images with green bounding boxes are positive samples, whereas those with red bounding boxes are negative samples for “bridge.” Note that SS-MKL will learn two sets of kernel weights even for two images with quite similar appearance (e.g.,
x
1
and
x
2
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-3-source-large.gif
2012,6126032,Fig. 4.,"Hierarchies of bicycle and aero plane from Pascal VOC2007. For better viewing, we demonstrate samples from the leaves only.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-4-source-large.gif
2012,6126032,Fig. 5.,"Performance comparison of different MKL methods (i.e., MKL-ES, UMK, MKL, SS-MKL, and GS-MKL) on Pascal VOC2007. The resulting MAPs are 50.0, 52.5, 54.5, 57, and 63.4, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-5-source-large.gif
2012,6126032,Fig. 6.,"Comparison of different multikernel combinations. x-Axis corresponds to the top-
K
nearest neighbor of a query sample. y-Axis denotes the mean percentage of the corresponding nearest neighbor being the same class of the query sample.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-6-source-large.gif
2012,6126032,Fig. 7.,"Performance of GS-MKL and other recent methods on Caltech101 data set. GS-MKL: number of training samples (average recognition rate), 10 (66.5), 15 (74.5), 20 (81.0), 25 (83.5), and 30 (84.4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6183042/6126032/6126032-fig-7-source-large.gif
2012,6179986,Fig. 1.,"(Top) Example of a multigraph representing the most general case, where no additional properties of relations are assumed. (Bottom) Examples of eight different types of relations in a graph of cardinality three. The following relational properties are illustrated: (C) crisp, (G) graded, (R) reciprocal, (S) symmetric, (T) transitive, and (I) intransitive. For the reciprocal relations, (I) refers to a relation that does not satisfy weak stochastic transitivity, while (T) is showing an example of a relation fulfilling strong stochastic transitivity. For the symmetric relations, (I) refers a relation that does not satisfy
T
-transitivity w.r.t. the Łukasiewicz t-norm
T
L
(a,b)=max(a+b−1,0)
, while (T) is showing an example of a relation that fulfills
T
-transitivity w.r.t. the product t-norm
T
P
(a,b)=ab
. See Section IV for formal definitions of transitivity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6362253/6179986/6179986-fig-1-source-large.gif
2012,6179986,Fig. 2.,"Comparison of the ordinary Kronecker product pairwise kernel
K
Φ
⊗
and the symmetric Kronecker product pairwise kernel
K
Φ
⊗S
on the Newsgroups dataset. The MSE is shown as a function of the training set size.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6362253/6179986/6179986-fig-2-source-large.gif
2012,6123212,Fig. 1.,"Comparison of the consumed time of the proposed technique and the modularity algorithm. (a) and (b)
⟨k⟩=16
. (c) and (d)
⟨k⟩=0.15V
.
M=4
communities. (a) and (c)
z
out
/⟨k⟩=0.2
. (b) and (d)
z
out
/⟨k⟩=0.4
. Each point in the trace is averaged over 20 realizations. The error bars represent the maximum and minimum values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-1-source-large.gif
2012,6123212,Fig. 2.,Simple network with 11 vertices distributed into two unbalanced communities. Vertices 1 to 4 compose a community (cyan or light gray) and vertices 5 to 11 comprise another community (red or dark gray).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-2-source-large.gif
2012,6123212,Fig. 3.,"Behavior of
N
¯
(t)
as the system progresses in time
(t)
. The input network is depicted in Fig. 2. (a) Average domination levels of particle 1 on vertices 1 to 4 and on vertices 5 to 11. (b) Average domination levels of particle 2 on the same group of vertices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-3-source-large.gif
2012,6123212,Fig. 4.,"Illustration of an artificial community detection process via particle competition. The total number of nodes is
V=128
,
M=4
equally sized communities,
⟨k⟩=16
, and
z
out
/⟨k⟩=0.3
.
K=4
particles are inserted in a random manner. Snapshot of the network when (a)
t=0
, (b)
t=300
, (c)
t=800
, and (d)
t=1700
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-4-source-large.gif
2012,6123212,Fig. 5.,"Illustration of an artificial community detection process via particle competition. The total number of nodes is
V=128
,
M=4
equally sized communities,
⟨k⟩=16
, and
z
out
/⟨k⟩=0.3
.
K=4
particles are purposefully inserted in the same community. Snapshot of the network when (a)
t=0
, (b)
t=300
, (c)
t=800
, and (d)
t=2000
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-5-source-large.gif
2012,6123212,Fig. 6.,"Determination of the optimal number of particles
K
in random clustered networks. (a) Correct community detection rate reached by the algorithm versus
K
. Networks with 5 (160 vertices), 10 (320 vertices), and 15 (480 vertices) communities are used. (b)
⟨R(t)⟩
versus
K
. The corresponding
⟨R(t)⟩
for the networks in (a). The constructed networks have the following characteristics:
⟨k⟩=15
and
z
out
/⟨k⟩=0.3
. Results are averaged over 20 simulations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-6-source-large.gif
2012,6123212,Fig. 7.,"Determination of the optimal number of particles
K
(the actual number of clusters) in real-world data sets. In all these simulations, we have used the
k
-nearest neighbor network formation technique with
k=4
. The number of classes that each data set originally possesses is: 1) Iris: 3; 2) credit approval: 2; 3) vowel: 11; and 4) letter recognition: 26. Results are averaged over 20 simulations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-7-source-large.gif
2012,6123212,Fig. 8.,"GN's community detection benchmark results obtained by the proposed technique. The constructed networks' configurations are
V=128
,
M=4
equally sized communities and
⟨k⟩=16
. For each point in the trajectory, 200 independent runs are performed. The error bars indicate the standard deviations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-8-source-large.gif
2012,6123212,Fig. 9.,"Comparison of three community detection algorithms using the benchmark of Lancichinetti et al. The modularity method, the algorithm developed in Quiles et al. (see [30]), and the proposed technique are utilized. The constructed networks' configurations are
V=10000
and
⟨k⟩=15
.
γ=2
. (a)
β=1
. (b)
β=2
. For each point in the trajectory, 30 independent runs are performed. The error bars indicate the standard deviations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-9-source-large.gif
2012,6123212,Fig. 10.,Community detection result of the Zachary's karate club network by using the proposed method. The red (dark gray) and blue (gray) colors denote the detected communities. Only the yellow or light gray vertex (vertex 3 in the original database) is incorrectly grouped.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-10-source-large.gif
2012,6123212,Fig. 11.,"Data clustering of toy data sets with different cluster shapes. Data with the same color represent the same cluster. The left and right panels report the result obtained by the optimized K-means and the proposed technique, respectively. (a) and (b) Banana-shaped clusters. (c) and (d) Highleyman clusters. (e) and (f) Lithuanian clusters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6123212/6123212-fig-11-source-large.gif
2012,6293841,Fig. 1.,"Scalar image simplification on a gray level image with different parameters values of
α,β
and different weight functions in local and non-local configurations, with 5 or 20 iterations. First row shows results for
α=1
,
β=0
(
Δ
∞
)
. Second row shows results for
α=0.5
,
β=0.5
. Finally, third column shows results for
α=0
,
β=1
(
Δ
2
)
. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-1-source-large.gif
2012,6293841,Fig. 2.,"Textured surface simplification with different parameters values of
α,β
and different weight functions. The graph is local and built from the triangulated mesh structure. First columns shows results using a constant mesh functions. Second column shows results with an adapted weight function. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-2-source-large.gif
2012,6293841,Fig. 3.,"Illustration of non-local morphological erosion and dilation. First and fourth columns show results for a 4-adjacency grid graph, with a constant weight (respectively 5 an 10 iterations). Second and fifth columns show results for a 4-adjacency grid graph, with image dependant weight (respectively 5 an 10 iterations). Finally, third and sixth columns show non-local graph using 15 × 15 neighborhood window and 5 × 5 patches (respectively 5 an 10 iterations). See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-3-source-large.gif
2012,6293841,Fig. 4.,"Non-local morphology (
NLD
and
NLE
) on a textured surface, using different weight functions in local and non-local configurations. The graph is local and built from the triangulated mesh structure. First columns shows results using a constant mesh functions. Second column shows results with an adapted weight function. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-4-source-large.gif
2012,6293841,Fig. 5.,"Semi-supervised natural image segmentation with
Δ
w,α,β
. First column presents results with a local 4-adjacency grid graph where each pixel is characterized by it's color feature vector. Second columns presents non-local results obtained with a larger neighborhood (each pixel
u
is linked with any pixel in a 11 × 11 window centered on
u
) and pixels are characterized by patches of size 3 × 3. In both cases, results are provided for
α=1
,
β=0
(
Δ
∞
)
,
α=0.5
,
β=0.5
and
α=0
,
β=1
. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-5-source-large.gif
2012,6293841,Fig. 6.,"Comparison between our approach and other local methods for semi-supervised image segmentation. Images come from the grabcut database and seeds are eroded versions of ones provided in the database. For each image, we provide the best result beyond all combinations of
α
and
β
and local or non-local configurations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-6-source-large.gif
2012,6293841,Fig. 7.,"Semi supervised natural image segmentation using a partitioned representation of the image: (a) Original image with superimposed two seeds (green and red). (b) Partitioned version of the image (given by the red boundaries). The
k
-ERAG is illustrated for a single vertex with local (green) and non-local (black) edges. (c) Segmentation result (with
α=1
,
β=0
(
Δ
∞
)
. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-7-source-large.gif
2012,6293841,Fig. 8.,"Semi supervised data clustering with
Δ
α,β
. See text for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-8-source-large.gif
2012,6293841,Fig. 9.,"Natural image inpainting, with different parameters values of
α,β
in local and non-local configurations. Left column presents results with a local 4-adjacency grid graph. Right column presents non-local results, using a 31 × 31 neighborhood window and 15 × 15 patches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-9-source-large.gif
2012,6293841,Fig. 10.,"Illustration of natural image inpainting using non-local graph configuration, using a 25 × 25 window and 5 × 5 patches. (a) Original image. (b)Missing information (black). (c) Image reconstruction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6329953/6293841/6293841-fig-10-source-large.gif
2012,6298965,Fig. 1.,"Training samples and the differences of the images caused by
ε
-draggings. (a) 60 training samples in three classes. (b) First components of the differences caused by
ε
-draggings. (c) Second components of the differences caused by
ε
-draggings. (d) Third components of the differences caused by
ε
-draggings.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6331030/6298965/6298965-fig-1-source-large.gif
2012,6298965,Fig. 2.,"Group I: The classification accuracy with different numbers of selected features. The final classification accuracy is calculated as the average of the 20 trials. in total, eight different numbers of selected features are evaluated, as indicated by the horizontal axis. To be clear, for each data set, two figures are employed to illustrate the accuracy and the standard deviation. Specifically, the first figure shows the results obtained by MI, Gini, T-test, FS, and our method, while the second figure illustrates those obtained by LS, ReliefF, mRMR, SBMLR, and our method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6331030/6298965/6298965-fig-2-source-large.gif
2012,6298965,Fig. 3.,"Group II: The classification accuracy with different numbers of selected features. The final classification accuracy is calculated as the average of the 20 trials. in total, eight different numbers of selected features are evaluated, as indicated by the horizontal axis. To be clear, for each data set, two figures are employed to illustrate the accuracy and the standard deviation. Specifically, the first figure shows the results obtained by MI, Gini, T-test, FS, and our method, while the second figure illustrates those obtained by LS, ReliefF, mRMR, SBMLR, and our method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6331030/6298965/6298965-fig-3-source-large.gif
2012,6189059,Fig. 1.,Visualization of the interaction index for the car evaluation data (numerical values are shown in terms of level of gray; values on the diagonal are set to 0). Groups of related criteria are indicated by the black lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6362253/6189059/6189059-fig-1-source-large.gif
2012,6189059,Fig. 2.,"Visualization of the interaction index for datasets CLR-1 (left) and CLR-7 (right). The reduction of prediction error is about twice as much as for CLR-7. For the ease of representation, the values on the diagonal are set to 0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6362253/6189059/6189059-fig-2-source-large.gif
2012,6104061,Fig. 1.,"Given a single bounding box defining the object location and extent in the initial frame (left), our system tracks, learns, and detects the object in real time. the red dot indicates that the object is not visible.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-1-source-large.gif
2012,6104061,Fig. 2.,The block diagram of the tld framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-2-source-large.gif
2012,6104061,Fig. 3.,The block diagram of the p-n learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-3-source-large.gif
2012,6104061,Fig. 4.,"Evolution of errors during p-n learning for different eigenvalues of matrix m. the errors are decreasing (left), static (middle), or grow (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-4-source-large.gif
2012,6104061,Fig. 5.,Performance of a detector as a function of the number of processed frames. the detectors were trained by synthetic p-n experts with a certain level of error. the classifier is improved up to error 50 percent (black); higher error degrades it (red).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-5-source-large.gif
2012,6104061,Fig. 6.,Illustration of a scanning grid and corresponding volume of labels. red dots correspond to positive labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-6-source-large.gif
2012,6104061,Fig. 7.,Illustration of the examples output by the p-n experts. the third row shows error compensation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-7-source-large.gif
2012,6104061,Fig. 8.,Detailed block diagram of the tld framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-8-source-large.gif
2012,6104061,Fig. 9.,Block diagram of the object detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-9-source-large.gif
2012,6104061,Fig. 10.,Conversion of a patch to a binary code.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-10-source-large.gif
2012,6104061,Fig. 11.,"Illustration of p-expert. (a) object model and the core in feature space (gray blob). (b) unreliable (dotted) and reliable (thick) trajectory. (c) the object model and the core after the update. red dots are positive examples, black dots are negative, and cross denotes end of a trajectory.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-11-source-large.gif
2012,6104061,Fig. 12.,Snapshots from the introduced tld data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6104061/6104061-fig-12-source-large.gif
2012,5961630,Fig. 1.,"(a) Graphical model representing our model:
x
i
is the
i
th sample (out of
N
samples) and
y
i
is the corresponding class label.
b
j
is the
j
th basis vector (out of
K
basis vectors) and
c
i
is the loading coefficient for the
i
th sample;
w
parametrizes the class-likelihood, i.e.,
p
w
(y|⋅)
; in other words, it parametrizes the classifier. Since samples and corresponding labels are observed variables, they are shaded with gray while unobserved variables (i.e.,
b
j
,
c
i
, and
w
) are white. (b) shows the same idea as a matrix factorization;
b
j
,
c
i
, and
x
i
are columns of
B,
C
, and
X
, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-1-source-large.gif
2012,5961630,Fig. 2.,"Due to nonnegativity constraints, only the addition operation is allowed. If a part is added to an image, it cannot be subtracted; thus the algorithm must choose proper basis vectors to represent an image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-2-source-large.gif
2012,5961630,Fig. 3.,"Graphical representation of boxed-sparsity for
R
3
, which is the intersection of
ℓ
∞
and
ℓ
1
norm balls in the positive orthant. The blue dots are vertices of the feasible set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-3-source-large.gif
2012,5961630,Fig. 4.,"This figure shows an example of a 3×3 image (hence
b∈
R
9
) that is segmented into three regions
(G={
g
1
,
g
2
,
g
3
})
.
b
|
g
1
and
∥b
∥
2,1
are shown as examples.
⟨⋅,⋅⟩
means inner product thus
∥
b
|
g
1
∥
2
=
⟨
b
|
g
1
,
b
|
g
1
⟩
−
−
−
−
−
−
−
−
√
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-4-source-large.gif
2012,5961630,Fig. 5.,"Examples of RAVENS maps for the tissue types created from the transformation
(ϕ)
that warp the template (top, left) to the subject (top, right). The image shows the RAVEN maps for the tree tissue type: gray matter (GM, bottom left), white matter (WM, bottom middle), and cerebral spinal fluid (CSF, bottom right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-5-source-large.gif
2012,5961630,Fig. 6.,"Three examples of basis vectors with three different methods
(
λ
3
/D=20%)
. (a) One of the basis vectors learned by the proposed method on sagittal cuts and; (b) coronal cuts. (c) One of the basis vectors learned by the NMF method on sagittal cuts and, (d) coronal cuts. (e) One of the basis vectors learned by the SVD method on sagittal cuts and, (f) coronal cuts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-6-source-large.gif
2012,5961630,Fig. 7.,"Average classification rates in ten-fold cross-validation for various ratios of
(
λ
2
)/(
λ
1
)
(discriminative versus generative) for different number of basis vectors; i.e., various
K
. To avoid occlusion, standard deviations of the accuracy rates are added as a separate figure in (b). The
y
-axis,
σ
(C.V. Accuracy), indicates the standard deviations of the accuracy rates. The colors are the same as (a).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-7-source-large.gif
2012,5961630,Fig. 8.,"An example basis vector for a strong sparsity constraint
(
λ
3
/D=10%)
in two orthogonal cuts. Compare it with two examples shown in Fig. 6
(
λ
3
/D=20%)
. (a) Coronal cuts. (b) Sagittal cuts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-8-source-large.gif
2012,5961630,Fig. 9.,"Investigation of sparsity level on the classification accuracy for the boxed-sparsity when (a) the generative term is dominant, (b) the discriminative term is dominant. Standard deviations of the accuracy rates are added as the bars to the figures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-9-source-large.gif
2012,5961630,Fig. 10.,An example of a basis vector for a case in which group-sparsity constraint is used. (a) Coronal cuts. (b) Sagittal cuts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-10-source-large.gif
2012,5961630,Fig. 11.,"Investigation of sparsity level on the classification accuracy for the group-sparsity when (a) the generative term is dominant, (b) the discriminative term is dominant. Standard deviations of the accuracy rates are shown as error bars.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-11-source-large.gif
2012,5961630,Fig. 12.,"The accuracy rates and AUC versus different number of labeled samples for different regularizations.
SF
and
SSF
stand for supervised and semi-supervised features respectively i.e., supervised basis learning with or without unlabeled data;
SC
and
SSC
denote supervised classifier (Logistic Model Trees [54]) or semi-supervised classifier (linear lapSVM), respectively. (a) The accuracy rates of AD/NC when the boxed-sparsity is used as regularization. (b) AUC for MCI-NC/MCI-C subjects when the boxed-sparsity is used as regularization. (c) The accuracy rates of AD/NC when the group-sparsity is used as regularization. (d) AUC for MCI-NC/MCI-C subjects when the group-sparsity is used as regularization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-12-source-large.gif
2012,5961630,Fig. 13.,"Presentation of a feasible set
(B)
for
b∈
R
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/6112725/5961630/5961630-fig-13-source-large.gif
2012,5963673,Fig. 1.,Sample images from each group in the first data set. (a) Highway. (b) Inside of cities. (c) Tall buildings. (d) Streets. (e) Forest. (f) Coast. (g) Mountain. (h) Open country.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-1-source-large.gif
2012,5963673,Fig. 2.,Additional categories in the second data set. (a) Suburb residence. (b) Bedroom. (c) Kitchen. (d) Livingroom. (e) Office.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-2-source-large.gif
2012,5963673,Fig. 3.,Additional categories in the third data set. (a)-(c) Store. (d)-(f) Industrial.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-3-source-large.gif
2012,5963673,Fig. 4.,"Average MML values obtained when fitting DM (row 1), GDM (row 2) and BLM (row 3) models to the different image data sets. (a) Data set 1. (b) Data set 1 (natural categories). (c) Data set 1 (man-made categories). (d) Data set 2. (e) Data set 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-4-source-large.gif
2012,5963673,Fig. 5.,"Classification accuracy for the different data set as a function of the number of neighbors
K
when using KNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-5-source-large.gif
2012,5963673,Fig. 6.,"Classification accuracy for the first data set as a function of (a) the number of training images, (b) the number of visual words, and (c) the number of topics. The euclidean exponential kernel is considered for SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-6-source-large.gif
2012,5963673,Fig. 7.,"An image with four possible orientations. (a)
0
∘
(correct orientation). (b)
90
∘
. (c)
180
∘
. (d)
270
∘
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-7-source-large.gif
2012,5963673,Fig. 8.,"A set of images whose orientations were detected correctly (first row: input images, second row: detected orientations).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-8-source-large.gif
2012,5963673,Fig. 9.,"Examples of misclassified images (first row: input images, second row: detected orientations, third row: correct orientations).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6335084/5963673/5963673-fig-9-source-large.gif
2012,6192275,Fig. 1.,Histogram of the ranging error for the LOS and NLOS condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-1-source-large.gif
2012,6192275,Fig. 2.,In some situations there is a clear difference between LOS (upper waveform) and NLOS (lower waveform) signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-2-source-large.gif
2012,6192275,Fig. 3.,"CDF of residual ranging error without mitigation, and using SVM and GP-based mitigation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-3-source-large.gif
2012,6192275,Fig. 4.,"Outage probability for
N
b
=5
anchors, with
P
NLOS
=0.2",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-4-source-large.gif
2012,6192275,Fig. 5.,"Outage probability for
N
b
=5
anchors, with
P
NLOS
=0.8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-5-source-large.gif
2012,6192275,Fig. 6.,"Outage probability for
N
b
=5
anchors, with
e
th
=50
cm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-6-source-large.gif
2012,6192275,Fig. 7.,"Outage probability for
N
b
=5
anchors, with
e
th
=2m
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/26/6216525/6192275/6192275-fig-7-source-large.gif
2012,5871594,Fig. 1.,"H-SVM structure for
m=4
levels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-1-source-large.gif
2012,5871594,Fig. 2.,"(a) The average accuracy rate of H-SVM with respect to training sample sizes for
m=7
and
σ
2
=0.1,0.2,0.3,0.4
. (b) The average accuracy rate of H-SVM with respect to training sample sizes for
m=3,5,7
and
σ
2
=0.3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-2-source-large.gif
2012,5871594,Fig. 3.,"(a) The average estimation error of analytical results and simulation results with respect to
m
for different SVM classification error
ε
. (b) The standard derivation of the estimation error of analytical results and simulation results with respect to
m
for different SVM classification error
ε
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-3-source-large.gif
2012,5871594,Fig. 4.,"The comparison of the upper bound of the average estimation error of LSVM and the exact average estimation error of H-SVM with respect to level numbers
m
and different SVM classification error
ϵ
for
D=1
and the uniform distribution of nodes locations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-4-source-large.gif
2012,5871594,Fig. 5.,"The learning complexity comparison between H-SVM and LSVM for level numbers
m=3,5,7
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-5-source-large.gif
2012,5871594,Fig. 6.,"The accuracy rate under the effect of mobility for different level number
m
provided that
D
equals
500m
and nodes are uniformly distributed in a square
D×D
region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-6-source-large.gif
2012,5871594,Fig. 7.,"The relationship between the total cost function
C(m)
and the level number
m
of H-SVM by assuming
C
SVM
=1
,
C
e
=100
, and
D=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/6142500/5871594/5871594-fig-7-source-large.gif
2012,6332444,Fig. 1.,An unsupervised two-view latent subspace MN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-1-source-large.gif
2012,6332444,Fig. 2.,"t-SNE 2D embedding of the discovered latent subspace representation by (left) MMH, (middle) DWH, and (right) TWH on the TRECVID video dataset (better viewed in color).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-2-source-large.gif
2012,6332444,Fig. 3.,"Example topics discovered by a 60-topic MMH on the Flickr animal dataset. For each topic, we show five topic-ranked images as well as the average probabilities of that topic on representing images from the 13 categories.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-3-source-large.gif
2012,6332444,Fig. 4.,Classification accuracy on the (a) TRECVID and (b) Flickr image datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-4-source-large.gif
2012,6332444,Fig. 5.,(a) Prediction R2. (b) Feature weights in MMH with 10 topics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-5-source-large.gif
2012,6332444,Fig. 6.,The average precision curve and the two precision-recall curves for image retrieval on TRECVID data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-6-source-large.gif
2012,6332444,Fig. 7.,"Top-
N
F1-measure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-7-source-large.gif
2012,6332444,Fig. 8.,"Example images from the 13 categories on the Flickr animal dataset with predicted annotations. Tags in blue and bold are correct annotations, while red and italic ones are wrong predictions. The other tags are neutral. We have repeated the categories “squirrel” and “cat” in the right corner to fill the empty space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-8-source-large.gif
2012,6332444,Fig. 9.,"(a) Classification accuracy of structured MMH and DWH models, and (b) training and testing time on hotel review data [50] for regression.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-9-source-large.gif
2012,6332444,Fig. 10.,"Sensitivity to
C
2
on the (a) TRECVID and (b) Flickr datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6332444/6332444-fig-10-source-large.gif
2012,6126047,Fig. 1.,"Learning curves for NCKLMS
(μ=1)
, ANCKLMS,
(μ=1/4)
, CKAPSM and ACKAPSM (filter length
L=5
) for the nonlinear channel identification problem with Gaussian input and Gaussian noise at 20 dB, for (a) circular input case
(ρ=
2
–
√
/2)
and (b) non-circular input case
(ρ=0.1)
. In the realization of the CKAPSM and ACKAPSM the
l
2
norm was employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-1-source-large.gif
2012,6126047,Fig. 2.,"Learning curves for NCKLMS
(μ=1)
, ANCKLMS,
(μ=1/4)
, CKAPSM and ACKAPSM (filter length
L=5
) for the nonlinear two-channels identification problem with Gaussian input and Gaussian noise at 20 dB, for(a) circular input case
(ρ=
2
–
√
/2)
and (b) non-circular input case
(ρ=0.1)
. In the realization of the CKAPSM and ACKAPSM the
l
2
norm was employed. After index
n=5000
, both the linear and the non-linear component of the channel have been changed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-2-source-large.gif
2012,6126047,Fig. 3.,"Learning curves for NCKLMS
(μ=1)
, ANCKLMS,
(μ=1/4)
, CKAPSM and ACKAPSM (filter length
L=5
) for the nonlinear channel identification problem with Gaussian input and heavy-tailed student noise
(ν=3)
at 20 dB, for the non-circular input case
(ρ=0.1)
. In the realization of the CKAPSM and ACKAPSM the Huber loss function was employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-3-source-large.gif
2012,6126047,Fig. 4.,"Learning curves for NCKLMS
(μ=1)
, ANCKLMS,
(μ=1/4)
, CKAPSM, and ACKAPSM (filter length
L=5
) for the nonlinear channel identification problem with uniform input and Gaussian noise at 20 dB, for (a) circular input case
(ρ=
2
–
√
/2)
and (b) non-circular input case
(ρ=0.1)
. In the realization of the CKAPSM and ACKAPSM the
l
2
loss function was employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-4-source-large.gif
2012,6126047,Fig. 5.,"Learning curves for NCKLMS
(μ=1/2)
, ANCKLMS,
(μ=1/4)
, CKAPSM, and ACKAPSM (filter length
L=5
, delay
D=2
) for the nonlinear channel equalization problem with Gaussian input and Gaussian noise at 20 dB, for (a) circular input case
(ρ=
2
–
√
/2)
and (b) non-circular input case
(ρ=0.1)
. In the realization of the CKAPSM and ACKAPSM the
l
2
loss function was employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-5-source-large.gif
2012,6126047,Fig. 6.,"Learning curves for NCKLMS
(μ=1/2)
, ANCKLMS,
(μ=1/4)
, CKAPSM and ACKAPSM (filter length
L=5
, delay
D=2
) for the nonlinear channel equalization problem with uniform input and Gaussian noise at 20 dB, for (a) circular input case
(ρ=
2
–
√
/2)
and (b) non-circular input case
(ρ=0.1)
. In the realization of the CKAPSM and ACKAPSM the
l
2
loss function was employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-6-source-large.gif
2012,6126047,Fig. 7.,"Symbol error rate versus signal-to-noise ratio (SNR) for NCKLMS
(μ=1/2)
and CKAPSM (filter length
L=5
, delay
D=2
) for the nonlinear QPSK equalization problem with Gaussian noise, for (a) circular input case and (b) non-circular input case. In the realization of the CKAPSM the
l
2
loss function was employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6126047/6126047-fig-7-source-large.gif
2012,6135800,Fig. 1.,"Base classifiers learned by using the labeled training instances from the source domains (and the target domain as well). For each unlabeled instance
x
in
D
T
u
, we define its virtual label
y
~
=
∑
P
s=1
γ
~
s
f
s
(x)
as a weighted summation of the decision values
f
s
(x)
's from the base classifiers
f
s
's on
x
, where
γ
~
s
=
γ
s
/
∑
P
s=1
γ
s
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6135800/6135800-fig-1-source-large.gif
2012,6135800,Fig. 2.,"Means and standard deviations (%) of APs of all methods on the 20 newsgroups dataset with different regularization parameter
C=0.01,0.1,1,10
, and 100. For better visualization, see the colored PDF file. (a) Rec versus sci, (b) comp versus rec, and (c) sci versus comp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6135800/6135800-fig-2-source-large.gif
2012,6135800,Fig. 3.,"Means and standard deviations (%) of APs of FastDAM and UniverDAM on the 20 newsgroups dataset with different
λ
D
=
λ
D
1
=
λ
D
2
=0.1,1,10,100
, and 1000. (a) Rec versus sci, (b) comp versus rec, and (c) sci versus comp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6135800/6135800-fig-3-source-large.gif
2012,6135800,Fig. 4.,"Means and standard deviations (%) of APs of FastDAM and UniverDAM on the 20 newsgroups dataset with different tradeoff parameter
λ=0.1,1,10,100
, and 1000. (a) Rec versus sci, (b) comp versus rec, and (c) sci versus comp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6135800/6135800-fig-4-source-large.gif
2012,6135800,Fig. 5.,"Means and standard deviations (%) of APs of FastDAM and UniverDAM on the 20 newsgroups dataset with different bandwidth parameter
β=0,0.01,1,100
, and 10 000 in (30). (a) Rec versus sci, (b) comp versus rec, and (c) sci versus comp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6157687/6135800/6135800-fig-5-source-large.gif
2012,6329964,Fig. 1.,"Examples of dimensionality reduction by LDA, LPP, and LDA over LPP. In the upper half part of each subfigure, the 2-D data set consists of two classes (red star and blue circle). Using three different subspace learning algorithms, we have achieved three 1-D projections. The red line means LDA over LPP, the green line represents LPP, and the purple line shows LDA. After projecting, the distribution of 1-D data can be seen in the bottom half part of each subfigure. (a) Data set 1. (b) Data set 2. (c) Data set 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6329964/6329964-fig-1-source-large.gif
2012,6329964,Fig. 2.,"Adjacency graph for between-class compactness of LFDA and LDE. Here, we only take one point in one class as the example to construct the between-class graph. In LDE, each point finds its
k
-nearest neighbors. Here, we set
k=4
. However, in some situations, for example, one subset may only find some of other different subsets and neglect others, as shown in (a). In LFDA, for each point, the points in different classes are connected to it, as shown in (b). The number of connections could be very large in LFDA for the entire set. (a) Between-class graph of LDE. (b) Between-class graph of LFDA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6329964/6329964-fig-2-source-large.gif
2012,6329964,Fig. 3.,"Adjacency graphs are for within-class and between-class of the hierarchical graph structure. Here, we only take one point in one class as the example to construct two graphs. We use
k=4
to construct the within-class graph, as shown in (a). In between-class graph, as shown in (b), subsets of the whole data set are shown in subgraph i. The between-class graph of one subset (we take the speed limits sign for example) is shown in subgraph ii. We use
k=3
, and each subset connects one line to the nearest point of the different classes. Subgraph iii shows the graph structure of all the subsets, and here,
k=2
. (a) Within-class graph. (b) Between-class graph.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6329964/6329964-fig-3-source-large.gif
2012,6329964,Fig. 4.,Samples of the six subsets of similar traffic signs of GTSRB. (a) Unique. (b) Derestriction. (c) Other prohibitory. (d) Speed limit. (e) Mandatory. (f) Danger.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6329964/6329964-fig-4-source-large.gif
2012,6329964,Fig. 5.,"Samples of the new subsets of GTSRB with various appearances (lighting, occlusion, and erosion conditions). (a) Traffic signs with partial occlusion. (b) Traffic signs with various lighting conditions. (c) Traffic signs with some erosion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6329964/6329964-fig-5-source-large.gif
2012,6329964,Fig. 6.,Samples of the five subsets of similar traffic signs of Belgium Traffic Sign Classification Benchmark. (a) Limit signs. (b) Unique signs. (c) Circle mandatory signs. (d) Rectangle mandatory signs. (e) Danger signs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6329964/6329964-fig-6-source-large.gif
2012,5620916,Fig. 1.,"Two forms of information release to a third party: (a) the data collector sends the preprocessed information (which was sanitized through extra techniques, such as cryptographic approaches or statistical database) at will or (b) hackers steal the original samples in storage without notifying the data collector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-1-source-large.gif
2012,5620916,Fig. 2.,"Unrealizing training samples in (a) by calling Unrealize-Training-Set
(
T
S
,
T
U
,{},{})
The resulting tables
T
P
and
T
′
are given in (b) and (c).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-2-source-large.gif
2012,5620916,Fig. 3.,"Illustration of Generate-Tree process by applying the conventional ID3 approach with the original samples
T
S
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-3-source-large.gif
2012,5620916,Fig. 4.,"Illustration of
Generate-
Tree
′
process by applying the modified ID3 approach with the unrealized samples (
T
′
+
T
P
). For each step the entropy values and resulting subtrees are exactly the same as the results of the traditional approach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-4-source-large.gif
2012,5620916,Fig. 5.,"Data Sets in
q
T
U
, where data sets contained in the rectangles belong to
T
S
and the rest belong to
[
T
′
+
T
P
]
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-5-source-large.gif
2012,5620916,Fig. 6.,"Unrealizing training samples in (a) with a dummy value Dummy added to the domain of attribute Wind. The resulting tables
T
P
and
T
′
are shown in (b) and (c).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-6-source-large.gif
2012,5620916,Fig. 7.,"(a) The case with the lowest variance distribution.
P
loss
(
T
L
|
T
S
,
T
S
)=
P
loss
(
T
L
|[
T
′
+
T
P
],
T
S
)=|
T
L
|∗(|
T
S
|/|
T
U
|)
. (b) The case with the highest variance distribution.
P
loss
(
T
L
|
T
S
,
T
S
)=
|
T
L
|∗|
T
S
|
and
P
loss
(
T
L
|[
T
′
+
T
P
],
T
S
)=0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-7-source-large.gif
2012,5620916,Fig. 8.,"Analytical results of experiments when data set complementation approach is applied to 1) normally distributed samples, 2) evenly distributed samples, and 3) extremely unevenly distributed samples, in the cases of (i) without creating any dummy attribute values and (ii) when the dummy attribute technique is used to double the size of sample domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-8-source-large.gif
2012,5620916,Fig. 9.,"Analytical results of experiments when the data set complementation approach is applied to some randomly picked samples, in the cases of (i) without creating any dummy attribute values and (ii) creating dummy attribute values in order to double the size of sample domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-9-source-large.gif
2012,5620916,Fig. 10.,Samples with normal distribution. This set of samples is used in many data mining teaching materials.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-10-source-large.gif
2012,5620916,Fig. 11.,Samples with even distribution. All data sets have the same counts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-11-source-large.gif
2012,5620916,Fig. 12.,"Samples with extremely uneven distribution. All data sets have 0 counts except one data set has
|
T
S
|
counts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-12-source-large.gif
2012,5620916,Fig. 13.,"Unrealized samples
T
′
(left) and
T
P
(right) derived from samples in Fig. 9, without the dummy attribute values technique.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-13-source-large.gif
2012,5620916,Fig. 14.,"Unrealized samples
T
′
(left) and
T
P
(right) derived from samples in Fig. 9, by inserting dummy attribute values into the attributes Temperature and Windy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-14-source-large.gif
2012,5620916,Fig. 15.,"Unrealized samples
T
′
(left) and
T
P
(right) derived from samples in Fig. 10, without the dummy attribute values technique.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-15-source-large.gif
2012,5620916,Fig. 16.,"Unrealized samples
T
′
(left) and
T
P
(right) derived from samples in Fig. 10, by inserting dummy attribute values into the attributes Temperature and Windy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-16-source-large.gif
2012,5620916,Fig. 17.,"Unrealized samples
T
′
(left) and
T
P
(right) derived from samples in Fig. 11, without the dummy attribute values technique. The attribute Count is added only to save printing space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-17-source-large.gif
2012,5620916,Fig. 18.,"Unrealized samples
T
′
(left) and
T
P
(right) derived from samples in Fig. 11, by inserting dummy attribute values into the attributes Temperature and Windy. The attribute Count is added only to save printing space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6108378/5620916/5620916-fig-18-source-large.gif
2012,6081929,Fig. 1.,"Diagram illustrating the proposed ensemble classifier. The random subspaces are constructed by selecting
d
sub
≪d
features randomly and uniformly from the entire feature space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6166811/6081929/6081929-fig-1-source-large.gif
2012,6081929,Fig. 2.,"Left: Detection error
P
E
quickly saturates with the number of fused learners
L
. Right: The detection error after saturation as a function of
d
sub
. The dots represent out-of-bag error estimates
E
OOB
(see Sections II-C). Feature sets considered:
F
inter
,
F
intra
, and
F
∗
with dimensionalities 1550, 2375, and 3925 (see Section III). Target algorithm: nsF5 with payload 0.1 bpac.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6166811/6081929/6081929-fig-2-source-large.gif
2012,6081929,Fig. 3.,"Graphical explanation of features
C
xy
(Δx,Δy)
. The symbols
⨀
and
◯
denote the first and the corresponding second DCT mode.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6166811/6081929/6081929-fig-3-source-large.gif
2012,6081929,Fig. 4.,"Evolution of
E
OOB
as individual feature sets are merged together.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6166811/6081929/6081929-fig-4-source-large.gif
2012,6081929,Fig. 5.,"Steganalysis of nsF5, YASS, and MBS. The performance of the ensemble classifier using
CF
∗
features is compared to the state-of-the-art G-SVM steganalysis with CC-PEV (or CDF) features. We report median (MED) testing error over ten different splits of the CAMERA database into a training and testing set, as well as the median absolute deviation (MAD) values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6166811/6081929/6081929-fig-5-source-large.gif
2012,6190762,Fig. 1.,Example of K-complex wave.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-1-source-large.gif
2012,6190762,Fig. 2.,Data processing using triangular smoothing technique. (a) Original EEG wave. (b) Smoothed EEG wave.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-2-source-large.gif
2012,6190762,Fig. 3.,Bag decomposition of an EEG segment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-3-source-large.gif
2012,6190762,Fig. 4.,Subsegment decomposition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-4-source-large.gif
2012,6190762,Fig. 5.,Support vector classifier on representative data instances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-5-source-large.gif
2012,6190762,Fig. 6.,Plot of feature weights for 29 feature set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-6-source-large.gif
2012,6190762,Fig. 7.,"Performance of RIC with different values of
(
δ
pos
,
δ
neg
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-7-source-large.gif
2012,6190762,Fig. 8.,Experiment results on the effect of oversampling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6190762/6190762-fig-8-source-large.gif
2012,6236215,Fig. 1.,Power flow in a split HEV configuration with arrows indicating positive power.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-1-source-large.gif
2012,6236215,Fig. 2.,"Projection of cost matrix
R
along the driving cycle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-2-source-large.gif
2012,6236215,Fig. 3.,ML-emo-HEV: a computational framework for machine learning of optimal energy management in HEV.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-3-source-large.gif
2012,6236215,Fig. 4.,"Segments of a speed profile. The
X
-axis represents time measured in seconds, and the
Y
-axis represents speed measured in miles per hour.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-4-source-large.gif
2012,6236215,Fig. 5.,Neural learning for roadway types and traffic congestion level prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-5-source-large.gif
2012,6236215,Fig. 6.,Neural network performance of roadway types and traffic congestion level prediction for la92.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-6-source-large.gif
2012,6236215,Fig. 7.,Driving trend of the NN structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-7-source-large.gif
2012,6236215,Fig. 8.,Performances of the driving trend prediction of neural network on various window sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-8-source-large.gif
2012,6236215,Fig. 9.,Computational steps of DP optimization in an HEV.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-9-source-large.gif
2012,6236215,Fig. 10.,DP results with ford escape model in PSAT for arterial LOS cd cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-10-source-large.gif
2012,6236215,Fig. 11.,"Energy management neural networks. (a)
N
N
i
P
batt
for Sierra FS cycle
R
i
,
i=1,…,11
. (b)
N
N
i
ω
eng
for Sierra FS cycle
R
i
,
i=1,…,11
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-11-source-large.gif
2012,6236215,Fig. 12.,Comparison of optimal engine speed and battery power generated by the neural networks and DP for freeway LOS c.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/6330019/6236215/6236215-fig-12-source-large.gif
2012,6392452,Fig. 1.,(a) Example of a microdrill and (b) tip schematic diagram from the top view.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-1-source-large.gif
2012,6392452,Fig. 2.,"Example images of the six phases (0.5-mm-diameter microdrill bit). (a) N, (b) ND, (c) K1, (d) K1D, (e) K2, and (f) K2D.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-2-source-large.gif
2012,6392452,Fig. 3.,Flowchart of the phase indemnification of microdrill bits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-3-source-large.gif
2012,6392452,Fig. 4.,(a) Automatic visual inspection system of microdrill bits and (b) drill bit image acquisition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-4-source-large.gif
2012,6392452,Fig. 5.,(a) Original smeared microdrill bit image and (b) segmented result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-5-source-large.gif
2012,6392452,Fig. 6.,(a) Segmented cutting before alignment and (b) direction aligned cutting plane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-6-source-large.gif
2012,6392452,Fig. 7.,Cutting plane land marking.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-7-source-large.gif
2012,6392452,Fig. 8.,"Point shapes of a cutting plane in six phases: (a) N, (b) ND, (c) K1, (d) K1D, (e) K2, and (f) K2D.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-8-source-large.gif
2012,6392452,Fig. 9.,Fraction of total variation versus eigenvalues: (a) PCA and (b) LDA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-9-source-large.gif
2012,6392452,Fig. 10.,"Mix identification rate using PCA for statistical-shape construction with three classifiers, SVMs, NNs, and kNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-10-source-large.gif
2012,6392452,Fig. 11.,"Mix identification rate using LDA for statistical-shape construction with three classifiers, SVMs, NNs, and kNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-11-source-large.gif
2012,6392452,Fig. 12.,Identification rates using PCA-SVMs and LDA-SVMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6330018/6392452/6392452-fig-12-source-large.gif
2012,6138848,Fig. 1.,"The architecture of the ensemble of BRNNs, followed by the filtering of the output. The PSSM values are given as input to six BRNNs, which predict the secondary structure of each residue in the amino acid sequence. Subsequently, the outputs are averaged and are given as input to the filtering methods investigated in this study.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6171967/6138848/6138848-fig-1-source-large.gif
2012,6138848,Fig. 2.,Experiments with different local window sizes for filtering PSSP (see text for more information) using four machine learning algorithms on the CB513 data set. The predictive accuracy (left) and the SOV score (right) strongly depend on the size of the local window used for filtering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6171967/6138848/6138848-fig-2-source-large.gif
2012,6138848,Fig. 3.,"Per state prediction after the application of filtering techniques on the PDB-Select25 data set. The
y
-axis corresponds to the number of residues and the
x
-axis to the combinations of observed and predicted state. For instance, HE corresponds to residues that are observed as helices (H) but are predicted as extended (E). The three columns at each state show the number of residues for the unfiltered classifier (ensemble of BRNNs), the LibSVM filtering, and the combination of LibSVM and SS-filt, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6171967/6138848/6138848-fig-3-source-large.gif
2012,6138848,Fig. 4.,Five examples that show the effect of filtering on PSSP. The first line in each case shows the PDB ID and the Chain ID. Sequences A and B are taken from CB513 and the remaining sequences from PDB-Select25. The mispredictions are shown in shadow. “PriStr” is the amino acid sequence; “Real SS” is the observed secondary structure; “No-Filt” is the PSSP from the ensemble of BRNNs; “LibSVM” is the PSSP filtered with LibSVM and “SS-Filt” is the application of the SS-Filt empirical rules on the output of LibSVM filtering. Secondary structure states are reported using the reduced three-state scheme (see Section 2.1).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6171967/6138848/6138848-fig-4-source-large.gif
2012,6153069,Fig. 1.,"Ripley dataset containing 250 training data points and 1000 test data points. (a) Training examples of the Ripley dataset with corresponding labels. (b) Nonpruned solution using Vapnik's SVM classifier for the nonseparable case with
σ=1
and
C=100
. (c) Pruned solution using SCA with removal of misclassified examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6163421/6153069/6153069-fig-1-source-large.gif
2012,6153069,Fig. 2.,"SSCA with removal of misclassified examples applied to the Ripley dataset with
σ=1
and
C=100
. Parameter
D
differs per figure. (d)
D=0.3
. (e)
D=1.3
. (f)
D=2.0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6163421/6153069/6153069-fig-2-source-large.gif
2012,6153069,Fig. 3.,"SSCA, Keerthi's method (Keer), and FS LS-SVM reducing the number of support vectors of SVM classifiers using a RBF kernel
(K(x,
x
k
)=exp(−
∥x−
x
k
∥
2
2
/
σ
2
))
or Pol kernel (
K(x,
x
k
)=(
x
T
x
k
+τ
)
d
with
τ≥0
). The bandwidth of the kernel used for the selection of the basis functions in FS LS-SVM is chosen according to the improved Silverman's normal rule of thumb (ISNR). For each randomization, the data points that belong to the training set are randomly selected. The remaining data points belong to the test set.
ACC
TEST,npr
is the average test set accuracy of the nonpruned SVM solution, and
SV
npr
is the average number of support vectors of the nonpruned SVM solution. In the SSCA method, we always remove misclassified data points instead of flipping their labels. The hyperparameters are optimized using 10-fold cross-validation. For FS LS-SVM, the hyperparameters depend on the number of basis functions and differ from the hyperparameters of the nonpruned SVM solution. Hyperparameters of the SVM solution have subscript “
svm
” Hyperparameters of Keerthi's method have subscript “
kee
” Given a certain dataset, kernel, and number of support vectors, the figures show one boxplot for each method: the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the most extreme data points not considered as outliers, and the outliers are plotted individually. (a)
adu(rbf)
; randomizations: 20; hyperparameters:
σ
svm
=8
,
C
svm
=5
,
σ
kee
=8
,
C
kee
=3
;
ACC
TEST,npr
=84.71%
; and
SV
npr
=11604
. (b)
pro(rbf)
; randomizations: 20; hyperparameters:
σ
svm
=7
,
C
svm
=20
,
σ
kee
=20
,
C
kee
=200
;
ACC
TEST,npr
=74.42%
; and
SV
npr
=8116
. (c)
adu(pol)
; randomizations: 20; hyperparameters:
d
svm
=3
,
τ
svm
=0.1
,
C
svm
=500
,
d
kee
=2
,
τ
kee
=0.9
,
C
kee
=0.01
;
ACC
TEST,npr
=84.75%
; and
SV
npr
=11583
. (d)
pro(pol)
; randomizations: 20; hyperparameters:
d
svm
=3
,
τ
svm
=1.1
,
C
svm
=500
,
d
kee
=3
,
τ
kee
=0.9
,
C
kee
=2000
;
ACC
TEST,npr
=74.37%
; and
SV
npr
=8002
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6163421/6153069/6153069-fig-3-source-large.gif
2012,6031933,Fig. 1.,"X
z
as defined by (3) for different
z
values. (a)
z=10
. (b)
z=20
. (c)
z=30
. (d)
z=100
. (e)
z=512
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-1-source-large.gif
2012,6031933,Fig. 2.,"Effect of changing
σ
in images. Images (i)–(o) are constructed from
U
,
V
of original “Lena” image (h), and the
σ
matrices of images from (a)–(g), respectively. Image (p) is constructed from
U
,
V
of original “Lena” image (h), and the average of
σ
matrices of images from (a)–(g).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-2-source-large.gif
2012,6031933,Fig. 3.,Behavior of singular values for noise and blur distortion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-3-source-large.gif
2012,6031933,Fig. 4.,"(a) White noise distorted hat part. (b) White noise distorted shoulder part. (c) White noise distorted building part. (d) White noise distorted plants part. The objective predictions from PSNR and
Q
TID
have been indicated below each image. For reference,
Q
TID
=5.7966
for the image with no distortions. The images have been cropped for visibility. (a)
PSNR=32.0705
,
Q
TID
=3.4677
. (b)
PSNR=32.1293
,
Q
TID
=3.3173
. (c)
PSNR=33.8977
,
Q
TID
=4.7885
. (d)
PSNR=33.1732
,
Q
TID
=5.4665
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-4-source-large.gif
2012,6031933,Fig. 5.,"(a)
C
P
comparison on different image databases. (b) RMSE for CSIQ, IVC, A57, and TID databases. (c) RMSE for LIVE and WIQ databases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-5-source-large.gif
2012,6031933,Fig. 6.,"(a) Average
ΔCS
values with respect to
Q
over the seven image databases for different metrics. (b)
C
P
values for five chunks of TID database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-6-source-large.gif
2012,6031933,Fig. 7.,"Scatter plot for the LIVE image database with
Q
CSIQ
as the objective metric.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-7-source-large.gif
2012,6031933,Fig. 8.,"(a)
C
P
values for the 500 images from TID database with five distortion types (see text for further explanation). (b)
C
P
and
C
S
values for LIVE and EPFL video databases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-8-source-large.gif
2012,6031933,Fig. 9.,"F
-test plot for different image and video databases (the points above the
F
critical
boundary denote the cases for the proposed scheme to be statistically better than the corresponding metric).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-9-source-large.gif
2012,6031933,Fig. 10.,"(a) Kernel similarity scores of the noisy and blurred images with the SVs corresponding to lower quality images
(MOS<2)
. (b) Kernel similarity scores of the noisy and blurred images with the SVs corresponding to lower higher images
(MOS>6.5)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6031933/6031933-fig-10-source-large.gif
2012,6133287,Fig. 1.,"Global cues for recognizing human interactions. (a) Relative spatial relations between people can be indicative of the type of interaction being performed. (b) Head orientation (here indicated by a conical beam) is a cue to which people, in a given frame, are more likely to be interacting with each other.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-1-source-large.gif
2012,6133287,Fig. 2.,Computing tracks. (a) Raw upper body detections. (b) Linking detections by KLT tracking of points inside the head region. The detections of a person can be linked even if the person is not detected for several frames.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-2-source-large.gif
2012,6133287,Fig. 3.,"Head pose estimation. (a) Area inside an upper body detection used to estimate the head pose. (b) Examples of the discrete head orientations used: profile-left, frontal-left, frontal-right, profile-right, and backward.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-3-source-large.gif
2012,6133287,Fig. 4.,Local context descriptor. Grid used to describe the person’s local context. Highlighted in the grid are the dominant gradient orientation of each cell (displayed with green lines) and cells that contain significant motion (in red).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-4-source-large.gif
2012,6133287,Fig. 5.,Discrete set of relative spatial relations between people in a frame. The dotted square represents an upper body bounding box and the black square the head area.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-5-source-large.gif
2012,6133287,Fig. 6.,"Performance of the upper body detector and tracks estimation. Precision-recall curve of the upper body detector (raw), generated tracks using the CP method of [5] and our new approach (CP only or KLT-CP combination). Results are given for two overlap thresholds. Computing tracks improves the precision and recall in both cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-6-source-large.gif
2012,6133287,Fig. 7.,"Performance of the discrete head pose classifier. (a) Confusion matrix. (b) Percentage of correctly classified head poses, before and after applying a quadratic smoothing, for each interaction class: hand shake (HS), high five (HF), hug (HG), kiss (KS), negative (NG).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-7-source-large.gif
2012,6133287,Fig. 8.,"Spatial weights (
γ
, see Fig. 5) learned using training data from manual annotations and automatically generated tracks, corresponding to experiments 2 and 10 in Table 3. Lighter intensity indicates a higher weight.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-8-source-large.gif
2012,6133287,Fig. 9.,Highest ranked videos divided by interaction class obtained using the structured prediction method with automatically computed tracks. The red squares indicate false positives.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6332439/6133287/6133287-fig-9-source-large.gif
2012,6280632,Fig. 1.,Structure of the LTMIL system for automatic detection of MI from patients’ ECGs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6361493/6280632/6280632-fig-1-source-large.gif
2012,6280632,Fig. 2.,Hierarchical framework of our LTMIL classification algorithm for automatic detection of MI.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6361493/6280632/6280632-fig-2-source-large.gif
2012,6280632,Fig. 3.,"Average sensitivity and specificity achieved by LTMIL change as the number of clusters
L
and the scale factor μ increase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6361493/6280632/6280632-fig-3-source-large.gif
2012,6280632,Fig. 4.,ROC points obtained by compared algorithms with varying number of clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6361493/6280632/6280632-fig-4-source-large.gif
2012,6280632,Fig. 5.,ROC points achieved by each algorithm with the best.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6361493/6280632/6280632-fig-5-source-large.gif
2012,5989791,Fig. 1.,Architecture of the ART2-based system for protein classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6138579/5989791/5989791-fig-1-source-large.gif
2012,5989791,Fig. 2.,"Typical architecture of ART2 neural network (G.A. Carpenter, and S. Grossberg, 1987).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6138579/5989791/5989791-fig-2-source-large.gif
2012,5989791,Fig. 3.,Comparison of F-measures of 507 domains data set from various substitution matrices and classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6138579/5989791/5989791-fig-3-source-large.gif
2012,5989791,Fig. 4.,Distribution of minimum E-value of 507 domains data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6138579/5989791/5989791-fig-4-source-large.gif
2012,5989791,Fig. 5.,"Clustering results and Error percent values (last column) of the 507 data set by ART2, Spectral k-means, Pairwise SVM, and Random forest. Rows 1 to 6 indicate true classes of six superfamilies: Globin-like (88), EF-hand (83), Cupredoxins (78), (Trans) glycosidases (83), Thioredoxin-like (81), and Membrane all-alpha (94), respectively. Columns indicate experimental classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6138579/5989791/5989791-fig-5-source-large.gif
2012,6319409,Fig. 1.,Images from WCE. (a) Image partially covered by intestinal content presenting a polyp (dashed circle). (b) Image where lumen (indicated by the arrow) is visualized surrounded by intestinal wall.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-1-source-large.gif
2012,6319409,Fig. 2.,"Example of images from WCE videos. (a) Clear images without pathologies. (b) Images presenting some pathologies: bleeding, celiac, polyps, and chron. (c) Images with turbid content. (d) Images with bubbles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-2-source-large.gif
2012,6319409,Fig. 3.,System architecture for detection and segmentation of intestinal content.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-3-source-large.gif
2012,6319409,Fig. 4.,"This figure shows the correlation between Gabor and SURF methods, both applied to the detection and segmentation of bubble frames. (a) Correlation graph (
r=0.95
) between Gabor surface and Surf points in bubble detection problem. Each point in the graph represents one single frame. Ordinate axis represents the percentage of frame surface covered by bubbles following the Gabor method. Abscise axis represents the number of SURF points detected in that frame. With numbers 1–4, some outliers have been marked. (b) Outliers and the output of SURF and Gabor methods for these frames.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-4-source-large.gif
2012,6319409,Fig. 5.,"Box plots of intestinal content classification results using different sets of features: (a) color moment features, (b) HSV histogram with 64 bins, (c) intes color map histogram with 64 bins and (d) intes color map histogram with 64 bins + bubbles. On each box, the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the most extreme data points not considered outliers, and outliers are plotted individually.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-5-source-large.gif
2012,6319409,Fig. 6.,"Each box plot represents the accuracy obtained by testing a classifier learned by the video of
x
-axis with all other 49 videos in our dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-6-source-large.gif
2012,6319409,Fig. 7.,"Each box plot represents the accuracy obtained by testing the video represented by
x
-axis using 49 different classifiers, which are learned using one different video in each case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-7-source-large.gif
2012,6319409,Fig. 8.,System accuracy for ten videos from different subjects. X-axis represents the number of videos in the training dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-8-source-large.gif
2012,6319409,Fig. 9.,Qualitative results obtained from nine random images by using the proposed intestinal content segmentation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-9-source-large.gif
2012,6319409,Fig. 10.,Qualitative results obtained from 12 random images by using the proposed bubble segmentation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-10-source-large.gif
2012,6319409,Fig. 11.,"Results of Intestinal content segmentation from 20 random images. Black areas mean clear region, white means region with bubbles and grey areas means turbid regions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6319409/6319409-fig-11-source-large.gif
2012,6263298,Fig. 1.,Proposed geometric distance-based risk score prediction method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6263298/6263298-fig-1-source-large.gif
2012,6263298,Fig. 2.,"Illustration of initial score calculation and score updating in the proposed score prediction algorithm. Circle patterns indicate patients with positive outcomes (cardiac arrest within 72 h) and square patterns indicate patients with negative outcomes. Triangle pattern is the testing data point. Note that
C
p
and
C
n
are virtual center points calculated from positive samples and negative samples, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6263298/6263298-fig-2-source-large.gif
2012,6263298,Fig. 3.,Prediction results with different number of nearest neighbors on the balanced dataset where five performance indicators are used.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6263298/6263298-fig-3-source-large.gif
2012,6263298,Fig. 4.,"ROC curves generated on the balanced dataset with three different types of features: the combined features, HRV parameters, and vital signs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6263298/6263298-fig-4-source-large.gif
2012,6263298,Fig. 5.,"ROC curves generated on the imbalanced dataset with three different types of features: the combined features, HRV parameters, and vital signs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4233/6353358/6263298/6263298-fig-5-source-large.gif
2012,5719616,Fig. 1.,"Average prequential accuracy (1) of the four ensembles analyzed for the circle problem considering 30 runs using “perfect” drift detections. The accuracy is reset when the drift starts (
f∈{1,1001}
). The new ensembles are created from scratch at the time steps 1 and 1, 001. The old ensembles correspond to the new ensembles before the beginning of the drift. (a) Low Sev, High Sp. (b) High Sev, High Sp. (c) Low Sev, Low Sp. (d) High Sev, Low Sp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6156881/5719616/5719616-fig-1-source-large.gif
2012,5719616,Fig. 2.,"Average prequential accuracy (1) of DDD, EDDM, DWM and an ensemble without drift handling, considering 30 runs. The accuracy is reset when the drift begins (
f∈{1,N+1}
). The vertical black bars represent the average time step in which a drift is detected at each concept. The numbers in brackets are the average numbers of drift detections per concept. The results of the comparisons aided by T tests at the time steps
0.99N
,
1.1N
,
1.5N
, and
2N
are also shown: “> ” means that DDD attained better accuracy than EDDM, “< ” means worse, and “= ” means similar. (a) SineH Low Sev High Sp
(1.13,2.23)>=≫
. (b) Plane Low Sev High Sp
(0.17,1.47)=>=>
. (c) SineH High Sev High Sp
(1.13,2.17)>=≫
. (d) Plane High Sev High Sp
(0.10,1.03)=≪<
. (e) SineH Low Sev Low Sp
(1.13,2.43)>=≫
. (f) Circle Low Sev Low Sp
(none,1.26)==≫
. (g) Circle High Sev Low Sp
(none,3.13)=≫>
. (h) Plane High Sev Low Sp
(0.10,2.09)====
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6156881/5719616/5719616-fig-2-source-large.gif
2012,5719616,Fig. 3.,"Average weight attributed by DDD to each ensemble, considering 30 runs. (a) SineH Low Sev High Sp. (b) Plane Low Sev High Sp. (c) SineH High Sev High Sp. (d) Plane High Sev High Sp. (e) SineH Low Sev Low Sp. (f) Circle Low Sev Low Sp. (g) Circle High Sev Low Sp. (h) Plane High Sev Low Sp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6156881/5719616/5719616-fig-3-source-large.gif
2012,5719616,Fig. 4.,"Average prequential accuracy (1) obtained by an approach which always chooses the same ensemble for prediction, considering 30 runs. The accuracy is reset when the drift begins (
f∈{1,501}
). The vertical black bars represent the average time step in which a drift was detected at each concept. The numbers in brackets are the average numbers of drift detections per concept. (a) Plane Low Sev High Sp (0.17, 1.47). (b) Plane High Sev Low Sp (0.10, 2.90).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6156881/5719616/5719616-fig-4-source-large.gif
2012,5719616,Fig. 5.,"Average prequential accuracy (1) reset at every third of the learning, using MLPs/NB as base learners. (a) Electricity using MLPs. (b) PAKDD using MLPs. (c) KDD using MLPs. (d) KDD using NB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6156881/5719616/5719616-fig-5-source-large.gif
2012,5719616,Fig. 6.,"Average false positive and negative error rates for PAKDD, reset at every third of the learning using MLPs as base learners and considering 30 runs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6156881/5719616/5719616-fig-6-source-large.gif
2012,6157603,Fig. 1.,Recognition rate comparisons with other methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6167335/6157603/6157603-fig-1-source-large.gif
2012,6157603,Fig.2.,"Visualization of (a) the original images; (b) LBP histograms; (c) histograms of local monogenic magnitude binary pattern; (d) histograms of local monogenic real binary pattern; (e) histograms of local monogenic imaginary binary pattern. Rows from up to bottom: normal, weak and dark illumination.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6167335/6157603/6157603-fig-2-source-large.gif
2012,6157603,Fig. 3.,"Recognition rates of LBPTOP, STLMMBP, STLMRBP, STLMIBP, STLMBP-C and STLMBP-J.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6167335/6157603/6157603-fig-3-source-large.gif
2012,6157603,Fig. 4.,Recognition rate of cross-illuminations on VIS images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6167335/6157603/6157603-fig-4-source-large.gif
2012,6069610,Fig. 1.,"Year-wise distribution of articles for neural networks, support vector machines, and genetic algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6217351/6069610/6069610-fig-1-source-large.gif
2012,6069610,Fig. 2.,Year-wise distribution of articles for hybrid classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6217351/6069610/6069610-fig-2-source-large.gif
2012,6069610,Fig. 3.,Top five most widely used baseline models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6217351/6069610/6069610-fig-3-source-large.gif
2012,6069610,Fig. 4.,Top seven most widely used datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6217351/6069610/6069610-fig-4-source-large.gif
2012,6069610,Fig. 5.,"Average accuracy of the bankruptcy-prediction models by single, ensemble, and hybrid machine-learning techniques.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6217351/6069610/6069610-fig-5-source-large.gif
2012,6243194,Fig. 1.,Blondie24 architecture [3].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6378413/6243194/6243194-fig-1-source-large.gif
2012,6335478,Fig. 1.,"Different detection scenarios. The circle is the ego car, and three signs are distributed along the road. The area highlighted in red illustrates the driver's area of attention. (a) Standard scenario used for autonomous cars. Here, all signs must be detected and processed. (b) and (c) System that tracks the driver's attention. In (b), the driver is attentive and spots all signs. Therefore, the system just highlights the sign that is known to be difficult for people to notice. In (c), the driver is distracted by a passing car and thus misses two signs. In this case, the system should inform the driver about the two missed signs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-1-source-large.gif
2012,6335478,Fig. 2.,Basic flow in most TSR systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-2-source-large.gif
2012,6335478,Fig. 3.,"Examples of european signs. These are danish, but many countries use similar signs. (a) Speed limit. Sign c55. (b) End speed limit. Sign c56. (c) Start of freeway. Sign e55. (d) Right turn. Sign a41.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-3-source-large.gif
2012,6335478,Fig. 4.,Examples of signs from the U.s. National MUTCD. Image source: [15]. (a) Stop. Sign r1-1. (b) Yield. Sign r1-2. (c) Speed limit. Sign r2-1. (d) Turn warning with speed recommendation. Sign w1-2a.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-4-source-large.gif
2012,6335478,Fig. 5.,Examples of similar signs from the MUTCD. The situation in (c) exists only in the california MUTCD. Image source: [15]. (a) Speed limit. Sign r2-1. (b) Minimum speed. Sign r2-4. (c) End speed limit. Sign r3 (ca).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-5-source-large.gif
2012,6335478,Fig. 6.,"Example sign images from (a) the GTSRB and (b) the STS data set, with the sign bounding boxes superimposed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-6-source-large.gif
2012,6335478,Fig. 7.,General flow followed by typical sign detection algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-7-source-large.gif
2012,6335478,Fig. 8.,"Example of thresholding, looking for red hues. (a) Before thresholding. (b) After thresholding.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-8-source-large.gif
2012,6335478,Fig. 9.,Biologically inspired detection stage from [21]. Image source: [21].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-9-source-large.gif
2012,6335478,Fig. 10.,Basic principle behind the radial symmetry detector. Image inspired by [55]. (a) Possible circles for a gradient. (b) Intersecting vote lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-10-source-large.gif
2012,6335478,Fig. 11.,Votes from a radial symmetry system superimposed on the original image. The brightest spot coincides with the center of the sign. This image is from a system developed in conjunction with this paper and is a radial symmetry voting algorithm extended to work for rectangles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-11-source-large.gif
2012,6335478,Fig. 12.,"Example of sign relevancy challenges in a crop from our own collected data set. The signs have been manually highlighted, and while both signs would likely be detected, only the one to the right is relevant to the driver. The sign to the left belongs to another road, where the black and white cars come from.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6357363/6335478/6335478-fig-12-source-large.gif
2012,6175108,Fig. 1.,Construction of multi-view committee by complimentary acoustic features and randomized decision trees (RDTs).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6195042/6175108/6175108-fig-1-source-large.gif
2012,6186826,Fig. 1.,"Separating planes of LapESVR with different
μ
and
n
ev
. Two labeled points are shown in different colors, while the other points are unlabeled samples. When setting
μ=0
, LapESVR reduces to the supervised method (see (a)). With the increase of
μ
, the separating plane becomes better (see (a)–(c)). Selecting the optimal number of eigenvectors can also lead to a better separating plane (see (b) and (d)).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6203443/6186826/6186826-fig-1-source-large.gif
2012,6186826,Fig. 2.,"Test error of LapESVR with different
n
ev
on the benchmark data sets. (a) g241c. (b) g241d. (c) Digit1. (d) USPS. (e) COIL2. (f) Text.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6203443/6186826/6186826-fig-2-source-large.gif
2012,6186826,Fig. 3.,"Test error and training CPU time (in seconds) of LapESVR with different
n
ev
on three data sets. (a), (c), (e) show the test errors and (b), (d), (f) show the training CPU time. Two settings MNIST1 (the first five digits versus. The last five digits) and MNIST2 (the odd digits versus. The even digits) are used for MNIST data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6203443/6186826/6186826-fig-3-source-large.gif
2012,6186826,Fig. 4.,Test error and training CPU time (in seconds) of LapESVR with respect to the number of unlabeled data on the extended USPS dataset. (a) Test Error. (b) Training CPU time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6203443/6186826/6186826-fig-4-source-large.gif
2012,5765955,Fig. 1.,A simplified description of rules-7.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-1-source-large.gif
2012,5765955,Fig. 2.,"A simplified description of
Induc
e
−
On
e
−
Rule",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-2-source-large.gif
2012,5765955,Fig. 3.,Illustration of the concept of scope of classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-3-source-large.gif
2012,5765955,Fig. 4.,A simplified description of edisc.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-4-source-large.gif
2012,5765955,Fig. 5.,A simplified description of rules-7 with EDISC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-5-source-large.gif
2012,5765955,Fig. 6.,"A simplified description of
Induce
−
One
−
Rule
with EDISC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-6-source-large.gif
2012,5765955,Fig. 7.,"A simplified description of revised SETA
V",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-7-source-large.gif
2012,5765955,Fig. 8.,A simplified description of find_interval.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6226556/5765955/5765955-fig-8-source-large.gif
2012,6042874,Fig. 1,"Histograms of the generated data from our one-dimensional simulation, overlaid with the fitted Gaussian component densities from Models IV (left) and I (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6155150/6042874/6042874-fig-1-source-large.gif
2012,6042874,Fig. 2,"Boxplots of 100 adjusted Rand indices for Models I-IV for data from each of the multivariate Gaussian and multivariate
t
-distributions used in our 2D simulations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6155150/6042874/6042874-fig-2-source-large.gif
2012,6042874,Fig. 3,Boxplots of adjusted Rand indices for wine and yeast data with 20 and 80 percent known observations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6155150/6042874/6042874-fig-3-source-large.gif
2012,6095564,Fig. 1.,"The flowchart of our unsupervised image-matching framework. The left is the source and target image sets of different attributes, e.g., face images under various poses, motion sequences of different subjects and landscape images captured under different illuminations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-1-source-large.gif
2012,6095564,Fig. 2.,"(a) the illustration of local feature matching. Parameterized distance curves (b)
G
S
and (c)
G
T
;(
d
1
distance matrix
D
C
;(e)
distance matrix
D
A
after manifold alignment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-2-source-large.gif
2012,6095564,Fig. 3.,Sampled image sets used in our experiments. The left is expressive face images captured under different viewpoints and the feature mesh with 86 markers annotated in the bu3def database [19]. The right is landscapes of different illuminations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-3-source-large.gif
2012,6095564,Fig. 4.,"Matching results of face images under different poses. The upper is input probe images in (0 degree, 0 degree) data set and the lower matching images in (30 degree, 0 degree) data set by our approach. Incorrectly matched images are outlined in green.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-4-source-large.gif
2012,6095564,Fig. 5.,"Pose transfer performed on the feret and the oriental databases. The upper is ground truths of four sets of posed face images and the lower three are transformed face images based on the correspondence obtained in the original feature space (dm-f), w09, and our approach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-5-source-large.gif
2012,6095564,Fig. 6.,"Motion sequence matching results of different subjects. (a) parameterized curve groups,
G
s
and
G
T
. (b) Distance matrix
D
A
after alignment. (c) the upper is the input motion sequence of mia and the lower is matching images by our approach in aragor's sequence. Incorrectly matched images are outlined in green.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-6-source-large.gif
2012,6095564,Fig. 7.,"Matching results of landscape images under different illuminations. (a) parameterized curve groups,
G
S
and
G
T
, (b) Distance matrix
D
A
after alignment. (c) the upper is input images captured at 5 am and the lower is matching images captured at 7 pm. Incorrectly matched images are outlined in green.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-7-source-large.gif
2012,6095564,Fig. 8.,"The alignment of protein structures. (a) the first and the 21st backbone structures of protein 1 g7o rendered with strands. (b), (c), and (d) show the overlapping of two structures in 3d, 2d, and 1d space after alignments, where the upper is by our approach and the lower by wang's method (w09).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-8-source-large.gif
2012,6095564,Fig. 9.,"Matching accuracy with ranks from 1 to 10 in our experiments. The four curves are related to the matching in original feature spaces (dm-f), direct matching in normalized embedding spaces (dm-m), wang's (w09), and our approach (ma). (a) the matching between face image set of (0 degree, 0 degree) and (30 degree, 0 degree). (b) the matching between motion sequences of mia and aragor. (c) the matching between landscape image sets of 5 am and 7 pm. (d) the matching between the first and the 21st structures of glutaredoxin protein pdb-1g7o.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6218247/6095564/6095564-fig-9-source-large.gif
2012,6362157,Fig. 1.,Information pipelines for the baseline and wizarded systems (a) Baseline CheckItOut (b) Embedded Wizard.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-1-source-large.gif
2012,6362157,Fig. 2.,Abbreviated example of wizards' dialogue strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-2-source-large.gif
2012,6362157,Fig. 3.,Excerpt from a transcript of a phone call placed by a patron to a librarian representing a single intentional segment. The patron mistakenly confirms an incorrect spelling of an author name at line 9.0.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-3-source-large.gif
2012,6362157,Fig. 4.,"Recognized title (ASR), parse, terminals from the parse (Term), and voice search (VS) return from CheckItOut.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-4-source-large.gif
2012,6362157,Fig. 5.,Example of poor ASR performance and WA's strategy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-5-source-large.gif
2012,6362157,Fig. 6.,Graphical user interface for embedded wizards.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-6-source-large.gif
2012,6362157,Fig. 7.,Pseudo-code for the error handling component of the new DM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4200690/6392940/6362157/6362157-fig-7-source-large.gif
2012,5728934,Fig. 1.,F1 score comparison with different number of human labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6151926/5728934/5728934-fig-1-source-large.gif
2012,5728934,Fig. 2.,Recall comparison with different number of human labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6151926/5728934/5728934-fig-2-source-large.gif
2012,5728934,Fig. 3.,Precision comparison with different number of human labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6151926/5728934/5728934-fig-3-source-large.gif
2012,5728934,Fig. 4.,F1 score with different number of SS iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6151926/5728934/5728934-fig-4-source-large.gif
2012,5728934,Fig. 5.,Recall comparison with different number of SS iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6151926/5728934/5728934-fig-5-source-large.gif
2012,5728934,Fig. 6.,Precision comparison with different number of SS iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5326/6151926/5728934/5728934-fig-6-source-large.gif
2012,6145754,Fig. 1.,Red light violation at signalized intersection. Adapted from www.drivingschool.ca.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-1-source-large.gif
2012,6145754,Fig. 2.,"Target vehicle approaching intersection. Classification is performed in the
T
w
time window, and warning is potentially sent to the host vehicle at
t
warn
measured as the expected time to reach the intersection. In some cases, the time corresponding to
d
min
can be larger than
TT
I
min
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-2-source-large.gif
2012,6145754,Fig. 3.,SVM-BF architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-3-source-large.gif
2012,6145754,Fig. 4.,HMM-based classification architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-4-source-large.gif
2012,6145754,Fig. 5.,"Basic HMM
λ(T,t,e)
with
n=3
states, transition probabilities
T
ij
, and emissions
e
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-5-source-large.gif
2012,6145754,Fig. 6.,"Satellite image of the Peppers Ferry intersection (U.S. 460 and Peppers Ferry Rd, Christiansburg, Montgomery, Virginia 24073) taken from Google Earth. CICAS-V data from vehicles at Peppers Ferry intersection were used to test the algorithms presented in this paper.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-6-source-large.gif
2012,6145754,Fig. 7.,Regression curves used as decision thresholds for the SDR algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-7-source-large.gif
2012,6145754,Fig. 8.,"Ten best parameter combinations for the SVM-BF classifier with corresponding true positive rates for a basic generalization test with
TT
I
min
=1.6 s
and
d
min
=10 m
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-8-source-large.gif
2012,6145754,Fig. 9.,"Ten best parameter combinations for the HMM-based classifier with corresponding true positive rates for a basic generalization test with
TT
I
min
=1.6 s
and
d
min
=10 m
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-9-source-large.gif
2012,6145754,Fig. 10.,"ROC curves for all five algorithms with insets showing area of interest around 5% false positives. (a) ROC curves for 10 000/1000 basic test at
TT
I
min
=1.0 s
. (b) ROC curves for 5000/1000 m-fold test at
TT
I
min
=1.0 s
. (c) ROC curves for 10 000/1000 basic test at
TT
I
min
=1.6 s
. ROC curves for 5000/1000 m-fold test at
TT
I
min
=1.6 s
. (d) ROC curves for 10 000/1000 basic test at
TT
I
min
=2.0 s
. (e) ROC curves for 5000/1000 m-fold test at
TT
I
min
=2.0 s
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6205705/6145754/6145754-fig-10-source-large.gif
2012,5975168,Fig. 1,Overview of our approach. See the main text for details.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-1-source-large.gif
2012,5975168,Fig. 2,"Left: Detection windows returned by the individual detectors (Green:
FB+UB1
, Blue: UB2, Red: F). Right: Corresponding regressed windows.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-2-source-large.gif
2012,5975168,Fig. 3,Example of an annotated image from the ETHZ PASCAL Stickmen data set. Left: The original stickman annotation. Right: The common reference frame we derived from the sticks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-3-source-large.gif
2012,5975168,Fig. 4,"Precision-recall curve for the individual detectors and the combined ones. We consider a detection as correct when the Intersection-Over-Union (IoU) with a ground-truth annotation is at least 50 percent. In parentheses are average precision values (AP), defined as the area under the respective curve.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-4-source-large.gif
2012,5975168,Fig. 5,Two images with three candidate windows each. The blue boxes indicate the location of the human calculated by the detector. The green boxes show possible action object locations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-5-source-large.gif
2012,5975168,Fig. 6,"A pair of training images from the “tennis serve” action. Candidate windows are depicted as white boxes. We employ a fully connected model, meaning that pairwise potentials (green lines) connect each pair of candidate windows between each pair of training images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-6-source-large.gif
2012,5975168,Fig. 7,"For each human-object cue we show three possible configurations of human-object windows. The two leftmost configurations have a low-pairwise energy, while the rightmost has high energy compared to either of the first two.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-7-source-large.gif
2012,5975168,Fig. 8,"Human pose has a high-discriminative power for distinguishing actions. The solid window is the original human detection, while the dashed window shows the area from which the pose-from-gradients descriptor is extracted.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-8-source-large.gif
2012,5975168,Fig. 9,"Columns 1-5: Example of action-object windows localized by our method in weakly supervised training images of the Sports data set [1] (columns 1-3) and of the TBH data set (columns 4-5). Both the human (green) and the object (dashed pink) are found automatically. Each column shows three images from the same class. The method is able to handle multimodal human-object spatial configurations. Columns 6-8: Action-object windows automatically selected from images of the PASCAL Action 2010 data set [2]. The human window is given beforehand, as prescribed by the PASCAL protocol. The object is localized automatically by our method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-9-source-large.gif
2012,5975168,Fig. 10,"Example results on the TBH data set for test images that were correctly classified by our approach. Two images are shown for each action class (from left to right, “playing trumpet,” “riding bike,” and “wearing hat”). First row: Results from the weakly supervised setting WS. Second row: Results from the fully supervised setting FS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-10-source-large.gif
2012,5975168,Fig. 11,"Example results from the sports data set of Gupta et al. [1] for test images that were correctly classified by our approach. One image per class is shown (from left to right: “cricket batting,” “cricket bowling,” “croquet,” “tennis forehand,” “tennis serve,” and “volleyball”). First row: Weakly supervised setting. Second row: Fully supervised setting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-11-source-large.gif
2012,5975168,Fig. 12,"Example failures of our method on several test images (after training in the WS setting). Action labels indicate the (incorrect) classes the images were assigned to. The main reasons are: missed humans due to tilted pose or poor visibility (first, fourth, and sixth image), similarities between different action classes (fifth image), truncation or poor visibility of the action object (second and third image).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-12-source-large.gif
2012,5975168,Fig. 13,"Human-object spatial distributions learned in the FS setting (top) and in the WS setting (bottom). (a)-(c): Relative location of the action object with respect to the human (
k
l
in Section 3.4). Dashed boxes indicate the size and location of the human windows. (a) “Cricket Batting,” (b) “Croquet,” (c) “Playing Trumpet.” (d): Distribution of the object scale relative to the human scale for the action “Volleyball” (
k
s
in Section 3.4). The horizontal axis represents the
x
-scale and the vertical the
y
-scale. A cross indicates the scale of the human.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-13-source-large.gif
2012,5975168,Fig. 14,"Example results on the PASCAL Action 2010 test set [2]. Each column shows two test images from a class. From left to right: “playing instrument,” “reading,” “taking photo,” “riding horse,” and “walking.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/5975168/5975168-fig-14-source-large.gif
2012,6095611,Fig. 1.,"Articulated OI (left), given the contour a tracking window
χ
(in blue) and rectangular blocks
ς
i
(in green) to approximate the shape of an actor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-1-source-large.gif
2012,6095611,Fig. 2.,Overview of proposed recognition framework using incrementally learnable classifiers and PHOG features extracted from adaptive blocks to approximate the contour of a moving object.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-2-source-large.gif
2012,6095611,Fig. 3.,"Tracking results using action videos of run, kick, golf, and dive (top to bottom) from UCF Sports dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-3-source-large.gif
2012,6095611,Fig. 4.,Absolute difference of PHOG features for action videos walk and jack performed by actor Lena from Weizmann dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-4-source-large.gif
2012,6095611,Fig. 5.,Simplified structure of ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-5-source-large.gif
2012,6095611,Fig. 6.,Data flow diagrams for recursive learning strategy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-6-source-large.gif
2012,6095611,Fig. 7.,"Example frames from Weizmann dataset (top row), KTH dataset (middle row), and UCF sports dataset (bottom row).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-7-source-large.gif
2012,6095611,Fig. 8.,"(a) Performance analysis of ELM by replacing PHOG features with different ST features, (b) separability of extracted PHOG features in eigen-space for different actions, and (c) training precision (%) analysis for varying amount of samples in incremental and batch-mode learning for Weizmann dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-8-source-large.gif
2012,6095611,Fig. 9.,"Lowest classification (among nine permutations) for changing number of frames in Weizmann dataset (a) using separate PHOG features of rectangular blocks, (b) using concatenated PHOG features of rectangular blocks, and (c) for varying number of frames and weight of tracking window estimate (this figure is best viewed in colors).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-9-source-large.gif
2012,6095611,Fig. 10.,Classification accuracy (%) for varying length of snippets using the incremental learning scheme for randomly selected actions from Weizmman dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-10-source-large.gif
2012,6095611,Fig. 11.,"(a) Performance analysis (averaged value of nine permutations) using changing number of frames from Weizmman dataset. (b) Confusion matrix, Weizmann dataset. (c) Confusion matrix, KTH dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-11-source-large.gif
2012,6095611,Fig. 12.,"Confusion matrix—UCF Sports dataset (snippet
length=6
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6341125/6095611/6095611-fig-12-source-large.gif
2012,6095566,Fig. 1.,"Our object segmentation framework. segments are extracted around regularly placed foreground seeds, with various background seeds corresponding to image boundary edges, for all levels of foreground bias, which has the effect of producing segments at different locations and spatial scales. the resulting set of segments is ranked according to their plausibility of being good object hypotheses, based on mid-level properties. ranking involves first removing duplicates, then diversifying the segment overlap scores using maximum marginal relevance measures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-1-source-large.gif
2012,6095566,Fig. 2.,"Different effects of uniform and color-based unary terms. for illustration, a single foreground seed was placed manually at the same location for two energy problems, one with uniform and another with color unary terms. shown are samples from the set of successive energy breakpoints (increasing
λ
values) from left to right, as computed by parametric max-flow. uniform unary terms are used in rows 1 and 3. color unary terms are used in even rows. uniform unary terms are most effective in images where the background and foreground have similar color. color unary terms are more appropriate for objects with elongated shapes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-2-source-large.gif
2012,6095566,Fig. 3.,"Frequency of the parametric max-flow breakpoints for each seed on the training set of the voc2010 segmentation data set. these results were obtained using a 6 × 6 uniform grid of seeds. the number of breakpoints has mean 110, and a heavier tail toward a larger number of breakpoints.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-3-source-large.gif
2012,6095566,Fig. 4.,"Feature importance for the random forests regressor learned on the voc2009 segmentation training set. the minor axis of the ellipse having the same normalized second central moments as the segment (here “minor axis length”) is, perhaps surprisingly, the most important. this feature used in isolation results in relatively poor rankings however (see fig. 5a). the graph properties have small importance. the “boundary fraction of low cut” features, being binary, do not contribute at all. gestalt features have above average importance, particularly the contour energies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-4-source-large.gif
2012,6095566,Fig. 5.,Ranking results on the weizmann and voc2009 data sets. different rankers are compared with the optimal ranker (“upper bound”) and with random ranking (“random selection”).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-5-source-large.gif
2012,6095566,Fig. 6.,"Segmentation and ranking results obtained using the random forests model learned on the voc2009 training set, with the features described in section 4. the green regions are the segment foreground hypotheses. the first image in each row shows the ground truth, the second and third images show the most plausible segments given by cpmc, the last two images show the least plausible segments, and the fourth and fifth images show segments intermediately placed in the ranking. the predicted segment scores are overlaid. the first three images are from the voc2009 validation set and rows 2, 4, and 6 show the diversified rankings, with
θ=0.75
. note that in the diversified ranking, segments scored nearby tend to be more dissimilar. the last three rows show results from the weizmann segmentation database. the algorithm has no prior knowledge of the object classes, but on this data set, it still shows a remarkable preference for segments with large spatial overlap with the imaged objects, yet there are neither chariots nor vases in the training set, for example. the lowest ranked object hypotheses are usually quite small reflecting perhaps the image statistics in the voc2009 training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-6-source-large.gif
2012,6095566,Fig. 7.,"Quality of the segments in the combined voc2009 train and validation sets, as a function of the area of the ground truth segments. object area has been discretized into 20 bins on a log scale. in the case of the ground truth curve, the y-axis corresponds to the number of segments assigned in each bin (ground truth segments have an overlap value of 1 with themselves). medium and large size objects, which are more frequent, are segmented significantly more accurately by cpmc than by gpb-owt-ucm [33]. subframe-cpmc is competitive with gpb-owt-ucm on small objects, but generates a larger segment pool than plain cpmc (on the order of 700 instead of 150 elements).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-7-source-large.gif
2012,6095566,Fig. 8.,"Learned feature weights for the subframe-cpmc model. the original set of mid-level features and region properties gets higher weights, texture features get intermediate weights and shape features get smaller weights. texture features might help discard amorphous “stuff” regions such as grass, water, and sky.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-8-source-large.gif
2012,6095566,Fig. 9.,"Segmentation results on images from the validation set of the voc2010 database. the first column contains the original images, the second gives the human ground truth annotations of multiple objects, the third shows the best segment in the subframe-cpmc pool for each ground truth object, and the fourth shows the best segment among the ones ranked in the top-200. the proposed algorithm obtains accurate segments for objects at multiple scales and locations, even when they are spatially adjacent. see Fig. 10 for challenging cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-9-source-large.gif
2012,6095566,Fig. 10.,"Examples, taken from the validation set of voc2010, where the cpmc algorithm encounters difficulties. the first column shows the images, the second shows the human ground truth annotations of multiple objects, the third shows the best segment in the entire subframe-cpmc pool for each ground truth object, and the fourth shows the best segment among the ones ranked in the top-200. partially occluded objects (first two rows), wiry objects (third row), and objects with low background contrast (fourth and fifth rows) can cause difficulties.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-10-source-large.gif
2012,6095566,Fig. 11.,"Average overlap between ground truth objects and the best subframe-cpmc segments on the validation set of voc2010. we compare results obtained when considering all segments, just the top ranked 100 or 200, and a baseline that selects 100 segments randomly from the pool of all segments. certain classes appear to be considerably harder to segment, such as bicycles, perhaps due to their wiry structure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-11-source-large.gif
2012,6095566,Fig. 12.,"Recall at 50 percent overlap between regions of ground truth objects and the best subframe-cpmc segments (top) and between ground truth bounding boxes and best subframe-cpmc segment bounding boxes (bottom). note that bicycles are difficult to segment accurately due to their wiry structure, but there is usually some segment for each bicycle that has an accurate bounding box, such as the ones shown in the third row of Fig. 2. these results are computed on the validation set of the voc2010 segmentation data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6200921/6095566/6095566-fig-12-source-large.gif
2012,6136516,Fig. 1,"This example illustrates the relation between slowly varying action concepts and quickly varying pixel values. (a) Three action samples are concatenated into one sequence, including boxing, hand waving, and walking. (b) The gray values of three pixels “P1,” “P2,” and “P3” over time. (c) High-level representation of action categories over time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-1-source-large.gif
2012,6136516,Fig. 2,"Diagram of the SFA-based method. First, a large amount of cuboids are collected in training sequences. Then, a number of slow feature functions are obtained by SFA. At the test stage, the detected cuboids in a given video are transformed by the learned slow feature functions. Afterward, a feature vector is calculated to encode the statistical distribution of slow features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-2-source-large.gif
2012,6136516,Fig. 3,"Examples of training cuboids denoted by the light gray area, where the solid black lines represent the foreground bounding boxes with the size of
110×80
and the image size is
120×160
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-3-source-large.gif
2012,6136516,Fig. 4,"The reformatting process of the cuboid. The white dashed box is the selected local cuboid in the action sequence. Then, the cuboid is reformatted so that the input vector at each time includes
Δt
successive patches. Here,
Δt=3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-4-source-large.gif
2012,6136516,Fig. 5,"Four kinds of SFA learning strategies: the unsupervised SFA, the supervised SFA, the discriminative SFA, and the spatial discriminative SFA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-5-source-large.gif
2012,6136516,Fig. 6,"An example of the computation of the ASD feature. A number of cuboids are collected by random sampling in motion boundaries. Then, the cuboids are transformed by all slow feature functions. Finally, the squared first order derivatives of all cuboids are accumulated to form the feature representation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-6-source-large.gif
2012,6136516,Fig. 7,Example of the SD-SFA-based feature representation. The features in subregions are concatenated into a long vector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-7-source-large.gif
2012,6136516,Fig. 8,"Sample images of interactions in the CASIA database. There are three categories of two-person interactions, i.e., meet, fight, and rob. For a clear illustration, the foreground areas of the three interactions are enlarged as shown at the right side.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-8-source-large.gif
2012,6136516,Fig. 9,"Example images in the UT-Interaction data set. There are six types of interactions, i.e., hug, kick, point, punch, push, and hand-shake. In total, 120 segments are divided into two sets, where Set #1 is taken at a parking lot with less noise and Set #2 is taken from a lawn on a windy day.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-9-source-large.gif
2012,6136516,Fig. 10,"The settings of control experiments and the corresponding experimental results. Each path from the root node to a leaf node denotes one test, where the nodes denote the experimental settings and the under leaf node is the recognition accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-10-source-large.gif
2012,6136516,Fig. 11,Some visualizations of the slow feature functions learned by different SFA strategies. (a) The slow feature functions learned by the U-SFA. (b) The functions learned by the S-SFA for the action boxing. (c) The functions learned by the D-SFA for the action boxing. (d) The functions learned by the SD-SFA for the action boxing in the first subregion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-11-source-large.gif
2012,6136516,Fig. 12,The squared derivatives of the cuboids transformed by the learned slow feature functions. The value under each figure is the corresponding average squared derivative over all cuboids.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-12-source-large.gif
2012,6136516,Fig. 13,"The average accumulated squared derivatives features of the original cuboids and the cuboids transformed by the U-SFA slow feature functions. (a) The ASD features of the original cuboids. (b) The ASD feature of the cuboids transformed by the U-SFA functions which are learned from the training cuboids of all action categories. (c) The functions are learned from the cuboids without jogging, running, and walking. (d) The functions are learned from the cuboids without jogging and running.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-13-source-large.gif
2012,6136516,Fig. 14,The ASD features of the cuboids transformed by the S-SFA and D-SFA slow feature functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-14-source-large.gif
2012,6136516,Fig. 15,Confusion matrices of the classification on the KTH data set obtained by different SFA learning strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-15-source-large.gif
2012,6136516,Fig. 16,"Examples of the ASD features on the Weizmann data set. The collected cuboids are transformed by 10 sets of slow feature functions learned by the D-SFA. For each set of slow feature functions, one function is selected as an example. The squared derivatives of the transformed cuboids are presented, where the magnitudes are represented by different gray levels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-16-source-large.gif
2012,6136516,Fig. 17,Confusion matrices of the classification on the Weizmann data set by different SFA learning strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-17-source-large.gif
2012,6136516,Fig. 18,Confusion matrices of multiperson interactions classification: D-SFA + ASD versus GD + BoW.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6136512/6136516/6136516-fig-18-source-large.gif
2012,6029340,Fig. 1.,(a) Example (left two) pedestrian and (right two) nonpedestrian images and (b)local image regions for extracting pixel statistical features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6029340/6029340-fig-1-source-large.gif
2012,6029340,Fig. 2.,"Example of a (tree-based) genetic program representing the mathematical expression
(
F
1
+
F
2
)−0.5
. The output of the genetic program (when evaluated on a data instance) is mapped onto two class labels using zero as the class threshold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6029340/6029340-fig-2-source-large.gif
2012,6029340,Fig. 3.,"Figure on the left shows an example of class distributions of genetic program outputs for minority and majority class instances, and two class thresholds (0 and
t
). The horizontal axis is the genetic program output, and the height of a point along either distribution is the frequency of class instances which evaluate the same output. The figure on the right shows an example ROC curve, where points
A
and
B
correspond to the performance of the genetic program using class thresholds 0 and
t
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6029340/6029340-fig-3-source-large.gif
2012,6029340,Fig. 4.,"Genetic program outputs for two classifiers;
X
denotes the classifier's outputs for different (minority class) examples, where equivalent
X
values are stacked above each other. The solid circle shows correct class predictions, and the dotted circle shows incorrect predictions. Classifier
(b)
earns more rewards than
(a)
, as
(b)
has more predictions that lie further away from the (zero) class threshold; these earn the highest reward per prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6029340/6029340-fig-4-source-large.gif
2012,6029340,Fig. 5.,Average AUC (training set) of the fittest evolved solution in the population using the GP fitness functions over 50 independent runs (Ped task).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6169194/6029340/6029340-fig-5-source-large.gif
2012,6146437,Fig. 1.,Overview of the web-supervised system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-1-source-large.gif
2012,6146437,Fig. 2.,Example images for images downloaded from the WWW for the concept “airplane.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-2-source-large.gif
2012,6146437,Fig. 3.,Example images for images downloaded from the WWW for the concept “car.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-3-source-large.gif
2012,6146437,Fig. 4.,Pseudo code for spam filtering based on textual data using hyponyms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-4-source-large.gif
2012,6146437,Fig. 5.,"Examples for two different subclasses (clusters) of training images for car, clustered using visual features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-5-source-large.gif
2012,6146437,Fig. 6.,"Two subclasses (rows) for “car” after
k
-means clustering using visual features, both discarded as spam.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-6-source-large.gif
2012,6146437,Fig. 7.,Abstraction of the random savanna concept.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-7-source-large.gif
2012,6146437,Fig. 8.,Pseudo code for random savanna update.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-8-source-large.gif
2012,6146437,Fig. 9.,Pseudo code for the scalability manager.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-9-source-large.gif
2012,6146437,Fig. 10.,Example images of the test sets TS1 and TS2. Three examples for each concept are shown.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-10-source-large.gif
2012,6146437,Fig. 11.,Results for nine concepts with and without spam filtering of training images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-11-source-large.gif
2012,6146437,Fig. 12.,"Mean AvP for nine concepts for the test sets TS1 and TS2 using the web-supervised learning system and a different number of training images (
x
-axis).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-12-source-large.gif
2012,6146437,Fig. 13.,"Mean AvP for five concepts of the VOC test set TS-VOC using the web-supervised learning system and a varying number of positive training images (
x
-axis).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-13-source-large.gif
2012,6146437,Fig. 14.,Results of Fig. 13 with mean AvP averaged for five subsequent points of measurement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-14-source-large.gif
2012,6146437,Fig. 15.,"Mean AvP for five concepts of the test set TS-VOC, when using EM algorithm for clustering positive training images based on visual features. \${X}\$ -axis. Different number of positive training images ( \${x}\$ -axis).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6239698/6146437/6146437-fig-15-source-large.gif
2012,6275431,Fig. 1.,LTR-RT and sLTR—rectangles represent LTRs; solid line represents viral genes; dotted line represents genomic DNA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-1-source-large.gif
2012,6275431,Fig. 2.,Formation of solitary LTR. 1 shows the original LTR-RT; 2 shows homologous recombination; 3 shows the resulting sLTR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-2-source-large.gif
2012,6275431,Fig. 3.,"Example of an evolved 6-state SEM. Arrows are labeled with IUPAC codes (shown in Table 1) for DNA base transitions. States 3 and 4 form a transient communicating class. States 0, 1, and 2 are transient states, and State 5 is an attracting state. States 1 and 3 create highly effective features discussed in Section 3.3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-3-source-large.gif
2012,6275431,Fig. 4.,Representation of SEM shown in Fig. 3 used in the genetic algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-4-source-large.gif
2012,6275431,Fig. 5.,Histogram of absolute values of correlations of pairs of features. Filled bars represent highly correlated pairs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-5-source-large.gif
2012,6275431,Fig. 6.,"Classification accuracy predicted by rfcv using different numbers of variables calculated using a data set combining RB, RM, and RT data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-6-source-large.gif
2012,6275431,Fig. 7.,Projection into two dimensions of sLTRs from the three different types of data represented using the four super-features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-7-source-large.gif
2012,6275431,Fig. 8.,"Accuracy of classifiers using different types of data sets for training and testing. Box plots represent the distribution of accuracies produced by classifiers created with individual 20-state evolved machines and with groups of 20 SEM features chosen by DS with random selection. Between the boxplots are shown the accuracies of classifiers built using all the 4- and 6-state SEM features (X), all the 20-state SEM features (L), and all the nonevolved features (N). Also shown as impulses are the accuracies of four classifiers created using DS with “center” and “best” selection methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-8-source-large.gif
2012,6275431,Fig. 9.,Classification accuracy of 137 feature sets generated by DC and 100 feature sets generated by individual evolved 20-state SEMs tested on mixed RM and RT data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-9-source-large.gif
2012,6275431,Fig. 10.,Projection into two dimensions of sLTRs and SINEs from all data sets represented using the four superfeatures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6375734/6275431/6275431-fig-10-source-large.gif
2012,6096441,Fig. 1.,Block diagram of the S-AC algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-1-source-large.gif
2012,6096441,Fig. 2.,Tile coding example. The dot represents a point in the state space. Two tilings each have one (shaded) tile to which that particular point belongs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-2-source-large.gif
2012,6096441,Fig. 3.,Block diagram of the MLAC algorithm. The solid lines indicate actual signals. The dashed lines indicate the use of a local linear model or gradient from a particular entity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-3-source-large.gif
2012,6096441,Fig. 4.,Block diagram of the RMAC algorithm. The solid lines indicate actual signals. The dashed lines indicate the use of a local linear model or gradient from a particular entity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-4-source-large.gif
2012,6096441,Fig. 5.,Inverted pendulum setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-5-source-large.gif
2012,6096441,Fig. 6.,"Results for the S-AC algorithm. The mean, max, and min bounds and 95% confidence region are computed from 40 learning curves.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-6-source-large.gif
2012,6096441,Fig. 7.,"Final critic
V(x)
for the S-AC algorithm after one learning experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-7-source-large.gif
2012,6096441,Fig. 8.,"Final actor
π(x)
for the S-AC algorithm after one learning experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-8-source-large.gif
2012,6096441,Fig. 9.,"Results for the S-AC algorithm with two initializations for the value function:
V
0
(x)=−1000
for all
x
and
V
0
(x)=(1/1−γ)
min
x,u
r(x,u)≈−4050
(pessimistic).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-9-source-large.gif
2012,6096441,Fig. 10.,"Results for the MLAC algorithm. The mean, max, and min bounds and 95% confidence region are computed from 40 learning curves.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-10-source-large.gif
2012,6096441,Fig. 11.,"Final critic
V(x)
for the MLAC algorithm after one learning experiment. Every point represents a sample
[x|V]
in the critic memory
M
C
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-11-source-large.gif
2012,6096441,Fig. 12.,"Final actor
π(x)
for the MLAC algorithm after one learning experiment. Every point represents a sample
[x|u]
in the actor memory
M
A
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-12-source-large.gif
2012,6096441,Fig. 13.,"Results for the RMAC algorithm. The mean, max, and min bounds and 95% confidence region are computed from 40 learning curves.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-13-source-large.gif
2012,6096441,Fig. 14.,"Final critic
V(x)
for the RMAC algorithm after one learning experiment. Every point represents a sample
[x|V]
in the critic memory
M
C
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-14-source-large.gif
2012,6096441,Fig. 15.,"Final reference model
R(x)
for the RMAC algorithm after one learning experiment. Every arrow represents a sample
[x|
x
^
]
in the reference model memory
M
R
. The arrow points to the desired next state
x
^
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-15-source-large.gif
2012,6096441,Fig. 16.,"Learning curves for S-AC, MLAC, and RMAC compared with each other.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-16-source-large.gif
2012,6096441,Fig. 17.,"Learning curves for S-AC, MLAC, and RMAC compared with each other, where the value function of S-AC was initialized with
V(x)=−1000
for all
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3477/6198838/6096441/6096441-fig-17-source-large.gif
2012,5710866,Fig. 1.,Problem classification process in multitier web environment. The monitoring data and error logs are manually labeled with the real underlying problem that occurred when the data were collected. The patterns specific for an exiting problem taxonomy are learned from this historical labeled data. Our system recognizes problems when given a new set of monitoring and log data based on the learned patterns.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-1-source-large.gif
2012,5710866,Fig. 2.,"An example of problem taxonomy represented using two-tree forest with
M=10
nodes. Each tree in the forest represents a different application. Each node is associated with a class label. Gray nodes indicate a problem trace of an error which is formed using a multilabel assignment in our learning algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-2-source-large.gif
2012,5710866,Fig. 3.,The impact of the learning parameter. The original hyperplanes are shown in solid lines. The new hyperplanes with different learning rates are in dash lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-3-source-large.gif
2012,5710866,Fig. 4.,"A demonstration of our framework. Size of RESERVE set is limited to be 50. Data are added incrementally from 0 to 2, 000.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-4-source-large.gif
2012,5710866,Fig. 5.,"A sample of error log trace from the IBM WebSphere application. The error contains timestamp, application ID, originating class as well as the error trace message.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-5-source-large.gif
2012,5710866,Fig. 6.,An example of CPU usage graph from the Snappimon monitoring suite. An error was injected between 08:00 am and 10:00 am that causes a low CPU usage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-6-source-large.gif
2012,5710866,Fig. 7.,"An example of error-feature matrix from one of the data sets used in this paper.
X
-axis corresponds to feature IDs, and
Y
-axis is error IDs. Each red dot indicates an appearance of a particular feature in an error message. The features contain both text features (logs) and numerical features (system metric). The matrix is more than 90 percent sparse.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-7-source-large.gif
2012,5710866,Fig. 8.,Two examples of feature correlations. (Left) Time and correlation plots of two highly correlated features. (Right) Time and correlation plots of two less correlated features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-8-source-large.gif
2012,5710866,Fig. 9.,"Servers and client setup for our experiments. Server 1 contains the database server. Server 2 installs web application server, database client, and a web application. A monitoring suite is installed in a separate machine.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-9-source-large.gif
2012,5710866,Fig. 10.,Problem taxonomy for our experiment. Two applications with seven different errors are presented in the tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-10-source-large.gif
2012,5710866,Fig. 11.,"Computational complexity of four learning algorithm. Our algorithm (online-Perc) exhibits a constant learning time with the increase of the training data, while traditional batch learning algorithm (
SVM
struct
) requires linear learning time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-11-source-large.gif
2012,5710866,Fig. 12.,"Comparison of hierarchical classification and flat classification. To classify an instance into the “Iptable” category, hierarchical classification only has to perform three cases of classification while flat classification has to perform binary classification in all nodes which potentially increases the error rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-12-source-large.gif
2012,5710866,Fig. 13.,Precision on the test data set. The combination of log and numerical data performs the best. Top: results using online Perceptron classifier. Bottom: results using online SVM classifier. Left: 100 percent features used. Middle: 70 percent features selected. Right: 40 percent features selected.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-13-source-large.gif
2012,5710866,Fig. 14.,Comparison of the test performance between hierarchical classification versus flat classification. Our Hierarchical approach shows better results in all three cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-14-source-large.gif
2012,5710866,Fig. 15.,Comparison of different size of the RESERVE set with the original Perceptron algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4629386/6280555/5710866/5710866-fig-15-source-large.gif
2013,6461419,Fig. 1.,Learning control structure of kernel ACDs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-1-source-large.gif
2013,6461419,Fig. 2.,Performance comparisons between K-ACDs and ACDs under different parameter settings such as (a) success rates and (b) average trials.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-2-source-large.gif
2013,6461419,Fig. 3.,Performance comparisons between K-ACDs and ACDs under (a) different conditions of noise levels and (b) different number of hidden layer nodes in actor networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-3-source-large.gif
2013,6461419,Fig. 4.,Angle variations of the real cart-pole system controlled by different learning controllers after convergence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-4-source-large.gif
2013,6461419,Fig. 5.,Ball and plate system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-5-source-large.gif
2013,6461419,Fig. 6.,Total squared errors of four algorithms in 18 trials.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-6-source-large.gif
2013,6461419,Fig. 7.,Performance comparisons of the final policies obtained by the four algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-7-source-large.gif
2013,6461419,Fig. 8.,Performance comparisons of KDHP and DHP algorithms for real-time control of the ball and plate system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461419/6461419-fig-8-source-large.gif
2013,6596491,Fig. 1.,Visual interface of the crane simulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6679569/6596491/6596491-fig-1-source-large.gif
2013,6596491,Fig. 2.,Agent learning process (observational learning only).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6679569/6596491/6596491-fig-2-source-large.gif
2013,6596491,Fig. 3.,Algorithm 1: Calculation of Similarity Factor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6679569/6596491/6596491-fig-3-source-large.gif
2013,6596491,Fig. 4.,Algorithm 2: Performance Rating calculation in crane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6679569/6596491/6596491-fig-4-source-large.gif
2013,6596491,Fig. 5.,LATA agent in the process of teaching a trainee.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6679569/6596491/6596491-fig-5-source-large.gif
2013,6425391,Fig. 1.,General active learning flowchart using the quintuplet terminology of [24].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-1-source-large.gif
2013,6425391,Fig. 2.,Comparison between (a) query-by-committee and (b) multiview approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-2-source-large.gif
2013,6425391,Fig. 3.,Correlation coefficient matrix of KSC hyperspectral data on area 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-3-source-large.gif
2013,6425391,Fig. 4.,"Overall classification accuracy for SVM classification of KSC area 1 with multiview active learning based on band correlation,
k
-means clustering, uniform band slicing, and random view generation compared to random sampling and margin sampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-4-source-large.gif
2013,6425391,Fig. 5.,Overall classification accuracy (OACC) and view performance derived from correlation-based view generation for KSC AVIRIS data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-5-source-large.gif
2013,6425391,Fig. 6.,"Spectral/spatial active learning with spatial/spectral information: (a) spectral MCLU criterion [27]; (b) distance to current support vectors in the spatial domain; (c) combined criterion exploiting nondominated solution between (a) and (b) [54]. In all three maps, bright tones highlight interesting regions for new samples. (See the Appendix for details about the ProSpecTIR data.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-6-source-large.gif
2013,6425391,Fig. 7.,"Active learning for labeling a segmentation hierarchy. Best classification per number of queries using bisecting
k
-means [57]: (top) result after building the clustering without spatial constraints; (middle and bottom) results using constraint on cluster contiguity using (middle) random queries; and (bottom) active queries [13]. for legend colors, refer to Table 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-7-source-large.gif
2013,6425391,Fig. 8.,Top: average spectral signature for classes (a) Graminoid marsh and (b) Salt marsh found in the two disjoint areas of the KSC image. Bottom: signatures of three similar classes for (c) area 1 and (d) area 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-8-source-large.gif
2013,6425391,Fig. 9.,(a) and (b) Feature space visualization of the ten classes for the two areas of the KSC image. (c) Superposition of ten training pixels per class taken from area 1 (large dots) on the area 2 samples (small dots). Each color represents a landcover class. (a) Area 1. (b) Area 2. (c) Combined.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-9-source-large.gif
2013,6425391,Fig. 10.,Active learning used to correct the shift between class distributions of the two disjoint areas of the KSC data set. Curves are averages over ten experiments using SVM. The initial training set is composed by 500 pixels randomly selected from area 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-10-source-large.gif
2013,6425391,Fig. 11.,"(a) and (c) RGB images. (b) and (d) Class label images of KSC area 1 (top row) and area 2 (bottom row), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-11-source-large.gif
2013,6425391,Fig. 12.,(a) RGB image. (b) Class label image of Indian Pine 2010 SpecTIR data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6461951/6425391/6425391-fig-12-source-large.gif
2013,6472238,Fig. 1.,"Illustration of representation-learning discovering explanatory factors (middle hidden layer, in red), some explaining the input (semi-supervised setting), and some explaining target for each task. Because these subsets overlap, sharing of statistical strength helps generalization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-1-source-large.gif
2013,6472238,Fig. 2.,"(Top) Samples from convolutionally trained
μ
-ssRBM from [53]. (Bottom) Images in the CIFAR-10 training set closest (L2 distance with contrast normalized training images) to corresponding model samples on top. The model does not appear to be overfitting particular training examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-2-source-large.gif
2013,6472238,Fig. 3.,"When data concentrate near a lower dimensional manifold, the corruption vector is typically almost orthogonal to the manifold, and the reconstruction function learns to denoise, map from low-probability configurations (corrupted inputs) to high-probability ones (original inputs), creating a vector field aligned with the score (derivative of the estimated density).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-3-source-large.gif
2013,6472238,Fig. 4.,"The tangent vectors to the high-density manifold as estimated by a contractive autoencoder [167]. The original input is shown on the top left. Each tangent vector (images on right side of first row) corresponds to a plausible additive deformation of the original input, as illustrated on the second row, where a bit of the third singular vector is added to the original, to form a translated and deformed image. Unlike in PCA, the tangent vectors are different for different inputs because the estimated manifold is highly nonlinear.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-4-source-large.gif
2013,6472238,Fig. 5.,"Reconstruction function
r(x)
(green) learned by a high-capacity autoencoder on 1D input, minimizing reconstruction error at training examples
x
(t)
(
r(
x
(t)
)
in red) while trying to be as constant as possible otherwise. The dotted line is the identity reconstruction (which might be obtained without the regularizer). The blue arrows show the vector field of
r(x)−x
pointing toward high-density peaks estimated by the model and estimating the score (log-density derivative).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-5-source-large.gif
2013,6472238,Fig. 6.,"Sampling from regularized autoencoders [170], [26]: Each MCMC step adds to current state
x
the noise
δ
, mostly in the direction of the estimated manifold tangent plane
H
and projects back toward the manifold (high-density regions) by performing a reconstruction step.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-6-source-large.gif
2013,6472238,Fig. 7.,"Top: Early, during training, MCMC mixes easily between modes because the estimated distribution has high entropy and puts enough mass everywhere for small-steps movements (MCMC) to go from mode to mode. Bottom: Later on, training relying on good mixing can stall because estimated modes are separated by wide low-density deserts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6472238/6472238-fig-7-source-large.gif
2013,6493458,Fig. 1.,Challenges with physiological signal analysis. (a) High-order models are required to distinguish targeted physiological states from background activity. (b) The manifestation of targeted states are different from patient-to-patient. (c) Physiological changes over time (particularly following acute events) result in signal dynamics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-1-source-large.gif
2013,6493458,Fig. 2.,"(a) SVM framework in an EEG-based seizure-detection example. The trainer generates a classifier model from previous observations. Real-time detection occurs in two steps: feature extraction and classification. (b) SVMs form a decision boundary from support vectors, which are sampled from the edge of the data distributions (only the principal components are shown).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-2-source-large.gif
2013,6493458,Fig. 3.,"Performance and energy of representative machine-learning algorithm (ECG-based cardiac arrhythmia detection using real medical data from [34]). (a) High-order models are required to achieve high accuracy, but for strong classification kernels (i.e., RBF kernel is used in example shown). (b) The classification energy scales with the model complexity (note, while the performance appears to saturate at
#SV=11000
for the data shown, additional support vectors may be selected by the training algorithm, which optimizes an objective function to derive the decision boundary).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-3-source-large.gif
2013,6493458,Fig. 4.,Processor architecture with machine-learning accelerators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-4-source-large.gif
2013,6493458,Fig. 5.,SVMA is configurable via memory-mapped registers to enable various classifier structures and kernels. The FSM controls the configurable data path unit (DPU) to realize the computations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-5-source-large.gif
2013,6493458,Fig. 6.,"The choice of specific kernel function strongly impacts the classification energy and memory requirements. For the polynomial kernel, the alternate formulation, which achieves linearization by transforming vectors to matrices, is also shown [23].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-6-source-large.gif
2013,6493458,Fig. 7.,"Batch data (a) without diversity metric, leading to clustered selections, and (b) with diversity metric, leading to coverage over large regions of the feature space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-7-source-large.gif
2013,6493458,Fig. 8.,"(a) Simulation of batch-selection algorithm on CPU, illustrating the need for background computation via an accelerator, and (b) ALDS accelerator computation flow.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-8-source-large.gif
2013,6493458,Fig. 9.,"Weighted metrics are stored as 15 b, and the MSB bit is used to indicate previously selected data points to be skipped in subsequent iterations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-9-source-large.gif
2013,6493458,Fig. 10.,"DPU employs two-stage pipeline to reduce glitch propagation, simulations show the energy benefit of the pipeline architecture.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-10-source-large.gif
2013,6493458,Fig. 11.,(a) In-line truncation of shifting prevents overflow yielding comparable performance to a floating-point implementation. (b) 16-b support-vector precision minimizes memory requirements and classification error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-11-source-large.gif
2013,6493458,Fig. 12.,Computation of the decision function with the RBF kernel. Accumulation over each support vector and accumulation over all support vectors is achieved in registers at the second stage of the pipeline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-12-source-large.gif
2013,6493458,Fig. 13.,Computation of the decision function with linear and polynomial kernels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-13-source-large.gif
2013,6493458,Fig. 14.,CORDIC implementation (left) and the resulting region of convergence with and without scaling (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-14-source-large.gif
2013,6493458,Fig. 15.,Die photo and prototype IC summary.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-15-source-large.gif
2013,6493458,Fig. 16.,"(a) EEG-based seizure detection extracts the energy from eight different frequency bins from each EEG channel in a 2-s epoch, and three epochs are combined to form a feature vector. (b) ECG-based arrhythmia detection samples 18 data points in each ECG beat along with the R-to-R information. Feature extraction runs on CPU and the SVMA executes SVM classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-16-source-large.gif
2013,6493458,Fig. 17.,Active learning block diagram (left) and demonstration setup (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-17-source-large.gif
2013,6493458,Fig. 18.,Active learning performance measured from the chip showing the benefit of the ALDS approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-18-source-large.gif
2013,6493458,Fig. 19.,Cycle count and energy savings for (a) seizure detection and (b) arrhythmia detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/6545290/6493458/6493458-fig-19-source-large.gif
2013,6461130,Fig. 1.,Architecture of the EE-SVM learning control system for biped robots with a single support phase (SSP) and a double support phase (DSP).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461130/6461130-fig-1-source-large.gif
2013,6461130,Fig. 2.,"Schematic of a seven-link robot in the
x--z
plane.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461130/6461130-fig-2-source-large.gif
2013,6461130,Fig. 3.,"Generating energy coefficients for the
N
sampling points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461130/6461130-fig-3-source-large.gif
2013,6461130,Fig. 4.,"After training the EE-SVM in a rough terrain (the width of a stair is 0.16 m) offline, the biped walking is realized in unlearned rough terrains (Case 1: the width of a stair is 0.18 m; Case 2: the width of a stair is 0.20 m). The height of one stair is 0.01 m, and three successive walking periods are implemented.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461130/6461130-fig-4-source-large.gif
2013,6461130,Fig. 5.,"Joint angles when different controllers are adopted for biped walking in level terrain, and the step length is 0.16 m: fuzzy (solid line), SVM (dot-dashed line), PID (dashed line), and the proposed EE-SVM (dotted line).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461130/6461130-fig-5-source-large.gif
2013,6461130,Fig. 6.,"ZMP trajectories when different controllers are adopted for biped walking in level terrain, and the step length is 0.16 m: PID (solid red line), SVM (dashed black line), fuzzy (dotted magenta line), and the proposed EE-SVM (dot-dashed green line).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461130/6461130-fig-6-source-large.gif
2013,6324460,Fig. 1.,"Saliency detection. From top to bottom, each row represents the original images, the saliency maps calculated by CA [15], and the saliency maps calculated by the proposed framework, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-1-source-large.gif
2013,6324460,Fig. 2.,"Selection of color space.
L
∗
a
∗
b
∗
, HSV, and RGB spaces are evaluated to choose a better one for saliency detection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-2-source-large.gif
2013,6324460,Fig. 3.,Statistical map of saliency distribution on 200 training images. Each pixel in the map indicates the possibility of being salient for a normalized image with fixed size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-3-source-large.gif
2013,6324460,Fig. 4.,"Selection of parameter
μ
for center-prone prior.
μ
is varied from 0 to 1 in a 0.01 spacing to evaluate the performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-4-source-large.gif
2013,6324460,Fig. 5.,"Extracted boundary. The boundary is learned from a set of training images by a logistic model. Then, it is employed as a high-level feature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-5-source-large.gif
2013,6324460,Fig. 6.,Precision-recall curves of saliency detection results by different algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-6-source-large.gif
2013,6324460,Fig. 7.,"Averaged precision, recall, and
F
-measure of saliency detection results by different algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-7-source-large.gif
2013,6324460,Fig. 8.,"(a) Original image. (b) Ground truth. Saliency maps produced by (c) AC [48], (d) CA [15], (e) FT [16], (f) GB [30], (g) IT [23], (h) LC [33], (i) MZ [49], (j) SR [24], (k) HC [31], and (l) the proposed EMDD in this paper.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-8-source-large.gif
2013,6324460,Fig. 9.,Performance of saliency detection on different sets of features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-9-source-large.gif
2013,6324460,Fig. 10.,"Original images (first row) and its corresponding noisy images. Each image in the data set is added with white Gaussian noise, keeping SNR as 20 dB (second row) and 60 dB (third row).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-10-source-large.gif
2013,6324460,Fig. 11.,Performance comparison before and after adding noise to images. The statistics reflect the algorithms' robustness to (a) 20-dB noise and (b) 60-dB noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-11-source-large.gif
2013,6324460,Fig. 12.,"(a) Original image. (b) Ground truth. (c) Saliency map by HC [31], (d) its corresponding carved seams, and (e) resized image. (f) Saliency map by the proposed algorithm EMDD, (g) its corresponding carved seams, and (h) resized image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-12-source-large.gif
2013,6324460,Fig. 13.,Performance comparison using feature representations of different dimensionalities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-13-source-large.gif
2013,6324460,Fig. 14.,Illustrative detection of multiple salient objects. (First column) Original images. (Second column) Ground-truth saliency results. (Third column) Saliency detection results by the proposed EMDD algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-14-source-large.gif
2013,6324460,Fig. 15.,Sample experimental results on images with rich textures. (First row) Original images. (Second row) Ground-truth saliency results. (Third row) Saliency detection results by the proposed EMDD algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-15-source-large.gif
2013,6324460,Fig. 16.,Illustration of several negative results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6324460/6324460-fig-16-source-large.gif
2013,6423895,Fig. 1.,Graphical example illustrating how spatial information can be used as a criterion for semisupervised self-learning in hyperspectral image classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-1-source-large.gif
2013,6423895,Fig. 2.,(a) False color composition of the AVIRIS Indian Pines scene. (b) (Right) Ground-truth map containing 16 mutually exclusive land-cover classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-2-source-large.gif
2013,6423895,Fig. 3.,(a) False color composition of the ROSIS Pavia scene. (b) Ground-truth map containing nine mutually exclusive land-cover classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-3-source-large.gif
2013,6423895,Fig. 4.,"Overall classification accuracies (as a function of the number of unlabeled samples) obtained for the AVIRIS Indian Pines data set using the (top) MLR and (bottom) probabilistic SVM classifiers, respectively. Estimated labels were used in all the experiments, i.e.,
l
r
=0
. (a) Five labeled samples per class
(
l
n
=80)
. (b) Ten labeled samples per class
(
l
n
=160)
. (c) Fifteen labeled samples per class
(
l
n
=240)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-4-source-large.gif
2013,6423895,Figure 10.,"Overall Average Individual Classification Accuracies (in Percentage) and
κ
Statistic Obtained Using the MLR and Probabilistic Classifiers When Applied to the AVIRIS Indian Pines Hyperspectral Data Set, With Ten Labeled Samples Per Class (160 Samples in Total) and
u
n
=750
Unlabeled Training Samples.
l
r
Denotes the Number of True Labels Available in
D
u
(Used to Implement an Optimal Version of Each Sampling Algorithm). The Standard Deviations are Also Reported for Each Test",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-table-1-source-large.gif
2013,6423895,Fig. 5.,"Overall classification accuracies (as a function of the number of unlabeled samples) obtained for the AVIRIS Indian Pines data set using the MLR classifier with BT sampling by using five labeled samples per class (in total, 80 samples). Two cases are displayed: The one in which all unlabeled samples are estimated by the proposed approach (i.e.,
l
r
=0
) and the optimal case, in which true labels are used whenever possible (i.e.,
l
r
=
u
r
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-5-source-large.gif
2013,6423895,Fig. 6.,"Classification maps and overall classification accuracies (in parentheses) obtained after applying the (top) MLR and (bottom) probabilistic SVM classifiers to the AVIRIS Indian Pines data set by using ten labeled training samples and 750 unlabeled samples, i.e.,
l
n
=160
,
u
n
=750
, and
l
r
=0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-6-source-large.gif
2013,6423895,Fig. 7.,"Overall classification accuracies (as a function of the number of unlabeled samples) obtained for the ROSIS Pavia University data set using the (top) MLR and (bottom) probabilistic SVM classifiers, respectively. Estimated labels were used in all the experiments, i.e.,
l
r
=0
. (a)
l
n
=45
. (b)
l
n
=90
. (c)
l
n
=135
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-7-source-large.gif
2013,6423895,Figure 11.,"Overall Average Individual Classification Accuracies (in Percentage) and
κ
Statistic Obtained Using the MLR and Probabilistic SVM Classifiers When Applied to the ROSIS University of Pavia Hyperspectral Data Set by Using Ten Labeled Samples Per Class (in Total, 90 Samples) and
u
n
=700
Unlabeled Training Samples.
l
r
Denotes the Number of True Labels Available in
D
u
(Used to Implement an Optimal Version of Each Sampling Algorithm). The Standard Deviations are Also Reported for Each Test",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-table-2-source-large.gif
2013,6423895,Fig. 8.,"Overall classification accuracies (as a function of the number of unlabeled samples) obtained for the ROSIS Pavia University data set using the MLR classifier with BT sampling by using 100 labeled samples per class (in total, 900 samples). Two cases are displayed: The one in which all unlabeled samples are estimated by the proposed approach (i.e.,
l
r
=0
) and the optimal case, in which true labels are used whenever possible (i.e.,
l
r
=
u
r
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-8-source-large.gif
2013,6423895,Fig. 9.,"Classification maps and overall classification accuracies (in parentheses) obtained after applying the (top) MLR and (bottom) probabilistic SVM classifiers to the ROSIS Pavia University data set (in all cases,
l
n
=90
and
l
r
=0
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6423895/6423895-fig-9-source-large.gif
2013,6310071,Fig. 1.,"Variation of
tr(
S
T
)∥ω
∥
2
,
R
2
∥ω
∥
2
, and the generalization error with respect to the base-kernel combination coefficient. In (a), two Gaussian kernels, with widths of one-half and two, respectively, are used as base kernels. The correlation coefficient between
tr(
S
T
)∥ω
∥
2
and the generalization error is 0.7537, and the correlation coefficient between
R
2
∥ω
∥
2
and the generalization error is 0.7446. In (b), a polynomial kernel with a degree of two and a Gaussian kernel with a width of one-half are used as base kernels. The correlation coefficient between
tr(
S
T
)∥ω
∥
2
and the generalization error is 0.4919, while the correlation coefficient between
R
2
∥ω
∥
2
and the generalization error becomes 0.1413. The same
C
that minimizes the generalization error is equally applied to compute the two criteria. (a) Gaussian + Gaussian. (b) Poly + Gaussian.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6310071/6310071-fig-1-source-large.gif
2013,6310071,Fig. 2.,Classification accuracy comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6310071/6310071-fig-2-source-large.gif
2013,6310071,Fig. 3.,MCC comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6310071/6310071-fig-3-source-large.gif
2013,6310071,Fig. 4.,"F1
score comparison.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6479286/6310071/6310071-fig-4-source-large.gif
2013,6516950,Fig. 1.,Window descriptive of the model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-1-source-large.gif
2013,6516950,Fig. 2.,Window descriptive of the model equations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-2-source-large.gif
2013,6516950,Fig. 3.,Window of user guide.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-3-source-large.gif
2013,6516950,Fig. 4.,Equivalent circuit model by phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-4-source-large.gif
2013,6516950,Fig. 5.,Window data entry.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-5-source-large.gif
2013,6516950,Fig. 6.,Virtual lab interface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-6-source-large.gif
2013,6516950,Fig. 7.,“Buttons Start/Stop”—-motor running.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-7-source-large.gif
2013,6516950,Fig. 8.,“Torque–Speed curve”—-at starting point.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-8-source-large.gif
2013,6516950,Fig. 9.,“Current–time graph”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-9-source-large.gif
2013,6516950,Fig. 10.,“Torque–Speed curve”—-Maximum torque point.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-10-source-large.gif
2013,6516950,Fig. 11.,“Torque–Speed curve”—-Rated point.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245520/6516927/6516950/6516950-fig-11-source-large.gif
2013,6336689,Fig. 1.,An intelligent design can transform the acquired information into knowledge by learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-1-source-large.gif
2013,6336689,Fig. 2.,The cognition cycle of an autonomous cognitive radio (referred to as the Radiobot) [5]. Decisions that drive Actions are made based on the Observations and Learnt knowledge. The impact of actions on the system performance and environment leads to new Learning. The Radiobots new Observations are guided by this Learnt Knowledge of the effects of past Actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-2-source-large.gif
2013,6336689,Fig. 3.,Supervised and unsupervised learning approaches for cognitive radios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-3-source-large.gif
2013,6336689,Fig. 4.,Typical problems in cognitive radio and their corresponding learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-4-source-large.gif
2013,6336689,Fig. 5.,"The reinforcement learning cycle: At the beginning of each learning cycle, the agent receives a full or partial observation of the current state, as well as the accrued reward. By using the state observation and the reward value, the agent updates its policy (e.g. updating the Q-values) during the learning stage. Finally, during the decision stage, the agent selects a certain action according to the updated policy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-5-source-large.gif
2013,6336689,Fig. 6.,System model of [77] which is formed of a Digital TV (DTV) cell and multiple WRAN cells.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-6-source-large.gif
2013,6336689,Fig. 7.,One realization of the Dirichlet process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-7-source-large.gif
2013,6336689,Fig. 8.,"The observation points
y
i
are classified into different clusters, denoted with different marker shapes. The original data points are generated from a Gaussian mixture model with 4 mixture components and with an identity covariance matrix.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-8-source-large.gif
2013,6336689,Fig. 9.,A diagram showing the basic idea of SVM: optimal separation hyperplane (solid red line) and two margin hyperplanes (dashed lines) in a binary classification example; Support vectors are bolded.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-9-source-large.gif
2013,6336689,Fig. 10.,A comparison among the learning algorithms that are presented in this survey.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6572931/6336689/6336689-fig-10-source-large.gif
2013,6531635,Fig. 1.,Application of the two-class SVM algorithm with the Gaussian kernel and a probability estimation [22]. Color code: the estimated probability.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-1-source-large.gif
2013,6531635,Fig. 2.,Application of the one-class SVM algorithm with the Gaussian kernel. Color code: distance from the boundary renormalized from 0 to 1 and with the threshold shifted to 0.5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-2-source-large.gif
2013,6531635,Fig. 3.,Application of the RVM algorithm with the Gaussian kernel. Color code: the estimated probability.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-3-source-large.gif
2013,6531635,Fig. 4.,"Overall results for all tested algorithms using the complete training set. The column cells contain the scores of following algorithms: two-class SVM with RBF kernel; two-class SVM with linear kernel; and RVM, one-class SVM. First row: the results for the test set. Second row: the scores for the training set. Finally, in each cell, the first box plot contains the following improvements of the training set: 1) artificial data and iterative removal of the worst outliers (left, black); 2) removal of the worst outliers (middle, red); and 3) none (right, blue).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-4-source-large.gif
2013,6531635,Fig. 5.,"Overall results for all tested algorithms using the small training set. The showed training score is, however, evaluated using the full training database. The arrangement of plots is the same as in Fig. 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-5-source-large.gif
2013,6531635,Fig. 6.,Time evolution of disruption prediction ratio for two-class SVM with iterative removal of outliers and artificial points. The red curve SVM is trained with the full database while the blue curve SVM with a small database (10% of discharges). JPS is a result of JET mode lock trigger system. Time points corresponding to 30 ms and 1 s before the disruption are identified by dashed lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-6-source-large.gif
2013,6531635,Fig. 7.,"Total score for the optimized two-class SVM in the JET campaigns C15 to C27. This figure shows the variance in score for different campaigns. Campaigns C19-C22 are used for training and campaigns C24, C25, C27 are used for testing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/27/6552859/6531635/6531635-fig-7-source-large.gif
2013,6338929,Fig. 1.,Effect of swapping the values of the ranking function of two feature values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338929/6338929-fig-1-source-large.gif
2013,6338929,Fig. 2.,Relation between the slopes of two consecutive line segments in a convex ROC curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338929/6338929-fig-2-source-large.gif
2013,6338929,Fig. 3.,Visualization of the ROC points in two-class discretization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338929/6338929-fig-3-source-large.gif
2013,6338929,Fig. 4.,Final cut-points after the first pass of convex hull algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338929/6338929-fig-4-source-large.gif
2013,6338929,Fig. 5.,The model learned for the single feature in Table 5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338929/6338929-fig-5-source-large.gif
2013,6365193,Fig. 1,"Vision problems arranged in order of “openness.” For some problems, we do not have knowledge of the entire set of possible classes during training and must account for unknowns during testing. In this paper, we develop a deeper understanding of those open cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-1-source-large.gif
2013,6365193,Fig. 2,"The open set recognition problem explicitly assumes not all classes are known a priori. Square images are from training, oval images are from testing. The class of interest (“dog”) is surrounded by other classes which can be known (“frog,” “birds”) or unknown (“owl,” “raccoon,” “?”). Plane
A
maximizes the SVM margin making “dog” a half-space—which is mostly open space. The 1-vs-set machine adds a second plane
Ω
and defines an optimization to adjust
A
and
Ω
to balance empirical and open space risk.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-2-source-large.gif
2013,6365193,Fig. 3,"The trouble with binary (1-vs-1) and multiclass (1-vs-All) classification for open set problems. In a 1-vs-1 scenario (a), good separation can be achieved between two classes during training, but this establishes margins that need not separate additional known or unknown classes. For instance, considering the margin between class 1 and class 2 above (labeled 1 vs 2), examples from class 3 and unknown classes fall indiscriminately across the margin. Similarly, in a 1-vs-All scenario (b), we see the same problem for unknown classes. In both cases; when considering just an additional training example (the red circle with a star in each figure), the results can be even worse, as the margins readjust for maximum separation. Far from the training classes, this produces very significant margin plane movement, which can be seen in the light gray new margin separation plane 1 vs
2
∗
in (a).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-3-source-large.gif
2013,6365193,Fig. 4,"Example of linear 1-vs-set machine showing the (a) base slab for both the 1-class and binary formulations, where the second class is only considered in the latter case, (b) the generalization, and (c) the specialization operators. Blue refers to generalization, red for specialization, and gray for the base linear 1-vs-set machine.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-4-source-large.gif
2013,6365193,Fig. 5,"A comparison between the 1-vs-set machine and typical SVMs with a linear kernel using two different statistics: (a) F-measure (b) and accuracy. These plots represent detail from the open universe of 88 classes with HOG features test found in Table 2. The classes shown here correspond to the top 25 for the binary 1-vs-set machine ranked by F-measure. Error bars reflect standard error. In every case shown, the binary 1-vs-set machine produces a higher F-measure and accuracy score compared to a binary SVM. The 1-class 1-vs-set machine shows more modest gains. In this very difficult open set scenario, accuracy places more emphasis on correct negative classification instances, where the F-measure provides a more meaningful balance between correct positive and negative classification instances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-5-source-large.gif
2013,6365193,Fig. 6,"Examples of the near and far plane pressure parameter space and corresponding F-measures when one of the two parameters is fixed at our selected default. The F-measure in this plot is calculated over all of the classes in the open world of 88 classes with HOG features. Notice how movement on the near and far planes during Algorithms 1 and 2 makes a difference in the resulting F-measures over the test data. Importantly, we see that the addition of a second plane
Ω
has an impact on recognition performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-6-source-large.gif
2013,6365193,Fig. 7,"An assessment of the F-measure as a function of openness (growing from left to right) for a collection of binary classifiers. The F-measure in this plot is calculated over all of the classes in the open universe of 88 classes with HOG features. As expected, all three machines decrease in accuracy as the universe grows to be more open. Even in the most open setting (82 percent), the 1-vs-set machine yields 8, 129 fewer false positives compared to the binary SVM with a linear kernel, and 10, 377 fewer false positives compared to the binary SVM with an RBF kernel. All 1-vs-set machine results are significantly better at a 95 percent confidence interval.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-7-source-large.gif
2013,6365193,Fig. 8,"Face verification results as a function of openness (growing from left to right) for a collection of binary classifiers and LBP-like and Gabor features. The F-measure in this plot is calculated over all of the classes in the subset of LFW we consider. Notice that in closed set testing (0 percent), there is not much difference between the 1-vs-set machines and the typical binary classifiers. In all open set cases, the 1-vs-set machine results are significantly better at a 95 percent confidence interval.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6516859/6365193/6365193-fig-8-source-large.gif
2013,6488857,Fig. 1.,(a) Number of misclassified points obtained by GMLVQ and (b) k-NN classifications (error bars report standard deviation across 12 training re-sampling) conducted on the MNIST dataset (images “5” and “8”).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6516643/6488857/6488857-fig-1-source-large.gif
2013,6488857,Fig. 2.,"Number of misclassified points obtained by the IT-TB in GMLVQ and the previously introduced
SVM+
based models for LUPI conducted on the MNIST dataset (images “5” and “8”).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6516643/6488857/6488857-fig-2-source-large.gif
2013,6488857,Fig. 3.,Mean misclassification rates (error bars report standard deviation across 10 training/test resampling) obtained using varying amounts of privileged information.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6516643/6488857/6488857-fig-3-source-large.gif
2013,6253273,Fig. 1.,"Illustration of the solution
K
in SimpleMKL and
G
in MK-ONJKL. Read the text for more detail.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6253273/6253273-fig-1-source-large.gif
2013,6253273,Fig. 2.,Comparison of training time between ELMMKL [23] and our proposed MK-ONJKL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6253273/6253273-fig-2-source-large.gif
2013,6253273,Fig. 3.,"Value of the objective function of our GK-ONJKL in the optimization procedure on Banana, Breast, Diabetis, Flare, German, and Heart.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6253273/6253273-fig-3-source-large.gif
2013,6253273,Fig. 4.,"Value of the objective function of our MK-ONJKL in the optimization procedure on Banana, Breast, Diabetis, Flare, German, and Heart.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6253273/6253273-fig-4-source-large.gif
2013,6253273,Fig. 5.,"Example images of background_graz, bikes, cars, and people in Graz02 used in this experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6253273/6253273-fig-5-source-large.gif
2013,6578595,Fig. 1.,Mapping input data to the feature space and decision space [30].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-1-source-large.gif
2013,6578595,Fig. 2.,The 2-D feature space with the optimal separating hyperplane [30].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-2-source-large.gif
2013,6578595,Fig. 3.,Fault classification flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-3-source-large.gif
2013,6578595,Fig. 4.,Three-terminal (teed) transmission system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-4-source-large.gif
2013,6578595,Fig. 5.,"Voltage
WT
C
2
s
in aerial mode in scale-2 for a single-phase-to-ground fault in line C-T at 235 mi from bus A.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-5-source-large.gif
2013,6578595,Fig. 6.,"Voltage
WT
C
2
s
in aerial mode in scale-2 for a single-phase-to-ground fault in line C-T 347 mi from bus A.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-6-source-large.gif
2013,6578595,Fig. 7.,"Voltage
WT
C
2
s
at bus A in aerial and ground mode in scale-2 for phase
a
–
c
-ground fault 95 mi away from bus A in line A-T.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/61/6605682/6578595/6578595-fig-7-source-large.gif
2013,5936063,Fig. 1.,An illustrative example for transductive multilabel classification problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-1-source-large.gif
2013,5936063,Fig. 2.,The TraM algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-2-source-large.gif
2013,5936063,Fig. 3.,"Results on automatic image annotation task under different label rates. The lower the value, the better the performance. Along with the curves, we also plot the mean ± std on each point for different random data set partitions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-3-source-large.gif
2013,5936063,Fig. 4.,"Results on yeast gene function analysis task with different label rates. The lower the value, the better the performance. Along with the curves, we also plot the mean ± std on each point for different random data set partitions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-4-source-large.gif
2013,5936063,Fig. 5.,Results on automatic web page categorization task with different label rates. Note that the values in each figure are reported as the geometrical means across the 11 data subsets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-5-source-large.gif
2013,5936063,Fig. 6.,"Results on text categorization task under different label rates. The lower the value, the better the performance. Along with the curves, we also plot the mean ± std on each point for different random data set partitions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-6-source-large.gif
2013,5936063,Fig. 7.,"Results on natural scene classification task with different label rates. The lower the value, the better the performance. Along with the curves, we also plot the mean ± std on each point for different random data set partitions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-7-source-large.gif
2013,5936063,Fig. 8.,Performances of TraM with different percentages of dimensions in Mddm step on automatic image annotation task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6419729/5936063/5936063-fig-8-source-large.gif
2013,6555905,Fig. 1.,Sample plankton images from five classes. (a) Ciliata. (b) Diatoms. (c) Crustacea. (d) Flagelata. (e) Others.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6632919/6555905/6555905-fig-1-source-large.gif
2013,6389680,Fig. 1.,"Left: Multinomial DBM model: The top layer represents M softmax hidden units
h
(3)
which share the same set of weights. Right: A different interpretation: M softmax units are replaced by a single multinomial unit which is sampled M times.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-1-source-large.gif
2013,6389680,Fig. 2.,"HDP prior over the states of the DBM's top-level features
h
(3)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-2-source-large.gif
2013,6389680,Fig. 3.,"A random subset of the training images along with the first and second layer DBM features and higher level class-sensitive HDP features/topics. To visualize higher level features, we first sample
M
words from a fixed topic
ϕ
t
, followed by sampling RGB pixel values from the conditional DBM model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-3-source-large.gif
2013,6389680,Fig. 4.,A typical partition of the 100 basic-level categories. Many of the discovered supercategories contain semantically coherent classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-4-source-large.gif
2013,6389680,Fig. 5.,"Learning to learn: Training examples along with the eight most probable topics
ϕ
t
, ordered by hand.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-5-source-large.gif
2013,6389680,Fig. 6.,"Class-conditional samples generated from the HDP-DBM model. Observe that the model despite extreme variability, the model is able to capture a coherent structure of each class. See in color for better visualization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-6-source-large.gif
2013,6389680,Fig. 7.,"Conditional samples generated by the HDP-DBM model when learning with only three training examples of a novel class: Top: Three training examples, Bottom: 49 conditional samples. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-7-source-large.gif
2013,6389680,Fig. 8.,"Performance of HDP-DBM, DBM, and SVMs for all object classes when learning with three examples. Object categories are sorted by their performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-8-source-large.gif
2013,6389680,Fig. 9.,"A random subset of the training images along with the first and second layer DBM features, as well as higher level class-sensitive HDP features/topics. To visualize higher level features, we first sample M words from a fixed topic
ϕ
t
, followed by sampling pixel values from the conditional DBM model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-9-source-large.gif
2013,6389680,Fig. 10.,Some of the learned supercategories that share the same prior distribution over “strokes.” Many of the discovered supercategories contain meaningful groupings of characters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-10-source-large.gif
2013,6389680,Fig. 11.,"Within each panel: Left: Examples of training characters in one supercategory: Each row is a different training character and each column is a drawing produced by a different subject. Right: Examples of novel sampled characters in the corresponding supercategory: Each row is a different sampled character, and each column is a different example generated at random by the model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-11-source-large.gif
2013,6389680,Fig. 12.,"Each panel shows three figures from left to right: 1) three training examples of a novel character class, 2) 12 synthesized examples of that class, and 3) training characters in the same supercategory that the novel character has been assigned to.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-12-source-large.gif
2013,6389680,Fig. 13.,Human motion capture data that corresponds to the “normal” walking style.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6389680/6389680-fig-13-source-large.gif
2013,6399478,Fig. 1.,"The shapes of the organs vary substantially and the shape of a liver with metastases can be very abnormal (e). Regions were labeled as described in the main text. Note how the exact outline of the organs is not always clear. Uncertainty in identifying the spleen was high as it is difficult to distinguish from the other nearby organs, for example, in (d).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-1-source-large.gif
2013,6399478,Fig. 2.,"A 4D DCE-MRI scan of a liver patient for a time course
1≤t≤40
with volume size of
256×256×7
. Each pixel of an image slice in a volume gives a time series of its brightness over 40 images. The time series represents the perfusion status of the tissue in the voxel and will vary with tissue types.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-2-source-large.gif
2013,6399478,Fig. 3.,"Two hundred fifty-six overcomplete (a) temporal and (b)
8×8
size visual feature set learned by unsupervised sparse feature learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-3-source-large.gif
2013,6399478,Fig. 4.,"Visualization of dimensionality reduction with a single-layer sparse autoencoder, where the size of the DCE-MRI temporal dimension is reduced from 40 to 16 elements. Different tissue types are visualized in different colors, and a liver tumor is represented as a complex pattern within liver (a), (b). Ambiguities in identifying some tissue types of different organs remain, with some subregions of the aorta, heart, liver, and spleen being represented as the same cyan color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-4-source-large.gif
2013,6399478,Fig. 5.,"(a) A conceptual visualization of max-pooling on a 2D feature space showing how it can capture the same feature for the patches at different locations in the kidney in Fig. 4c. (b) A conceptual visualization of
3×3
max-pooling on a 3D temporal feature space with 256 temporal features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-5-source-large.gif
2013,6399478,Fig. 6.,The overall architecture of the visual feature learning networks (top) and temporal feature extraction networks (bottom). The first and second hidden layers are unsupervised feature learning networks and the third hidden layer is a classification network which is trained with supervision to classify patches of different organs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-6-source-large.gif
2013,6399478,Fig. 7.,"Scatter plots showing 1,500 randomly sampled patches of the organ object classes (red: liver, yellow: heart, green: kidney, blue: spleen) in the training dataset with each of the feature learning methods and projected onto 2D space using PCA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-7-source-large.gif
2013,6399478,Fig. 8.,"Scatter plots showing 1,500 randomly sampled patches of a different subset of the training dataset and the CV liver patient dataset. The patches are processed and displayed in the same way as for Fig. 7. Since the scans of the CV dataset are focused on the kidneys, heart does not appear in those images due to its anatomical location; therefore heart is absent in the CV dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-8-source-large.gif
2013,6399478,Fig. 9.,A conceptual visualization of the usage of context-specific features with SAE in classification. Patches of different modalities are sampled from the dataset and go through different feature networks to be classified as an object part of an organ category.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-9-source-large.gif
2013,6399478,Fig. 10.,"Classification results of part-based organ detection (yellow: liver, magenta: heart, cyan: kidney, red: spleen, blue: NOI). (a), (b): Liver patient training dataset, (c), (d): kidney patient CV dataset, (e), (f): liver patient from a clinical trial. The patch size for liver and heart is
16×16
, for kidney
27×27
, spleen
18×18
, and for NOI
24×24
. The various parameters, including the patch sizes for each organ class, are chosen based on the results shown in Table 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-10-source-large.gif
2013,6399478,Fig. 11.,"Source image of a liver tumor patient (a), with probability maps for: (b) liver, (c) heart, (d) kidney, and (e) spleen.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-11-source-large.gif
2013,6399478,Fig. 12.,"Some examples of the final multiorgan detection (yellow: liver; magenta: heart; cyan: kidney; red: spleen) on the training dataset (first row), CV dataset (second row), and test dataset (third row). Liver and kidney are well detected, whereas spleen is less well detected. In some images, the detected heart region also contains aorta ((a) and (i)), which is probably because the signal uptake pattern in the aorta and the heart are similar. The liver class detected includes both normal appearing tissues and tumor tissues. Liver tumor is seen in most of the liver patient images (first and third row), with some largely abnormal liver shapes ((e) and (m)).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6399478/6399478-fig-12-source-large.gif
2013,6461129,Fig. 1.,"Constraint check in the environment; the predicate
∀x
a
1
(x)∧
a
3
(x)⇒
a
2
(x)
cannot be formally deduced from the given knowledge base (KB), but it holds true with the given probability distribution depicted in the figure. Notice that this is independent of the probability distribution associated with
a
4
(x)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461129/6461129-fig-1-source-large.gif
2013,6461129,Fig. 2.,"Outcome of an SCM trained using constraints on labeled examples and the ones of (6). The nodes are the available data points, and only a few of them are labeled. The mean absolute error (MAE) of the constraints (6) is reported on the right.
SCM
L
learns from constraints on labeled examples only, whereas
SCM
LC
learns also from (6).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461129/6461129-fig-2-source-large.gif
2013,6461129,Fig. 3.,"Top row: the original four classes. The green dashed lines show the real boundaries of the classes. Middle row: the functions
f
1
,
f
2
,
f
3
,
f
4
in SCMs that use labeled examples only,
SCL
L
. Bottom row: the functions
f
1
,
f
2
,
f
3
,
f
4
in SCMs with FOL clauses,
SCM
LC
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6461129/6461129-fig-3-source-large.gif
2013,6191358,Fig. 1.,Architecture of a PC-based motion controller.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-1-source-large.gif
2013,6191358,Fig. 2.,Block diagram of a servo control system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-2-source-large.gif
2013,6191358,Fig. 3.,Frequency responses of the velocity loop. (a) x-axis. (b) y-axis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-3-source-large.gif
2013,6191358,Fig. 4.,Block diagram of a command-based ILC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-4-source-large.gif
2013,6191358,Fig. 5.,Butterfly curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-5-source-large.gif
2013,6191358,Fig. 6.,Output tracking errors at different iterations. (a) x-axis. (b) y-axis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-6-source-large.gif
2013,6191358,Fig. 7.,Spectrum of the tracking error at the tenth iteration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-7-source-large.gif
2013,6191358,Fig. 8.,Flowchart of the EMD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-8-source-large.gif
2013,6191358,Fig. 9.,IMF1 and IMF2 at different iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-9-source-large.gif
2013,6191358,Fig. 10.,IMF5 at different iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-10-source-large.gif
2013,6191358,Fig. 11.,Output tracking error at different iterations. (a) Using ILC. (b) Using ILC-EMD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-11-source-large.gif
2013,6191358,Fig. 12.,"Output tracking errors in the x-axis at different iterations (butterfly trajectory, maximum feedrate: 6000 mm/min).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-12-source-large.gif
2013,6191358,Fig. 13.,"RMS value of output tracking errors in the x-axis at different iterations. (Butterfly trajectory, maximum feedrate: 6000 mm/min.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-13-source-large.gif
2013,6191358,Fig. 14.,"Output tracking errors in the x-axis at different iterations (butterfly trajectory, maximum feedrate: 3000 mm/min).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-14-source-large.gif
2013,6191358,Fig. 15.,Dragon curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-15-source-large.gif
2013,6191358,Fig. 16.,"Output tracking errors in the x-axis at different iterations (dragon trajectory, maximum feedrate: 6000 mm/min).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/6415325/6191358/6191358-fig-16-source-large.gif
2013,6226427,Fig. 1.,Classifier F-measure by project.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6487349/6226427/6226427-fig-1-source-large.gif
2013,6226427,Fig. 2.,Buggy F-measure versus features using Naive Bayes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6487349/6226427/6226427-fig-2-source-large.gif
2013,6338928,Fig. 1.,Comparison of MKFDA+SVM and SMKE algorithms on the MULTIFEAT data set in terms of the average test accuracies and the average kernel weights with their standard deviations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-1-source-large.gif
2013,6338928,Fig. 2.,Two-dimensional embeddings of MKFDA on the MULTIFEAT data set. Left: Training. Right: Test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-2-source-large.gif
2013,6338928,Fig. 3.,Two-dimensional embeddings of SMKE on the MULTIFEAT data set. Left: Training. Right: Test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-3-source-large.gif
2013,6338928,Fig. 4.,Comparison of MKFDA+SVM and SMKE algorithms on the MULTIFEAT data set in terms of the average training times with their standard deviations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-4-source-large.gif
2013,6338928,Fig. 5.,Two-dimensional embeddings of MKFDA on the PLANT data set. Left: Training. Right: Test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-5-source-large.gif
2013,6338928,Fig. 6.,Two-dimensional embeddings of SMKE on the PLANT data set. Left: Training. Right: Test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-6-source-large.gif
2013,6338928,Fig. 7.,Two-dimensional embeddings of MKFDA on the PSORT+ data set. Left: Training. Right: Test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-7-source-large.gif
2013,6338928,Fig. 8.,Two-dimensional embeddings of SMKE on the PSORT+ data set. Left: Training. Right: Test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6583920/6338928/6338928-fig-8-source-large.gif
2013,6547747,Fig. 1.,Illustration of approximation with Gaussian functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-1-source-large.gif
2013,6547747,Fig. 2.,Illustration of approximation with Ridgelet functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-2-source-large.gif
2013,6547747,Fig. 3.,Flowchart of the proposed GMRSVT based image noise reduction algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-3-source-large.gif
2013,6547747,Fig. 4.,Comparison of different support vector filters. (a) Gaussian Support Vector filters (GSVFs) b) Multiscale Gaussian support vector filters (MGSVFs) (c) Ridgelet support vector filters (RSVFs) (d) Multiscale Ridgelet support vector filters (MRSVFs).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-4-source-large.gif
2013,6547747,Fig. 5.,"Comparison of UWT, MGSVT, and GMRSVT. (a) Decomposition result of Lena by UWT('haar' wavelets) (b) Decomposition result of Lena by MGSVT (c) Decomposition result of Lena by GMRSVT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-5-source-large.gif
2013,6547747,Fig. 6.,"The denoised result of our proposed
GMRSVT+KSVD
algorithm. (a) The difference between the multiscale Ridgelet Support Vectors of the noisy image and that of original image (b) The difference between the multiscale Ridgelet support values of the denoised image and that of original image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-6-source-large.gif
2013,6547747,Fig. 7.,"The denoised Lena images by different methods. (a) Lee: 25.6903 dB (b) BST: 29.9855 dB (c) NLM: 31.3995 dB (d) RDCT: 32.0381 dB (e) BM3D: 33.0348 dB (f) BLS-GSM: 32.6600 dB (g) KSVD: 32.3819 dB (h)
GSVT+KSVD
: 33.1151 dB (i)
GMRSVT+KSVD
: 33.3421 dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-7-source-large.gif
2013,6547747,Fig. 8.,"The amplification version of the denoised images. (a) Original images (b) Denoised image by BM3D (c) Denoised image by KSVD (d) Denoised image by
GSVT+KSVD
(e) Denoised image by
GMRSVT+KSVD
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-8-source-large.gif
2013,6547747,Fig. 9.,The test images. (a) Barbara (b) Cartoon (c) Baboon (d) peppers (e) Goldenhill (f) Geometrics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-9-source-large.gif
2013,6547747,Fig. 10.,The amplification of local region of the original and denoised images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-10-source-large.gif
2013,6547747,Fig. 11.,The amplification of local region of the original and denoised images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6595034/6547747/6547747-fig-11-source-large.gif
2013,6459569,Fig. 1.,"Flow chart of D-ELM. (
Ψ
n
(x)
: the approximated function obtained at the
n
th step;
E
n
: the error of
Ψ
n
;
L
n
: the number of hidden nodes at the
n
th step.).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6670128/6459569/6459569-fig-1-source-large.gif
2013,6459569,Fig. 2.,"The
C−γ
contour plot of SVR with a coarse gird search for Abalone case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6670128/6459569/6459569-fig-2-source-large.gif
2013,6459569,Fig. 3.,Testing RMSE updating curves of D-ELM and EM-ELM for Abalone case.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6670128/6459569/6459569-fig-3-source-large.gif
2013,6459569,Fig. 4.,Hidden-node number updating progress in 100 steps for Abalone case.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6670128/6459569/6459569-fig-4-source-large.gif
2013,6263281,Fig. 1.,Block diagram of the personalized and cost-effective classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6384876/6263281/6263281-fig-1-source-large.gif
2013,6263281,Fig. 2.,Average cost (in GBP) of the biomarker included at each iteration for the personalized classification of CN versus AD subjects with α = 0.15 and AUC as feature selection criterion for the modes minimizing the number (dashed black line) or cost (full gray line) of the biomarkers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6384876/6263281/6263281-fig-2-source-large.gif
2013,6392289,Fig. 1.,Schematic diagram of metacognitive learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6392289/6392289-fig-1-source-large.gif
2013,6392289,Fig. 2.,"Results of the unified segmentation and smoothing steps performed on MRI of an AD patient. from right: sagittal, coronal, and axial views. (a) MRI of an AD patient. (b) Segmented gray matter tissue class. (c) Smoothed gray matter image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6392289/6392289-fig-2-source-large.gif
2013,6392289,Fig. 3.,Maximum intensity projections of the significant areas with increased gray matter density in the healthy persons relative to the AD patients. (a) Sagittal view. (b) Coronal view. (c) Axial view.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6392289/6392289-fig-3-source-large.gif
2013,6392289,Fig. 4.,Schematic diagram of PBL-McRBFN application on extracted ADNI dataset features from OASIS dataset SPM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6392289/6392289-fig-4-source-large.gif
2013,6387742,Fig. 1.,"Average CPU time on PKU dataset by varying
C
on five random samples of training and testing datasets. (a) TP1. (b) TP2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-1-source-large.gif
2013,6387742,Fig. 2.,Learned template weights for Chinese word segmentation in TP1 and TP2 on PKU. (a) TP1. (b) TP2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-2-source-large.gif
2013,6387742,Fig. 3.,"F1 measure by varying
C
in TP2 on the Chinese word segmentation datasets. (a) AS. (b) MSR. (c) CityU. (d) PKU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-3-source-large.gif
2013,6387742,Fig. 4.,"CPU time on Spanish and Dutch dataset by varying
C
. (a) ESP. (b) NED.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-4-source-large.gif
2013,6387742,Fig. 5.,Relative learned template weights for named entity recognition on Spanish and Dutch. The horizontal line with 1 stands for methods with the average weights. (a) ESP. (b) NED.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-5-source-large.gif
2013,6387742,Fig. 6.,"Relative learned template weights for dependency parsing on Danish and Swedish. The horizontal line with
10
0
stands for methods with the average weights. (a) Danish. (b) Swedish.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-6-source-large.gif
2013,6387742,Fig. 7.,"Normalized weights of
p
MTL
hmm
on the large set of templates by varying
p
values on PKU and ESP datasets, respectively. (a) PKU. (b) ESP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6387742/6387742-fig-7-source-large.gif
2013,6514914,Fig. 1.,Traditional way of providing secondary frequency control reserve (top) versus rapid-start way (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-1-source-large.gif
2013,6514914,Fig. 2.,RS-EB algorithm [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-2-source-large.gif
2013,6514914,Fig. 3.,Overview of the RS machine learning based algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-3-source-large.gif
2013,6514914,Fig. 4.,Description of the RS machine learning based algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-4-source-large.gif
2013,6514914,Fig. 5.,Power generated trend (PInc) and clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-5-source-large.gif
2013,6514914,Fig. 6.,Power generated trend and clusters separately represented.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-6-source-large.gif
2013,6514914,Fig. 7.,Decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-7-source-large.gif
2013,6514914,Fig. 8.,Control zone model [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-8-source-large.gif
2013,6514914,Fig. 9.,"Example of the behavior of both algorithms once the control zone achieves the power that indicates a new start up
(
P
Start
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/6627990/6514914/6514914-fig-9-source-large.gif
2013,6531681,Fig. 1.,RGA with noiseless data and noisy data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6531681/6531681-fig-1-source-large.gif
2013,6531681,Fig. 2.,"Criteria to choose parameter
k
for the univariate case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6531681/6531681-fig-2-source-large.gif
2013,6531681,Fig. 3.,"Criteria to choose parameter
k
for the multivariate case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6531681/6531681-fig-3-source-large.gif
2013,6459603,Fig. 1.,The average number of selected base kernels for each of the methods on the benchmark data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6459603/6459603-fig-1-source-large.gif
2013,6459603,Fig. 2.,Performance of MKL when using different loss functions on kernel slack variables with respect to the level of noisy features for “Diabetes”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6477170/6459603/6459603-fig-2-source-large.gif
2013,6635250,Fig. 1.,Modular architecture of the proposed CSS framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-1-source-large.gif
2013,6635250,Fig. 2.,Two scenarios of user locations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-2-source-large.gif
2013,6635250,Fig. 3.,Example scatter plots of energy vectors in two scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-3-source-large.gif
2013,6635250,Fig. 4.,The CR network topology used for simulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-4-source-large.gif
2013,6635250,Fig. 5.,The ROC curves when a single PU is present. We use 500 training energy vectors to train each classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-5-source-large.gif
2013,6635250,Fig. 6.,The ROC curves when there are two PUs. We use 500 training energy vectors to train each classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-6-source-large.gif
2013,6635250,Fig. 7.,"The detection probability according to the transmission power of a PU when the false alarm probability is 0.1 and there are 25
(5×5)
SUs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/6635244/6635250/6635250-fig-7-source-large.gif
2013,6357182,Fig. 1.,"The training and testing process for the manifold learning methods (S-Isomap, W-Isomap, and PCA).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6496213/6357182/6357182-fig-1-source-large.gif
2013,6357182,Fig. 2.,"Performance of dimensionality reduction techniques (S-Isomap, W-Isomap, and PCA) in different embedding dimensions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6496213/6357182/6357182-fig-2-source-large.gif
2013,6357182,Fig. 3.,"Feature selection performance as a function of feature vector length. Note that for dimensionality reduction methods (S-Isomap, W-Isomap, and PCA), feature vector lengths under three are embedded using the corresponding smaller dimension. All higher dimensional feature vectors are embedded to 3D space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6496213/6357182/6357182-fig-3-source-large.gif
2013,6357182,Fig. 4.,"Visualizations of the used database using the best performing embeddings. Circle (∘), cross (
×
), square (
$⊔$
⊓
), and diamond (
⋄
) corresponds to neutral, sad, angry, and happy emotion, respectively. All visualizations are projected alike corresponding to looking through the tip of the emotional cone model to best show the underlying emotional structure. Axis values are in arbitrary units and each data point represents one sample of around 13 seconds of speech.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6496213/6357182/6357182-fig-4-source-large.gif
2013,6375845,Fig. 1.,"Structure of the LNF model with
p
inputs and
M
LMs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-1-source-large.gif
2013,6375845,Fig. 2.,Hierarchical model structure with five LMs (at leaves).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-2-source-large.gif
2013,6375845,Fig. 3.,Training and validation RMSE for the Mackey–Glass time series.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-3-source-large.gif
2013,6375845,Fig. 4.,Target and predicted test series for the Mackey–Glass time series (500 six-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-4-source-large.gif
2013,6375845,Fig. 5.,Target and predicted test series for sunspot number time series (59 one-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-5-source-large.gif
2013,6375845,Fig. 6.,Laser intensity time series.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-6-source-large.gif
2013,6375845,Fig. 7.,Target and predicted test series for laser intensity time series (100 one-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-7-source-large.gif
2013,6375845,Fig. 8.,Target and predicted test series for Henon map time series (250 one-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-8-source-large.gif
2013,6375845,Fig. 9.,Target and predicted test series for Ikeda map time series (250 one-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-9-source-large.gif
2013,6375845,Fig. 10.,Target and predicted test series for Lorenz model time series (250 one-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-10-source-large.gif
2013,6375845,Fig. 11.,Target and predicted test series for Rossler model time series (250 one-step-ahead predictions).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6410064/6375845/6375845-fig-11-source-large.gif
2013,6508918,Fig. 1.,"Illustration of Online Metric Learning Procedure: We first collect labeled data and train an initial model. Then, with video data arriving sequentially, after extracting the features, online metric learning and label propagation are used to make a prediction. The confident samples are inserted into the training set queue to online update the model incrementally.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-1-source-large.gif
2013,6508918,Fig. 2.,The work flow of the proposed online learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-2-source-large.gif
2013,6508918,Fig. 3.,"Comparison of the accuracy between our methods and the state-of-the-art methods when varying the rank and fixing the feature dimension
(dim=200)
, where the y-axis is the accuracy and the x-axis denotes the rank.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-3-source-large.gif
2013,6508918,Fig. 4.,"An example of the simulation result, where the x-axis is the number of iterations (10 k per step) and the y-axis is the accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-4-source-large.gif
2013,6508918,Fig. 5.,"Sample images from the Sport 8 datasets, including badminton, bocce, croquet, polo, rock climbing, rowing, sailing, and snowboarding.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-5-source-large.gif
2013,6508918,Fig. 6.,"Confusion matrix for the Sport 8 dataset, where the label of each row is the ground truth and the label of each column is the predicted category. The average accuracy is 77.03%, and random chance is 12.5%. For a better view, please check the electronic version.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-6-source-large.gif
2013,6508918,Fig. 7.,"The comparison of our OMLLR with OASIS using the Sport 8 dataset, where the x-axis is the number of iteration (10 k per step) and the y-axis is the accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-7-source-large.gif
2013,6508918,Fig. 8.,"The comparison of the accuracy between our OMLLR and OASIS [2], [35] for home1–6. In each figure, the x-axis corresponds to the iteration steps (10 k for each) and the y-axis is the current accuracy, where the accuracy of “Ours1,” “Ours2” and OASIS is denoted by sold green line, dash red line and dash blue line, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-8-source-large.gif
2013,6508918,Fig. 9.,"Comparison of the performance of OMLLR, OASIS, LMNN, MCML, LEGO and the Euclidean metric in feature space. Each curve shows the precision at top
k
as a function of
k
neighbors. The results are averaged across 5 train/test partitions (40 training images, 25 test images), error bars are standard error of the means, black dashed line denotes chance performance. (a) 10 classes. (b) 20 classes. (c) 50 classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-9-source-large.gif
2013,6508918,Fig. 10.,"The results of scene categorization for VPC 09. The images of the left column are examples of each home. Each figure of the right column is the frame-level result, where the red and blue line correspond to the predicted result of our methodology after smooth filter and the ground truth, respectively, and the x-axis is the frame index and the y-axis is the 5 class labels (bed, bath, kitchen, living and dining correspond to label 1, 2, 3, 5 and 6 respectively with label 4 absent).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6519343/6508918/6508918-fig-10-source-large.gif
2013,6165285,Fig. 1.,"A logic view of ASVM in RKHS. Two margins, the core-margin (
ρ/∥w∥
) and class-margin (
γ/∥w∥
), are maximized simultaneously to allow classifying the negative class and the core of the positive class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-1-source-large.gif
2013,6165285,Fig. 2.,"Toy examples. (a) and (b) Distributions of the first and second data sets. (c) Decision boundary given by the SVM classifier. (d) Enclosing ball of the positive class returned by the one-class SVM. (e) Decision boundary given by the ASVM. (f) Enclosing balls returned by ASVM. (g) Increasing
μ
of ASVM results in a larger class-margin. (h) Obtaining a high confidence region of the positive class by increasing
τ
. (i) The ROCs achieved by the SVM output in (c) and the ASVM output in (h). (j) The areas under respective ROCs that meet a user tolerance 0.1 to the false-positive rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-2-source-large.gif
2013,6165285,Fig. 3.,The scalability of ASVM based on the SMO implementation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-3-source-large.gif
2013,6165285,Fig. 4.,"The ROC curves of TH and ASVM given
t=0.1
and 0.05 in training time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-4-source-large.gif
2013,6165285,Fig. 5.,"Decision planes in RKHS. (a) In PT, the movement of a decision plane is unpredictable when the values of
C
+
and
C
−
are changed. (b) In ASVM, changing the value of
τ
effectively shifts the decision boundary toward the positive class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-5-source-large.gif
2013,6165285,Fig. 6.,"The ROC curves of PT and ASVM given
t=0.1
and 0.05 in training time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-6-source-large.gif
2013,6165285,Fig. 7.,Number of iterations required to complete a grid search.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-7-source-large.gif
2013,6165285,Fig. 8.,"The asymptotic property of
τ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-8-source-large.gif
2013,6165285,Fig. 9.,"The
t
-AUC achieved by TH, PT, and ASVM given imbalanced data sets. (a) TH versus ASVM when there are more negative instances. (b) PT versus ASVM when there are more negative instances. (c) TH versus ASVM when there are more positive instances. (d) PT versus ASVM when there are more positive instances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6486492/6165285/6165285-fig-9-source-large.gif
2013,6409361,Fig. 1.,VNUML development phases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6478748/6409361/6409361-fig-1-source-large.gif
2013,6409361,Fig. 2.,VNUML in execution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6478748/6409361/6409361-fig-2-source-large.gif
2013,6409361,Fig. 3.,Example of scenario for Practice 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6478748/6409361/6409361-fig-3-source-large.gif
2013,6409361,Fig. 4.,Example of scenario for Practice 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6478748/6409361/6409361-fig-4-source-large.gif
2013,6409361,Fig. 5.,Questions valoration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/6478748/6409361/6409361-fig-5-source-large.gif
2013,6520895,Fig. 1.,Block diagram illustrating the GP background estimation algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-1-source-large.gif
2013,6520895,Fig. 2.,Illustration of the minimum search approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-2-source-large.gif
2013,6520895,Fig. 3.,Block diagram illustrating the prediction process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-3-source-large.gif
2013,6520895,Fig. 4.,Block diagram illustrating the background reconstruction process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-4-source-large.gif
2013,6520895,Fig. 5.,Measured background gamma-ray spectrum.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-5-source-large.gif
2013,6520895,Fig. 6.,Estimated background using the Matérn kernel versus actual background.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-6-source-large.gif
2013,6520895,Fig. 7.,"Distribution of the count difference between measured and estimated background (i.e.,
difference=measured−estimated
) for (a) Matérn, (b) Gaussian, and (c) linear kernel in demonstration case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-7-source-large.gif
2013,6520895,Fig. 8.,(a) A representative source-plus-background gamma-ray spectrum used in scenario 11. The inset shows part of the low energy region of the spectrum. (b) The background spectrum used in scenarios 1 and 11.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-8-source-large.gif
2013,6520895,Fig. 9.,Geometry of the source models used in scenarios 11–20.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-9-source-large.gif
2013,6520895,Fig. 10.,"Distribution of the count difference between measured and estimated background (i.e.,
difference=measured−estimated
) for (a) Matérn, (b) Gaussian, and (c) linear kernel in scenario 11 (i.e., source-plus-background).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-10-source-large.gif
2013,6520895,Fig. 11.,"Presentation of each scenario's highest correlation coefficient computed by any of Matérn, Gaussian, linear kernel, and comparison of each pure background spectrum versus the respective source-plus-background spectrum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/6530641/6520895/6520895-fig-11-source-large.gif
2013,6470638,Fig. 1.,"Pseudocolor image, ground truth map, and DTMKL classification map for hyperspectral data. The green rectangles in the pseudocolor images represent the source domain area. (a) WDC data set. (b) PC data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6531644/6470638/6470638-fig-1-source-large.gif
2013,6470638,Fig. 2.,"Weight proportion values of different kernels and
μ
sensitive analysis for the (top row) WDC data set and (bottom row) PC data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6531644/6470638/6470638-fig-2-source-large.gif
2013,6470638,Fig. 3.,Results for the (top row) WDC data set and (bottom row) PC data set. OA over the validation set as a function of the ratio of labeled samples using different methods is presented in the left side. Kappa statistic surface is presented in the right side.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6531644/6470638/6470638-fig-3-source-large.gif
2013,6220277,Fig. 1.,Pseudocode for creating similarity matrix in phase II.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220277/6220277-fig-1-source-large.gif
2013,6220277,Fig. 2.,"Pseudocode for calculating similarity indices in phase III for a test bag
B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220277/6220277-fig-2-source-large.gif
2013,6220277,Fig. 3.,"Pseudocode for classification of a test bag
B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220277/6220277-fig-3-source-large.gif
2013,6220277,Fig. 4.,Dinosaur images from Corel's data: Before (left column) and After (right column) preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220277/6220277-fig-4-source-large.gif
2013,6220277,Fig. 5.,Images from Corel's data after preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220277/6220277-fig-5-source-large.gif
2013,6220277,Fig. 6.,Accuracy of mi-DS on data with missing values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220277/6220277-fig-6-source-large.gif
2013,6637077,Fig. 1.,An illustration of our proposed idea.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-1-source-large.gif
2013,6637077,Fig. 2.,The proposed age estimation framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-2-source-large.gif
2013,6637077,Fig. 3.,"An illustration for the dilemma of large constraint set. The x-axis is the number of used constraints, and the y-axis represents (a) the percentage (%) of unsatisfied constraints and (b) the time complexity (in seconds). (c) We zoom in the first 1000 constraints of (a) to show the threshold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-3-source-large.gif
2013,6637077,Fig. 4.,"The distribution of age (left) and age difference (right) on FG-NET, MORPH1, and MORPH2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-4-source-large.gif
2013,6637077,Fig. 5.,The cumulative score (CS) in comparison with supervised subspace learning. (a) FG-NET. (b) MORPH1. (c) MORPH2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-5-source-large.gif
2013,6637077,Fig. 6.,The cumulative score (CS) in comparison with state-of-the-art age estimation methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-6-source-large.gif
2013,6637077,Fig. 7.,The age difference (x-axis) versus average ranking accuracy (y-axis). (a) FG-Net. (b) MORPH2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-7-source-large.gif
2013,6637077,Fig. 8.,The performance evaluation of FG-NET dataset by semi-supervised learning using (a) RS strategy and (b) SFS strategy to determine the ranking subset from unlabeled data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-8-source-large.gif
2013,6637077,Fig. 9.,The performance evaluation of MORPH2 dataset by semi-supervised learning using (a) RS strategy and (b) SFS strategy to determine the ranking subset from unlabeled data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-9-source-large.gif
2013,6637077,Fig. 10.,The performance evaluation using different percentages of incorrect ranking pairs on (a) FG-NET and (b) MORPH2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-10-source-large.gif
2013,6637077,Fig. 11.,The performance evaluation in comparison with SDA-based semi-supervised learning on (a) FG-NET and (b) MORPH2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-11-source-large.gif
2013,6637077,Fig. 12.,"The feature distribution among the first two principal dimensions. (a) LDA. (b) OPLDA. (c) MFA. (d) OPMFA. The left and right columns indicate labeled and unlabeled data, respectively, and the color changing from blue to red indicate the age labels ranging from 0 to 80.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-12-source-large.gif
2013,6637077,Fig. 13.,The feature distribution among the first two principal dimensions. (a) SemiRank-NS. (b) SemiRank-FS. (c) SemiRank-SS. (d) SemiRank-RS. The column arrangements and color changes use the same setting described in Fig. 12.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6656843/6637077/6637077-fig-13-source-large.gif
2013,6634165,Fig. 1.,"The tasks of searching video clips are performed routinely by sport analysts in order to meet various objectives for collecting videos such as event analysis, match planning and meeting presentations. The specification for the video clips to be retrieved are typically not well defined beforehand and may change dynamically during the search. Such tasks exhibit some common characteristics of many visual analytics applications.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-1-source-large.gif
2013,6634165,Fig. 2.,"The user interface consists of a number of interactive panels to encourage data exploration. The sketch input panel allows a user to draw a particular search query using intuitive sketch tools. The search results panel displays the top 12 video segments that correspond with the user sketch based on the current model parameters. The model visualization allows the user to analyse how each video segment corresponds to the different similarity metrics within the model. The search space visualization shows the overall similarity against the video timeline, in conjunction with match event data to provide context to the game. When the user accepts or rejects results, they are moved to the corresponding panels and the model is updated to reflect their decision, depicted by the weighting parameters in the model visualization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-2-source-large.gif
2013,6634165,Fig. 3.,"Detection and tracking of players from top-down view. (a) Image converted to HSV colour space to extract shirt colour of the home team (e.g., red). (b) Connected component analysis used to determine position of each shirt. (c) Convex hull applied to point set and centroid is calculated. (d)-(f) Subsequent video frames using same process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-3-source-large.gif
2013,6634165,Fig. 4.,"User sketch interface. The user can define three forms of action: motion, defined by the red arrow, position, defined by the blue circle, and distance, defined by the green cord. The user can also choose for each tool whether to apply this to the team centroid, the forwards (players in front of the team centroid) or the backs (players behind the team centroid).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-4-source-large.gif
2013,6634165,Fig. 5.,Model Visualization. Parallel co-ordinates show each video frame (first axis) against each similarity metric (subsequent axes) in our model. Each polyline represents a video segment (which consisting of a video frame and duration). Weighting of each similarity metric is shown using a dial view above the axis. This indicates its contribution to the overall simiarlity measure. User can brush polylines to explore data (shown in yellow). The selected search thumbnail is shown in red.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-5-source-large.gif
2013,6634165,Fig. 6.,"Search Space Visualization. This plot indicates the search similarity between the user sketch and the current model (based on video data and training data). Colour and height are used to emphasise similarity (red to green indicating minimum to maximum). The coloured segments along the bottom indicate ten-minute time periods, and the blue arrow shows the current selection of the user.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-6-source-large.gif
2013,6634165,Fig. 7.,"Search results illustrated using the Normalized Manoeuvre Visualization. The NMV shows the team starting position (dark red star), the team motion (orange path), and the current position (light red star). Individual player positions are also depicted (red dots).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-7-source-large.gif
2013,6634165,Fig. 8.,"Thumbnail viewer of NMV. The user can interact with the NMV by adjusting the duration slider to animate the motion path. The corresponding video keyframes are also displayed. The user can overlay additional information onto the video, including the NMV and distances between the team, the forwards and backs, in both the x and y directions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-8-source-large.gif
2013,6634165,Fig. 9.,"Example to show the analyst's workflow. The analyst wanted to see when the team advance down the pitch, and sketched this motion. The analyst found a suitable result (highlighted by the green circle) by examining the search space visualization and the NMV thumbnails. From browsing the NMV, the analyst could see the correspondence in the motion, and by adjusting the duration this revealed that two tries had actually been scored, in quick succession of each other.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-9-source-large.gif
2013,6634165,Fig. 10.,"Four keyframes to illustrate the opposition scoring using the NMV (Top-to-bottom: NMV showing full team, wide-angle video keyframe, close-up video keyframe). From the opposition lineout, the NMV shows the home team being pushed back towards their defensive half. The analysts can now assess whether the team were positioned correctly in order to defend such an attack.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-10-source-large.gif
2013,6634165,Fig. 11.,"Four keyframes to illustrate analysing forwards and backs using the NMV (Top-to-bottom: NMV showing forwards and backs, NMV showing full team, wide-angle video keyframe, close-up video keyframe). When the forwards are in the lineout, or in a maul, the analysts can analyse the positioning of the backs in order to cover the pitch whilst support the forwards.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6634084/6634165/6634165-fig-11-source-large.gif
2013,6232463,Fig. 1.,Algorithmic process flow diagram for the integrated P–A learning framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-1-source-large.gif
2013,6232463,Fig. 2.,Ideally complete set of Highway-Code-relevant detector outputs within camera frame.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-2-source-large.gif
2013,6232463,Fig. 3.,"Layered system architecture and related terminology: (p) Percepts, (a) actions, (f) feedback, and (c) control.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-3-source-large.gif
2013,6232463,Fig. 4.,"State diagram of two interacting PACFs grounding module (GM) and SP with the internal states
r
and
r
log
and the percepts
p=detections
and
p
log
=f
(the feedback from GM). The predictions are
p
~
=predictions
and
p
~
log
. Furthermore, the SP controls through
a
log
=c
the GM, but there is no action
a
of the GM. Also, there is no control of the SP
c
log
. The feedback of SP is
f
log
=symbolic output
. Finally,
z
−1
denotes a unit time delay.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-4-source-large.gif
2013,6232463,Fig. 5.,"Graph of the product T-norm defined over the interval [0, 1].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-5-source-large.gif
2013,6232463,Fig. 6.,Two examples for the generated synthetic data. The images show the 3-D point traces in pinhole projection. The black line indicates the horizon.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-6-source-large.gif
2013,6232463,Fig. 7.,"Typical SP module output, with all visualizable predicates (signs, cars, and road topologies) represented by icons with alpha blending proportional to fuzzy confidence (expected path trajectories are indicated by yellow arrows, and intentions are indicated by red arrows).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-7-source-large.gif
2013,6232463,Fig. 8.,(Vertical axis) Confidence of intention versus (horizontal axis) time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-8-source-large.gif
2013,6232463,Fig. 9.,Confidence in association of Class ID: 4 with green traffic light over time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-9-source-large.gif
2013,6232463,Fig. 10.,Confidences in association of ID: 0 with stop sign/give-way sign over time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6232463/6232463-fig-10-source-large.gif
2013,6490057,Fig. 1.,"Example images of PASCAL VOC'07 including person, bird, cat, cow, dog, horse, sheep, airplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, and tv/monitor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6490057/6490057-fig-1-source-large.gif
2013,6490057,Fig. 2.,"mAP of different SVM methods on 2250 labeled images. For each feature/view, the methods from left to right are SVM, LapSVM, and HesSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6490057/6490057-fig-2-source-large.gif
2013,6490057,Fig. 3.,"AP of different multiview methods on some classes including boat, bottle, car, cow, dog, horse, person, and potted plant. The upper eight subfigures are SVM methods, and the lower eight are LS methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6490057/6490057-fig-3-source-large.gif
2013,6490057,Fig. 4.,"mAP of different multiview methods. The upper six subfigures are SVM methods, and the lower six are LS methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6490057/6490057-fig-4-source-large.gif
2013,6484166,Fig. 1.,SURT display on the right part of the driving simulator cockpit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6521414/6484166/6484166-fig-1-source-large.gif
2013,6484166,Fig. 2.,"Performances of different classifiers (see legend). The measure of performance, i.e., the CR that gives the number of correct instances, is shown for each subject and for each type of classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6521414/6484166/6484166-fig-2-source-large.gif
2013,6484166,Fig. 3.,"Performances of different classifiers (see the legend) concerning the SENS parameter, which measures the proportion of actual positives that are correctly identified as such. This is shown for the first ten subjects, to include LRNN as well.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6521414/6484166/6484166-fig-3-source-large.gif
2013,6484166,Fig. 4.,"Performances of different classifiers (see the legend) concerning the SPEC parameter, which measures the proportion of negatives that are correctly identified. This is shown for the first ten subjects, to include LRNN as well.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6521414/6484166/6484166-fig-4-source-large.gif
2013,6516503,Fig. 1.,"Examples of sparse codes using different dictionary learning approaches on the six evaluated datasets. Each waveform indicates a sum of absolute sparse codes for different testing samples from the same class. The curves in the first, second, third, fourth, fifth, and sixth rows correspond to class 35 (32 testing frames) in extended YaleB, class 69 (six testing frames) in AR face, class 78 (29 testing frames) in Caltech101, class 32 (71 testing frames) in Caltech256, class 10 (115 testing frames) in 15 scene categories, and class 10 (four testing sequences) in the UCF dataset, respectively. (a) Sample images from these classes. (f) and (g) The sparse codes using LLC with 30 and 70 local bases, respectively, in five datasets, except for the UCF dataset, where we used 10 and 30 correspondingly. Each color from the color bars in (h) and (i) represents one class for a subset of dictionary items. The black-dashed lines demonstrate that the curves are highly peaked in one class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-1-source-large.gif
2013,6516503,Fig. 2.,Evaluated face databases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-2-source-large.gif
2013,6516503,Fig. 3.,"Effects of parameter selection of
α
and
β
on the classification accuracy on the extended YaleB database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-3-source-large.gif
2013,6516503,Fig. 4.,Performance comparisons on the Caltech101. (a) Performance on the Caltech101 with varying dictionary size. (b) Training time on the Caltech101 with varying dictionary size. (c) Performance on the Caltech101 with different spatial-pyramid-matching settings.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-4-source-large.gif
2013,6516503,Fig. 5.,Example images from classes with high classification accuracy from the Caltech101 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-5-source-large.gif
2013,6516503,Fig. 6.,Recognition results using different approaches with different dictionary sizes on the Caltech256.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-6-source-large.gif
2013,6516503,Fig. 7.,Example images from classes with high classification accuracy from the Caltech256 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-7-source-large.gif
2013,6516503,Fig. 8.,Ten categories in the 15 scene category dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-8-source-large.gif
2013,6516503,Fig. 9.,"Confusion matrices on the 15 scene category dataset using the dictionary size
K=450
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-9-source-large.gif
2013,6516503,Fig. 10.,The UCF sports action dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-10-source-large.gif
2013,6516503,Fig. 11.,Confusion matrices on the UCF dataset using the five-fold cross-validation scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-11-source-large.gif
2013,6516503,Fig. 12.,The optimization process of the objective function for incremental dictionary learning on the Caltech101 dataset with 20 iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-12-source-large.gif
2013,6516503,Fig. 13.,The classification and memory consumption comparisons on the Caltech101 dataset between LC-KSVD2 and the incremental dictionary learning approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6516503/6516503-fig-13-source-large.gif
2013,6482624,Fig. 1.,Second-order face recognition datasets. (a) Yale64×64 samples. (b) ORL64×64 samples. (c) C05 samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6482624/6482624-fig-1-source-large.gif
2013,6482624,Fig. 2.,Gait silhouette sequence for third-order gait recognition datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6482624/6482624-fig-2-source-large.gif
2013,6482624,Fig. 3.,"Test accuracy versus on (a) Yale32×32, Yale64×64, (b) ORL32×32, ORL64×64, (c) C05, C09, (d) USFGait17_32×22×10, and USFGait17_128×88×20, where the red triangles indicate the peak positions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6482624/6482624-fig-3-source-large.gif
2013,6482624,Fig. 4.,"Training time versus
R
on (a) Yale32×32, Yale64×64, (b) ORL32×32, ORL64×64, (c) C05, C09, (d) USFGait17_32×22×10, and USFGait17_128×88×20, where the red diamonds indicate the optimal values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6515702/6482624/6482624-fig-4-source-large.gif
2013,6384531,Fig. 1.,The conceptual view of the HSMILE. Dots denote negative instances and triangles represent positive samples. The size of the triangle implies the likelihood of an instance to be genuinely positive.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6648553/6384531/6384531-fig-1-source-large.gif
2013,6384531,Fig. 2.,"Validation of the probable positive sample distribution on synthetic multi-instance data sets. Each point in (b)-(e) denotes the largest distribution value using (9) of the negative instance (
x
-axis) and positive instance (
y
-axis) in each positive bag, respectively. Each synthetic data set contains about 200 positive and 200 negative bags. Density specifies the percentage of positive instances in a positive bag. (a) A conceptual view of the synthetic data set; (b) low density (23.47 percent positive instances in each positive bag) with five instances/bag; (c) low density (19.66 percent) with 10 instances/bag; (d) high density (83.65 percent) with five instances/bag; (e) high density (72.64 percent) with 10 instances/bag.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6648553/6384531/6384531-fig-2-source-large.gif
2013,6384531,Fig. 3.,"Prediction accuracies (
y
-axis) with respect to different number of times for intra-bag sampling (
x
-axis).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6648553/6384531/6384531-fig-3-source-large.gif
2013,6384810,Fig. 1.,Using a device as a mediator object between the human and the robot to control the movements of a personal robot allows nonexpert users to teach it how to recognize new visually grounded objects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-1-source-large.gif
2013,6384810,Fig. 2.,"To allow users to designate a particular object to a robot in a cluttered environment, we need to provide them with a robust and accurate pointing detection. Otherwise, it may lead to restrictive interaction and even to false learning examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-2-source-large.gif
2013,6384810,Fig. 3.,"To make the robot collect a new learning example, users have to first draw the robot's attention toward the object they want to teach through simple gestures. Once the robot sees the object, they touch the head of the robot to trigger the capture. Then, they directly encircle the area of the image that represents the object on the screen. The selected area is then used as the new learning example. (a) Draw the attention toward an object. (b) Trigger the capture. (c) Encircle the area of the object. (d) New learning example.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-3-source-large.gif
2013,6384810,Fig. 4.,Video stream of the camera of the robot on the screen. This allows accurate monitoring of what the robot is seeing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-4-source-large.gif
2013,6384810,Fig. 5.,"Drawing attention toward an object: The user first sketches directions to position the robot such that the object is in its field of view (left), and if he wants to center the robot's sight on a specific spot, the user can just tap on the screen (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-5-source-large.gif
2013,6384810,Fig. 6.,"Once the user asks the robot to take a picture of a new object, he can directly encircle it, thus providing a useful rough object segmentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-6-source-large.gif
2013,6384810,Fig. 7.,"Users can move the robot by using the directional cross or directly orienting the Wiimote to aim its head. However, the lack of feedback makes it very difficult to estimate whether the robot really sees the object the user wants to teach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-7-source-large.gif
2013,6384810,Fig. 8.,Users can drive the robot with a Wiimote and draw its attention toward an object by pointing at it with a laser pointer as the robot is automatically tracking the laser spot.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-8-source-large.gif
2013,6384810,Fig. 9.,"With this interface, users can draw the robot's attention with a laser pointer toward an object. The laser spot is automatically tracked by the robot. They can ensure that the robot detects the spot, thanks to the haptic feedback on the Wiimote. Then, they can touch the head of the robot to trigger the capture of a new learning example. Finally, they encircle the object with the laser pointer to delimit its area, which will be defined as the new learning example. (a) Draw the attention toward an object. (b) Trigger the capture. (c) Encircle the area of the object. (d) New learning example.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-9-source-large.gif
2013,6384810,Fig. 10.,Encircling with a laser pointer raises difficulties mostly due to the projection of the laser spot in the plane of the camera.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-10-source-large.gif
2013,6384810,Fig. 11.,"In this mode of interaction, the robot is guided by the hand and arm gestures made by the user. In order to have a robust recognition, we used a WOZ framework, where the wizard was only seeing the interaction through the robot's viewpoint.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-11-source-large.gif
2013,6384810,Fig. 12.,"For the experiment, we used 12 textured objects directly related to football: beer, ball, gloves, coke, a poster of Zidane, a jersey of Beckham, a poster of a stadium, a jersey of the Bordeaux team, shoes, a gamepad, a video game, and magazines. Each participant had to teach four randomly chosen objects to the robot to help it better understand football.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-12-source-large.gif
2013,6384810,Fig. 13.,Real-world environment designed to reproduce a typical living room. Many objects were added in the scene in order to make the environment cluttered.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-13-source-large.gif
2013,6384810,Fig. 14.,"Behaviors, such as “happy” (on the left) or scratching its head (on the right) were designed to make the robot look more lively and help the users better understand its behavior.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-14-source-large.gif
2013,6384810,Fig. 15.,Story of the game was told through a video displayed on our game interface. This display was also used to provide users with step-by-step instructions of the tutorial.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-15-source-large.gif
2013,6384810,Fig. 16.,"Partition of the collected images into three categories: The object is 1) entirely, 2) partially, or 3) not at all visible on the images. We can see that without any feedback (Wiimote or Gesture interfaces), the object was entirely visible in only 50% of the examples. Providing feedback significantly improves this result (80% for the laser and more than 85% for the iPhone).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-16-source-large.gif
2013,6384810,Fig. 17.,"Similar charts to Fig. 16, but here, the collected images were split into two subsets: small and big objects. We can see that the difference between the interfaces is even more accentuated for small objects: With the Wiimote and Gesture interfaces,’ participants failed to provide correct learning examples in about one third of cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-17-source-large.gif
2013,6384810,Fig. 18.,"Recognition rate for all the 12 objects: Impact of the interface on the quality of the learning examples and, therefore, on the generalization performance of the overall learning system. In particular, we can see that the iPhone interface allows users to collect significantly higher quality learning examples than the other interfaces. Furthermore, it allows even nonexpert users to provide the learning system with examples of a quality close to the “gold” examples provided by an expert user.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-18-source-large.gif
2013,6384810,Fig. 19.,"Recognition rate for the five big objects: We can see that all the mediator interfaces allow users to collect equally good learning examples. Therefore, for the big objects, the interface does not seem to have a strong impact on the recognition rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-19-source-large.gif
2013,6384810,Fig. 20.,Recognition rate for the seven small objects: We can see that the iPhone interface allows users to provide higher quality learning examples than the other three interfaces (especially with few learning examples). The other three interfaces gave approximately equal results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-20-source-large.gif
2013,6384810,Fig. 21.,"Impact of encircling with the iPhone interface on the recognition rate. As we can see, this intuitive gesture allows us to improve the recognition rate, especially when the system is trained with very few learning examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-21-source-large.gif
2013,6384810,Fig. 22.,"While the feedback provided by the Laser interface allows users to make sure that the object is visible, it does not help them to realize how the object is actually perceived by the robot. (Left) Video game is almost entirely occluded by the table. (Center) Cluttered foreground in front of the poster of Zidane. (Right) Image of the magazine has been taken with an almost horizontal point of view.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-22-source-large.gif
2013,6384810,Fig. 23.,"Usability: Participants found the gestures interface significantly less intuitive and harder to use than the other interfaces. They also stated that the iPhone interface was overall more pleasant than the Laser interface. Q1: It was easy to learn how to use this interface. Q2: It was easy to move the robot. Q3: It was easy to make the robot look at an object. Q4: It was easy to interact with a robot. Q5: The robot was slow to react. Q6: Overall, it was pleasant to use this interface.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-23-source-large.gif
2013,6384810,Fig. 24.,"Robotic game: Our robotic game was stated as entertaining by all participants. They found the game significantly harder with the gestures interfaces, but it increased the feeling of cooperation with the robot. Q1: Completing the game was easy. Q2: The game was entertaining. Q3: I felt like cooperating with the robot. Q4: I picture myself playing other robotic games in the future.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/6491466/6384810/6384810-fig-24-source-large.gif
2013,6247498,Fig. 1.,SVM for binary classification. (a) Hard margin classifier. (b) Soft margin classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-1-source-large.gif
2013,6247498,Fig. 2.,Flowchart of SKRSSE algorithm with kernel parameter optimization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-2-source-large.gif
2013,6247498,Fig. 3.,Contour plots of the SVM based binary classification for an illustrative 2-D toy problem. (a) Single-bandwidth parameters. (b) Optimized full-diagonal bandwidth parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-3-source-large.gif
2013,6247498,Fig. 4.,Error plots for the spambase data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-4-source-large.gif
2013,6247498,Fig. 5.,Error plots for the white wine data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-5-source-large.gif
2013,6247498,Fig. 6.,Error plots for the red wine data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-6-source-large.gif
2013,6247498,Fig. 7.,Error plots for the white wine data set (five bands) with varying number of training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-7-source-large.gif
2013,6247498,Fig. 8.,Error plots for the hyperspectral chemical plume data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-8-source-large.gif
2013,6247498,Fig. 9.,Error plots for the Indian pines hyperspectral data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-9-source-large.gif
2013,6247498,Fig. 10.,Optimized full-diagonal bandwidth parameters for chemical plume data set. (a) Single SVM with all bands. (b) Highest weighted SVM using 34 bands.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-10-source-large.gif
2013,6247498,Fig. 11.,Optimized full-diagonal bandwidth parameters for Indian pines data set. (a) Single SVM with all bands. (b) Highest weighted SVM using 40 bands.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-11-source-large.gif
2013,6247498,Fig. 12.,Error plots for various data sets using only 10% of original training samples for training. (a) Spambase. (b) White wine. (c) Red wine. (d) Chemical plume. (e) Indian pines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-12-source-large.gif
2013,6247498,Fig. 13.,"Classification results in the subset image of Indian pines hyperspectral data set (total error rates of single SVM, RSSE and SKRSSE over all the four classes using
σ
are 4.95%, 4.95 %, and 4.81%, and using
Σ
are 4.86 %, 4.5%, and 4.04%, respectively). (a) Single SVM
(σ)
. (b) Single SVM
(Σ)
. (c) RSSE
(σ)
. (d) RSSE
(Σ)
. (e) SKRSSE
(σ)
. (f) SKRSSE
(Σ)
. (g) Labeled scene.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247498/6247498-fig-13-source-large.gif
2013,6423292,Fig. 1.,"Demonstrating the lattice join
(⋎)
operation between trivial Type-2 INs. (a) Trivial Type-2 INs
[
C
1
,
C
1
]=
C
1
,
[
C
2
,
C
2
]=
C
2
, and
[
C
3
,
C
3
]=
C
3
. (b) Type-2 IN
C
1
⋎
C
2
=[
C
1
⋏
C
2
,
C
1
⋎
C
2
]
is shown in its membership-function representation. (c) Type-2 IN
C
1
⋎
C
2
=[
C
1
⋏
C
2
,
C
1
⋎
C
2
]
is shown again, this time in its (equivalent) interval representation for
L=32
different levels spaced uniformly over the interval
[0,1]
on the vertical axis. (d) Type-2 IN
C
2
⋎
C
3
=[
C
2
⋏
C
3
,
C
2
⋎
C
3
]=[∅,
C
2
⋎
C
3
]
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6423292/6423292-fig-1-source-large.gif
2013,6423292,Fig. 2.,"flrART neural architecture for clustering, where an input pattern
X
is in the lattice
(
I
N
1
,⊆)
of intervals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6423292/6423292-fig-2-source-large.gif
2013,6423292,Fig. 3.,"flrFAM neural architecture for classification, where
X∈(
I
N
1
,⊆)
and
ℓ(X)
is the category label of
X
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6423292/6423292-fig-3-source-large.gif
2013,6423292,Fig. 4.,"Seven different facial expressions from the JAFFE benchmark dataset, including (a) neutral, (b) angry, (c) disgusted, (d) fear, (e) happy, (f) sad, and (g) surprise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6423292/6423292-fig-4-source-large.gif
2013,6423292,Fig. 5.,"Eight different emotional expressions from the RADBOUD benchmark dataset, including (a) angry, (b) contemptuous, (c) disgusted, (d) fear, (e) happy, (f) neutral, (g) sad, and (h) surprise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6423292/6423292-fig-5-source-large.gif
2013,6423292,Fig. 6.,"A row of the 7×7 Table above (excluding the header) displays one 6-dimensional Type-2 IN induced for each of the seven human facial expressions (classes) of the JAFFE benchmark dataset. One Type-2 IN corresponds to one kind of moment. At the end of a row, the corresponding class name is shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6613560/6423292/6423292-fig-6-source-large.gif
2013,6259849,Fig. 1.,Flowchart of the point selection process. One point is selected in each iteration. The process stops when the number of selected points arrives at the maximum or the required colorization quality is achieved.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-1-source-large.gif
2013,6259849,Fig. 2.,Image compression example. The original 384×256 compressed image of 25% size and the decompressed results from various algorithms are shown. (a) Original image. (b) Gray image. (c) Cheng. (d) GRED. (e) TEM. (f) TEM-C.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-2-source-large.gif
2013,6259849,Fig. 3.,Histogram of the difference image. Histogram of (a) Cb and (b) Cr channels. Near-zero values account for the majority.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-3-source-large.gif
2013,6259849,Fig. 4.,"Colorization result for the Bus image. (a) Original image of size 256×384. (b)–(e) Colorization results at the compression ratio 55:1. (f) PSNR, (g) VSNR, (h) MS-SSIM, and (i) NQM value varying with the b/p for storing (a) Bus (b) Cheng:24.9 (c) GRED:28.3 (d) TEM:29.7 (e) TEM-C:34.2 (f) PSNR plot (g) VSNR plot (h) MS-SSIM plot (i) NQM plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-4-source-large.gif
2013,6259849,Fig. 5.,"Demonstration of selected color pixels. Color pixels selected by (a) Cheng's method, (b) GRED, and (c) TEM (a) Cheng (b) GRED (c) TEM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-5-source-large.gif
2013,6259849,Fig. 6.,"Colorization result for the Man image. (a) Original image of size 256×384. (b)–(e) Colorization results at the compression ratio 55:1. (f) PSNR, (g) VSNR, (h) MS-SSIM, and (i) NQM value varying with the b/p for storing (a) Man(b) Cheng:32.8 (c) GRED:35.3 (d) TEM:36.8 (e) TEM-C:41.5 (f) PSNR plot (g) VSNR plot (h) MS-SSIM plot (i) NQM plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-6-source-large.gif
2013,6259849,Fig. 7.,"Colorization result for the Baboon image. (a) Original image of size 500×480. (b)–(e) Colorization results at the compression ratio 100:1. (f) PSNR, (g) VSNR, (h) MS-SSIM, and (i) NQM value varying with the b/p for storing (a) Baboon (b) Cheng:23.4 (c) GRED:25.2 (d) TEM:26.1 (e) TEM-C:29.9 (f) PSNR plot (g) VSNR plot (h) MS-SSIM plot (i) NQM plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-7-source-large.gif
2013,6259849,Fig. 8.,"Colorization result for the Lenna image. (a) Original image of size 512×512. (b)–(e) Colorization results at the compression ratio 103:1. (f) PSNR, (g) VSNR, (h) MS-SSIM, and (i) NQM value varying with the b/p for storing (a) Lenna (b) Cheng:31.6 (c) GRED:36.1 (d) TEM:36.4 (e) TEM-C:38.7 (f) PSNR plot (g) VSNR plot (h) MS-SSIM plot (i) NQM plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-8-source-large.gif
2013,6259849,Fig. 9.,"Colorization result for the Pepper image. (a) Original image of size 512×512. (b)–(e) Colorization results at the compression ratio 103:1. (f) PSNR, (g) VSNR, (h) MS-SSIM, and (i) NQM value varying with the b/p for storing (a) Pepper (b) Cheng:27.7 (c) GRED:30.9 (d) TEM:31.2 (e) TEM-C:36.2 (f) PSNR plot (g) VSNR plot (h) MS-SSIM plot (i) NQM plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-9-source-large.gif
2013,6259849,Fig. 10.,Sample images from the benchmark dataset we used. Each image is of size 384×256 or 256×384.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-10-source-large.gif
2013,6259849,Fig. 11.,PSNR plots on the benchmark image dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-11-source-large.gif
2013,6259849,Fig. 12.,Performance and computation time varying with the size of the kernel PCA map. (a) Average time consumption per point. (b) PSNR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/6491457/6259849/6259849-fig-12-source-large.gif
2013,6463465,Fig. 1.,KSC dataset. (a) RGB composite image of three bands. (b) Groundtruth map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6541987/6463465/6463465-fig-1-source-large.gif
2013,6463465,Fig. 2.,University of Pavia dataset. (a) RGB composite image of three bands. (b) Groundtruth map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6541987/6463465/6463465-fig-2-source-large.gif
2013,6463465,Fig. 3.,Value of eq. (8) as a function of the number of iteration for four pairs of land cover types. (a) In the KSC dataset; (b) In the Pavia dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6541987/6463465/6463465-fig-3-source-large.gif
2013,6463465,Fig. 4.,"Similarity matrix generated by (left) RBF and (right) the learned similarity metric weights. (a) Graminoid,/Spartina in the KSC (20 samples per class); (b) Willow,/CP-Hammock in the KSC (20 samples per class); (c) Asphlt/Metal-sheet in the Pavia (10 samples per class); (d) Bare-soil,/Brick in the Pavia (10 samples per class).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6541987/6463465/6463465-fig-4-source-large.gif
2013,6463465,Fig. 5.,"Experimental results for (top row) the KSC dataset and (bottom row) the Pavia dataset. (Left) Overall Accuracy (OA, in percent) and (right) Kappa statistic as a function of the number of labeled training samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6541987/6463465/6463465-fig-5-source-large.gif
2013,6516530,Fig. 1.,Example morning ritual represented in an action map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-1-source-large.gif
2013,6516530,Fig. 2.,Three-layered global architecture of LFPUBS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-2-source-large.gif
2013,6516530,Fig. 3.,Example where transformation layer can easily transform sensor information.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-3-source-large.gif
2013,6516530,Fig. 4.,Example where inferring a meaningful action demands the combination of different actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-4-source-large.gif
2013,6516530,Fig. 5.,Template for the action “Go into Bathroom.”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-5-source-large.gif
2013,6516530,Fig. 6.,Action pattern for turning on fan.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-6-source-large.gif
2013,6516530,Fig. 7.,Steps to be performed by the learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-7-source-large.gif
2013,6516530,Fig. 8.,Basic representation of Michael's behavior.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-8-source-large.gif
2013,6516530,Fig. 9.,Michael's behavior with repetitive actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-9-source-large.gif
2013,6516530,Fig. 10.,Michael's behavior with unordered subsets of actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-10-source-large.gif
2013,6516530,Fig. 11.,Michael's behavior without considering the allowed maximum granularity parameter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-11-source-large.gif
2013,6516530,Fig. 12.,Time distances between occurrences of “Shower Off” and “BathroomFan On.”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-12-source-large.gif
2013,6516530,Fig. 13.,Action pattern for turning on fan with time relation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-13-source-large.gif
2013,6516530,Fig. 14.,“Shower Off-BathroomFan On” and “Shower Off-BathroomLights Off” tables with calendar and context information.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-14-source-large.gif
2013,6516530,Fig. 15.,Specific condition for the action pattern.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-15-source-large.gif
2013,6516530,Fig. 16.,(a) Sensors installed in different items. (b) Distribution of context and motion sensors [33].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-16-source-large.gif
2013,6516530,Fig. 17.,Action map after general conditions were discovered.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-17-source-large.gif
2013,6516530,Fig. 18.,(a) Distribution of motion sensors. (b) Cabinet and other sensors' distribution [34].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-18-source-large.gif
2013,6516530,Fig. 19.,Action map after topology and time relations were discovered.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6626361/6516530/6516530-fig-19-source-large.gif
2013,6165264,Fig. 1.,"The red line denotes the average NDCG@10 value over 10 times of random sorting. The dots are the NDCG@10 values of 64 features, respectively. We can see that only a few dots are above the red line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/6509861/6165264/6165264-fig-1-source-large.gif
2013,6165264,Fig. 2.,Ranking accuracies of fenchelrank and RANKSVM-primal on TD2004 (top) and NP2004 (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/6509861/6165264/6165264-fig-2-source-large.gif
2013,6165264,Fig. 3.,Ranking accuracies of fenchelrank and rank SVM-primal on ohsumed (top) and MQ2008 (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/6509861/6165264/6165264-fig-3-source-large.gif
2013,6165264,Fig. 4.,Ranking accuracies of fenchelrank and ranksvm-primal on HP2004.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/6509861/6165264/6165264-fig-4-source-large.gif
2013,6550864,Fig. 1.,Schematic diagrams for generating new descriptors called HKM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6651932/6550864/6550864-fig-1-source-large.gif
2013,6550864,Fig. 2.,"Prediction results of residues within four amino acid sequences. (A) The A chain of complex structure 3A6P (PDB ID), true positives (TP) 7, false negatives (FN) 17, false positives (FP) 23, and true negatives (TN) 975, with a 25.93 percent F1 score and a 0.789 AUC value. (B) The C chain of complex structure 3A6P, TP 2, FN 0, FP 12, and TN 146, with a 25 percent F1 score and a 0.962 AUC value. (C) The A chain of complex structure 3ADI, TP 6, FN 0, FP 33, and TN 22, with a 26.67 percent F1 score and a 0.985 AUC value. (D) The B chain of complex structure 3TS0, TP 23, FN 6, FP 49, and TN 38, with a 45.54 percent F1 score and a 0.741 AUC value. The correctly identified binding residues (TP) are in blue space fill; the correctly identified nonbinding residues (TN) are in red space fill; the binding residues with negative predictions (FN) are in yellow space fill; the nonbinding residues but wrongly predicted as positives (FP) are in green space fill. The total 10 residues located in the N-terminal and C-terminal of the four amino acid sequences are not used in reporting prediction performance by our model and shown in white space fill. The miRNA molecules are indicated in gray wire frame. The presentation in the format of 3D structures is generated with PyMOL ( http://www.pymol.org).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6651932/6550864/6550864-fig-2-source-large.gif
2013,6509470,Fig. 1.,"Two cases of
|
∑
i∈
S
S
y
i
|=|
S
S
|
during the adjustments for
α
c
. (a) Labels of margin support vectors in
S
S
are all positive. (b) Labels of margin support vectors in
S
S
are all negative.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6558950/6509470/6509470-fig-1-source-large.gif
2013,6205648,Fig. 1.,Conceptual structure of ontology graph in KnowledgeSeeker.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6356011/6205648/6205648-fig-1-source-large.gif
2013,6205648,Fig. 2.,Bottom–up approach of the DOG learning process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6356011/6205648/6205648-fig-2-source-large.gif
2013,6420927,Fig. 1.,McFIS: An analogy to the Nelson and Narens model of metacognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6675054/6420927/6420927-fig-1-source-large.gif
2013,6420927,Fig. 2.,Schematic diagram describing the influence of the metacognitive component in sequential learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6675054/6420927/6420927-fig-2-source-large.gif
2013,6420927,Fig. 3.,"Architecture of the McFIS classifier. The inset figure describes the internal structure of the
i
th Gaussian node.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6675054/6420927/6420927-fig-3-source-large.gif
2013,6514601,Fig. 1.,Graphical user interface of the in-house P300 speller BCI [17].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6582530/6514601/6514601-fig-1-source-large.gif
2013,6514601,Fig. 2.,"Spelling accuracy of the BCI Competition III dataset II for Subjects A and B. The SUST-LSSVM learning involved the unlabeled input characters with the number from 0 to 60. The Rakoto results from [18] were the best accuracies in the competition, and were achieved based on supervised learning with the 85-character labeled training set. (a) One labeled training char, 15 repeats. (b) Two labeled training chars, 10 repeats.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6582530/6514601/6514601-fig-2-source-large.gif
2013,6514601,Fig. 3.,"Spelling accuracy of the online P300 BCI speller in Experiment 1 for Subjects 1 to 8. The sizes of the initial training sets are given in Table II. The SUST-LSSVM learning involved the unlabeled input characters with the number from 0 to 25 (∗: SUST-LSSVM,
−−
: SVM,
−⋅−
: LS-SVM.).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6582530/6514601/6514601-fig-3-source-large.gif
2013,6514601,Fig. 4.,"Spelling accuracy of the online P300 BCI speller in Experiment 2 for Subjects 1 to 8. The size of the initial labeled training set for each subject is two characters. The SUST-LSSVM learning involved the unlabeled input characters with the number from 0 to 35. (
△
: SUST-LSSVM, ∗: SUST-LSSVM with thresholding. threshold
θ
0
= 0.15.).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6582530/6514601/6514601-fig-4-source-large.gif
2013,6472075,Fig. 1.,Generative model of multimodal EEG and hemodynamic signal corresponding to (1) and (3) (noise terms omitted).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-1-source-large.gif
2013,6472075,Fig. 2.,"Schematic comparison of two popular unsupervised analysis approaches to multimodal neuroimaging data, as well as our own novel approach. The three processing streams are to be red from to top to bottom, i.e., multivariate measures of neural oscillations (EEG/MEG) and of hemodynamic responses (fMRI/NIRS) come in at the top and are then subjected to several processing steps, such as a unmixing, the estimation of spectral features, and convolution with an HRF-like function. Left: PCA/ICA based approaches usually perform a separate unmixing before bandpower estimation and convolution. The unmixing stage is uninformed of the respective other modality. Middle: In approaches based on PLS/CCA there is bandpower extraction and convolution as preprocessing steps before the unmixing state. Here the unmixing stage acknowledges the inherent multimodality of the data, yet the nonlinear preprocessing may hinder the linear unmixing. See Sections III and IV for more details. Right: In this paper we propose an (iterative) approach to the ummixing that is multimodal and takes the nonlinear features of the resulting components into account. See Section V for more details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-2-source-large.gif
2013,6472075,Fig. 3.,"Illustration for the limitations of channel-wise band power representations. Left: Scatter plot of data from two sources that have been linearly mixed. The original source directions are indicated with a red and black line. The band power of the source corresponding to the red direction (target source) is coupled to a variable
z
(not shown). Middle: The same data after applying a channel-wise nonlinearity (here variance as a proxy for band power). Note that information about polarity (sign) is lost. Right: Covariance between
z
and the band power of a projection of the data in the left scatter plot (blue line), and covariance between
z
and the projection of the data in the middle scatter plot (green line), both as a function of projection angle with respect to the
x
2
axis. The vertical red line corresponds to the direction of the target source. In this simple example the maximum covariance obtained by unmixing the nonlinearly transformed data does not lead to the true source direction. Unmixing the data before applying the nonlinearity, however, allows to find the true source direction. See text for further explanation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-3-source-large.gif
2013,6472075,Fig. 4.,"Obtained correlations (medians over 250 repetitions) as a function of signal-to-noise ratio for simulated EEG and NIRS data. Black stars indicate significant differences in performance
(p<0.01)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-4-source-large.gif
2013,6472075,Fig. 5.,"Cross-validated correlations between EEG and NIRS (left HbR, right HbO) in the motor execution task for each subject. Results of mSPoC and CCA are compared. Each point corresponds to the correlations obtained for the first set of
w
x
,
w
y
, and
w
τ
from a single subject by CCA (
x
-axis) and mSPoC (
y
-axis).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-5-source-large.gif
2013,6472075,Fig. 6.,"Spatial patterns corresponding to
w
x
and
w
y
, as well as the temporal filter for extracted EEG power
w
τ
. Each row shows the results obtained for a single subject, where on the left side we show the patterns obtained from mSPoC and on the right side the patterns obtained from CCA. We chose 4 representative subjects for each NIRS chromophore but not the same subject twice, thus showing 8 different subjects in total (out of 14). In the middle column we show the subject code, the respective NIRS chromophore and the cross-validated correlation values between the convolved EEG component power time course and the time course of the NIRS component, as obtained by mSPoC and CCA, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-6-source-large.gif
2013,6472075,Fig. 7.,"Exemplary results for one subject (VPean) as derived by mSPoC. The scalp-plots on the left side show the EEG pattern that corresponds to the obtained filter
w
x
. In the middle plot we show the temporal filter for the EEG power of the component shown left. The rightmost scalp-plots depict the spatial pattern that corresponds to the filter
w
y
, i.e., the NIRS patterns. The top row shows the results for applying mSPoC to left hand movement trials, while in the bottom row results for right hand movement trials are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/6558949/6472075/6472075-fig-7-source-large.gif
2013,6400253,Fig. 1.,"Illustration of
ε
controlling the number of nonzero
α
~
∗
t+1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-1-source-large.gif
2013,6400253,Fig. 2.,Number of support vectors for different thresholds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-2-source-large.gif
2013,6400253,Fig. 3.,Classification errors for each method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-3-source-large.gif
2013,6400253,Fig. 4.,Time cost for each method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-4-source-large.gif
2013,6400253,Fig. 5.,"Illustration of
OLK
R
with the number of training data
l
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-5-source-large.gif
2013,6400253,Fig. 6.,"RMSE on
D
test
of algorithms with respect to
l
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-6-source-large.gif
2013,6400253,Fig. 7.,Illustration of novelty points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-7-source-large.gif
2013,6400253,Fig. 8.,"ROC curves of novelty detection by
OLK
N
and NORMA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-8-source-large.gif
2013,6400253,Fig. 9.,Original Jug data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-9-source-large.gif
2013,6400253,Fig. 10.,"Results of processing Jug117 data with
r=0.001
,
v=0.1
,
p=10
, and
C=0.2
by
OLK
N
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-10-source-large.gif
2013,6400253,Fig. 11.,"Jug117 data with
λ=0.0001
,
p=8
, and
η=0.001
by NORMA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-11-source-large.gif
2013,6400253,Fig. 12.,Original data of Video-DSCF6721.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-12-source-large.gif
2013,6400253,Fig. 13.,"Results of processing Video-DSCF6721 with nine-frame data (r=0.001,v=0.1,p=20,C=0.5)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6425530/6400253/6400253-fig-13-source-large.gif
2013,6359938,Fig. 1.,Flowchart of the proposed LSVM suction detection algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-1-source-large.gif
2013,6359938,Fig. 2.,"Example of in vivo data based on two different pumps. (a) MedQuest pump. (b) Nimbus pump. PS is the pump speed, LVP is the left ventricular pressure, PIP is the pump inlet pressure, and PF is the pump flow.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-2-source-large.gif
2013,6359938,Fig. 3.,Structure of a simple SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-3-source-large.gif
2013,6359938,Fig. 4.,Suction indices extracted from the MedQuest pump flow signal. (a) PF. (b) SI1. (c) SI2. (d) SI3. (e) SI4. (f) SI5. (g) SI6.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-4-source-large.gif
2013,6359938,Fig. 5.,Box plots of the features per pump state for MedQuest pump. (a) SI1. (b) SI2. (c) SI3. (d) SI4. (e) SI5. (f) SI6.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-5-source-large.gif
2013,6359938,Fig. 6.,"Comparison of classification sensitivity, specificity, and accuracy for the two-state classification (the brackets indicate SD). (a) MedQuest pump. (b) Nimbus pump.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-6-source-large.gif
2013,6359938,Fig. 7.,Comparison of ROC curves for the two-state classification (zoomed). (a) MedQuest pump. (b) Nimbus pump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-7-source-large.gif
2013,6359938,Fig. 8.,"Comparison of classification sensitivity, specificity, and accuracy for the three-state classification (the brackets indicate SD). (a) MedQuest pump. (b) Nimbus pump.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-8-source-large.gif
2013,6359938,Fig. 9.,Comparison of ROC curves for the three-state classification (zoomed). (a) MedQuest pump. (b) Nimbus pump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6511984/6359938/6359938-fig-9-source-large.gif
2013,6578079,Fig. 1.,(a) Original image (b) Trimap (c) Unlabeled region segmentation (d) Alpha matte produced by our approach (e) New composite.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6584011/6578079/6578079-fig-1-source-large.gif
2013,6578079,Fig. 2.,An illustration of candidate samples collection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6584011/6578079/6578079-fig-2-source-large.gif
2013,6578079,Fig. 3.,An illustration of candidate samples collection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6584011/6578079/6578079-fig-3-source-large.gif
2013,6578079,Fig. 4.,Visual comparison of various matting algorithms on benchmark [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6584011/6578079/6578079-fig-4-source-large.gif
2013,6214595,Fig. 1.,Example of multitask problem with four tasks and three different relations among them.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-1-source-large.gif
2013,6214595,Fig. 2.,"Modified kernel
K
˘
is just the original kernel
K
modulated by a kernel matrix
B
constructed from the known task relations between samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-2-source-large.gif
2013,6214595,Fig. 3.,"Left: Cloud screening weighted error rate, averaged over ten experiments, and standard deviation bars using different MERIS images and sizes of training samples. Right: Best MTL hyperparameters,
δ
and
γ
, for both methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-3-source-large.gif
2013,6214595,Fig. 4.,"From top to bottom: Color composition of MERIS FR images over Spain (BR-2003-07-14 and BR-2004-07-14), Tunisia (TU-2004-07-15), Finland (FI-2005-02-26), and France (FR-2005-03-19) and the comparison between the ground truth and the classification maps for the standard aggregate and pairwise multitask methods. Discrepancies are shown in red when analyzed methods detect cloud and in yellow when pixels are classified as cloud free.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-4-source-large.gif
2013,6214595,Fig. 5.,Range of the input variables for the two kinds of tasks defined by the foliated and desertic land-mine fields and for the (left) negative and (right) positive samples. Markers identify the mean values while bars identify one standard deviation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-5-source-large.gif
2013,6214595,Fig. 6.,"Land-mine data set. Results of methods (left) under the flat assumption, (center) under the two-cluster assumption, and (right) under the two-level assumption.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-6-source-large.gif
2013,6214595,Fig. 7.,Averaged results over 20 experiments on multitemporal urban classification from multispectral and SAR data using different amounts of training data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-7-source-large.gif
2013,6214595,Fig. 8.,"(a) RGB image composition of the 1999 Naples image using Landsat bands 3, 2, and 1. (b) Labeled ground truth with urban areas in black, nonurban in white, and unknown in gray. (c), (d) Classification maps of independent and relational multitask methods, with 6.6% and 4.23% test errors, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6387651/6214595/6214595-fig-8-source-large.gif
2013,6489877,Fig. 1.,Types of phishing attacks,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-1-source-large.gif
2013,6489877,Fig. 2.,Life cycle of phishing email.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-2-source-large.gif
2013,6489877,Fig. 3.,Screenshot of the eBay phishing email [11],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-3-source-large.gif
2013,6489877,Fig. 4.,Screenshot of embedded URL on the eBay phishing web page [11],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-4-source-large.gif
2013,6489877,Fig. 5.,Phishing email transfer procedure in the computer network,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-5-source-large.gif
2013,6489877,Fig. 6.,Taxonomy of email message structure[13],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-6-source-large.gif
2013,6489877,Fig. 7.,An example of message structure for the purpose of feature,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-7-source-large.gif
2013,6489877,Fig. 8.,Approaches to protect the user from phishing attack [33],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-8-source-large.gif
2013,6489877,Fig. 9.,Microsoft's Sender ID integration into an anti-phishing solution [39],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-9-source-large.gif
2013,6489877,Fig. 10.,NetCraft- A Client side tool [46],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-10-source-large.gif
2013,6489877,Fig. 11.,Comic strip presented to people in generic condition [64],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-11-source-large.gif
2013,6489877,Fig. 12.,Support Vector Machine [78],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-12-source-large.gif
2013,6489877,Fig. 13.,K-Nearest Neighbour Algorithm [16],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-13-source-large.gif
2013,6489877,Fig. 14.,Neural network,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-14-source-large.gif
2013,6489877,Fig. 15.,Five steps in Ontology Concept proposed method [42].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-15-source-large.gif
2013,6489877,Fig. 16.,Block diagram of PHONEY architecture [92],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-16-source-large.gif
2013,6489877,Fig. 17.,The machine learning approach [14],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-17-source-large.gif
2013,6489877,Fig. 18.,Robust classifier model architecture for phishing email detection [96],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-18-source-large.gif
2013,6489877,Fig. 19.,Feature selection using modified global k-mean [19],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-19-source-large.gif
2013,6489877,Fig. 20.,FRALEC Email Classification Flow model [102],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-20-source-large.gif
2013,6489877,Fig. 21.,Block diagram of the multi-tier classification model[103],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-21-source-large.gif
2013,6489877,Fig. 22.,Working of Phishing Email Clustering Approach (PECM),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-22-source-large.gif
2013,6489877,Fig. 23.,Working of PENFF,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-23-source-large.gif
2013,6489877,Fig. 24.,Working of PDENFF,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6661627/6489877/6489877-fig-24-source-large.gif
2013,6509481,Fig. 1.,Framework of our experimental studies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/6521443/6509481/6509481-fig-1-source-large.gif
2013,6509481,Fig. 2.,"Scatter plots of (PD, PF) points of the seven training methods on the ten SDP data sets. (a) Balance criterion; (b) G-mean criterion; (c) AUC criterion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/6521443/6509481/6509481-fig-2-source-large.gif
2013,6509481,Fig. 3.,"Bar plots of average PD, balance, G-mean, and AUC of the five class imbalance learning methods based on different parameter searching criteria. (a) Mean PD; (b) mean balance; (c) mean G-mean; (d) mean AUC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/6521443/6509481/6509481-fig-3-source-large.gif
2013,6263287,Fig. 1.,"OmniBiomarker web interface for data upload and knowledge mining. (a) After uploading gene expression datasets, users can assign a cancer subtype to each class label. (b) omniBiomarker provides an interactive graphical search tool as well as (c) a keyword-based disease search tool for browsing the NCI thesaurus of cancer subtypes. (d) Assignment of specific cancer subtypes to class labels enables the system to automatically identify disease-specific genes for use in knowledge-driven biomarker identification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6670086/6263287/6263287-fig-1-source-large.gif
2013,6263287,Fig. 2.,"Case study design. Predictive modeling with microarray data involves choosing (1) feature selection method, (2) feature size, and (3) classifier. Isolating the first step can elucidate the relationship between biological relevance of feature selection and prediction performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6670086/6263287/6263287-fig-2-source-large.gif
2013,6263287,Fig. 3.,"Biological relevance of feature selection affects microarray-based prediction performance. Prediction models with minimum biological relevance tend to perform worse than prediction models optimized with cross validation (AUCmin–AUCdata tends to be less than zero, orange boxplots). Models with maximum biological relevance perform similarly to models optimized with cross validation (AUCmax–AUCdata is close to zero, green boxplots) and perform better than models with minimum biological relevance (AUCmax–AUC min tends to be greater than zero, blue boxplots).
P
-values, computed using a
t
-test, indicate whether the changes in performance are significantly different from zero. Liver cancer results show no statistically significant trends.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6670086/6263287/6263287-fig-3-source-large.gif
2013,6548033,Fig. 1.,Learning from observation with GenCL [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6548010/6548033/6548033-fig-1-source-large.gif
2013,6548033,Fig. 2.,Pictorial representation of context partitioning and clustering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6548010/6548033/6548033-fig-2-source-large.gif
2013,6548033,Fig. 3.,High-level work/data flow for CFA and COPAC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6548010/6548033/6548033-fig-3-source-large.gif
2013,6475945,Fig. 1.,"A mapping unit [1]. The triangle symbolizes multiplicative interactions between the three variables
z
k
,
x
i
, and
y
j
. The value of any one of the three variables is a function of the product of the others.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-1-source-large.gif
2013,6475945,Fig. 2.,Top: Feature learning graphical model. Bottom: Autoencoder network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-2-source-large.gif
2013,6475945,Fig. 3.,"(a) The diagonal of
L:=x
y
T
contains evidence for the identity transformation. (b) The secondary diagonals contain evidence for shifts. (c) A hidden unit that pools over one of the diagonals can detect transformations. Such a hidden unit would need to compute a sum over products of pixels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-3-source-large.gif
2013,6475945,Fig. 4.,"Learning to encode relations: We consider the task of learning latent variables
z
that encode the relationship between images
x
and
y
, independently of their content.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-4-source-large.gif
2013,6475945,Fig. 5.,Relating images using multiplicative interactions. Two views of the same model. Left: Input-modulated feature learning. Right: Mixture of warps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-5-source-large.gif
2013,6475945,Fig. 6.,"A gated autoencoder is an autoencoder that learns to represent an image,
y
, using parameters that are modulated by another image,
x
. This makes it possible to learn relationships between
x
and
y
with gradient-based learning and back-prop.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-6-source-large.gif
2013,6475945,Fig. 7.,"Inferring transformations from test data. (a) Coherent motion across the whole image. (b) “Factorial motion” that is independent in different image regions. In both plots, the meaning of the five columns is as follows (left-to-right): Random test images
x
, random test images
y
, inferred flow-field, new test-image
x
^
, inferred output
y
^
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-7-source-large.gif
2013,6475945,Fig. 8.,Top: Factorizing the parameter tensor. Bottom: Interpreting factorization as filter matching.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-8-source-large.gif
2013,6475945,Fig. 9.,"Input filters learned from various types of transformation. Top-left: translation, Top-right: rotation, Bottom-left: split-screen translation, Bottom-right: natural videos. See Fig. 10 for corresponding output filters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-9-source-large.gif
2013,6475945,Fig. 10.,"Output filters learned from various types of transformation. Top-left: translation, Top-right: rotation, Bottom-left: split-screen translation, Bottom-right: natural videos. See Fig. 9 for corresponding input filters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-10-source-large.gif
2013,6475945,Fig. 11.,"Illustration of ISA applied to an image pair
(x,y)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-11-source-large.gif
2013,6475945,Fig. 12.,"Left: Inference in a gated feature learning model is equivalent to extracting rotation angles from two-dimensional invariant subspaces. Right: By absorbing eigenvalues into the eigenvectors, a mapping unit can learn to detect rotations by a particular angle (its preferred angle): The inner product between the projections of the two images
x
and
y
in the figure will be maximal when
ϕ
y
=
ϕ
x
+θ
, that is, when the rotation that the detector applies to
x
has the effect of aligning
x
with
y
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-12-source-large.gif
2013,6475945,Fig. 13.,Implementing a cross-correlation model via an energy model via a cross-correlation model. Sequence of filters learned from the concatenation of 10 frames of moving random dots.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6541932/6475945/6475945-fig-13-source-large.gif
2013,6476738,Fig. 1.,"NDCG@10 results on (a) OHSUMED, (b) HP2004, and (c) NP2004 from LETOR 3.0 distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6494635/6476738/6476738-fig-1-source-large.gif
2013,6476738,Fig. 2.,"MAP results on (a) OHSUMED, (b) HP2004, and (c) NP2004 from LETOR 3.0 distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6494635/6476738/6476738-fig-2-source-large.gif
2013,6476738,Fig. 3.,NDCG@10 and MAP results on (a) and (b) MQ2008 from LETOR 4.0 distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6494635/6476738/6476738-fig-3-source-large.gif
2013,6476738,Fig. 4.,"Comparison of the features in HP2004, NP2004, and MQ2008. The dots are the NDCG@10 values of 64 features on each dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6494635/6476738/6476738-fig-4-source-large.gif
2013,6476738,Fig. 5.,Comparison results of training time on four datasets from LETOR distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6494635/6476738/6476738-fig-5-source-large.gif
2013,6324391,Fig. 1.,An example of a bipartite graph representing the instance-feature occurrence relationship.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-1-source-large.gif
2013,6324391,Fig. 2.,Propagate information gain through the bipartite graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-2-source-large.gif
2013,6324391,Fig. 3.,Performance using maximum gradient length query with soft decisions vs. hard decisions compared to the random baseline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-3-source-large.gif
2013,6324391,Fig. 4.,Bias of posterior probability produced by label propagation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-4-source-large.gif
2013,6324391,Fig. 5.,"Performance of maximum batch network gain and maximum graph-cut queries, compared to a random baseline.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-5-source-large.gif
2013,6324391,Fig. 6.,Performance on Amazon product review (books).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-6-source-large.gif
2013,6324391,Fig. 7.,Performance on Amazon product review (dvd).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-7-source-large.gif
2013,6324391,Fig. 8.,Performance on Amazon product review (electronics).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-8-source-large.gif
2013,6324391,Fig. 9.,Performance on Amazon product review (kitchen).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-9-source-large.gif
2013,6324391,Fig. 10.,Performance on movie review (positive/negative task).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-10-source-large.gif
2013,6324391,Fig. 11.,Performance on movie review (subjective/objective task).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-11-source-large.gif
2013,6324391,Fig. 12.,"Percentage of the required training data relative to the [Random] baseline system for [Random+LProp], [ActiveEnt+LProp], [ActiveMGL+LProp] and [ActiveBNG+LProp] to reach the baseline system's performance on the 6th iteration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10376/6359939/6324391/6324391-fig-12-source-large.gif
2013,6542747,Fig. 1.,Stopping area for TASC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-1-source-large.gif
2013,6542747,Fig. 2.,Control procedure of TASC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-2-source-large.gif
2013,6542747,Fig. 3.,TASC system with online learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-3-source-large.gif
2013,6542747,Fig. 4.,Reward function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-4-source-large.gif
2013,6542747,Fig. 5.,RLA process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-5-source-large.gif
2013,6542747,Fig. 6.,TASC simulation platform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-6-source-large.gif
2013,6542747,Fig. 7.,Train braking system,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-7-source-large.gif
2013,6542747,Fig. 8.,Velocity and deceleration of a train using PID.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-8-source-large.gif
2013,6542747,Fig. 9.,Velocity and deceleration of a train using HOA (RLA).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-9-source-large.gif
2013,6542747,Fig. 10.,Velocity and deceleration of a train using GOA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-10-source-large.gif
2013,6542747,Fig. 11.,Stopping errors of PID under BST disturbances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-11-source-large.gif
2013,6542747,Fig. 12.,Stopping errors of HOA under BST disturbances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-12-source-large.gif
2013,6542747,Fig. 13.,Stopping errors of GOA under BST disturbances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-13-source-large.gif
2013,6542747,Fig. 14.,Stopping errors of RLA under BST disturbances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/6587572/6542747/6542747-fig-14-source-large.gif
2013,6477123,Fig. 1.,SS-DPSOM scheme for machine-fault diagnosis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-1-source-large.gif
2013,6477123,Fig. 2.,Transmission diagram of SG135-2 gearbox.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-2-source-large.gif
2013,6477123,Fig. 3.,"Gear-pitting defect, transmission test bed, and sensor configuration. (a) Gear-pitting defect. (b) Transmission test rig. (c) Sensor configuration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-3-source-large.gif
2013,6477123,Fig. 4.,Gearbox vibration signals. (a) Normal. (b) Gear pitting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-4-source-large.gif
2013,6477123,Fig. 5.,Magnitude spectra of gearbox vibration signals. (a) Normal. (b) Gear pitting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-5-source-large.gif
2013,6477123,Fig. 6.,"Incipient gear-pitting detection using different learning methods. (a) SOM hit histogram. (b) DPSOM. (c) Kernel PCA (RBF kernel
σ=0.5
). (d) SS-DPSOM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-6-source-large.gif
2013,6477123,Fig. 7.,"Defect classification with different classifiers. (a) SOM. (b) DPSOM. (c) SS-DPSOM. (d) SVM (RBF kernel
σ=1
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-7-source-large.gif
2013,6477123,Fig. 8.,Feature selection verification for defect classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-8-source-large.gif
2013,6477123,Fig. 9.,Scatterplot of data along selected feature direction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-9-source-large.gif
2013,6477123,Fig. 10.,"Severity classification with different classifiers. (a) SOM classification. (b) DPSOM classification. (c) SS-DPSOM. (d) SVM (RBF kernel
σ=1
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-10-source-large.gif
2013,6477123,Fig. 11.,Feature selection verification for severity classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6493455/6477123/6477123-fig-11-source-large.gif
2013,6081866,Fig. 1.,Problems with (a) a complex decision boundary and (b) multiple classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-1-source-large.gif
2013,6081866,Fig. 2.,"Illustrative synthetic example of rare class discovery and classification. Bottom: true class indicated by symbols, observed points by bold symbols, predicted class by shade/color. Second row: badly explained points preferred by likelihood criteria. Third row: ambiguous points preferred by the uncertainty criteria. Fourth row: accuracy and likelihood versus uncertainty criteria weighting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-2-source-large.gif
2013,6081866,Fig. 3.,"Discovery and classification performance for UCI data sets. (a) Shuttle, (b) Thyroid, (c) Glass, (d) Pageblocks, (e) Covertype, (f) Ecoli, (g) KDD.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-3-source-large.gif
2013,6081866,Fig. 4.,Criteria weight adaptation for (a) shuttle and (b) thyroid data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-4-source-large.gif
2013,6081866,Fig. 5.,Classifier switching by multiclass classification entropy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-5-source-large.gif
2013,6081866,Fig. 6.,"Digits data set. (a) Discovery rate, (b) classification rate, (c) entropy-based classifier switching, (d) selected query strategy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-6-source-large.gif
2013,6081866,Fig. 7.,"Discovering and learning digits. Labels indicate iteration number, and whether the instance was queried by the (l) likelihood or (u) uncertainty criterion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-7-source-large.gif
2013,6081866,Fig. 8.,"Gait view data set. (a) Discovery rate, (b) classification rate, (c) entropy-based classifier switching, (d) query criteria weights (average).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-8-source-large.gif
2013,6081866,Fig. 9.,"Discovering and learning view angles. Labels indicate iteration, angle and if the instance was queried by the (l) likelihood criterion or (u) uncertainty criterion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-9-source-large.gif
2013,6081866,Fig. 10.,Similar gait images chosen for merging.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-10-source-large.gif
2013,6081866,Fig. 11.,Effect of including density weighted likelihood or entropy criterion in GSsw/GSadapt framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6389669/6081866/6081866-fig-11-source-large.gif
2013,6338362,Fig. 1.,"Structure of data-driven biomedical detection algorithms, including offline training and online detection (employing biomarker extraction and interpretation).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-1-source-large.gif
2013,6338362,Fig. 2.,"Reducing model complexity by reducing NSV, degrading the accuracy of classification for (a) RBF and polynomial kernels of order 3 and (b) polynomial kernels of order 2 and 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-2-source-large.gif
2013,6338362,Fig. 3.,"Arrhythmia detector. (a) Classification energy scales with
N
SV
dominate those of feature extraction. (b) Classification energy scales with
D
SV
.
N
SV
and
D
SV
represent classification complexity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-3-source-large.gif
2013,6338362,Fig. 4.,"Custom-instruction based implementation resulting in only modest energy improvement. Classification energy still dominates
preprocessing+featureextraction
(a) by
1146×
for wavelet features and (b) by
264×
for morphological features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-4-source-large.gif
2013,6338362,Fig. 5.,General architecture of a biomedical processor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-5-source-large.gif
2013,6338362,Fig. 6.,Architecture and layout of the classification coprocessor designed in an FD-SOI process. The coprocessor architecture comprises an array of MAC units to compute the dot product of SVs and TVs. The output is then transformed by a kernel function in order to evaluate the classification result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-6-source-large.gif
2013,6338362,Fig. 7.,"E
act
and
E
lk
profiles for a MAC unit with the minimum total energy occurring at
V
dd
=0.4V
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-7-source-large.gif
2013,6338362,Fig. 8.,"Operating frequency at
V
dd
=0.4V
is 520 kHz at 285 K (low temperature is slowest in subthreshold).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-8-source-large.gif
2013,6338362,Fig. 9.,Variable-precision MAC unit. Partial product additions can be terminated at CBA-0/1/2 to scale the precision for 8/10/12-bit inputs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-9-source-large.gif
2013,6338362,Fig. 10.,Scaling precision of input data enabling a second level of energy optimization. The figure shows energy savings of 17.6% while scaling the data-representation precision from 12 to 8 b.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-10-source-large.gif
2013,6338362,Fig. 11.,Programmable kernel enabling choice among kernels of degree 1–4.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-11-source-large.gif
2013,6338362,Fig. 12.,FD-SOI device having a steep subthreshold slope to minimize leakage and maintain high transistor on-to-off ratios in subthreshold CMOS gates.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-12-source-large.gif
2013,6338362,Fig. 13.,"Classification coprocessor enabling energy reductions of
228×
at 1.2 V. Energy reductions are increased to
1170×
at
V
dd
=0.4V
and 8 b of precision.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-13-source-large.gif
2013,6338362,Fig. 14.,"Coprocessor energy versus
D
SV
per TV (at 12-bit data precision).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-14-source-large.gif
2013,6338362,Fig. 15.,"Coprocessor energy versus
V
dd
per TV (at
N
SV
=10
,
D
SV
=8
, and 12-bit data precision).
V
dd
scaling enables energy reduction by up to 77%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-15-source-large.gif
2013,6338362,Fig. 16.,"Total application energy
(segmentation+featureextraction+classification)
reduced by up to
1062×
through the use of a coprocessor for classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-16-source-large.gif
2013,6338362,Fig. 17.,"Energy proportions for preprocessing, feature extraction, and classification computations, illustrating the benefits of voltage and precision scaling applied to the classification coprocessor for low-energy operation. (a) Base Tensilica processor operating at
V
dd
=1.2V
. (b) Custom-instruction-based implementation. (c) Hardware—software codesign with custom instructions for
preprocessing+featureextraction
and the classification computations implemented on a coprocessor at
V
dd
=1.2V
and 12 b of data precision. (d) Coprocessor at
V
dd
=1.2V
and 8 b. (e) Coprocessor at
V
dd
=0.4V
and 8 b.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-17-source-large.gif
2013,6338362,Fig. 18.,"Benefits of voltage scaling applied to the coprocessor (with the Tensilica processor at 1.2 V) using a fourth-order polynomial kernel and a beat classification rate
(
R
CLASS
)
of 3 beats/s with wavelet features
(
D
SV
=256)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-18-source-large.gif
2013,6338362,Fig. 19.,"Voltage scaling enabling the full detector computations at less than 500
μW
. Results are shown for morphological features
(
D
SV
=26)
with a quartic kernel at
R
CLASS
=3beats/s
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/6594872/6338362/6338362-fig-19-source-large.gif
2013,6423937,Fig. 1.,General architecture of the battery test workbench.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-1-source-large.gif
2013,6423937,Fig. 2.,Initial charging and discharging cycles used for battery conditioning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-2-source-large.gif
2013,6423937,Fig. 3.,DST profile.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-3-source-large.gif
2013,6423937,Fig. 4.,Experimental dataset (static test).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-4-source-large.gif
2013,6423937,Fig. 5.,Experimental dataset (DST test).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-5-source-large.gif
2013,6423937,Fig. 6.,Predicted SOC versus experimental SOC (static test).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-6-source-large.gif
2013,6423937,Fig. 7.,Predicted SOC versus experimental SOC (DST test).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-7-source-large.gif
2013,6423937,Fig. 8.,Zoomed-in view of SOC prediction for the static test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-8-source-large.gif
2013,6423937,Fig. 9.,SOC prediction errors for the static dataset (upper) and DST dataset (lower).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-9-source-large.gif
2013,6423937,Fig. 10.,Predicted and experimental cell voltages using the DST dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-10-source-large.gif
2013,6423937,Fig. 11.,Relative errors between the predicted and experimental cell voltages (expressed in percentage).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6525347/6423937/6423937-fig-11-source-large.gif
2013,6220279,Fig. 1.,"Experiments on unfolding surfaces embedded in
R
3
. In each subfigure,
X
stands for the training data, and
Z
stands for the generating data.
Y
SNPPE
,
Y
LLE
,
Y
NPP
, and
Y
ONPP
stand for the embedding given by SNPPE, LLE, NPP, and ONPP, respectively. (a) Learning results on
SwissRoll
. (b) Learning results on
SwissHole
. (c) Learning results on
Gaussian
. (d) Bar plot of PM values between
Y
and
Z
. (Note that the smaller the PM value is, the better the embedding result is; PM values for results obtained by SNPPE are too small to be clearly seen and are given in Table IV.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-1-source-large.gif
2013,6220279,Fig. 2.,"Experiment on locating new samples for randomly distributed
SwissRoll
data set
rsw1k
. (a) Training and generating data. (b)–(f) Locating results by SNPPE, KE, LLEoos, g-KONPP, and p-KONPP, respectively. In (b)–(f),
N=0
stands for the training result.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-2-source-large.gif
2013,6220279,Fig. 3.,"Experiment on locating new samples that are out of the range of training data. (a) Training (lower part) and testing data (upper and red part). (b) Ground truth of training and testing data (red part). (c)–(h) Locating results (red part) by SNPPE, NPPE, KE, LLEoos, g-KONPP, and p-KONPP, respectively. (i) Bar plot of PM values. (Note that the smaller the PM value is, the better the embedding result is; PM values for results obtained by SNPPE are too small to be clearly seen and are given in Table V.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-3-source-large.gif
2013,6220279,Fig. 4.,"Experiment on
lleface
data. Training results are plotted by blue dots, whereas testing results are marked with filled red circles. (a) and (f) Learning and testing results by SNPPE. (b) and (g) Learning and testing results by LLEoos. (c) and (h) Learning and testing results by KE. (d) and (i) Learning and testing results by g-KONPP. (e) and (j) Learning and testing results by p-KONPP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-4-source-large.gif
2013,6220279,Fig. 5.,"Experiment on
usps
data. Training results are plotted by blue dots, whereas testing results are marked with filled red circles. (a) and (f) Learning and testing results by SNPPE. (b) and (g) Learning and testing results by LLEoos. (c) and (h) Learning and testing results by KE. (d) and (i) Learning and testing results by g-KONPP. (e) and (j) Learning and testing results by p-KONPP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-5-source-large.gif
2013,6220279,Fig. 6.,"Experiment on
PIE
data. Training results are plotted by blue dots, whereas testing results are marked with filled red circles. (a) and (f) Learning and testing results by SNPPE. (b) and (g) Learning and testing results by LLEoos. (c) and (h) Learning and testing results by KE. (d) and (i) Learning and testing results by g-KONPP. (e) and (j) Learning and testing results by p-KONPP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-6-source-large.gif
2013,6220279,Fig. 7.,"Time cost and PM values on experiments conducted in Section VI. (a) PM value versus number of testing samples on
rsw10k
. (b) Log plot of time cost versus number of testing samples on
rsw10k
. (c) Log plot of time cost versus number of testing samples on
lleface
. (d) Log plot of time cost versus number of testing samples on
usps
. (e) Log plot of time cost versus number of testing samples on
pie
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6340245/6220279/6220279-fig-7-source-large.gif
2013,6212515,Fig. 1,A Bayesian network structure (DAG).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-1-source-large.gif
2013,6212515,Fig. 2,The BCD algorithm used for solving (2).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-2-source-large.gif
2013,6212515,Fig. 3,The shooting algorithm used for solving (3).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-3-source-large.gif
2013,6212515,Fig. 4,A general tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-4-source-large.gif
2013,6212515,Fig. 5,A general inverse tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-5-source-large.gif
2013,6212515,Fig. 6,"(a) General tree used in the simulation study in Section 5.1; (b) general inverse tree used in the simulation study in Section 5.2 (regression coefficients of arcs generated from
±Uniform(0.5,1)
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-6-source-large.gif
2013,6212515,Fig. 7,"(a) Frequency of
X
1
being identified as a parent of
X
i
,
i=2,…,7
; (b) ratio of number of correctly identified arcs in learned BN to number of arcs in true BN; (c) ratio of total learning error in learned BN (false positives plus false negatives) to number of arcs in true BN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-7-source-large.gif
2013,6212515,Fig. 8,"(a) Frequency of
X
i
being identified as parents of their respective child in true BN,
i=1,…,30
; (b) ratio of number of correctly identified arcs in learned BN to number of arcs in true BN; (c) ratio of total learning error in learned BN (false positives plus false negatives) to number of arcs in true BN",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-8-source-large.gif
2013,6212515,Fig. 9,"(a) Ratio of total learning error in the learned BN (false positives plus false negatives) to the number of arcs in the true BN for the 10 competing algorithms and SBN on 11 benchmark networks; (b) ratio of correctly identified arcs in the learned BN (i.e., true positives) to the number of arcs in the true BN; (c) ratio of falsely identified arcs in the learned BN (i.e., false positives) to the number of arcs in the true BN; (d) ratio of the total learning error in the learned PDAG to the number of arcs in the true PDAG. The learned BN and PDAG in (a)-(d) are based on a simulation dataset of sample size 1, 000. Dots are means and error bars are standard deviations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-9-source-large.gif
2013,6212515,Fig. 10,"(a) Ratio of total learning error in the learned BN (false positives plus false negatives) to the number of arcs in the true BN for the 10 competing algorithms and SBN on 11 benchmark networks; (b) ratio of correctly identified arcs in the learned BN (i.e., true positives) to the number of arcs in the true BN; (c) ratio of falsely identified arcs in the learned BN (i.e., false positives) to the number of arcs in the true BN; (d) ratio of the total learning error in the learned PDAG to the number of arcs in the true PDAG. The learned BN and PDAG in (a)-(d) are based on a simulation dataset of sample size 100. Dots are means and error bars are standard deviations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-10-source-large.gif
2013,6212515,Fig. 11,"Scalability of SBN with respect to (a) the number of variables,
p
, (b) the sample size,
n
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-11-source-large.gif
2013,6212515,Fig. 12,"Comparison of SBN with competing algorithms on CPU time in structure learning.
Y
-axis is the CPU time for each sweep through all the columns of
B
on a computer with Intel Core 2, 2.2 GHz, 4 GB memory. The
X
-axis is the first nine networks in Table 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-12-source-large.gif
2013,6212515,Fig. 13,Brain effective connectivity models by SBN. (a) AD; (b) NC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6504892/6212515/6212515-fig-13-source-large.gif
2013,6645367,Fig. 1.,"Emotions as componential phenomena, consisting of changes in appraisal, motivation, physiology, expression, and feeling. Adapted from [11], Fig. 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6776147/6645367/6645367-fig-1-source-large.gif
2013,6645367,Fig. 2.,Clustering of emotion centroids using hierarchical clustering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6776147/6645367/6645367-fig-2-source-large.gif
2013,6645367,Fig. 3.,Estimated decision boundaries of the Random Forest classifier (left) versus the LASSO multinomial regression without interactions (right) in the principal appraisal space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6776147/6645367/6645367-fig-3-source-large.gif
2013,6645367,Fig. 4.,Regression diagram for predicting positive versus negative emotions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6776147/6645367/6645367-fig-4-source-large.gif
2013,6645367,Fig. 5.,"Clockwise from top left: regression diagrams for predicting the shame/guilt cluster, rage, despair, and fear, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165369/6776147/6645367/6645367-fig-5-source-large.gif
2013,6331562,Fig. 1.,The effect of different table size ratio and signal quality. (a) 5 customers; (b) 10 customers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6401194/6331562/6331562-fig-1-source-large.gif
2013,6331562,Fig. 2.,"Price of anarchy with different utility functions. (a)
U(R,n)=
R
n
; (b)
U(R,n)=log(
R
n+10
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6401194/6331562/6331562-fig-2-source-large.gif
2013,6331562,Fig. 3.,"Average utility of customers in resource pool scenario when
r=0.4
. (a) 5 customers; (b) 3 customers; (c) best response when
N=3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6401194/6331562/6331562-fig-3-source-large.gif
2013,6331562,Fig. 4.,"Average utility of customers in available/unavailable scenario when
r=0
. (a) 5 customers; (b) 3 customers; (c) best response when
N=3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6401194/6331562/6331562-fig-4-source-large.gif
2013,6331562,Fig. 5.,Sequential cooperative spectrum sensing and accessing. (a) Channel sensing; (b) channel selection and signal broadcast; (c) data transmission.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6401194/6331562/6331562-fig-5-source-large.gif
2013,6331562,Fig. 6.,Spectrum accessing in cognitive radio network under different schemes. (a) Secondary user 1; (b) secondary user 3; (c) secondary user 4; (d) secondary user 7; (e) average utility; (f) SUs interfering PU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/6401194/6331562/6331562-fig-6-source-large.gif
2013,6171871,Fig. 1.,"Diagram of the whole process of the proposed two-stage enhancement scheme (dire = ridge direction, freq = ridge frequency).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-1-source-large.gif
2013,6171871,Fig. 2.,Demonstration of an enhanced window along the local ridge orientation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-2-source-large.gif
2013,6171871,Fig. 3.,Diagram of the whole process of the first-stage enhancement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-3-source-large.gif
2013,6171871,Fig. 4.,"Designed radial filter in the Fourier domain with
ρ
c
=5.0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-4-source-large.gif
2013,6171871,Fig. 5.,"Curves of enhanced image entropy values with (a) different
α,
when
β=1
and (b) different
β,
when
α=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-5-source-large.gif
2013,6171871,Fig. 6.,(a) Original fingerprint image. (b) Globally normalized image. (c) Locally normalized image. (d) First-stage enhanced image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-6-source-large.gif
2013,6171871,Fig. 7.,"Extracted minutiae from two input contexts. (a), (c), (e) Original images. (b), (d), (f) Our first-stage enhanced images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-7-source-large.gif
2013,6171871,Fig. 8.,"Results of the proposed two-stage enhancement algorithm that is applied to fingerprints from four FVC2004 databases DB1–DB4. (a), (c), (e), (g) Original images. (b), (d), (f), (h) Our two-stage enhanced images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-8-source-large.gif
2013,6171871,Fig. 9.,"(a), (b) Original fingerprints (FVC2004 db2 16_8.tif and 4_8.tif), compared with enhanced images of (c), (d) first-stage enhanced algorithm; (e), (f) Hong's Gabor-filter-based algorithm; (g), (h) Chikkerur's STFT algorithm; and (i), (j) our proposed two-stage enhancement algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-9-source-large.gif
2013,6171871,Fig. 10.,"(a) Original fingerprint (FVC2004DB2 4_8.tif), compared with enhanced images (b) Hong's Gabor filter + second-stage filter; (c) Chikkerur's STFT + second-stage filter; and (d) our proposed ridge-compensation filter + second-stage filter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-10-source-large.gif
2013,6171871,Fig. 11.,"Detection accuracy graphs with different methods over FVC2004 four subdatabases (no enhancement, Gabor-filter enhancement, Gabor filter + second-stage enhancement, STFT enhancement, STFT + second-stage enhancement, and our two-stage enhancement).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/6461502/6171871/6171871-fig-11-source-large.gif
2013,6420928,Fig. 1.,Spectral curves of a set of “scrub” pixels collected at different spatial locations from KSC hyperspectral data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-1-source-large.gif
2013,6420928,Fig. 2.,Pure and mixed pixels' location in Hilbert Space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-2-source-large.gif
2013,6420928,Fig. 3.,"Three kinds of spaces of pixels: (a) Input space of each pixel, (b) the RKHS induced by each basis kernel, and (c) the MKHS combined by every single RKHS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-3-source-large.gif
2013,6420928,Fig. 4.,"University of Pavia data set. (a) RGB composite image of bands 31, 21, and 11. (b) Ground-reference map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-4-source-large.gif
2013,6420928,Fig. 5.,"Indian Pine data set. (a) RGB composite image of three bands 50, 27, and 17. (b) Ground-reference map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-5-source-large.gif
2013,6420928,Fig. 6.,Tenth band (centered at 483.4 nm) of the subimage of AVIRIS Cuprite Nevada data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-6-source-large.gif
2013,6420928,Fig. 7.,Values of kernel alignment for basis kernels with different parameters. (a) “Scrub-willow.” (b) “Cattail marsh-salt marsh.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-7-source-large.gif
2013,6420928,Fig. 8.,"Kernel alignment for kernels constructed by samples from three classes. (a) “Scrub,” “willow,” and “CP hammock.” (b) “Cattail marsh,” “salt marsh,” and “spartina marsh.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-8-source-large.gif
2013,6420928,Fig. 9.,Overall rmse of unmixing results. (a) Data set 1. (b) Data set 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-9-source-large.gif
2013,6420928,Fig. 10.,(a) Subhyperspectral imagery of the Pavia data. (b) Related ground-reference map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-10-source-large.gif
2013,6420928,Fig. 11.,"Abundance map corresponding to (a) “metal sheet,” (b) “shadow,” (c) “meadows” by FCLS, (d) “metal sheet,” (e) “shadow,” (f) “meadows” by eSVM, (g) “metal sheet,” (h) “shadow,” and (i) “meadows” by MK-eSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-11-source-large.gif
2013,6420928,Fig. 12.,(a) Subhyperspectral imagery of Indiana Pine. (b) Related ground-reference map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-12-source-large.gif
2013,6420928,Fig. 13.,"Abundance map corresponding to (a) “woods,” (b) “corn-no till,” (c) “soybeans-min,” and (d) “soybeans-no till” by FCLS; (e) “woods,” (f) “corn-no till,” (g) “soybeans-min,” and (h) “soybeans-no till” by eSVM; and (i) “woods,” (j) “corn-no till,” (k) “soybeans-min,” and (l) “soybeans-no till” by MK-eSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-13-source-large.gif
2013,6420928,Fig. 14.,Abundance map of Cuprite. (a) “Alunite.” (b) “Kaolinite #1.” (c) “Dumortierite.” (d) “Sphene.” (e) “Kaolinite #2.” (f) “Pyrope #1.” (g) “Pyrope #2.” (h) “Muscovite.” (i) “Montmorillonite.” (j) “Kaolinite #3.” (k) “Kaolinite #4.” (l) “Buddingtonite.” (m) “Nontronite.” (n) “Andradite.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-14-source-large.gif
2013,6420928,Fig. 15.,(a) and (c) Endmember signatures provided by the USGS library. (b) and (d) Spectra of 100% abundance pixels identified by MK-eSVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6544631/6420928/6420928-fig-15-source-large.gif
2013,6517188,Fig. 1.,Illustration to compare FDA (left) and NCMML (right); the obtained projection direction is indicated by the gray line on which the projected samples are also plotted. For FDA the result is clearly suboptimal since the blue and green classes are collapsed in the projected space. The proposed NCMML method finds a projection direction which separates the classes reasonably well.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6517188/6517188-fig-1-source-large.gif
2013,6517188,Fig. 2.,"The learned class-specific biases
s
c
and the norm of the projected means
b
c
are strongly correlated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6517188/6517188-fig-2-source-large.gif
2013,6517188,Fig. 3.,"The nearest classes for two reference classes using the
ℓ
2
distance and metric learned by NCMML. Class probabilities are given for a simulated image that equals the mean of the reference class; see text for details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6517188/6517188-fig-3-source-large.gif
2013,6517188,Fig. 4.,"Results of the NCMC-test classifier, which uses
k=1
at train time and
k>1
at test time, for the 4K (left) and 64K (right) features, for several values of
k
during evaluation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6517188/6517188-fig-4-source-large.gif
2013,6517188,Fig. 5.,"Results of NCM as a function of the number of images used to compute the means for test classes. Comparison of the ML (blue) and MAP (red) mean estimates, for the 4K (left) and 64K (right) features, in a 1,000-way classification task, including baseline (black) when trained on all classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6601604/6517188/6517188-fig-5-source-large.gif
2013,6425443,Fig. 1.,"Data classification according ISO criteria. Red right-pointing triangles indicate true negatives, magenta downward-pointing triangles indicate true positives, black left-pointing triangles indicate false positives, and blue upward-pointing triangles indicate false negatives.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6532373/6425443/6425443-fig-1-source-large.gif
2013,6425443,Fig. 2.,"Profiles depending on the septic status of patients. Green squares and red circles correspond to the RTCGMS measurements that are classified as correct and incorrect measurements by the classifier, respectively. Magenta diamonds correspond to the ABG measurements that are used as the gold standard. A cyan right-pointing triangle indicates a true negative, a yellow downward-pointing triangle indicates a true positive, a black left-pointing triangle indicates a false positive, and a blue upward-pointing triangle indicates a false negative in the RTCGMS measurements. Black asterisks correspond to the calibration points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6532373/6425443/6425443-fig-2-source-large.gif
2013,6202711,Fig. 1.,"Simplified business process for fulfilling an order (Petri net
N
0
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-1-source-large.gif
2013,6202711,Fig. 2.,"Reachability graph for Petri net
N
0
(Fig. 1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-2-source-large.gif
2013,6202711,Fig. 3.,"PDFA
A
0
corresponding to Petri net
N
0
(Fig. 1), with the addition of transition probabilities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-3-source-large.gif
2013,6202711,Fig. 4.,"Example of process substructures in Petri net
N
0
and PDFA
A
0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-4-source-large.gif
2013,6202711,Fig. 5.,"Petri net and PDFA fragments depicting various substructures. (a) xor split. (b) Parallel (and) split. (c) Complex splits and joins (substructures
A
and
B
) and “extra” parallel activities (dotted ellipses).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-5-source-large.gif
2013,6202711,Fig. 6.,"(a) The Alpha relations on a pair of tasks partition the possible logs of
n
traces, (b) illustration of
(C∩D)∖(A∪B)
for Proposition 5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-6-source-large.gif
2013,6202711,Fig. 7.,"Probabilistic behavior of Alpha relations. (a) Probability of discovery of
a
→
n
b
for 10 traces, varying
π(ab),π(ba)
. (b) Probability of discovery of
a
∥
n
b
for varying numbers of traces and
π(ab)
(π(ab)+π(ba)=1.0)
. (c) Number of traces for 95% discovery probability of three-way xor substructure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-7-source-large.gif
2013,6202711,Fig. 8.,"Results showing convergence of Alpha to the ground truth, mining from logs of increasing size simulated from PDFA
A
0
. (a) Average metrics against number of traces. (b) Probability of approximately correct model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-8-source-large.gif
2013,6202711,Fig. 9.,"Petri Net
N
3
representing more complex example process with a selection of basic substructures and “extra” parallel relations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-9-source-large.gif
2013,6202711,Fig. 10.,"PDFA
A
3
corresponding to Petri net
N
3
, with the addition of transition probabilities, used for producing simulated event logs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-10-source-large.gif
2013,6202711,Fig. 11.,"Results showing convergence of Alpha to the ground truth, mining from logs of increasing size simulated from larger process model (PDFA
A
3
). (a) Average metrics against number of traces. (b) Probability of approximately correct model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-11-source-large.gif
2013,6202711,Fig. 12.,"PDFA and Petri nets produced by various mining algorithms. (a) PDFA
A
1
differing from
A
0
(Fig. 1) in probabilities only. (b) Petri net
N
1
structurally different from
N
0
(Fig. 1). (c) PDFA
A
2
corresponding to Petri net
N
1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-12-source-large.gif
2013,6202711,Fig. 13.,Comparison of metrics: varying probability of parallel substructure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6461142/6202711/6202711-fig-13-source-large.gif
2013,6247502,Fig. 1.,(In the white circles) Six examples of pixels selected to be labeled by a standard AL criterion. Most of them are placed in areas under shadows or at the boundary between several classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-1-source-large.gif
2013,6247502,Fig. 2.,"Images considered in the experiments, along with their corresponding Ground Truth (GTs) used for testing purposes. (a) Brüttisellen. (b) Highway.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-2-source-large.gif
2013,6247502,Fig. 3.,"Graphic user interface developed for testing the AL-UC model. On the left, the image to be labeled; in the middle, class buttons and a zoom; on the right, the current classification map, the confidence map, and the spectrum of the pixel in the white circle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-3-source-large.gif
2013,6247502,Fig. 4.,"AL-UC ingredients. (a) Classical AL heuristic: MCLU. (Uncertain samples correspond to dark colors.) (b) User's confidence estimation: SVM trained on user's responses. (Easily labelizable samples correspond to light colors.) (c) Mask obtained by thresholding the confidence map at
θ=0.6
. Only pixels in the black areas can now be selected by minimizing the MCLU criterion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-4-source-large.gif
2013,6247502,Fig. 5.,"Zoom images into the confidence map of Fig. 4(b) at iteration
ϵ=5
. (a) Image. (b) Confidence map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-5-source-large.gif
2013,6247502,Fig. 6.,Numeric results for the Brüttisellen data set. (a) Kappa statistic with traditional AL setting. (b) Number of queries per iteration involving 20 valid labeled samples. (c) Kappa statistic related to average real effort provided by the user.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-6-source-large.gif
2013,6247502,Fig. 7.,Comparison of the uncertainty maps returned by MCLU and EQB (multiplied by a −1 factor). Dark areas correspond to uncertain areas. (a) MCLU. (b) EQB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-7-source-large.gif
2013,6247502,Fig. 8.,"Cross-validation surface showing the overall accuracy of the confidence classifier for the Brüttisellen data set. On the right, confidence maps corresponding to the (in green/solid) maximum and (in red/dashed) two minima.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-8-source-large.gif
2013,6247502,Fig. 9.,Numeric results for the Highway data set using the MCLU heuristic. (a) Kappa statistic with traditional AL setting. (b) Number of queries per iteration involving 20 valid labeled samples. (c) Kappa statistic related to average real effort provided by the user.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-9-source-large.gif
2013,6247502,Fig. 10.,"(a) Confidence map and (b) corresponding binary mask for the Highway data set at iteration
ϵ=5
,
|
X
5
|=170
,
|
X
5
θ
|=218
. (a) Confidence map. (b) Confidence mask.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-10-source-large.gif
2013,6247502,Fig. 11.,Average number of queries for a committee of different users performing a single run of each model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6414652/6247502/6247502-fig-11-source-large.gif
2013,6497459,Fig. 1.,A cascade approach to statistical ST.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-1-source-large.gif
2013,6497459,Fig. 2.,An illustration of the speech recognition process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-2-source-large.gif
2013,6497459,Fig. 3.,An illustration of the SMT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-3-source-large.gif
2013,6497459,Fig. 4.,An illustration of the word alignment and phrase pair extraction. (a) Solid squares indicate the alignment links between source and target words. Boxes indicate valid source–target phrase pairs. (b) Part of the bilingual phrase table extracted from this sentence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-4-source-large.gif
2013,6497459,Fig. 5.,The generative process for phrase-based SMT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-5-source-large.gif
2013,6497459,Fig. 6.,"Inputs (a)
F
and (b)
F
′
, computed on the fly according to the reordering model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-6-source-large.gif
2013,6497459,Fig. 7.,Pseudocode of the Viterbi algorithm of DAG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-7-source-large.gif
2013,6497459,Fig. 8.,Pseudocode for the Viterbi search of DAH.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-8-source-large.gif
2013,6497459,Fig. 9.,An instance of the generic Viterbi search on the multilevel graph for WFST-based translation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-9-source-large.gif
2013,6497459,Fig. 10.,Hypergraph decoding without LM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-10-source-large.gif
2013,6497459,Fig. 11.,Hypergraph decoding with bigram LM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/6504474/6497459/6497459-fig-11-source-large.gif
2014,6657748,Fig. 1.,"Function
ψ(t)
used by NR method to rescale inequality constraints.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6828828/6657748/kwon1-2288101-large.gif
2014,6657748,TABLE I,Numerical Results,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6828828/6657748/kwon.t1-2288101-large.gif
2014,6730683,Fig. 1.,"Boxplot of dataset after removing excused absentees. Median values are shown as central marks, edges of the box represent 25th-75th percentile. Whiskers are shown for data up to 1.5 times the interquartile range. Outliers are plotted individually with a “+”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6730683/6730683-fig-1-source-large.gif
2014,6730683,Fig. 2.,Comparison of performance of linear prediction models using varying levels of quiz information. Error based on predicted final grade compared to actual final grade.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6730683/6730683-fig-2-source-large.gif
2014,6730683,Fig. 3.,Comparison of performance of logistic regression prediction models using varying levels of quiz information.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6730683/6730683-fig-3-source-large.gif
2014,6701391,Fig. 1.,"Standard approaches for incorporating humans into the machine learning process [1] have focused on individual human annotators for labeling difficult or ambiguous training data for continual improvement of a class model. Here we propose a new approach, wherein sets of queries are posed to crowds of citizen scientists on the web. In the framework of psychophysical experiments we can model patterns of error, which can be translated to human weighted loss functions that apply penalties for margins that are not consistent with human data during training. Steps that are different from traditional supervised learning or active learning are highlighted in bold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-1-source-large.gif
2014,6701391,Fig. 2.,"(a) For many problems in computer vision, humans are still considerably better than machines. These psychometric curves for face detection performance on our “Face in the Branches” test show that as the visible percentage of the face increases, both human and computer performance improves, but in all conditions human performance reaches a high level of accuracy at a level of face visibility where the comparison algorithms (Google's Picasa algorithm, and the face.com algorithm, recently acquired by Facebook) were unable to successfully detect. Note that the curves for humans have been normalized so that performance ranges from zero to one hundred percent accuracy; non-normalized chance accuracy (e.g., random guessing) on a three alternative forced choice task is 33 percent. This normalization allows for a direct comparison of performance with the algorithms, which were given discrete stimuli and asked to make a binary decision. (b) Example occluded stimuli with Portilla-Simoncelli backgrounds arranged from left to right in order of decreasing difficulty and increasing face area visible.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-2-source-large.gif
2014,6701391,Fig. 3.,"Number of support vectors selected by SVMs with hinge loss (
ϕ
h
) and human weighted loss (
ϕ
ψ
) during training for the same SVM parameter
C
. In all experiments described in this paper, we observed a solution for human weighted loss that was sparse compared to the corresponding classifier trained with hinge loss. All bars are the average for the 10 classifiers for each experiment (error bars reflecting standard error were too small to be visible).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-3-source-large.gif
2014,6701391,Fig. 4.,"An increase in accuracy is achieved when the hinge loss (HL) function of a linear SVM is replaced with a human weighted loss (HWL) function. Each curve represents an exhaustive tenfold cross validation experiment where a classifier for each fold of FDDB was trained on 200 images (100 +/−) from that fold and 100 images (50 +/−) from an outside data set. Classifiers were tested on 1,000 images (500 +/−) from each fold not used for training, for a total of 90 classification tests. All classifiers making use of the same data sets saw the exact same training data, and all classifiers were trained with the same SVM
C
parameter. Shaded regions represent standard error. Other possible configurations not shown did not differ significantly in pattern of results. (a) Accuracy increases when HL is replaced with HWL loss, using either the AFLW or Portilla-Simoncelli perceptually annotated data. Performance did not significantly differ between these data sets. (b) Biologically-inspired features outperform HOG features. Baseline performance increases in both cases when HL is replaced with HWL. (c) HWL using either accuracy or reaction time from the psychometric measure. Both improved baseline performance and did not significantly differ from each other.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-4-source-large.gif
2014,6701391,Fig. 5.,"FDDB results for the discrete score metric (Eq. (6)). The perceptually annotated classifiers trained using the biologically-inspired features of Cox and Pinto [41] produce the best results compared to all prior published approaches reporting on this data set [37]–[44]. The biologically-inspired features are especially effective at reducing false positives at higher true positive rates. Note the large measure of improvement between the baseline Viola-Jones algorithm, which is used as a first stage by our detection approach, and perceptual annotation with both feature types.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-5-source-large.gif
2014,6701391,Fig. 6.,"FDDB results for the continuous score metric (Eq. (7)). For these curves, each individual score contributes to the final result. The perceptually annotated classifiers trained with both feature types yield the highest accuracy, producing much higher quality detections based on the criterion of Eq. (5), compared to prior published approaches reporting on this data set [37]–[44]. The curve labeled “SVM,
ϕ
h
, HOG” highlights the difference in performance when the hinge loss function is replaced with human weighted loss for a well-known feature approach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-6-source-large.gif
2014,6701391,Fig. 7.,"Visual examples of detected FDDB faces from the full perceptual annotation approach and the baseline Viola-Jones algorithm. Perceptually annotated classifiers are better at detecting low-resolution and occluded faces, as well as those that are highly impacted by artifacts such as non-uniform illumination. Note that recall is not perfect for the perceptually annotated classifiers: at least one face is missed in each image shown above.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6848879/6701391/6701391-fig-7-source-large.gif
2014,6756997,Fig. 1.,Primal and dual networks of sparse ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai1-2298235-large.gif
2014,6756997,Fig. 2.,Training accuracy of sparse ELM with sinusoid nodes (ionosphere).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai2-2298235-large.gif
2014,6756997,Fig. 3.,Testing accuracy of sparse ELM with sinusoid nodes (ionosphere).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai3-2298235-large.gif
2014,6756997,Fig. 4.,"Performance of sparse ELM with sinusoid nodes for Australian and Diabetes
(C=1)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai4-2298235-large.gif
2014,6756997,Fig. 5.,"Performance of sparse ELM with sinusoid nodes for Iris and Segment
(C=1)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai5-2298235-large.gif
2014,6756997,Fig. 6.,SVM (Gaussian kernel) for ionosphere data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai6-2298235-large.gif
2014,6756997,Fig. 7.,Sparse ELM (Gaussian kernel) for ionosphere data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6756997/bai7-2298235-large.gif
2014,6678329,Fig. 1.,Decision boundaries of Iris classification multitask problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6828828/6678329/li1-2291772-large.gif
2014,6678329,TABLE I,Learned Coefficients for the Iris Classification Multitask Problem,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6828828/6678329/li.t1-2291772-large.gif
2014,6678329,TABLE II,Experimental Comparison of PSCS With the CS. IS Methods on Six Benchmark Data Sets in Terms of Percent Correct Classification Accuracy,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6828828/6678329/li.t2-2291772-large.gif
2014,6678329,TABLE III,Experimental Comparison of PSCS With the CS. IS Methods on Multitask Data Sets in Terms of Percent Correct Classification Accuracy,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6828828/6678329/li.t3-2291772-large.gif
2014,6617696,Fig. 1.,"Illustration of the behavior of slack variables
ξ
i
due to the slack and margin rescaling, respectively. Each vertical axis corresponds to a subset of constraints in a corresponding optimization problem with respect to the same data point
(x,y)
. The score value of the true output
y
is marked by a red bar, whereas the blue bars mark the score values of two support sequences
y
′
and
y
′′
with
Δ(y,
y
′
)>Δ(y,
y
′′
)
. (a) Green bars illustrate the effect of slack rescaling on the corresponding slack variables, where the distance between a red bar and the dotted line corresponds to the margin. (b) Distance between a dotted black line and a red bar corresponds to the margin of the length one, whereas the effect on slack variables due to the margin rescaling is visualized by the green dotted line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer1ab-2281761-large.gif
2014,6617696,Fig. 2.,"Simulation of Algorithm 1 for the case
⊙=⋅
,
T=4
,
N=3
, and
y=(3,2,1,3)
with
h
being the identity. On the left, the values
S
t,k
(j)
of the variable
S
are presented, where the edges between nodes denote the predecessor relationship. The red edges mark the decision in the current iteration step and the circled numbers correspond to the values
L
t,k
(j)
. On the right, the values
g
t
(i,j)
are shown in a matrix form.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer2-2281761-large.gif
2014,6617696,Fig. 3.,"Backtracking part of the simulation of Algorithm 1 in Fig. 2, where the red nodes mark an optimal solution
y
∗
=(2,3,3,1)
. The corresponding sum value is 13 yielding the optimal value
L
∗
=13⋅h(4)=13⋅4=52
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer3-2281761-large.gif
2014,6617696,Fig. 4.,"Illustration of the averaged running time (in seconds) over 10 repetitions of loss augmented Viterbi's algorithm, generalized Viterbi's algorithm, and a brute force algorithm via exhaustive searching in dependence on the length of the involved sequences for
N=4
. The exhaustive searching requires already for the short sequences of the length
T=10
about 160 s, while the computation time of the generalized Viterbi's algorithm for sequence length
T=2000
is less than 0.2 s.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer4-2281761-large.gif
2014,6617696,Fig. 5.,"Illustration of the distribution of the highest objective values of (14) for margin
(⊙=+)
and slack rescaling
(⊙=⋅)
in dependence on the sequence length (the vertical axis) and the Hamming loss with respect to the true output (the horizontal axis). The pictures were constructed by averaging over 10 randomly generated data sets, where the values
g
t
(i,j)
were drawn uniformly at random from an interval
[−50,50]
with
N=4
and
h
being identity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer5-2281761-large.gif
2014,6617696,Fig. 6.,"Illustration of data model for synthetic experiment. The graphic on the left shows a visualization of an example from the synthetic data. The first row (red) represents the label sequence consisting of two positive
(+1)
and three negative
(−1)
blocks. The remaining rows (blue) are the corresponding observation sequences generated by adding a Gaussian noise with mean
μ=0
and standard deviation
σ=1
to the label sequence. The graph on the right provides a corresponding state diagram describing the data model. There can be arbitrary many positive and negative states in a label sequence, while the first site always corresponds to the start and the last site to the end state.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer6ab-2281761-large.gif
2014,6617696,Fig. 7.,"(a) and (b) Averaged accuracy and averaged number of iterations taken by the cutting-plane algorithm for slack rescaling over 10 different synthetic data sets as a function of the number of training examples. Each curve corresponds to a fixed value of the standard deviation
σ∈{1,2,5,7,10,12}
of the Gaussian noise in the data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6617696/bauer7ab-2281761-large.gif
2014,6766243,Fig. 1.,Testing error with respect to different number of labeled data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang1abc-2307349-large.gif
2014,6766243,Fig. 2.,Testing error with respect to different number of unlabeled data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang2abc-2307349-large.gif
2014,6766243,Fig. 3.,"Visualization of the original IRIS data, and the embedded IRIS data using LE and US-ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang3abc-2307349-large.gif
2014,6766243,Fig. 4.,Clustering accuracy on IRIS as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang4-2307349-large.gif
2014,6766243,Fig. 5.,Clustering accuracy on WINE as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang5-2307349-large.gif
2014,6766243,Fig. 6.,Clustering accuracy on SEGMENT as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang6-2307349-large.gif
2014,6766243,Fig. 7.,Clustering accuracy on COIL20 as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang7-2307349-large.gif
2014,6766243,Fig. 8.,Clustering accuracy on USPST as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang8-2307349-large.gif
2014,6766243,Fig. 9.,Clustering accuracy on YALEB as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang9-2307349-large.gif
2014,6766243,Fig. 10.,Clustering accuracy on ORL as a function of the dimension of the embedded space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang10-2307349-large.gif
2014,6766243,Fig. 11.,"Example images extracted from the clean and noise USPST data set that we used. The images shown in the first row are the noise free examples. The second through the sixth row are the contaminated images with 10%, 20%, 30%, 40%, and 50% of original pixels replaced by random values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang11-2307349-large.gif
2014,6766243,Fig. 12.,Clustering accuracy of US-ELM and LE with respect to different levels of noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6936410/6766243/huang12-2307349-large.gif
2014,6698348,Fig. 1.,Test error differences on the real data sets. (a) CCAT. (b) Astro-physics. (c) Covertype. (d) A9a. (e) Gisette. (f) Mnist.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6914636/6698348/tao1abcdef-2294741-large.gif
2014,6698348,Fig. 2.,"Comparison results of the
l
1
-regularized problems. (a) CCAT. (b) Astro-physics. (c) Covertype. (d) A9a. (e) Gisette. (f) Mnist.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6914636/6698348/tao2abcdef-2294741-large.gif
2014,6698348,Fig. 3.,"Comparison results of the
l
2
-regularized problems. (a) CCAT. (b) Astro-physics. (c) Covertype. (d) A9a. (e) Gisette. (f) Mnist.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6914636/6698348/tao3abcdef-2294741-large.gif
2014,6763041,Fig. 1.,Sequence of processes in the BoW framework takes us from the image representation to the category label. The hierarchical visual coding architecture (gray) is responsible for transforming local features into feature codes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh1-2307532-large.gif
2014,6763041,Fig. 2.,"BoW framework map representations from image pixels to low-level descriptors, mid-level visual codes and, eventually, to a high-level image signature, which is used for categorization. In the hierarchical architecture, the feature dimensionality increases progressively, enhancing the richness of the representation. Meanwhile, the features are gradually abstracted through the layers and the number of features (i.e., cardinality) over the image space is reduced. The graph illustrates the relation between dimensionality and cardinality of the representation through the various processes in the framework.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh2-2307532-large.gif
2014,6763041,Fig. 3.,(a) Structure of the RBM. (b) Single RBM layer is used to encode a single SIFT descriptor in a shallow architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh3ab-2307532-large.gif
2014,6763041,Fig. 4.,Two-layer hierarchical architecture for coding SIFT-based MFs to local codes and subsequently to higher level macrocodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh4-2307532-large.gif
2014,6763041,Fig. 5.,"Comparing different coding strategies with the activation matrix
Z
. (a) Well-trained dictionary exhibits both selectivity and sparsity. Activations have high variance and are uncorrelated. (b) Selective-only coding may cause many features to be ignored or over represented. (c) Sparse-only coding alone might lead to codewords that have strong correlation or are silent.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh5abc-2307532-large.gif
2014,6763041,Fig. 6.,"Succession of two steps transforms a set of data-sampled activations to their targets. For illustrative purposes, the sequence of latent activations on the left is sorted in an ascending order of their activation level (middle) and its histogram is displayed on the right. (a) Original activation sequence may take the form of any empirical distribution. (b) Step 1 ranks the signals and scales them between 0 and 1, resulting in a uniform distribution within that interval. (c) Step 2 maps the ranked signals to fit a predefined long-tailed distribution to obtain
p
. Only few target activations are encouraged to be high, while most are low.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh6abc-2307532-large.gif
2014,6763041,Fig. 7.,"Overall, the parameters of the BoW model are optimized in six steps. The steps are grouped into three phases. 1: greedy supervised learning, 2: top-down regularized deeps learning, and 3: supervised discriminative learning. Inference consists of a single feedforward pass through the BoW pipeline.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh7-2307532-large.gif
2014,6763041,Fig. 8.,Transfer learning results between Caltech-101 and 15-Scenes. Transfer learning works well when the representation transferred is shallow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh8-2307532-large.gif
2014,6763041,Fig. 9.,"(a) Visualization of visual dictionary learned from SIFT descriptors by a shallow architecture. The codewords are observed to encode a variety of image structure, such as (b) smooth gradients, (c) lines and edges, (d) textured gratings, or (e) other complex features, such as corners, bends, and center-surround features. Hypothetical pixel intensities leading to strong responses for the codewords are shown below each set of codeword examples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh9abcde-2307532-large.gif
2014,6763041,Fig. 10.,Results on 15-Scenes with unsupervised RBM learning jointly regularized with sparsity and selectivity. Performance degrades as the coding gets too sparse or too distributed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh10-2307532-large.gif
2014,6763041,Fig. 11.,Relative contributions of sparsity and selectivity on categorization results on Caltech-101 (15 training examples) using a shallow architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh11-2307532-large.gif
2014,6763041,Fig. 12.,Results for 10 trials on Caltech-101 (30 examples) for the (a) shallow and (b) deep architectures using MFs. When supervised fine-tuning improves the results for every trial. The performance boost is substantial for the deep model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6960135/6763041/goh12ab-2307532-large.gif
2014,6851844,Fig. 1.,Supervised training schema.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6945396/6851844/guidi1-2337752-large.gif
2014,6851844,Fig. 2.,Patient management interface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6945396/6851844/guidi2-2337752-large.gif
2014,6851844,Fig. 3.,Score-based prognosis interface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6945396/6851844/guidi3-2337752-large.gif
2014,6851844,Fig. 4.,Input mask.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6945396/6851844/guidi4-2337752-large.gif
2014,6851844,Fig. 5.,Graphical view of follow-ups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6945396/6851844/guidi5-2337752-large.gif
2014,6851844,Fig. 6.,Decision process of pruned CART in HF severity assessment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6945396/6851844/guidi6-2337752-large.gif
2014,6615928,Fig. 1.,Variations of accuracies of SBELM and ELM with increment of hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6763171/6615928/6615928-fig-1-source-large.gif
2014,6615928,Fig. 2.,Variations of number of remaining hidden neurons with increment of hidden neurons in SBELM and TROP-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6763171/6615928/6615928-fig-2-source-large.gif
2014,6570512,Fig. 1.,"ROC curves of two selected feature combinations (Count2 and Leakage) and some individual features, window size = 5 s, on training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6815992/6570512/6570512-fig-1-source-large.gif
2014,6780607,Fig. 1.,Block diagram describing the conceptual framework for AL and SSL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse1-2305805-large.gif
2014,6780607,Fig. 2.,Sample selection by the query function with different SSL and AL strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse2-2305805-large.gif
2014,6780607,Fig. 3.,"Distribution of the synthetically generated samples. Dark points refer to the pool, while bright colored (red and blue) ones refer to the training set (two-moon toy data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse3-2305805-large.gif
2014,6780607,Fig. 4.,"Average (over ten trials) OA on the test set versus the number of semilabeled training samples used by the classifier in the five experiments using
PS
3
VM
(two-moon toy data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse4-2305805-large.gif
2014,6780607,Fig. 5.,"Average (over ten trials) OA on the test set versus the number of semilabeled training samples used by the classifier in the five experiments using
PS
3
VM-D
(two-moon toy data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse5-2305805-large.gif
2014,6780607,Fig. 6.,Average (over ten trials) OA on the test set versus the number of training samples used by the classifier in the five experiments using AL (two-moon toy data set).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse6-2305805-large.gif
2014,6780607,Fig. 7.,"Distribution of unlabeled and semilabeled data at different iterations using the
PS
3
VM
method for experiment 1: (a) Original training set, (b) 600 semilabeled points, (c) 1200 semilabeled points, and (d) 1800 semilabeled points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse7abcd-2305805-large.gif
2014,6780607,Fig. 8.,"Distribution of unlabeled and semilabeled data at different iterations using the
PS
3
VM-D
method for experiment 1: (a) Original training set, (b) 300 semilabeled points, (c) 500 semilabeled points, and (d) 1000 semilabeled points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse8abcd-2305805-large.gif
2014,6780607,Fig. 9.,"Distribution of labeled data at different iterations of the AL method (MCLU-ECBD) for experiment 1: (a) Original training set, (b) 40 labeled points, (c) 60 labeled points, and (d) 70 labeled points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse9abcd-2305805-large.gif
2014,6780607,Fig. 10.,"Distribution of the synthetically generated samples. Dark points refer to the pool, while bright colored ones refer to the training set (multiclass toy data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse10-2305805-large.gif
2014,6780607,Fig. 11.,True color composition of the considered QuickBird image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse11-2305805-large.gif
2014,6780607,Fig. 12.,"Distribution of training and pool samples considering bands 3 and 4 of the multispectral image for (a) experiment 1, (b) experiment 2, (c) experiment 3, and (d) experiment 4 (multispectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse12abcd-2305805-large.gif
2014,6780607,Fig. 13.,"Average (over ten trials) OA on the test set versus the number of semilabeled samples considered by
PS
3
VM
classifier in the four experiments (multispectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse13-2305805-large.gif
2014,6780607,Fig. 14.,Average (over ten trials) OA on the test set versus the number of semilabeled samples considered by \$\hbox{PS}^{3}\hbox{VM-D}\$ classifier in the four experiments (multispectral RS data set).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse14-2305805-large.gif
2014,6780607,Fig. 15.,Average (over ten trials) OA on the test set versus the number of training samples used by the classifier in the four experiments using AL (MCLU-ECBD) (multispectral RS data set).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse15-2305805-large.gif
2014,6780607,Fig. 16.,"Distribution of unlabeled and semilabeled data at different iterations using the \$\hbox{PS}^{3}\hbox{VM}\$ method for experiment 2 considering bands 3 and 4. (a) Original training set, (b) 500 semilabeled points, (c) 1000 semilabeled points, and (d) 2000 semilabeled points (multispectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse16abcd-2305805-large.gif
2014,6780607,Fig. 17.,"Distribution of unlabeled and semilabeled data at different iterations using the \$\hbox{PS}^{3}\hbox{VM}\$ method for experiment 4 considering bands 3 and 4. (a) Original training set, (b) 1000 semilabeled points, (c) 2000 semilabeled points, and (d) 3000 semilabeled points (multispectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse17abcd-2305805-large.gif
2014,6780607,Fig. 18.,"Distribution of unlabeled and labeled data at different iterations of the AL process (MCLU-ECBD) for experiment 2 considering bands 3 and 4. (a) Original training set, (b) 5 added labeled points, (c) 25 added labeled points, and (d) 100 added labeled points (multispectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse18abcd-2305805-large.gif
2014,6780607,Fig. 19.,"Distribution of unlabeled and labeled data at different iterations of the AL process (MCLU-ECBD) for experiment 4 considering bands 3 and 4. (a) Original training set, (b) 5 added labeled points, (c) 25 added labeled points, and (d) 100 added labeled points (multispectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse19abcd-2305805-large.gif
2014,6780607,Fig. 20.,"Average (over ten trials) OA obtained by the \$\hbox{PS}^{3}\hbox{VM-D}\$ with respect to the number of semilabeled samples added to the training set for experiments 1, 2, 3, 5, 7, and 9 (hyperspectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse20-2305805-large.gif
2014,6780607,Fig. 21.,"Average (over ten trials) OA obtained versus the number of labeled samples using the following methods: 1) Random sampling (RAND), 2) MCLU, 3) MCLU-ECBD, 4) multiview-based AL using five views generated by correlation-partition-based clustering as reported in [47] (multiview C), and 5) multiview-based AL using five views randomly generated (multiview R) (hyperspectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse21-2305805-large.gif
2014,6780607,Fig. 22.,"Average (over ten trials) OA obtained versus the number of labeled/semilabeled samples added to the training using different combination strategies for AL and SSL: 1) sequential strategy (AL+SSL) and 2) SSL encapsulation in the AL process (encSSL-AL). For the sequential strategy, solid and dashed lines correspond to AL and SSL phases, respectively (hyperspectral RS data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6780607/perse22-2305805-large.gif
2014,6582514,Fig. 1.,"Comparisons of indicator function and its differentiable approximation
ξ
β
(β=3)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6582514/6582514-fig-1-source-large.gif
2014,6582514,Fig. 2.,"Comparisons of Laplace approximation, EP, and hybrid Monte Carlo (HMC, 2000000 sampling points) in terms of generalization error and CPU time. (a) Synth. (b) Heart.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6582514/6582514-fig-2-source-large.gif
2014,6582514,Fig. 3.,"Comparison of classification of synthetic data sets using a RBF kernel. Two classes are shown as dots and crosses. The separating lines are obtained by projecting test data over a grid. Kernel and regularization parameters for SVM, RVM, and EPCVM are obtained by tenfold cross-validation. (a) Spiral: SVM. (b) Spiral: RVM. (c) Spiral: EPCVM. (d) Overlap: SVM. (e) Overlap: RVM. (f) Overlap: EPCVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6582514/6582514-fig-3-source-large.gif
2014,6582514,Fig. 4.,"Comparison of CPU time and the ERR of
EPCVM
Lap
, SVM, SMLR, and RVM on adult data set. (a) CPU time on adult data set. (b) Error rate on adult data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6582514/6582514-fig-4-source-large.gif
2014,6582514,Fig. 5.,Illustration of KL divergence between truncated posterior and truncated Gaussian prior.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6582514/6582514-fig-5-source-large.gif
2014,6704311,Fig. 1.,Entire scheme for the CP- and DP-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6914636/6704311/wang1-2296048-large.gif
2014,6704311,Fig. 2.,"Model selection of CP- and DP-ELM using Sigmoid hidden nodes for nonlinear time series
(σ=0.1)
. (a) Average ARER. (b) Selected hidden node numbers versus evaluation trials. (c) Testing RMSE versus evaluation trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6914636/6704311/wang2abc-2296048-large.gif
2014,6704311,Fig. 3.,"Identification results of (a) CP-, (b) DP-, (c) OP-, and (d) ELM using Sigmoid hidden nodes for nonlinear time series with
σ=0.1
(‘
⋅
’, ‘
∗
’, and ‘
∘
’ denote noisy, noise-free, and identification points, respectively).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6914636/6704311/wang3abcd-2296048-large.gif
2014,6810000,Fig. 1.,"Illustration of face-race perception. The quick glimpse of the picture will activate three “primitive” conscious evaluations of a person: a young white female, although this figure was a computer-generated “artificial” face with a mix of several races. (Photo source: The New Face of America, TIME Magazine, November 18, 1993.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he1-2321570-large.gif
2014,6810000,Fig. 2.,"Illustrative example showing that skin tone is not the major determinant of perceived racial identity: observers look at a series of facial photos where a central face (racially different from the surroundings) appeared lightened or darkened, which nevertheless produce no influence on their final race identification judgement of the central face. The behavior result showed that race perception is not determined by skin color information, but by morphological characteristics of the subject (Figure source: [55]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he2-2321570-large.gif
2014,6810000,Fig. 3.,"Illustrative genetic variance distribution of human races, while in practice it is often accepted that 3- to 7-races classification system would be enough for regular applications (Figure source: http://www.faceresearch.org/).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he3-2321570-large.gif
2014,6810000,Fig. 4.,"Eye tracking data shows different attention regions for Western and Eastern subjects (observers) viewing same race and other race faces from JACFEE [72]. Left part: Western subject’s gazing region for Western face. Right part: Eastern subject’s gazing region for Eastern face. As can be seen from the figure, fixation distributions for both observer group varied significantly (color-area indicates the density of fixations), which can be roughly grouped into nose-centric for Western (evenly distribution across the face) and eye-centric for Eastern (biased for upper part of face). Biased attention may cause ambiguity in affective perception (Figure source: [73]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he4-2321570-large.gif
2014,6810000,Fig. 5.,"Illustrative time course of face perception in human brain, recorded by ERP potential peaking. Race sensitive modulation was observed mainly in N100 (mean peak latency around 120ms) and P200. Gender modulation was observed in P200 (mean peak latency around 180ms). N170 component indicates structural encoding of face (versus non-face). Social membership cognition was observed in P300. Data source: [83] .",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he5-2321570-large.gif
2014,6810000,Fig. 6.,Illustration of race processing regions [36].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he6-2321570-large.gif
2014,6810000,Fig. 7.,"Illustrative visualization representing the relationship between eigenface and facial information, note that (b) and (c) are multiplied by 3
σ
, the square root of eigenvalue. Clearly the facial properties such as race, pose, illumination and gender are controlled by the different eigenfaces [114].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he7-2321570-large.gif
2014,6810000,Fig. 8.,"Discriminative race feature representation by multiple layer convolution neural networks (CNN). (a): supervised CNN filters, (b): CNN with transfer learning filters [126].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he8-2321570-large.gif
2014,6810000,Fig. 9.,Illustrative example showing racial sensitive regions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he9-2321570-large.gif
2014,6810000,Fig. 10.,"Illustrative example showing different periocular region, from left to right: East Asian, African-American, Caucasian, Asian Indian. Compared with iris image, periocular region could provide more rich semantic information for race classification (Source: Google Images).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he10-2321570-large.gif
2014,6810000,Fig. 11.,"Framework of a typical racial face processing system. Following the basic visual cortex scheme, the preprocessing part includes the detection of facial regions, illumination normalization, and edge detection. The second level functions like Gabor filter, sending output to the perceptual level for extracting features robust for selectivity and variance, after being grouped and classified, the category level gives the output [122].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he11-2321570-large.gif
2014,6810000,Fig. 12.,"The anatomically anthropometric landmarks used in [155]. It should be noted that those landmarks are highly redundant, therefore, how to chose a distinct subset according to some predefined criterion is essential.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he12-2321570-large.gif
2014,6810000,Fig. 13.,3D generic models and depth image of different races used in [119] for 3D reconstruction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he13-2321570-large.gif
2014,6810000,Fig. 14.,"The anatomical landmark regions used in [158], showing the five most relevant iso-geodesic stripe pairs for Asian and Caucasian subjects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he14-2321570-large.gif
2014,6810000,Fig. 15.,"Diagram showing the whole configuration of the CUN face database (with varying poses, expressions, accessories, illumination, photographic room and a multi-camera system [122] [174].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he15-2321570-large.gif
2014,6810000,Fig. 16.,3D facial stimuli generated by FaceGen Modeller [195].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he16-2321570-large.gif
2014,6810000,Fig. 17.,"Systematic illustration of a computationally intelligent biometric video surveillance system. From the top: the first level divides the input data into two branches: physical body tracking and face tracking. With interaction of databases, in the person session the system focuses on the overall biometrics such as height, behavior and trajectory following; while in the face session the system mainly concerns about detailed facial information, such as facial expressions, gender, race, identity, etc., in a coarse-to-fine order. Weighted outputs are sent to fusion center where the final decision making is performed. The system can be accomplished in a multi-camera networks. The extraction of these biometric sensitive information also helps to build intelligent CCTV for ensuring privacy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he17-2321570-large.gif
2014,6810000,Fig. 18.,"Silhouette profiles across races, from left to right: African-American (Male/Female), Asian (Male/Female), Caucasian American(Male/Female) (Figure source: [213]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he18-2321570-large.gif
2014,6810000,Fig. 19.,"Computational illustration for overall cognitive race processing regions and a hypothetical two-level working scheme hierarchy: Primary level (L1) includes ventral visual pathway (from retina to IT/TEO, for general object recognition and understanding), this level is bottom-up. Higher level (L2) includes VLPFC/DLPFC/MPFC, amygdala, ACC/PCC/FFA, it controls social interaction and application of race perception, which might exert top-down regulation to the L1 level. It is generally considered that PCC and FFA are engaged in early detection and categorization stage of race, and Amygdala is involved in the regulation of emotion and social status inference (e.g., racial attitude judgments). ACC/DLPFC/VLPFC are responsible for behavior control, such as race-biased response or stereotype, and prejudice. MPFC deals with inference with interaction among race groups. Selective attention exerts top-down administration. At present, most neuro-computational models have been focused on ventral pathway, and detailed knowledge about the precise role of cortical regions involved in racial cortex part is still missing. Note that several possible subcortical pathways also exist as shortcuts, such as the green dashed line involving the SC and the pulvinar nucleus, which provides fast and direct access to the amygdala, or LGN-MT and V2-TEO connections. Also note that the scheme is a rough hypothetical illustration and thus not strictly hierarchical. LGN, lateral geniculate nucleus; MT, medial temporal area (also known as V5); SC, superior colliculus; TEO, inferior temporal area; VLPFC, ventrolateral prefrontal cortex; DLPFC, dorsolateral prefrontal cortex; FFA, fusiform face area (fusiform gyrus).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6810000/he19-2321570-large.gif
2014,6740813,Fig. 1.,Undirected bipartite graphical model of an RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang1-2303478-large.gif
2014,6740813,Fig. 2.,Block diagrams of the keypoint recognition method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang2-2303478-large.gif
2014,6740813,Fig. 3.,"Example of generating training image patches. Left: selected keyframe. Right: set of image patches generated using (10) corresponding to the feature point extracted from the keyframe, as shown in the red circle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang3ab-2303478-large.gif
2014,6740813,Fig. 4.,Matching results of the first indoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang4-2303478-large.gif
2014,6740813,Fig. 5.,Matching results of the second indoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang5-2303478-large.gif
2014,6740813,Fig. 6.,Matching results of the first outdoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang6-2303478-large.gif
2014,6740813,Fig. 7.,Matching results of the second outdoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang7-2303478-large.gif
2014,6740813,Fig. 8.,Comparison results using the RBM and the L&F methods for the first indoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang8-2303478-large.gif
2014,6740813,Fig. 9.,Comparison results using the RBM and the L&F methods for the second outdoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang9-2303478-large.gif
2014,6740813,Fig. 10.,Average training time with different numbers of keypoint classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang10-2303478-large.gif
2014,6740813,Fig. 11.,Matching results using different numbers of point classes: 400 (top) and 200 (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang11-2303478-large.gif
2014,6740813,Fig. 12.,Average training time with different batch sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang12-2303478-large.gif
2014,6740813,Fig. 13.,Recognized keypoints with different batch sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang13-2303478-large.gif
2014,6740813,Fig. 14.,Comparison results using the RBM and the SIFT methods for the indoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang14-2303478-large.gif
2014,6740813,Fig. 15.,Comparison results using the RBM and the SIFT methods for the outdoor experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6740813/tang15-2303478-large.gif
2014,6620871,Fig. 1.,Two examples of using some source knowledge on fruits and animals while learning the target objects guava and okapi.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma1-197-large.gif
2014,6620871,Fig. 2.,Three ways in which transfer might improve the learning performance when the number of target training samples increases. Forcing the target learning process to rely on unrelated sources produces the negative transfer effect. (Figure reproduced and adapted from [16].).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma2-197-large.gif
2014,6620871,Fig. 3.,Scheme of the possible transfer learning conditions in visual object categorization. The number of source sets can increase with different possible levels of relatedness with respect to the target category. The tasks are heterogeneous (homogeneous) if the samples are represented with different (the same) descriptors. The target task can be supervised with an increasing number of training samples or unsupervised when the target samples are not annotated.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma3-197-large.gif
2014,6620871,Fig. 4.,"For Multi-KT the source and target models must live in the same space identified by the kernel
K
. For MultiK-KT all the sources can be defined independently in their own space and the target solution lives in the space obtained by orthogonal combination. We show also a geometrical interpretation of the kernel combination.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma4-197-large.gif
2014,6620871,Fig. 5.,"Top line: Performance of the Multi-KT method with various settings for the constraint on the source knowledge weights. The results correspond to average recognition rate over the categories, considering each class out experiment repeated ten times. Bottom line: output of the bidimensional scaling applied on the
β
vector values. For 10 mixed classes we also show the assigned weigths with a heat map where each row corresponds to a target class and each column to a source class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma5-197-large.gif
2014,6620871,Fig. 6.,"Left and middle columns: recognition rate as a function of the number of positive training samples. In each experiment we consider in turn one of the classes as target and the others as source, on ten random training sets. The final results are obtained as average over all the runs. Top Right: (up) the histogram bars represent the recall produced by the source model (indicated on the x-axis) when used to classify on the target class; (down) we compute separately the true positive and true negative recognition rate of Multi-KT and we show the value of their difference with no transfer in the related case. Bottom Right: average norm of the difference between two
β
vectors obtained for a pair of subsequent training steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma6-197-large.gif
2014,6620871,Fig. 7.,Performance of the MultiK-KT method in comparison with the single kernel Multi-KT formulation. The curves identified by no transfer and no transfer multiK corresponds respectively to learning from scratch by using only the Gaussian kernel or the combination of generalized Gaussian kernels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma7-197-large.gif
2014,6620871,Fig. 8.,"Recognition rate as a function of the number of positive training samples. Each source model is defined by using a Gaussian kernel with a different
γ
parameter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma8-197-large.gif
2014,6620871,Fig. 9.,Multi-KT performance for high number of source knowledge sets. Right: one shot learning recognition rate results when varying the number of prior known object categories.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma9-197-large.gif
2014,6620871,Fig. 10.,"Top line: recognition rate as a function of the number of positive training samples. Each experiment is defined by considering in turn one of the classes as target and the others as sources. The final results are obtained as average over ten runs. Bottom line: Maximum value over the elements of the
β
vector averaged over the classes and the splits.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6809246/6620871/tomma10-197-large.gif
2014,6682887,Fig. 1.,"A user interface to allow end users to test a classifier [32]. This classifier takes a message (input) and produces a topic label (output). The pie chart depicts the classifier's confidence that the message belongs in the red category (Cars), the blue category (Motorcycles), etc. A user can mark any message's topic label as right or wrong; these marked messages are the test cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-1-source-large.gif
2014,6682887,Fig. 2.,"Active learning versus classifier testing. The methods differ in 1) whether the classifier or the user drives interaction, 2) the aim of instance selection (maximize learning or maximize bug-detection) and 3) the outcome of the process (a better classifier, or user knowledge of classifier quality).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-2-source-large.gif
2014,6682887,Fig. 3.,"Failure detection effectiveness in different classifiers: All three methods outperformed RANDOM, with CONFIDENCE usually in the lead except for (f). (a-b): 20 Newsgroups by SVM (a) and Naive Bayes (b). (c), (d): Reuters by SVM and Naive Bayes. (e), (f): Enron by SVM and Naive Bayes. (g): SVM-20NG's confidence intervals at three training set sizes (200, 1,000, and 2,000).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-3-source-large.gif
2014,6682887,Fig. 4.,"Failure detection effectiveness of the baseline techniques. Both “intuitive” baseline methods performed worse than RANDOM. The classifiers shown are (a), (b): 20 Newsgroups by SVM (a) and Naive Bayes (b). (c), (d): Reuters by SVM and Naive Bayes. (e-f): Enron by SVM and Naive Bayes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-4-source-large.gif
2014,6682887,Fig. 5.,"Failure detection with confidence thresholds, at training set size 2,000 (the x-axis is the confidence level of the classifier). CONFIDENCE performed poorly as the confidence threshold increased, but COS-DIST and LEAST-RELEVANT outperformed RANDOM. (a-b): 20 Newsgroups by SVM (a) and Naive Bayes (b). (c), (d): Reuters by SVM and Naive Bayes. (e), (f): Enron by SVM and Naive Bayes. (Legend is same as Fig. 3.).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-5-source-large.gif
2014,6682887,Fig. 6.,"After removing items that might expose “the same fault” (by distance), CONFIDENCE and COS-DIST approaches still performed very well.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-6-source-large.gif
2014,6682887,Fig. 7.,"The CONFIDENCE (left), COS-DIST (middle), and LEAST-RELEVANT (right) visualizations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-7-source-large.gif
2014,6682887,Fig. 8.,"A user can mark a predicted topic wrong, maybe wrong, maybe right, or right (or “?” to revert to untested). Prior research found these four choices to be useful in spreadsheet testing [21].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-8-source-large.gif
2014,6682887,Fig. 9.,"(Top): The user tested three of the messages (the dark
✓
and × marks), so they no longer show a priority. Then the computer inferred the third message to be correct (light gray
✓
). Because the user's last test caused the computer to infer new information, the History column shows the prior values of what changed. (These values move right with each new change, until they are pushed off the screen.) (Bottom): A test coverage bar informs users how many topic predictions have been judged (by the user or the computer) to be correct (
✓
) or incorrect (×).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/32/6780997/6682887/6682887-fig-9-source-large.gif
2014,6705657,Fig. 1.,"The framework of online multi-target tracking using offline learned weights or online learned weights for association between detections and ongoing trajectories. (a), (c). The candidate associations between detection responses
o
1
,
o
2
and trajectories
r
1
,
r
2
,
r
3
,
r
4
at frame
t
. (b). According to the offline learned constant weights, as shown by same size dashed ellipses, the erroneous association
(
r
1
,
o
1
)
may be obtained. (d). The linking errors can be reduced using online learned weights, which are overall weights
w
t
or trajectory-specific weights
{
w
t
k
}
m
k=1
, and updated using online SOSVM learning or online discriminative learning respectively.
y
t
is the association result at frame
t
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6687259/6705657/6705657-fig-1-source-large.gif
2014,6705657,Fig. 2.,"Illustration of the weights online learned during multi-target tracking. (a) The current tracking scenario in frame 325 of ETHMS bahnnof sequence. (b) The weights learned with online learning method in subsection II-A. (c), (d) The weights of trajectories 77 and 71 learned with discrimination-regularized online method in Section II-B. The weights of trajectory 77 are larger in LBP parts than trajectory 71, which coincides with its distinctive textural features. Whereas the weights in (b) reflect that RGB features are more important than LBP features on the whole.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6687259/6705657/6705657-fig-2-source-large.gif
2014,6705657,Fig. 3.,The cumulative augmented structure loss of association results using online method PAMTT in Section II-A and offline SOSVM method [12].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6687259/6705657/6705657-fig-3-source-large.gif
2014,6705657,Fig. 4.,"Tracking examples for (a) PETS09, (b) ETHZ bahnnof and (c) TUD.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6687259/6705657/6705657-fig-4-source-large.gif
2014,6589012,Fig. 1.,"Proposed combination of unsupervised and supervised learning for function approximation. Left: the segmented line boxes show the original learning task, the key assumption, and the transformed learning task. Right: the solid thick line boxes show the proposed actual learning tasks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6740874/6589012/6589012-fig-1-source-large.gif
2014,6632881,Fig. 1.,"Watson's massively parallel evidence-based architecture [9], [10].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-1-source-large.gif
2014,6632881,Fig. 2.,Original Watson Jeopardy! machine-learning phases [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-2-source-large.gif
2014,6632881,Fig. 3.,Domain adaptation process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-3-source-large.gif
2014,6632881,Fig. 4.,Accuracy versus percentage of questions answered with lambda 0.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-4-source-large.gif
2014,6632881,Fig. 5.,Rank histogram for lambda 0.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-5-source-large.gif
2014,6632881,Fig. 6.,Accuracy versus percentage of questions answered with lambda 1.0E-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-6-source-large.gif
2014,6632881,Fig. 7.,Rank histogram for lambda 1.0E-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-7-source-large.gif
2014,6632881,Fig. 8.,Accuracy versus percentage of questions answered with lambda 10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-8-source-large.gif
2014,6632881,Fig. 9.,Rank histogram for lambda 10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/6766678/6632881/6632881-fig-9-source-large.gif
2014,6663672,TABLE I,"Classification of Feature Selection Algorithms for Learning to Rank Into Filter, Wrapper, and Embedded Categories",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor.t1-2286696-large.gif
2014,6663672,Fig. 1.,"Comparison of several nonconvex regularization terms.
ϵ=1
and
γ=1
are the parameters, respectively, for the log and MCP regularizations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor1-2286696-large.gif
2014,6663672,TABLE II,Derivatives of the Nonconvex Regularization Terms,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor.t2-2286696-large.gif
2014,6663672,TABLE III,Characteristics of Letor 3.0 and Letor 4.0 Distributions,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor.t3-2286696-large.gif
2014,6663672,Fig. 2.,Ratio of removed features for each algorithm and regularization on Letor 3.0 and 4.0 corpora.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor2-2286696-large.gif
2014,6663672,TABLE IV,Comparison of Sparsity Ratio Between Convex and Nonconvex Regularizations and State-of-the-Art Algorithms,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor.t4-2286696-large.gif
2014,6663672,TABLE V,"Comparison of MAP Between the Best Method on Each Dataset and Other Algorithms. Best MAP Is in Bold. the
∼
Symbol Indicates Equivalence Between Two Methods. Percentage of Decrease Is Presented When Statistically Significant Under the 5% Threshold (
p
-Values in Italics)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor.t5-2286696-large.gif
2014,6663672,TABLE VI,"Comparison of NDCG@10 Between the Best Method on Each Dataset and Other Algorithms. Best MAP Is in Bold. the
∼
Symbol Indicates Equivalence Between Two Methods. Percentage of Decrease Is Presented When Statistically Significant Under the 5% Threshold (
p
-Values in Italics)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor.t6-2286696-large.gif
2014,6663672,Fig. 3.,MAP versus sparsity ratio for three representative datasets. Dotted lines represented average MAP obtained with the different algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6815990/6663672/lapor3-2286696-large.gif
2014,6871342,Fig. 1.,(a) Schematic diagram: OSA: optical spectrum analyzer; FBG: fiber Bragg grating; PC: personal computer. (b) Spectra of the two-FBG sensor array measured from OSA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/6898892/6871342/jiang1ab-2345062-large.gif
2014,6871342,Fig. 2.,Flow chart of the ELM-based approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/6898892/6871342/jiang2-2345062-large.gif
2014,6871342,Fig. 3.,The performance of ELM on a wide range of number of hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/6898892/6871342/jiang3-2345062-large.gif
2014,6871342,Fig. 4.,The training time and testing time of ELM on a wide range of number of hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/6898892/6871342/jiang4-2345062-large.gif
2014,6871342,Fig. 5.,"(a) Measured Bragg wavelengths of the two FBGs, when different strain was applied to FBG2. (b) Input and output of the ELM model when step
=8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/6898892/6871342/jiang5ab-2345062-large.gif
2014,6871342,Fig. 6.,Cumulative percentile of RMS error for different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/6898892/6871342/jiang6-2345062-large.gif
2014,6807760,Fig. 1.,(a) Classification of EMG contaminated with power line interference. Effect of power line interference on simulated EMG at 5 dB in (b) time and (c) frequency domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6940066/6807760/frase1abc-2317296-large.gif
2014,6807760,Fig. 2.,(a) Classification of EMG contaminated with motion artifact. The effect of motion artifact on simulated EMG at 0 dB in (b) time and (c) frequency domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6940066/6807760/frase2abc-2317296-large.gif
2014,6807760,Fig. 3.,"(a) Classification of EMG contaminated with ECG interference. The effect of ECG interference on simulated EMG at
−5 dB
in (b) time and (c) frequency domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6940066/6807760/frase3abc-2317296-large.gif
2014,6807760,Fig. 4.,"(a) Classification of EMG contaminated with quantization noise. The effect of quantization noise on simulated EMG at an ADC step size of
2
−3
(29 dB) in (b) time and (c) frequency domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6940066/6807760/frase4abc-2317296-large.gif
2014,6807760,Fig. 5.,(a) Classification of EMG contaminated with ADC clipping. The effect of ADC clipping on simulated EMG with 310 samples clipped (26 dB) in (b) time and (c) frequency domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6940066/6807760/frase5abc-2317296-large.gif
2014,6807760,Fig. 6.,(a) Classification of EMG contaminated with amplifier saturation. The effect of amplifier saturation on simulated EMG with 34% of the signal in the amplifier nonlinear region (15 dB) in (b) time and (c) frequency domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6940066/6807760/frase6abc-2317296-large.gif
2014,6722919,Fig. 1.,Rotation and scaling of an image results in shift in HT features as the filter responses are captured by correspondingly rotated and scaled filters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6712948/6722919/6722919-fig-1-source-large.gif
2014,6722919,Fig. 2.,Texture images from the Brodatz texture album.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6712948/6722919/6722919-fig-2-source-large.gif
2014,6690171,Fig. 1.,"Faults in a five-phase machine, from left to right: open-circuit fault and short-circuit fault.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham1ab-2295537-large.gif
2014,6690171,Fig. 2.,Block diagram of BEM-based FTC scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham2-2295537-large.gif
2014,6690171,Fig. 3.,"Block diagram of the ILC-based and
BEM+ILC
-based FTC schemes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham3-2295537-large.gif
2014,6690171,Fig. 4.,Speed-normalized BEM waveform of the five-phase PM machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham4-2295537-large.gif
2014,6690171,Fig. 5.,FEA results for input currents and output torque with ILC-based FTC scheme. (a) and (b) Iteration 0. (c) and (d) Iteration 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham5abcd-2295537-large.gif
2014,6690171,Fig. 6.,"FEA results for input currents and output torque with
BEM+ILC
-based FTC scheme. (a) and (b) Iteration 0. (c) and (d) Iteration 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham6abcd-2295537-large.gif
2014,6690171,Fig. 7.,Two-norm of torque error versus iteration number for ILC-based FTC scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham7-2295537-large.gif
2014,6690171,Fig. 8.,"Two-norm of torque error versus iteration number for
BEM+ILC
-based FTC scheme.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham8-2295537-large.gif
2014,6690171,Fig. 9.,Five-phase motor drive system used for experimental tests.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham9-2295537-large.gif
2014,6690171,Fig. 10.,Five-phase current and output torque under normal operation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham10ab-2295537-large.gif
2014,6690171,Fig. 11.,Current of faulty phase and output torque under short-circuit fault condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham11ab-2295537-large.gif
2014,6690171,Fig. 12.,Output torque under open-circuit fault condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham12-2295537-large.gif
2014,6690171,Fig. 13.,Output torque under short-circuit fault condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham13-2295537-large.gif
2014,6690171,Fig. 14.,Experimental results for step change in load torque under star connection and single-phase open-circuit fault condition. (a) Current in a healthy phase. (b) Output torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham14ab-2295537-large.gif
2014,6690171,Fig. 15.,Experimental results for transition from healthy condition to single-phase open-circuit fault condition. (a) Current of faulty phase. (b) Current of adjacent healthy phase. (c) Current of nonadjacent healthy phase. (d) Output torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/6783703/6690171/moham15abcd-2295537-large.gif
2014,6775322,Fig. 1.,"(a) Triaxial ACC signals recorded from the wrist extensor site during resting tremor while sitting, and (b) ACC sensor signals collected from the wrist extensor site during kinetic tremor recorded while the patient was voluntarily moving his arm. In the absence of movement as in (a), the identification of tremor is straightforward and depends on the recognition of the periodicity that characterizes movements due to tremor; however, the presence of the much slower and more energetic voluntary movement in (b) can obscure the periodicity of tremor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-1-source-large.gif
2014,6775322,Fig. 2.,"(a) One channel of the ACC signals recorded by the sensor above the wrist extensor muscle while the subject performed voluntary movement and experienced dyskinesia, respectively. (b) The power spectral densities associated with the signals shown in (a). From these PSDs, we can tell that energy during dyskinesia is concentrated in the higher frequencies, whereas energy during voluntary movement is concentrated in the lower frequencies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-2-source-large.gif
2014,6775322,Fig. 3.,"Signals recorded from the ACC sensors above the wrist extensor muscle in patients experiencing tremor (top row) and dyskinesia (bottom row). In the ACC sensor signals, tremor presents as a sinusoidal wave with periodicity in the 3–6 Hz range, whereas dyskinesia produces aperiodic movements with no fundamental frequency.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-3-source-large.gif
2014,6775322,Fig. 4.,"Signals recorded from the ACC and EMG sensors above the wrist extensor muscle in patients experiencing tremor (top row) and dyskinesia (bottom row). In the sEMG sensor signals, tremor produces regular bursts of energy at the same fundamental frequency seen in the ACC signals, whereas dyskinesia is represented by sporadic bursts of energy that vary in amplitude, duration, and time between bursts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-4-source-large.gif
2014,6775322,Fig. 5.,"Diagram of the DNN used to recognize tremor. There are seven input nodes, four hidden nodes, and one output node. Each FIR filter in the hidden layer has five weights, whereas each filter in the output layer has one weight. The DNN used to recognize dyskinesia is structured similarly, with five input nodes, two hidden nodes, and one output node.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-5-source-large.gif
2014,6775322,Fig. 6.,"Diagram of the HMMs used to recognize tremor and nontremor segments in the isolated word recognizer framework from Rabiner [12]. Model is a left-right model with five total states. Number of states was fixed at the number of windows, such that moving from one window to the next always represented a state transition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-6-source-large.gif
2014,6775322,Fig. 7.,Locations of the eight wireless sensors worn by the subjects in our studies. The classifiers described in this paper rely on data from one hybrid sensor (denoted by stars) to track the evolution of both tremor and dyskinesia in a particular limb. A picture of one of these wireless sensors is visible in the inset. The sensor collects three channels of data from a triaxial accelerometer and one channel of surface electromyographic data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6893060/6775322/6775322-fig-7-source-large.gif
2014,6471714,Fig. 1.,Summary of major multi-label evaluation metrics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang1-39-large.gif
2014,6471714,Fig. 2.,Categorization of representative multi-label learning algorithms being reviewed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang2-39-large.gif
2014,6471714,Fig. 3.,Pseudo-code of binary relevance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang3-39-large.gif
2014,6471714,Fig. 4.,Pseudo-code of classifier chains.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang4-39-large.gif
2014,6471714,Fig. 5.,Pseudo-code of calibrated label ranking.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang5-39-large.gif
2014,6471714,Fig. 6.,"Pseudo-code of random
k
-labelsets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang6-39-large.gif
2014,6471714,Fig. 7.,"Pseudo-code of ML-
k
NN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang7-39-large.gif
2014,6471714,Fig. 8.,Pseudo-code of ML-DT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang8-39-large.gif
2014,6471714,Fig. 9.,Pseudo-code of rank-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang9-39-large.gif
2014,6471714,Fig. 10.,Pseudo-code of CML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/6851970/6471714/zhang10-39-large.gif
2014,6690197,Fig. 1.,"Examples of discrimination functions learned on probabilistic datasets via SVM and P-SVM. (a) training dataset spatial distribution along with the associated class probability
p
i
=P(
l
i
=1|X=
x
i
)
. (b) learned functions for the regular SVM (in green) and the P-SVM (in orange) while considering binary and probabilistic class labels
l
i
respectively. The regular SVM separating hyperplane is impacted by the yellow
(
p
i
=0.55)
and green
(
p
i
=0.45)
points which are considered as ‘
+1
’ and ‘
−1
’ examples respectively; the P-SVM algorithm accounts for the low class probability of these two examples
(
p
i
≃0.5)
, thus resulting in a more robust discriminative hyperplan, close to the Bayes decision (in black).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-1-source-large.gif
2014,6690197,Fig. 2.,"Localisation constraints representation depending on
p
. Localisation constraints for prediction
f(x)
are defined by boundaries
z
+
et
z
−
. They aim at maintaining predictions between defined limits depending on
p
. The nearest to 0 or 1 (up to minimum distance
η
) label
p
is, the softest the localisation constraint on
f(x)
is
(→∞)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-2-source-large.gif
2014,6690197,Fig. 3.,"Introduction of one outlier. We simulate two gaussian datasets labelled ‘
−1
’ (blue) and ‘
+1
’ (red), to visualize the position of the constructed P-SVM frontier (in black) and the margins (in green) depending on label
l
of the outlier (in pink): (a)
l=+1
, (b)
l=P(Y=1∣X=x)=0.51
, with
η=0.1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-3-source-large.gif
2014,6690197,Fig. 4.,"Probability estimations comparison. Top plot (a) shows the true probability distributions for classes ‘
−1
’ (blue) and ‘
+1
’ (red) (ground truth); the overlaying circles represent the
n
t
learning examples. Middle plot (b) shows the true posterior probability (black) with
SVM+Platt
(blue),
F-SVM+Platt
(green) and P-SVM (red) estimations overlaying. Lower plot (c) shows the distance between true probabilities and estimations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-4-source-large.gif
2014,6690197,Fig. 5.,"Comparison of P-SVM, regular
SVM+Platt
and
F-SVM+Platt
robustness to labelling noise. (a) True probability distribution together with noisy learning data points, plotted in blue (class ‘
−1
’) and red (class ‘
+1
’) circles. Probability estimations of (b) P-SVM, (c)
SVM+Platt
and (d)
F-SVM+Platt
over a grid when trained on the noisy data points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-5-source-large.gif
2014,6690197,Fig. 6.,"Evolution of P-SVM,
SVM+Platt
and
F-SVM+Platt
classification and prediction performances depending on noise amplitude
δ
(
n
l
=100
,
n
t
=1000
, drawing repeated 100 times). (a) Classification performances. (b) Probability estimations performances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-6-source-large.gif
2014,6690197,Fig. 7.,"Classification and probability prediction performances of the P-SVM and
F-SVM+Platt
depending on the proportion of probabilistic labels. Comparison to classical
SVM+Platt
performances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-7-source-large.gif
2014,6690197,Fig. 8.,"Prostate MRI: (a) axial T2-weighted, (b) Apparent Diffusion Coefficient and (c) Dynamic Contrast-Enhanced (after Gd-injection) MR images together with the corresponding (d) histology slice. Histologically assessed cancers (A and B) were outlined on MR images (scored 0.75 and 1 respectively) during the blinded a priori MR analysis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/6717077/6690197/6690197-fig-8-source-large.gif
2014,6481455,Fig. 1.,SDC2 block diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-1-source-large.gif
2014,6481455,Fig. 2.,Photograph of the SDC2 in the laboratory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-2-source-large.gif
2014,6481455,Fig. 3.,Block diagram describing the web publishing method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-3-source-large.gif
2014,6481455,Fig. 4.,DFIM drive schematics as implemented in the laboratory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-4-source-large.gif
2014,6481455,Fig. 5.,Rotor-side converter control system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-5-source-large.gif
2014,6481455,Fig. 6.,Grid-side converter control system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-6-source-large.gif
2014,6481455,Fig. 7.,"DFIM equivalent circuits in the qd reference frame, using induction machine model 0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-7-source-large.gif
2014,6481455,Fig. 8.,Simulated rotor current and phase voltage (top) and speed (bottom) in motoring mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-8-source-large.gif
2014,6481455,Fig. 9.,Simulated rotor current and phase voltage (top) and speed (bottom) in motoring mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-9-source-large.gif
2014,6481455,Fig. 10.,"Screen capture in generating mode, subsynchrounous speed, dc motor armature current 1 A. Oscilloscope plots include stator line-to-line voltage on Channel 1, top, lighter waveform [250 V/div] and current on Channel 2, top, darker waveform [1 A/div] (top). Rotor line-to-line voltage on Channel 3, bottom waveform with larger ripple [25 V/div] and current on Channel 4, bottom waveform with smaller ripple [0.5 A/div].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-10-source-large.gif
2014,6481455,Fig. 11.,"Simulated plots of stator line-to-line voltage [250 V/div] and current [1 A/div] (top) and rotor line-to-line voltage [25 V/div] and current [0.5 A/div] (bottom) to match the scope plots in Fig. 4. Generating mode, subsynchronous speed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-11-source-large.gif
2014,6481455,Fig. 12.,"Web interface screen capture while the DFIM is running in generating mode, supersynchronous speed. Oscilloscope plots include stator line-to-line voltage on Channel 1, top, lighter waveform, [250 V/div] and current on Channel 2, top darker waveform, [1 A/div]. Rotor line-to-line voltage on Channel 3, bottom waveform with larger ripple, [10 V/div] and current on Channel 4, bottom waveform with smaller ripple [0.5 A/div].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-12-source-large.gif
2014,6481455,Fig. 13.,"Simulated plots of stator line-to-line voltage [250 V/div] and current [1 A/div] (top) and rotor line-to-line voltage [10 V/div] and current [0.5 A/div] (bottom) to match the scope plots in Fig. 13. Generating mode, supersynchronous speed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-13-source-large.gif
2014,6481455,Fig. 14.,Experimental data and simulated data (circles) plotted versus speed. Motoring mode of operation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-14-source-large.gif
2014,6481455,Fig. 15.,Experimental data and simulated data (circles) plotted versus speed. Generating mode of operation with dc motor armature current 0.5 A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-15-source-large.gif
2014,6481455,Fig. 16.,Experimental data and simulated data (circles) plotted versus speed. Generating mode of operation with dc motor armature current 1 A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-16-source-large.gif
2014,6481455,Fig. 17.,Experimental data and simulated data (circles) plotted versus speed. Generating mode of operation with dc motor armature current 1.5 A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6563118/6481455/6481455-fig-17-source-large.gif
2014,6803991,Fig. 1.,"The data points distribution in the 2-D embedded space, where (A) Interface residues after Isomap; (B) Non-interface residues after Isomap; (C) Interface residues after k-means clustering; (D) Non-interface residues after k-means clustering; (E) Reconstructed dataset, where green points are interface residues and red points denote non-interface residues.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6823760/6803991/6803991-fig-1-source-large.gif
2014,6803991,Fig. 2.,The numbers of residues in reconstructed dataset when the dimension of embedding space is changed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6823760/6803991/6803991-fig-2-source-large.gif
2014,6803991,Fig. 3.,The six performance measures in different experimental runs. (A) svm_ER; (B) svm_SP; (C) svm_SPER.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6823760/6803991/6803991-fig-3-source-large.gif
2014,6803991,Fig. 4.,"The comparison of prediction performance between the predictors based on the original and reconstructed dataset, where the last letter of “O” denotes the original dataset, and “R” the reconstructed dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/6823760/6803991/6803991-fig-4-source-large.gif
2014,6766742,Fig. 1.,(a) Schematic diagram of the USB-port-based capacitance measurement system employing FSVV and (b) input circuit of the system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka1ab-2307991-large.gif
2014,6766742,Fig. 2.,USB-port-based FSVV module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka2-2307991-large.gif
2014,6766742,Fig. 3.,Actual USB-port-based capacitance measurement system developed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka3-2307991-large.gif
2014,6766742,Fig. 4.,GUI interface developed for the capacitance measurement system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka4-2307991-large.gif
2014,6766742,Algorithm 1,ELM Algorithm in Training Phase,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka8-2307991-large.gif
2014,6766742,Fig. 5.,"Complete capacitance measurement system, in schematic form.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka5-2307991-large.gif
2014,6766742,Algorithm 2,Automatic Amplifier Gain Selection or Control Module,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka9-2307991-large.gif
2014,6766742,Fig. 6.,"Real capacitance measurement performance comparisons at gain 1, at sample frequencies. (a) 30, (b) 50, (c) 70, and (d) 90 Hz for schemes employing hybrid BPNN-polynomial, hybrid ELM1-polynomial, hybrid ELM2-polynomial, and hybrid ELM3-polynomial-based reinforcements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka6abcd-2307991-large.gif
2014,6766742,Fig. 7.,"Capacitance measurement error comparisons, at gain 1, at 50 Hz, between schemes employing either no-compensation in level 2 or polynomial-based compensation in level 2 with (a) ELM1-based, (b) ELM2-based, and (c) ELM3-based reinforcements in level 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/6873368/6766742/sarka7abc-2307991-large.gif
2014,6858016,Fig. 1.,Relevant EM wave transformations between transmitter (Tx) and receiver (Rx) in dielectric media. Transmission (T): propagation without attenuation; attenuation (A): decay in signal amplitude; reflection (Fl): change in signal direction at interface; refraction (Fr): change in signal direction and speed at interface; diffraction (D): spreading and interference of waves at discontinuity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray1-2332872-large.gif
2014,6858016,Fig. 2.,"Radargram of a crevasse with the three indicative patterns: clear snow and firn layering, diffractions, and void. Amplitude values are mapped according to the colorbar. y-axis is depth in samples, corresponding to 12.5 m, x-axis is trace number, which can be converted to distance or time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray2-2332872-large.gif
2014,6858016,Fig. 3.,"Crevasse cross-sectional view. Note sagging and possible collapse of current and old snow bridges, and the result on the internal geometry. Snow layering is equivalent to density variation in upper firn layers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray3-2332872-large.gif
2014,6858016,Fig. 4.,"Yeti robot with GPR in tow near McMurdo Station, Antarctica, in 2010, with Pisten Bully tractor behind to show scale.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray4-2332872-large.gif
2014,6858016,Fig. 5.,Rosette survey patterns in a Greenland crevasse field. Anti-diagonal lines are crevasses drawn as shapefiles over 0.5-m resolution WorldView-2 satellite imagery. Each rosette is 55 m in diameter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray5-2332872-large.gif
2014,6858016,Fig. 6.,"SVM classification results for crevasse crossings in Rosette 3, downsampling by 8. Crevasses are concatenated for visualization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray6-2332872-large.gif
2014,6858016,Fig. 7.,"Diagram of HMM computation in relation to crevasse radar patterns. Each trace in a radargram is an observation
o
t
, and time progresses to the right.
∑
for the void model was not positive definite and could not be decomposed to compute the model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray7-2332872-large.gif
2014,6858016,Fig. 8.,Evaluation criteria for HMM classification. (a) Four cases of evaluation criteria for diffraction areas associated with a crevasse. (b) Two cases of evaluation criteria for firn.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray8ab-2332872-large.gif
2014,6858016,Fig. 9.,HMM–SVM classification results for Rosette 3 crevasse crossings downsampled by 8.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray9-2332872-large.gif
2014,6858016,Fig. 10.,"HMM–SVM results for Rosette 3 geo-referenced to GPS track file and satellite imagery. (a) Rosette 3 hand-labels. Each open circle represents 1 full second of “void” labels. (b) HMM–SVM results for Rosette 3, binned according to number of “void” predictions per second. FNs are plotted as closed circles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7015648/6858016/ray10ab-2332872-large.gif
2014,6805162,Fig. 1.,Illustration example of node localization in WSNs in 3D space using supervised neural networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat1-2320099-large.gif
2014,6805162,Fig. 2.,Example of non-linear support vector machines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat2-2320099-large.gif
2014,6805162,Fig. 3.,Simple 2D visualization of the principal component analysis algorithm. It is important to note that the potential of the PCA algorithm is high mainly when dealing with high-dimensional data [62].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat3-2320099-large.gif
2014,6805162,Fig. 4.,Visualization of the Q-learning method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat4-2320099-large.gif
2014,6805162,Fig. 5.,"Example of a sensor network routing problem using a graph along with each path routing cost, traditional spanning tree routing, and the generated sub-problems using machine learning that require only local communication to achieve optimal routing (i.e., require only single-hop neighborhood information exchange). (a) Original graph. (b) Traditional routing. (c) Simplified problems using machine learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat5abc-2320099-large.gif
2014,6805162,Fig. 6.,"SOM construction of the SIR algorithm, where routing link is selected based on the multi-hop path QoS metrics (latency, throughput, error rate, and duty cycle) and the Dijkstra's algorithm [70].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat6-2320099-large.gif
2014,6805162,Fig. 7.,"Data aggregation example in a clustered architecture, where the nodes are marked as working, dead and cluster heads.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat7-2320099-large.gif
2014,6805162,Fig. 8.,"Event detection and query processing enhancement using machine learning methods by assessing event validity and delimiting queried areas. System controller initiates query that is spread by the query processing unit to intended nodes. In contrast, events are detected by nodes to monitor specific signs within the monitored area.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat8-2320099-large.gif
2014,6805162,Fig. 9.,Human activity recognition using the hidden Markov model and the naive Bayes classifier [92].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat9-2320099-large.gif
2014,6805162,Fig. 10.,Workflow of the query optimization and reduction system using PCA proposed in [93].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat10-2320099-large.gif
2014,6805162,Fig. 11.,Localization using few beacon nodes by utilizing machine learning algorithms and other signal strength indicators (reformulated from [96]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat11-2320099-large.gif
2014,6805162,Fig. 12.,Example of the Q-values of a node over three frames in a WSN that employs ALOHA-QIR to manage medium access [114].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat12-2320099-large.gif
2014,6805162,Fig. 13.,Decision tree classifier used to select the optimal MAC algorithm in the SAML architecture [115].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat13-2320099-large.gif
2014,6805162,Fig. 14.,Example of anomaly detection in phenomena monitoring sensor system using machine learning clustering and classification techniques (data set in Euclidean space).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat14-2320099-large.gif
2014,6805162,Fig. 15.,Example of task management using the DIRL middleware algorithm: Object tracking application [134].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat15-2320099-large.gif
2014,6805162,Fig. 16.,Hierarchical clustering of network's nodes based on data spatial and temporal correlations in a temperature monitoring system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/6963743/6805162/niyat16-2320099-large.gif
2014,6747343,Fig. 1.,Graphical representations of Bagging and RS. Left subgraph: Bagging; Right subgraph: RS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-1-source-large.gif
2014,6747343,Fig. 2.,Pseudo-code description for FSS generation algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-2-source-large.gif
2014,6747343,Fig. 3.,Schematic illustration of FSS generation algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-3-source-large.gif
2014,6747343,Fig. 4.,Pseudo-code for asBagging_FSS algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-4-source-large.gif
2014,6747343,Fig. 5.,Schematic illustration of asBagging_FSS algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-5-source-large.gif
2014,6747343,Fig. 6.,Performance comparison for asBagging_FSS based on different dimensions of feature subspace. 1st column: Colon data set; 2 nd column: Lung data set; 3rd column: Ovarian I data set; 4th column: Ovarian II data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-6-source-large.gif
2014,6747343,Fig. 7.,"Comparison of average diversity for asBagging_FS, asBagging_FSS and asBagging_RS ensemble learning methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-7-source-large.gif
2014,6747343,Fig. 8.,"Comparison of average accuracy for asBagging_FS, asBagging_FSS and asBagging_RS ensemble learning methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6747343/6747343-fig-8-source-large.gif
2014,6754133,Fig. 1.,Architecture of a SOM neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz1-2305516-large.gif
2014,6754133,Fig. 2.,Flowchart of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz2-2305516-large.gif
2014,6754133,Fig. 3.,Linearly separable toy data set in a 2-D feature space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz3-2305516-large.gif
2014,6754133,Fig. 4.,Color image used in the second experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz4-2305516-large.gif
2014,6754133,Fig. 5.,Multispectral image used in our experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz5-2305516-large.gif
2014,6754133,Fig. 6.,Hyperspectral image used in our experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz6-2305516-large.gif
2014,6754133,Fig. 7.,"SOM at the convergence of the network training phase. (a) Distances between neighbor neurons arranged in the hexagonal lattice. The blue hexagons represent the neurons, and the red lines connect neighboring neurons. The colors in the regions containing the red lines indicate the distances between neurons. The darker colors represent larger distances, and the lighter colors represent smaller distances. (b) Positions of the weight vectors associated with the neurons in the lattice (toy data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz7ab-2305516-large.gif
2014,6754133,Fig. 8.,"Average classification accuracy over ten runs versus the number of training samples provided by the MS-proposed, MCLU-proposed, CAHT, MCLU-ECBD, MS-cSV, EQB, and RS methods (multispectral data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz8-2305516-large.gif
2014,6754133,Fig. 9.,"Average classification accuracy over ten runs versus the number of training samples provided by the MS-proposed, MCLU-proposed, CAHT, MCLU-ECBD, MS-cSV, EQB, and RS methods (hyperspectral data set).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz9-2305516-large.gif
2014,6754133,Fig. 10.,"Average classification accuracy provided by the MCLU-proposed technique with different values of
h
1
for the (a) multispectral and (b) hyperspectral data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/6823767/6754133/bruzz10ab-2305516-large.gif
2014,6654166,Fig. 1.,First row shows the surface graphs that demonstrate the influence of different kernel combination weights on the mean average precision score for three different classes. Four examples from each class are given in the second row.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak1-212-large.gif
2014,6654166,TABLE 1,Comparison of MKL Baselines and Simple Baselines (“Single” for Single Best Performing Kernel and “AVG” for the Average of All the Base Kernels) in Terms of Classification Accuracy,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t1-212-large.gif
2014,6654166,TABLE 2,Comparison of Computational Efficiency of MKL Methods,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t2-212-large.gif
2014,6654166,Fig. 2.,Summary of representative MKL optimization schemes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak2-212-large.gif
2014,6654166,TABLE 3,Description of the 48 Kernels Built for the Caltech-101 Dataset,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t3-212-large.gif
2014,6654166,TABLE 4,Classification Results (MAP) for the Caltech 101 Dataset,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t4-212-large.gif
2014,6654166,Fig. 3.,"Mean average precision (MAP) scores of different
L
1
-MKL methods vs number of iterations. (a) Caltech 101 - class 3. (b) Caltech 101 - class 10. (c) Caltech 101 - class 15.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak3abc-212-large.gif
2014,6654166,TABLE 5,Classification Results (MAP) for the VOC 2007 Dataset,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t5-212-large.gif
2014,6654166,TABLE 6,Comparison with the State-of-the-art Performance for Object Classification on Caltech 101 (Measured by Classification Accuracy) and VOC 2007 Datasets (Measured by MAP),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t6-212-large.gif
2014,6654166,Fig. 4.,Change in MAP score with respect to the number of base kernels for the Caltech 101 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak4-212-large.gif
2014,6654166,Fig. 5.,Change in MAP score with respect to the number of base kernels for the VOC 2007 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak5-212-large.gif
2014,6654166,TABLE 7,"Total Training Time (Secs), Number of Iterations, and Total Time Spent on Combining the Base Kernels (Secs) for Different MKL Algorithms vs Number of Training Examples for the Caltech 101(Left) and VOC 2007 Datasets (Right)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t7ab-212-large.gif
2014,6654166,TABLE 8,"Total Training Time (Secs), Number of Iterations, and Total Time Spent on Combining the Base Kernels (Secs) for Different MKL Algorithms vs Number of Base Kernels for the Caltech 101 (Left) Dataset and VOC 2007 (Right) Datasets",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t8ab-212-large.gif
2014,6654166,TABLE 9,Comparison of Training Time Between MKL-SMO and MKL-SIP,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak.t9-212-large.gif
2014,6654166,Fig. 6.,Number of active kernels learned by the MKL-SIP algorithm vs number of iterations for the Caltech 101 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak6-212-large.gif
2014,6654166,Fig. 7.,Number of active kernels learned by the MKL-SIP algorithm vs number of iterations for the VOC 2007 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak7-212-large.gif
2014,6654166,Fig. 8.,Classification performance for different training set sizes for the ImageNet dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak8-212-large.gif
2014,6654166,Fig. 9.,"Training times for
L
1
-MKL and
L
2
-MKL on different training set sizes for the ImageNet dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6840869/6654166/bucak9-212-large.gif
2014,6817586,Fig. 1.,"Illustration of the protein function prediction task with exact matches for Multi-Instance Multi-Label (MIML) learning frameworks. The protein “type 1 Insulin-like growth factor receptor (PDB ID: 1IGR)” is presented as the illustration where contains four domains (instances) highlighted with dashed oval and six GO Ontology molecular function terms (labels). An object (protein) has many alternative input instances (domains) and output labels (GO molecular function terms), and MIML considers the ambiguity in the input and output spaces simultaneously.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817586/wu1-2323058-large.gif
2014,6817586,Fig. 2.,Architecture of the MIMLNN algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817586/wu2-2323058-large.gif
2014,6817586,Fig. 3.,The performance of EnMIMLNN{metric} on all seven datasets under different values of scaling factor μ when the fraction parameter α is fixed to 0.1 and different values of the fraction parameter α when the scaling factor μ is fixed to 0.8. The performance of EnMIMLNN{metric} reaches the perk in most cases by setting the scaling factor μ to 0.8 and the fraction parameter α to 0.1.GS: Geobacter sulfurreducens; AV: Azotobacter vinelandii; HM: Haloarcula marismortui; PF: Pyrococcus furiosus; SC: Saccharomyces cerevisiae; CE: Caenorhabditis elegans; DM: Drosophila melanogaster.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817586/wu3-2323058-large.gif
2014,6990727,FIGURE 1.,"Schematic representation of the development and application of the SP machine. Reproduced from Figure 2 in [20], with permission.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff1-2382753-large.gif
2014,6990727,FIGURE 2.,"The best multiple alignment created by the SP model, with a set of New patterns (in column 0) that describe some features of an unknown plant, and a set of Old patterns, including those shown in columns 1 to 6, that describe different categories of plant, with their parts and sub-parts, and other attributes. Reproduced from [20, Fig. 16], with permission.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff2-2382753-large.gif
2014,6990727,FIGURE 3.,"Schematic representation of inter-connections amongst pattern assemblies as described in the text. Not shown in the figure are lateral connections within each pattern assembly, and inhibitory connections. Reproduced from [18, Fig. 11.2], with permission.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff3-2382753-large.gif
2014,6990727,FIGURE 4.,"The best multiple alignment created by the SP model with a store of Old patterns like those in rows 1 to 8 (representing grammatical structures, including words) and a New pattern (representing a sentence to be parsed) shown in row 0. Reproduced from [23. Fig. 1], with permission.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff4-2382753-large.gif
2014,6990727,FIGURE 5.,"A simple multiple alignment from which patterns may be derived. Reproduced from [18, Fig. 9.2], with permission.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff5-2382753-large.gif
2014,6990727,FIGURE 6.,"Patterns derived from the multiple alignment shown in Figure 5. Reproduced from [18, Fig. 9.3], with permission.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff6-2382753-large.gif
2014,6990727,FIGURE 7.,"Plan view of a 3D object, with each of the five lines around it representing a view of the object, as seen from the side. Reproduced from [21, Fig. 11], with permission",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff7-2382753-large.gif
2014,6990727,FIGURE 8.,A multiple alignment produced by the SP computer model showing how two instances of the pattern ‘I N F O R M A T I O N’ may be detected despite the interpolation of non-matching symbols throughout both instances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff8-2382753-large.gif
2014,6990727,FIGURE 9.,An example of a 2D SP pattern showing how it may be used to represent parallel streams of information. Each row (S1 to S5) represents a stream of information and each column is a time slot. Each numbered circle is an SP symbol.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6990727/wolff9-2382753-large.gif
2014,6680633,Fig. 1.,Three-stage algorithm for grading DR severity using fundus images. The system flow depicts the functionalities of the three individual stages and their interconnections.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6889048/6680633/6680633-fig-1-source-large.gif
2014,6680633,Fig. 2.,"Original blurry images are enhanced by spatial filtering. (a) and (c) Fundus image (
I
), (b) and (d) Filtered image with enhanced red lesions marked in red circles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6889048/6680633/6680633-fig-2-source-large.gif
2014,6680633,Fig. 3.,"Detection of OD, vasculature, and foreground candidate regions. (a) Illumination-corrected preprocessed image. (b) Intersecting red regions and bright regions (blue), out of which OD is selected as the bright region with highest solidity. (c) OD (
R
OD
) detected. (d) vasculature (
R
vasc
) detected. (e) Candidate bright lesion regions (
R
BL
) detected. (f) Candidate red lesion regions (
R
RL
) detected.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6889048/6680633/6680633-fig-3-source-large.gif
2014,6680633,Fig. 4.,"Two-step hierarchical classification of lesions for an image from DIARETDB1. In Stage 1 of the automated system, background regions corresponding to vasculature and optic disc are first detected. Candidate regions for bright (
R
BL
) and red lesions (
R
RL
) are then detected as the foreground. In Stage 2, hierarchical two-step classification is performed for identification of the type of lesions. In the first hierarchical classification step, foreground regions for bright lesions are classified as true lesions (red,
R
TBL
) and nonlesions (blue,
R
NBL
), and candidate regions for red lesions are classified as true red lesions (red,
R
TRL
) and nonlesions (blue,
R
NRL
). In hierarchical second-step classification, true bright lesions are classified into hard exudates (yellow,
R
HE
^
), and cotton wool spots (pink,
R
CWS
^
), while true red lesions are classified as microaneurysms (red,
R
MA
^
), and hemorrhages (black,
R
HA
^
). Corresponding to the 30 features mentioned in Table I, the average feature values for all the candidate lesion regions in the sample image is presented in the adjoining table (i.e.,
f
1
corresponds to feature with rank 1, which is area of the region). The features measuring distance are in terms of pixels, while the mean and variance of intensity values are scaled in [0, 1] range.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6889048/6680633/6680633-fig-4-source-large.gif
2014,6680633,Fig. 5.,DREAM system on public domain images with severe retinal degeneration. (a) and (d) Original image. (b) and (e) OD region detected with error. (c) and (f) Vasculature detected in first stage. Both images (a) and (d) are classified as images with DR by the DREAM system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/6889048/6680633/6680633-fig-5-source-large.gif
2014,6727483,Fig. 1.,Illustrations of a part of (a) the pairwise graph (b) and the triplet graph built on superpixels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-1-source-large.gif
2014,6727483,Fig. 2.,Example of segmentation result by PW-CC. (a) Original image. (b) Ground-truth. (c) Superpixels. (d) Segments obtained by PW-CC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-2-source-large.gif
2014,6727483,Fig. 3.,Hypergraph construction from multiple partitionings. (a) Multiple partitionings from baseline superpixels. (b) Hyperedge (yellow line) corresponding to a region in the second layer. (c) Hyperedge (yellow line) corresponding to a region in the third layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-3-source-large.gif
2014,6727483,Fig. 4.,Obtained evaluation measures from segmentation results on the SBD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-4-source-large.gif
2014,6727483,Fig. 5.,Results of image segmentation on the SBD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-5-source-large.gif
2014,6727483,Fig. 6.,Obtained evaluation measures from segmentation results according to the different set of features on the SBD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-6-source-large.gif
2014,6727483,Fig. 7.,Examples of partitionings by multiple human subjects and single probabilistic (real-valued) ground-truth partitioning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-7-source-large.gif
2014,6727483,Fig. 8.,Boundary precision-recall curve on the BSDS300 test set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-8-source-large.gif
2014,6727483,Fig. 9.,"Obtained evaluation measures from segmentation results of gPb-owt-ucm, PW-CC, and HO-CC on the BSDS300 test set according to the average number of regions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-9-source-large.gif
2014,6727483,Fig. 10.,Boundary precision-recall curve on the BSDS500 test set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-10-source-large.gif
2014,6727483,Fig. 11.,Results of image segmentation on the BSDS test set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-11-source-large.gif
2014,6727483,Fig. 12.,Results of image segmentation on the MSRC test set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6868318/6727483/6727483-fig-12-source-large.gif
2014,6819854,Fig. 1.,Tactile Learning using OIESGP Online Learning Experts. Both our discriminative and generative learners work directly on temporal data and are capable of creating new spatio-temporal experts “on-the-fly” as new objects are taught and refining models of familiar objects. A probability distribution over object classes/clusters is maintained and updated throughout the grasping action.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh1-2326159-large.gif
2014,6819854,Fig. 2.,"iCub with Objects: Plastic bottles (full, half-full, empty), Soda cans (empty, half-full), Teddy-bear, Monkey soft-toy, Lotion bottle and Hardcover Book.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh2-2326159-large.gif
2014,6819854,Fig. 3.,"Sample tactile sensor spatio-temporal blocks for the each of the 10 classes where each object generates an individual “signature”. In each plot, the left horizontal axis represents time and each vertical 5
×
5 slice represents the 25 tactile features (mean, standard deviation, skewness, maximum and minimum for each finger). Note that these the features change across time (horizontally-stacked slices) as the fingers press on each object. Our classifiers are based on the notion that each of these signatures can be learned and represented by an OIESGP expert model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh3-2326159-large.gif
2014,6819854,Fig. 4.,"The iCub Hand has 19 joints and 9 DoFs; this induces a coupling between certain joints as indicated by the colour codes and labelings. In our experiments, we use the the tactile (capacitive) sensors on each fingertip. Each fingertip incorporates a flexible printed circuit board underneath an electrically conductive silicone and provides 12 pressure measurements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh4-2326159-large.gif
2014,6819854,Fig. 5.,"The Grasping controller would first initialise the iCub hand into a “pre-shape” as shown in Fig. 5a. It would then close the fingers onto the object via the motion depicted in Fig. 5b, pressing onto the object to obtain tactile sensory input.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh5-2326159-large.gif
2014,6819854,Fig. 6.,"Classification accuracy as trials progressed. To summarise and visualise the results, we grouped each subsequent set of 19 objects (a “trial set”) and computed the accuracy scores over this set. The generative line plot is offset by 0.5 for improved visualisation. Both the discriminative and generative models improved classification performance as the trials progressed. The best results were obtained when both tactile and encoder features were used; by the fourth trial set, both methods achieved perfect accuracy. However, using only the tactile features also resulted in strong performance, particularly with the generative classifier. The models trained only on the encoder features attained comparatively poor performance, suggesting that the tactile data was more informative for object identification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh6-2326159-large.gif
2014,6819854,Fig. 7.,"Confusion matrices for the online tactile classification experiment across all samples using different feature sets (All, Tactile only or Encoder only). The object labels from one to ten are (1) Empty Plastic Bottle, (2) Half-Full Plastic Bottle, (3) Full Plastic Bottle, (4) Empty Soda Can, (5) Half-Full Soda Can, (6) Teddy-bear, (7) Monkey Soft-toy, (8) Book, (9) Lotion Bottle and (10) None (no object). The best results were obtained when using both encoder and tactile signals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh7-2326159-large.gif
2014,6819854,Fig. 8.,"Classification performance under low-noise (
σ
2
n
=100
) and high-noise (
σ
2
n
=1,000
) conditions. A fall in accuracy was observed (as expected), but performance remained superior to the incremental SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh8-2326159-large.gif
2014,6819854,Fig. 9.,Median classification accuracies at different stages of the pressing motion. High accuracies are achievable even before the motion is complete—perfect identifications are made using only the first 20 and 30 percent of the sequence for the discriminative and generative classifiers respectively.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh9-2326159-large.gif
2014,6819854,Fig. 10.,"Human online classification experiment with the three plastic bottles (of varying fullness). Participants were provided true-label feedback after each sample, giving them the opportunity to learn from mistakes. (b) Human Classification Accuracy as trials progressed (with medians shown in the boxes) show an improvement in accuracy between trials 1-10 and 11-20.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh10-2326159-large.gif
2014,6819854,Fig. 11.,"Purity and ARI scores fell as the novelty threshold was increased. The NMI penalises the large number of effective clusters spawned by the discriminative model and hence, we observe a rise from approximately
0.5
to 0.9 (at
γ
D
=0.3
), followed by a fall back to the initial score.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh11-2326159-large.gif
2014,6819854,Fig. 12.,The average number of clusters and effective clusters generated decreased as the novelty threshold was increased. Note that the relationship is non-linear. The discriminative model kept active the same number of clusters that it generated—200 when the threshold was at 0.1 and only one at 0.95. The generative model generated many initial clusters (up to 50 at low novelty thresholds) but kept effective clusters a much smaller number (only up to 18).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh12-2326159-large.gif
2014,6819854,Fig. 13.,"NMI scores for all four methods increased with the number of effective clusters, up to 15 clusters. After the respective maxima, NMI fell slowly (within the window of 20 clusters). Both the generative and discriminative model achieved higher NMI scores relative to sequential k-means and SOM (
p<
10
−4
from 11 effective clusters onwards).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4543165/6983644/6819854/soh13-2326159-large.gif
2014,6746640,Fig. 1.,Structure of the probabilistic neural network model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6746640/ferna1-2304976-large.gif
2014,6746640,Fig. 2.,Structure of the probabilistic neural network model proposed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6746640/ferna2-2304976-large.gif
2014,6746640,Fig. 3.,NNORELM framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6746640/ferna3-2304976-large.gif
2014,6746640,Fig. 4.,"Ranking test diagrams for the mean generalization
MZE
G
and
MAE
G
(α=0.10)
. (a) Nemenyi CD diagram comparing the generalization MZE mean rankings of the different methods. (b) Nemenyi CD diagram comparing the generalization MAE mean rankings of the different methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6924821/6746640/ferna4ab-2304976-large.gif
2014,6628006,Fig. 1.,DeSTIN hierarchical architectures employed for visual sensory processing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young1-2283730-large.gif
2014,6628006,Fig. 2.,"(a) Bump circuit, which computes
tanh(
V
1
−
V
2
)
and its derivative simultaneously. (b) Floating-gate memory using feedback for improved update linearity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young2ab-2283730-large.gif
2014,6628006,Fig. 3.,"Block diagram of an analog clustering circuit.
o
i
refers to the dimension
i
of the input.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young3-2283730-large.gif
2014,6628006,Fig. 4.,"Simulated current mirror mismatch as a function of (top) bias current with area fixed at 1
μ
m
2
and (bottom) transistor area with current fixed at 1 nA. Mirror mismatch is plotted as the ratio of the standard deviation of a current signal to its mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young4-2283730-large.gif
2014,6628006,Fig. 5.,"Input gain. Gain errors can cause the input to form centroids in a skewed space. This figure demonstrates the effect of a gain error that is too large,
4x
, in the second dimension of each centroid versus a case with no gain error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young5-2283730-large.gif
2014,6628006,Fig. 6.,"Centroid offset. As the size of the increments relative to the decrements becomes larger, the centroid becomes more offset from the true center of the data. Depending on the nature of the data being clustered over, the amount of error that is acceptable may vary. It is also important to remember that extremely accurate clustering is not needed, and even with fairly inaccurate clustering meaningful beliefs can be calculated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young6-2283730-large.gif
2014,6628006,Fig. 7.,"Memory adaptation bias. This figure demonstrates what happens when one centroid has a bias error in the memory adaptation. In this figure, Centroid B has a bias error toward the top right of the plot. As it moves toward the other data cluster, it swaps positions with Centroid A before reaching a steady-state position offset from its data cluster by approximately the magnitude of the bias error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young7-2283730-large.gif
2014,6628006,Fig. 8.,"Translinear circuit for computing
x
2
/y
needed to obtained the Mahalanobis distance and belief calculation from (6) and (7).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young8-2283730-large.gif
2014,6628006,Fig. 9.,Illustration of a distance calculation error within the clustering process.. Centroid B has an error causing it to appear artificially more distant from any input. This results in Centroid A to claiming some observations that should belong to Centroid B.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young9-2283730-large.gif
2014,6628006,Fig. 10.,Modeling of the current noises in a transistor as a function of bias current. (a) Shot noise. (b) Flicker noise. (c) Total integrated RMS noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young10abc-2283730-large.gif
2014,6628006,Fig. 11.,SNR as a function of bias current in the analog floating-gate memory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young11-2283730-large.gif
2014,6628006,Fig. 12.,(a) Clean and (b) noisy synthetic clustering data used for evaluation of analog computation inaccuracies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young12ab-2283730-large.gif
2014,6628006,Fig. 13.,"Accuracy versus level of error
(σ)
. Gain errors on clean dataset (top) and noisy dataset (bottom). This figure illustrates that the update and input errors have the lowest impact on performance, while noise has the most significant impact. In addition to the individual effects of each noise, this figure includes the effect of all the error sources combined and all the error sources and noise combined.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young13-2283730-large.gif
2014,6628006,Fig. 14.,"Accuracy versus level of error
(σ)
. Bias errors on clean dataset (top) and noisy dataset (bottom). The update and input errors have the lowest impact, while the remaining error components have an impact similar to that of the additive noise of the same level. In addition to the individual effects of each noise, this figure includes the effect of all the error sources combined and all the error sources and noise combined.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young14-2283730-large.gif
2014,6628006,Fig. 15.,"MNIST performance versus system error/noise level. The results here demonstrate the effect of different error/noise levels when the analog error and noise sources are modeled in different ways. The best results here are 97.6%. However, when using a decorrelated ensemble of classifiers and using elastic distortions [34], results of 99% have been achieved with the DeSTIN architecture, which is more comparable to state of the art. These methods were not used to produce this plot, however, in order to produce results for such a wide range of variables in a timely manner.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young15-2283730-large.gif
2014,6628006,Fig. 16.,MNIST performance versus error level. Providing the classifier with belief states from all layers is always better than only providing the belief states from the bottom layer for every error/noise level. These results are created with all errors modeled as gain errors and with additive noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6786374/6628006/young16-2283730-large.gif
2014,6547983,Fig. 1.,System structure diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/6740850/6547983/6547983-fig-1-source-large.gif
2014,6824789,Fig. 1.,"Proposed framework for domain adaptation of the SVM-based deformable part-based model . The figure shows the adaptation of a DPM-based pedestrian detector from a virtual-world source domain to a real-world target domain. As DA module we propose an adaptive structural SVM (A-SSVM) and a structure-aware A-SSVM, see Section 4 . A-SSVM and SA-SSVM require target-domain labeled samples (e.g., a few pedestrians and background) that can be provided by a human oracle. Alternatively, we propose a strategy inspired by self-paced learning and supported by a Gaussian Process Regression for the automatic labeling of samples in unlabeled or weakly labeled target domains. The combination of SPL/GPR with either A-SSVM or SA-SSVM gives rise to our self-adaptive DPM (see Section 5).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos1-2327973-large.gif
2014,6824789,Fig. 2.,Domain adaptation for DPM: Structure-aware adaptive structural SVM (SA-SSVM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos2-2327973-large.gif
2014,6824789,Fig. 3.,"Sample selection by GPR (see main text for details). The horizontal axis runs on the sample features projected to 1D for visualization. The triangles are re-scored values from the diamonds, with vertical segments indicating the
±3
σ
∗,i
variance range. The solid horizontal line draws the threshold
r
¯
¯
and the dashed one
r
¯
¯
+θ
. An uncertain sample is selected if its variance range is over
r
¯
¯
+θ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos3-2327973-large.gif
2014,6824789,Fig. 4.,Results of adapting PASCAL VOC 2007 DPM person detector to work on the INRIA pedestrian data set. Percentages correspond to the average miss rate within the plotted FPPI range. Vertical segments illustrate the variance over five runs per experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos4-2327973-large.gif
2014,6824789,Fig. 5.,Supervised adaptation of DPM from virtual world to specific real-world scenes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos5-2327973-large.gif
2014,6824789,Fig. 6.,Self-adaptive DPM from virtual world to specific real-world scenes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos6-2327973-large.gif
2014,6824789,Fig. 7.,Sample selection in U-SA-SSVM-GPR. See Section 6.4.2 for a complete explanation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6824789/ramos7-2327973-large.gif
2014,6587612,Fig. 1.,"Surface EMG signals measured from (a) an ALS subject and (b) a control subject. For each subject, a 1-s analysis epoch during a voluntary contraction is shown with six representative channels selected from the high-density electrode array. The area values of two corresponding channels from above to below between (a) and (b) are approximately the same. The values of clustering index (CI), kurtosis of signal sample distribution
(
K
A
)
and kurtosis of crossing rate expansion
(
K
CR
)
are also presented on the right side of each channel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6704338/6587612/6587612-fig-1-source-large.gif
2014,6587612,Fig. 2.,CI-area plot (left panel) and the corresponding Z-scores (right panel) for the two subject groups: ALS and healthy controls. The healthy control group can also be divided into the younger and age-matched sub-groups. The reference range (dotted line) is presented within ±2.5 of the standard error (SE) of the linear regression for the reference cloud in the left-panel and within ±2.5 of Z-scores in the right panel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6704338/6587612/6587612-fig-2-source-large.gif
2014,6587612,Fig. 3.,"The
K
A
-area plot (left panel) and the corresponding Z-scores (right panel) for the two subject groups: ALS and healthy controls. The healthy control group can also be divided into the younger and age-matched sub-groups. The reference range (dotted line) is presented within ±2.5 of the standard error (SE) of the linear regression for the reference cloud in the left-panel and within ±2.5 of Z-scores in the right panel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6704338/6587612/6587612-fig-3-source-large.gif
2014,6587612,Fig. 4.,"The
K
CR
-area plot (left panel) and the corresponding Z-scores (right panel) for the two subject groups: ALS and healthy controls. The healthy control group can also be divided into the younger and age-matched sub-groups. The reference range (dotted line) is presented within ±2.5 of the standard error (SE) of the linear regression for the reference cloud in the left-panel and within ±2.5 of Z-scores in the right panel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6704338/6587612/6587612-fig-4-source-large.gif
2014,6587612,Fig. 5.,LDA-transformed features for all subjects and the corresponding decision boundary during a repetition of cross-validation when one ALS subject (marked by a downward arrow) was not correctly identified.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/6704338/6587612/6587612-fig-5-source-large.gif
2014,6809169,Fig. 1.,"Open set recognition must address both the known and unknown classes that might occur in the real world. For instance, considering OCR for a parcel delivery application, classifiers must recognize known characters (e.g., “3”) and reject an unspecifiable variety of other input including symbols, marks, stickers, and photos that can appear on a package. Standard statistical learning, using any mixture of discriminative and generative models, does not address unknown classes. The goal of this work is to approach the problem of multi-class open set recognition by limiting open space risk using labeled training sets of finite measure. The compact abating probability model we introduce bounds probability estimates of feature space that decay away from the training data. By truncating the abating probability, CAP models provably reduce open space risk. In addition, probability estimates from a calibrated binary kernel machine can have even better estimates than the CAP bounds, further reducing the open set risk. By taking advantage of the CAP model and the statistical extreme value theory for probability estimation, the novel W-SVM provides solutions for non-linear multi-class classification in an open set scenario. In the multi-step example above, classes marked in red indicate rejection, and those marked in green indicate acceptance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6914638/6809169/schei1-2321392-large.gif
2014,6809169,Fig. 2.,"Performance on an open set binary object detection task for an open universe of 88 classes [27]. Results are calculated over a five-fold cross-data set style test with images from Caltech 256 used for training and images from Caltech 256 and ImageNet for testing; error bars reflect standard deviation. The W-SVM significantly outperforms the prior state-of-the-art (1-vs-Set Machine), with a 20-26 percent improvement in F-measure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6914638/6809169/schei2-2321392-large.gif
2014,6809169,Fig. 3.,"F-measure for multi-class open set recognition on OLETTER. As openness increases, the W-SVM has the best performance. Common multi-class SVMs, probabilistic multi-class SVMs with a rejection option, logistic regression, and nearest neighbor degrade quickly (1-vs-All Mult. RBF, Pairwise Mult. RBF, and MAS are all comparable and visually overlap). The MAS and NN algorithms with a CAP model do a bit better than their baselines, but still degrade much more than the W-SVM. Error bars reflect standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6914638/6809169/schei3-2321392-large.gif
2014,6809169,Fig. 4.,"F-measure for multi-class open set recognition on OMNIST. W-SVM again maintains high F-measure scores as the problem grows to be more open, but common multi-class SVMs and existing thresholded probability estimators degrade quickly. NN+CAP and MAS+CAP are again better than their baselines. All algorithms except the W-SVM, NN+CAP, MAS+CAP, and Logistic Regression are comparable and visually overlap. Error bars reflect standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6914638/6809169/schei4-2321392-large.gif
2014,6778011,Fig. 1.,"(a) Cumulative distributions of margins, and (b) Training error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6778818/6778011/6778011-fig-1-source-large.gif
2014,6778011,Fig. 2.,"Weight updating for sample
x
i
in Real AdaBoost.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6778818/6778011/6778011-fig-2-source-large.gif
2014,6778011,Fig. 3.,"Weight updating for sample
x
i
in Parameterized AdaBoost.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6778818/6778011/6778011-fig-3-source-large.gif
2014,6778011,Fig. 4.,"(a)–(c) Cumulative distributions of margins, and (d) Training error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6778818/6778011/6778011-fig-4-source-large.gif
2014,6778011,Fig. 5.,"(a) Training error of Diabetes, and (b) Test error of Diabetes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6778818/6778011/6778011-fig-5-source-large.gif
2014,6515597,Fig. 1.,Overview of our proposed approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-1-source-large.gif
2014,6515597,Fig. 2.,Converge curve of objective function. (a) Musk1 dataset. (b) Musk2 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-2-source-large.gif
2014,6515597,Fig. 3.,Images randomly sampled from 10 categories and the corresponding segmentation results. Segmented regions are shown in their representative colors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-3-source-large.gif
2014,6515597,Fig. 4.,Sensitivity to labeling noise (Musk dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-4-source-large.gif
2014,6515597,Fig. 5.,Sensitivity to labeling noise (Corel dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-5-source-large.gif
2014,6515597,Fig. 6.,Sensitivity to labeling noise (20 Newsgroup dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-6-source-large.gif
2014,6515597,Fig. 7.,Results of multiclass classification on the SIVAL dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6766657/6515597/6515597-fig-7-source-large.gif
2014,6695785,Fig. 1.,"(a)–(d) Two class synthetic data sets with four ML constraints and four CL constraints, with the group assignments of samples obtained using the methods in [17] (with single component per class), [11], and our method. The true class labels of samples are shown in (a) with different symbols and colors. The ML constraints are shown with a solid line and the CL constraints are shown with a dotted line. The samples involved in constraints are shown with larger symbols for clarity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu1abcd-2294459-large.gif
2014,6695785,Fig. 2.,Plots of synthetically generated data used in experiments. Points from different classes are denoted by different colors and symbols. (a) Dataset 1. (b) Dataset 2. (c) Dataset 3. (d) Dataset 4. (e) Dataset 5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu2abcde-2294459-large.gif
2014,6695785,Fig. 3.,"Solutions learned by the MCP and MCGMM methods on the data set in Fig. 2(e), for a particular set of constraints. The shaded regions show the learned class boundaries, and the samples are shown with their true class labels using different colors and symbols. For MCGMM, the class membership discontinuities at the location of the constrained samples are not shown. (a) Solution learned by the MCP method. (b) Solution learned by the MCGMM method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu3ab-2294459-large.gif
2014,6695785,Fig. 4.,Average F-score and average NMI versus number of constraints for all methods on the synthetically generated data sets shown in Fig. 2. (a) Dataset 1. (b) Dataset 2. (c) Dataset 3. (d) Dataset 4. (e) Dataset 5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu4abcde-2294459-large.gif
2014,6695785,Fig. 5.,Average F-score and average NMI versus number of constraints for all methods on the UC Irvine and the texture image segmentation data sets. (a) Balance scale. (b) Image segmentation. (c) Cardiotocography. (d) Ionosphere. (e) Wall following robot. (f) Vehicle silhouettes. Average F-score and average NMI versus number of constraints for all methods on the UC Irvine and the texture image segmentation data sets. (g) Breast cancer. (h) Pen-based recognition. (i) Waveform database. (j) Pima indians. (k) Texture image segmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu5abcdefghijk-2294459-large.gif
2014,6695785,Fig. 6.,"Example illustrating the effect of class overlap on the solutions learned by the MCP and MCGMM methods on a data set with three classes, randomly generated from a Gaussian mixture. (a) True class labels of samples (markers with different symbols and colors), MLs (solid lines), and CLs (disconnected lines). (b) and (c) Class boundaries learned by MCP and MCGMM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu6abc-2294459-large.gif
2014,6695785,Fig. 7.,"Plot of the average F-score and average NMI of the MCP method as a function of the number of mixture components for the UCI data sets. 1) Ionosphere. 2) Vehicle silhouettes. 3) Pen-based recognition (from left to right) for a constraint set size 10%. The average number of components selected by the method described in Section II-C-V
K
1
, and the average number of components at the BIC minimum
K
2
are also shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6695785/raghu7-2294459-large.gif
2014,6744583,Fig. 1.,"Conceptual diagram for the timeline of light stimuli in the LLR assay. The larvae were dark-adapted in the Zebrabox for 3.5 hours immediately before the experiment, then the assay lasted 3 hours with light stimuli: ON-OFF-ON-OFF-ON-OFF. Each stimulus was sustained for 30 min.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6744583/6744583-fig-1-source-large.gif
2014,6744583,Fig. 2.,Illustration of key features used.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6744583/6744583-fig-2-source-large.gif
2014,6744583,Fig. 3.,Conceptual illustration of data extracted from each individual zebrafish for different classification schemes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6744583/6744583-fig-3-source-large.gif
2014,6744583,Fig. 4.,"Classification results of 8 dpf zebrafish larvae using 30-min LLR. Activities of both BT = 0 and BT = 4 pixel were analyzed. We used four different classification methods, as bars from left to right: 3 Nearest Neighbor (3NN), Naive Bayes (NB), Support Vector Machine (SVM) and Expectation-Maximization algorithm with Gaussian Mixture Model (EM). In addition, 4 different classification problems were tested, in which W1, W2, P2 denoted WT-#1, WT-#2, pde6c-#2, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6744583/6744583-fig-4-source-large.gif
2014,6744583,Fig. 5.,"SVM classification results using both Baseline and LLR data of 5 dpf (Top Row) and 8 dpf (Bottom Row) zebrafish larvae. For classification using baseline data, the activities recorded in the last 30 min of the initial 3.5-hour dark period before the first light-ON stimulus was used as input. While for LLR classification, three different amounts of data were used, i.e., first 1-min, first 2-min and all 30-min LLR data. Activities of both BT = 0 and BT = 4 pixel were analyzed. Four different classification problems were tested, as the four subfigure columns. The other parameters are the same with Fig. 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6744583/6744583-fig-5-source-large.gif
2014,6744583,Fig. 6.,"Light-OFF activity profiles of inliers and outliers of 8 dpf zebrafish WT-#2 versus pde6c-#2, where BT was 0 and behavioral data length was 30 min. The activity was measured by Burst Duration, which has been defined in Section 2. The light-OFF stimulus was given at time 0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6873374/6744583/6744583-fig-6-source-large.gif
2014,6544206,Fig. 1.,Flow chart of the proposed active learning approach for large-scale classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6658878/6544206/6544206-fig-1-source-large.gif
2014,6544206,Fig. 2.,"False-color composite (RGB: bands 1, 2, 3) of the MODIS image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6658878/6544206/6544206-fig-2-source-large.gif
2014,6544206,Fig. 3.,OA (%) achieved on the (a) target 1 and (b) target 2 domains as a function of the number of added target samples and averaged over ten runs of the algorithm. Shaded areas: standard deviation of the OA over the ten considered runs. R: random sampling. MS: margin sampling. MS(Ran): MS with random initialization. MS(Clu): MS(Clup). MS(Clud): MS with clustering-based initialization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6658878/6544206/6544206-fig-3-source-large.gif
2014,6544206,Fig. 4.,Classification maps at convergence. R: random sampling. MS: margin sampling. MS(Clu): MS with clustering-based initialization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6658878/6544206/6544206-fig-4-source-large.gif
2014,6748917,Fig. 1.,Scheme of traditional sliding window approach with linear classifier for detecting objects in video.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan1-2301453-large.gif
2014,6748917,Fig. 2.,Framework of DOD in video.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan2-2301453-large.gif
2014,6748917,Fig. 3.,Subweight vector imposes different weights on different dimensions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan3-2301453-large.gif
2014,6748917,Fig. 4.,Orientation quantization and magnitude decomposition. The orientation is quantized into nine bins.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan4-2301453-large.gif
2014,6748917,Fig. 5.,Distributed feature extraction of HOG features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan5-2301453-large.gif
2014,6748917,Fig. 6.,"Framework of CHOG-based DOD.
⊗
means inner-product operation and
⊕
means addition operation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan6-2301453-large.gif
2014,6748917,Fig. 7.,"Histogram-based feature vector is sensitive to shift. (a) The gradients of an edge. (b) A right-shift version of (a). The histograms of oriented gradients in cell 0 and cell 1 corresponding to (a) and (b) are shown in (c) and (d), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan7abcd-2301453-large.gif
2014,6748917,Fig. 8.,"Gradient smoothing makes the feature extraction shift-invariant. Here, smoothing along horizontal direction is conducted. (a) The gradients of an edge. (b) The smoothed result of (a). (c) The histograms of oriented gradients in cell 0 and cell 1 in (b). (d) The shift-version of (a). (e) The smoothed result of (d). (f) The histograms of oriented gradients in cell 0 and cell 1 in (e).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan8abcdef-2301453-large.gif
2014,6748917,Fig. 9.,"Overlapping block and even un-overlapping block is unnecessary. Here, smoothing along horizontal direction is conducted. (a) The gradients. (b) The smoothed gradients along horizontal direction. (c) The six cells are grouped by two overlapping blocks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan9abc-2301453-large.gif
2014,6748917,Fig. 10.,Normalized palm images for training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan10-2301453-large.gif
2014,6748917,Fig. 11.,Examples of successful hand detection results. Top row: images indoors. Bottom row: images outdoors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan11-2301453-large.gif
2014,6748917,Fig. 12.,Detection results on hand videos (a) indoors and (b) outdoors where SVM classifier is employed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan12ab-2301453-large.gif
2014,6748917,Fig. 13.,Detection results on hand videos (a) indoors and (b) outdoors where LDA classifier is employed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan13ab-2301453-large.gif
2014,6748917,Fig. 14.,Examples of successful face detection results. Top row: images indoors. Bottom row: images outdoors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan14-2301453-large.gif
2014,6748917,Fig. 15.,Detection results on face videos (a) indoors and (b) outdoors where SVM classifier is employed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan15ab-2301453-large.gif
2014,6748917,Fig. 16.,Detection results on face videos (a) indoors and (b) outdoors where LDA classifier is employed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan16ab-2301453-large.gif
2014,6748917,Fig. 17.,Examples of successful human detection results. Top row: images indoors. Bottom row: images outdoors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan17-2301453-large.gif
2014,6748917,Fig. 18.,Detection results on pedestrian videos (a) indoors and (b) outdoors where SVM classifier is employed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan18ab-2301453-large.gif
2014,6748917,Fig. 19.,Detection results on pedestrian videos (a) indoors and (b) outdoors where LDA classifier is employed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6748917/yuan19ab-2301453-large.gif
2014,6853357,Fig. 1.,"Six enzyme structures are shown, five of which correspond to a known EC number. The catalytic similarity
Q
is depicted on the edges of the graph. The algorithm that we present allows us to infer for the unannotated query (denoted as EC ?.?.?.?) a ranking of the annotated enzymes. To this end, the unsupervised approach solely uses cavity-based similarity measures, whereas the supervised approach also takes the EC numbers of annotated enzymes into account.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6980148/6853357/stock1-2338308-large.gif
2014,6853357,Fig. 2.,"(a) Heat maps of the values used for ranking the data set II for one fold in the testing phase. Each row of the heat map corresponds to one query. The four figures on top visualize the cavity-based similarities
K
ϕ
(v,
v
′
)
that are used to construct an unsupervised ranking. The four figures at the bottom visualize the model output
h(v,
v
′
)
, which is used to derive a supervised ranking. (b) The ground truth catalytic similarity that had to be learned.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6980148/6853357/stock2-2338308-large.gif
2014,6853357,Fig. 3.,"Box-and-whisker plots of the values used for ranking data set II for one randomly chosen query as an example. The different populations on the x-axis denote the groups that are formed by subdividing the database enzymes according to the number of EC number digits they share with the query. Given a query
v
and a database enzyme
v
′
, the y-axis shows the distribution of the values
h(v,
v
′
)
and
K
ϕ
(v,
v
′
)
for the supervised and the unsupervised approach, respectively. For nearly all cavity-based similarity measures, one can observe a much better separation of the groups for the supervised approach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6980148/6853357/stock3-2338308-large.gif
2014,6853357,Fig. 4.,Receiver operating characteristic curves for the unsupervised and supervised ranking methods for data set I. An enzyme is considered functionally similar to the query if the first three digits of the EC number are identical to those of the query.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6980148/6853357/stock4-2338308-large.gif
2014,6736057,Fig. 1.,"(a) Synopsis of the CALOR procedure (see Section III). The feedback process is done in a top-down manner: we start by building the classifier
C
L
at the
L
th level of the hierarchy. The set
T
L−1
of patches to be further processed consists of the patches at level
L−1
that intersect with positively classified patches of the set
T
L
. We repeat this procedure until we reach the lowest level of the hierarchy. The classifier
C
l
is built using an interactive relevance feedback process. (b) Extraction of the set of overlapping patches, for each level
l
of the hierarchy, using a window that slides incrementally at each step by half its size, horizontally or vertically.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-1-source-large.gif
2014,6736057,Fig. 2.,"Patches of size 50 pixels (right) intersecting with a patch of size 100 pixels (left). Only some of them contains the target object (“roundabouts”), those marked with a green plus sign. This example illustrates the difficulty of propagating training samples from one level of the hierarchy to the level below.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-2-source-large.gif
2014,6736057,Fig. 3.,"(a) Minimum enclosing rectangle. (b) Extracting the neighborhood of size
t
l−1
from a user-tagged patch of size
t
l
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-3-source-large.gif
2014,6736057,Fig. 4.,"Examples of elements of the 10 target classes used for the experiments. First row, from left to right: roundabouts, storehouses, tall buildings, marina, and moving boats. Second row, from left to right: gas holders, swimming pools, crossroads, planes, and baseball grounds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-4-source-large.gif
2014,6736057,Fig. 5.,"Amount of calculations (averaged over the 10 ground truth classes) at each iteration of the feedback loop. The scale is logarithmic on the
Y
-axis. The blue curve corresponds to our method, and the green curve corresponds to the
SVM
baseline
. For both methods, the amount of computation corresponds to the number of evaluations of the decision function of the current SVM classifier multiplied by the number of support vectors defining it. We see that, at each iteration of the feedback loop, the amount of computations generated by our method is approximately two orders of magnitude lower than the one generated by the baseline method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-5-source-large.gif
2014,6736057,Fig. 6.,"Precision as a function of the number of active learning iterations. The blue curves represent the results given by our method and the green curves the results given by
SVM
baseline
: (a) roundabouts, (b) storehouses, (c) tall buildings, (d) marina, (e) moving boats, (f) gas holders, (g) swimming pools, (h) crossroads, (i) planes, and (j) baseball grounds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-6-source-large.gif
2014,6736057,Fig. 7.,"Recall as a function of the number of active learning iterations. The blue curves represent the results given by our method and the green curves represent the results given by
SVM
baseline
: (a) roundabouts, (b) storehouses, (c) tall buildings, (d) marina, (e) moving boats, (f) gas holders, (g) swimming pools, (h) crossroads, (i) planes, and (j) baseball grounds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-7-source-large.gif
2014,6736057,Fig. 8.,"(a) Average recall as a function of the percentage of top-ranked elements we keep in the whole database for four different sizes of patches. The ranking function is the decision function of
SVM
baseline
after 10 iterations of feedback. (b) and (c) Average precision and recall as a function of the number of active learning iterations. The blue curves represent the results given by our method and the green curves represent the results given by
SVM
baseline
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-8-source-large.gif
2014,6736057,Fig. 9.,Google earth overlay of the test site: (a) ascending orbit and (b) descending orbit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-9-source-large.gif
2014,6736057,Fig. 10.,"Examples of six different temporal classes for indexing: (a) house, (b) flood, (c) flooded fields, (d) field, (e) beach, and (f) mountain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-10-source-large.gif
2014,6736057,Fig. 11.,"Precision curves as a function of the number of learning iterations for the six classes: (a) house, (b) flood, (c) flooded fields, (d) field, (e) beach, and (f) mountain. It is obvious that cascade active learning performs better in precision than SVM active learning. Despite the accuracy fluctuation in the first few iterations, cascade active learning converges quite fast after the first level.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-11-source-large.gif
2014,6736057,Fig. 12.,"Recall curves as a function of the number of learning iterations for the six classes: (a) house, (b) flood, (c) flooded fields, (d) field, (e) beach, and (f) mountain. Apparently, the recall of cascade active learning is better or the same in worst case. The jump in F-score occurs when moving to a new level because negative patches are dropped. If the discarded negative patches contained positive elements, the accuracy would decrease when moving to a new level.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-12-source-large.gif
2014,6736057,Fig. 13.,"Amount of calculations (averaged over the six ground truth classes) at each iteration of the feedback loop. The blue curve corresponds to our method and the green curve corresponds to the
SVM
baseline
. For both methods, the amount of computation corresponds to the number of evaluations of the decision function of the current SVM classifier multiplied by the number of support vectors defining it. We see that, at each iteration of the feedback loop, the amount of computations generated by our method is much smaller compared to the baseline.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6803891/6736057/6736057-fig-13-source-large.gif
2014,6702489,Fig. 1.,Level of experience against pragmatic information during ramp-up [1].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse1-2294155-large.gif
2014,6702489,Fig. 2.,Causes of learning and improvement [2].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse2-2294155-large.gif
2014,6702489,Fig. 3.,"Learning during ramp-up, an overview.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse3-2294155-large.gif
2014,6702489,Fig. 4.,MDP model for different ramp-up cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse4-2294155-large.gif
2014,6702489,Fig. 5.,Reinforcement learning during ramp-up.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse5-2294155-large.gif
2014,6702489,Fig. 6.,Ramp-up break down.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse6-2294155-large.gif
2014,6702489,Fig. 7.,SMC HAS production station.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse7-2294155-large.gif
2014,6702489,Fig. 8.,Experimental process flow chart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse8-2294155-large.gif
2014,6702489,Fig. 9.,Reward variation during ramp-up episodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse9-2294155-large.gif
2014,6702489,Fig. 10.,Learning iterations against the variation of the learning step and the discount factor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse10-2294155-large.gif
2014,6702489,Fig. 11.,Q-value change for different learning step and discount factor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse11-2294155-large.gif
2014,6702489,Fig. 12.,Policy comparison and variation through iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse12-2294155-large.gif
2014,6702489,Fig. 13.,Policy comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse13-2294155-large.gif
2014,6702489,Fig. 14.,Exported policy evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/6878502/6702489/lohse14-2294155-large.gif
2014,6558520,Fig. 1.,2-dimensional feature space with the optimal separating hyperplane [30].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-1-source-large.gif
2014,6558520,Fig. 2.,A 230-kV transmission line including an overhead line combined with an underground cable.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-2-source-large.gif
2014,6558520,Fig. 3.,Faulty-section identification flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-3-source-large.gif
2014,6558520,Fig. 4.,Faulty-half identification DT-SVM diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-4-source-large.gif
2014,6558520,Fig. 5.,Bewley diagram of faults in underground cable.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-5-source-large.gif
2014,6558520,Fig. 6.,Overhead line tower structure and underground cable layout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-6-source-large.gif
2014,6558520,Fig. 7.,"Voltage
WTC
2
s in aerial mode in scale-2 for a single-phase-to-ground fault in overhead line at 87 miles from bus S.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-7-source-large.gif
2014,6558520,Fig. 8.,"Voltage
WTC
2
s in aerial mode in scale-2 for a single-phase-to-ground fault in underground cable at 103 miles from bus S.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-8-source-large.gif
2014,6558520,Fig. 9.,"Voltage
WTC
2
s at bus S in aerial and ground mode in scale-2 for a phase-b-ground fault at 105 miles away from bus S in underground cable.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6693741/6558520/6558520-fig-9-source-large.gif
2014,6719563,Fig. 1.,SAMME training algorithm framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6719563/ricca1-2299291-large.gif
2014,6719563,Fig. 2.,AdaBoost(ELM) training algorithm framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6719563/ricca2-2299291-large.gif
2014,6719563,Fig. 3.,Graphical illustration of the AdaBoost(ELM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6719563/ricca3-2299291-large.gif
2014,6719563,Fig. 4.,"Radar illustration of the results on
Acc
and
MAE
. (a)
Acc
results: Comparison to AdaBoost models. (b)
MAE
results: Comparison to AdaBoost models. (c)
Acc
results: AdaBoost(ELM).ORC3 versus the state-of-the-art models. (d)
MAE
results: AdaBoost(ELM).ORC3 versus the state-of-the-art models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6719563/ricca4abcd-2299291-large.gif
2014,6719563,Fig. 5.,"Hypeparameters study on
Acc
for the AdaBoost(ELM).ORC3 algorithm and the parameters:
M
(ensemble size),
C
(regularization coefficient), and
k
(width of the Gaussian kernel).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6719563/ricca5-2299291-large.gif
2014,6719563,Fig. 6.,"Hypeparameters study on
MAE
for the AdaBoost(ELM).ORC3 algorithm and the parameters:
M
(ensemble size),
C
(regularization coefficient), and
k
(width of the Gaussian kernel).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6898047/6719563/ricca6-2299291-large.gif
2014,6874510,Fig. 1.,Desirable scenario for an examplar serial fusion biometric system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin1-2346703-large.gif
2014,6874510,Fig. 2.,The flowchart of the proposed SSL based enhancement techniques. The enhancement process is repeated with the accumulation of more unlabeled sample pairs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin2-2346703-large.gif
2014,6874510,Fig. 3.,"Representative face images in the combined face database. Images in the first row are from the FacePix database, and images in the second row are from the database constructed by SDUMLA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin3-2346703-large.gif
2014,6874510,Fig. 4.,"Accuracy of the face matcher under the proposed SSL based method with different setting of
thr
for the DMDR method. Results at each step of the promotion are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin4-2346703-large.gif
2014,6874510,Fig. 5.,"Accuracy of face matchers using the proposed method and the Self-update methods, respectively, with the increment of imposed unlabeled samples. The threshold for the Self-update method to accept unlabeled samples is set to zero FAR and 1% FAR respectively in (a) and (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin5ab-2346703-large.gif
2014,6874510,Fig. 7.,"Accuracy of face matchers using the proposed method and the TCU methods, respectively, with the increment of imposed unlabeled samples. The threshold for the TCU method to accept unlabeled samples is set to zero FAR and 1% FAR respectively in (a) and (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin7ab-2346703-large.gif
2014,6874510,Fig. 6.,"Accuracy of face matchers using the proposed method and the Mincut methods, respectively, with the increment of imposed unlabeled samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin6-2346703-large.gif
2014,6874510,Fig. 8.,Representative gait sequence in the database presented by 5 frames.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/6891522/6874510/yin8-2346703-large.gif
2014,6872787,Fig. 1.,Diagram of the proposed islanding detection technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic1-2338736-large.gif
2014,6872787,Fig. 2.,Periodogram versus AR PSD estimation of the voltage signal at PCC for the 0% active and reactive power mismatch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic2-2338736-large.gif
2014,6872787,Fig. 3.,Anti-islanding scheme design and validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic3-2338736-large.gif
2014,6872787,Fig. 4.,Diagram of the IEEE 13 distribution test system [44].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic4-2338736-large.gif
2014,6872787,Fig. 5.,DG system used in the study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic5-2338736-large.gif
2014,6872787,Fig. 6.,(a) PCC voltage and (b) PCC current for an islanding event for a zero active/reactive power mismatch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic6ab-2338736-large.gif
2014,6872787,Fig. 7.,(a) PCC voltage and (b) PCC current for a-phase grid fault event at bus 675.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic7ab-2338736-large.gif
2014,6872787,Fig. 8.,(a) PCC voltage and (b) PCC current for capacitor switching event at bus 692.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic8ab-2338736-large.gif
2014,6872787,Fig. 9.,(a) PCC voltage and (b) PCC current for load switching event at bus 675.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic9ab-2338736-large.gif
2014,6872787,Fig. 10.,"AR coefficients of the voltage signal at PCC for islanding, fault, load switching and capacitor switching events.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic10-2338736-large.gif
2014,6872787,Fig. 11.,"AR coefficients of the current signal at PCC for islanding, fault, load switching and capacitor switching events.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic11-2338736-large.gif
2014,6872787,Fig. 12.,AR coefficients of the voltage signal at PCC for grid fault event.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic12-2338736-large.gif
2014,6872787,Fig. 13.,AR coefficients of the current signal at PCC for grid fault event.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6928524/6872787/matic13-2338736-large.gif
2014,6875913,Fig. 1.,"The interface from our user study in which participants found Waldo while we recorded their mouse interactions. Inset (a) shows Waldo himself, hidden among the trees near the top of the image. Distractors such as the ones shown in inset (b) and (c) help make the task difficult.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6935054/6875913/20tvcg12-brown-2346575-fig-1-source-large.gif
2014,6875913,Fig. 2.,"Visualizations of transitions between viewpoints seen by participants during the study (see Section 5). Subfigures (a) and (b) show slow and fast users respectively, as determined by the mean_nomed splitting method (see Section 6). Subfigures (c) and (d) are split with the mean_nomed method based on locus of control, a personality measure of a person's perceived control over external events on a scale from externally controlled to internally controlled.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6935054/6875913/20tvcg12-brown-2346575-fig-2-source-large.gif
2014,6875913,Fig. 3.,"This is the decision tree generated as a classifier for fast versus slow completion time with mean class splitting. Each internal node represents an individual decision to be made about a data point. The text within the node is the n-gram used to make the choice, and the labels on the out-edges indicate how to make the choice based on the count for a given data point of the n-gram specified. Leaf nodes indicate that a decision is made and are marked with the decided class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6935054/6875913/20tvcg12-brown-2346575-fig-3-source-large.gif
2014,6875913,Fig. 4.,"Graphs showing the ability to classify participants' completion time as a function of the extent of data collected. The x-axis represents the number of seconds of observation, or number of clicks for the sequence based data. The y-axis is the accuracy achieved after that amount of observation. Accuracy values are calculated with leave-one-out cross validation, and use the mean splitting method (see Section 6).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6935054/6875913/20tvcg12-brown-2346575-fig-4-source-large.gif
2014,6875913,Fig. 5.,This graph shows the dependence of the ability to classify the personality trait extraversion on the amount of time the participants are observed. The x-axis represents the number of seconds of observation. The y-axis is the accuracy achieved after that amount of time. This example uses the edge space encoding and the mean splitting method (see Section 6). Accuracy values are calculated with leave-one-out cross validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/6935054/6875913/20tvcg12-brown-2346575-fig-5-source-large.gif
2014,6844831,Fig. 1.,"Single layer AE for hyperspectral data classification. The model learns a hidden feature “
y
” from input “
x
” by reconstructing it on “
z
.” Corresponding parameters are denoted in the network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen1-2329330-large.gif
2014,6844831,Fig. 2.,"Instance of a SAE connected with a logistic regression layer. It has five layers: one input layer, three hidden layers, and an output layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen2-2329330-large.gif
2014,6844831,Fig. 3.,"Classifying with spectral feature. The classification scheme shown here has five layers: one input layer, three hidden layers of AEs, and an output layer of logistic regression. If we want to learn a shallower feature set, we just remove the higher layers of AE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen3-2329330-large.gif
2014,6844831,Fig. 4.,"Spatial-dominated information classification scheme. The first step of procesing is PCA compressing over spectral dimension, then after flatening the data, AEs are introduced to extract layer-wise deep features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen4-2329330-large.gif
2014,6844831,Fig. 5.,"Joint spectral–spatial classification framework. Spectral and spatial information are extracted separately via the former mentioned schemes, and feature extraction is conducted via a deep architecture like SAEs. Final classification is implemented as the final layer of the neural network, using classical neural network classifiers like logistic regression.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen5-2329330-large.gif
2014,6844831,Fig. 6.,"NASA data, KSC. Band 20 and corresponding ground truth areas representing 13 land cover classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen6-2329330-large.gif
2014,6844831,Fig. 7.,"ROSIS-3 data, Pavia, Italy. False-color composite (Band 5, 28, 56) and representing nine land cover classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen7-2329330-large.gif
2014,6844831,Fig. 8.,"Reconstructions of a same input in different iteration epochs. (a) Input spectrum. (b)–(f) Reconstructions of (a) in epoch 1, 10, 100, 1000, and 3500, respectively. Vertical axis stands for normalized reflectance, whereas horizontal axis stands for band numbers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen8abcdef-2329330-large.gif
2014,6844831,Fig. 9.,Filter images learned by an AE on (a) KSC dataset and (b) Pavia dataset. Each N-pixel tiny rectangle stands for N input-to-hidden weights that connects each input unit to a same hidden unit. The intensity of each pixel stands for the absolute value of corresponding weights.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen9ab-2329330-large.gif
2014,6844831,Fig. 10.,Factors influencing training time. (a) Training time of an AE with different hidden and input sizes. (b) Training time elapsed on each epoch whereas varying hidden sizes. (c) Training time elapsed on each epoch whereas varying input size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen10abc-2329330-large.gif
2014,6844831,Fig. 11.,"(a) AE-LR and (b) AE-SVM performance with respect to hidden sizes on the KSC dataset. Dashed lines stands for performance of the control group and the red solid line stands for AE-based methods. Horizontal axis stands for the number of features we extract in the control group and number of hidden units we use while training an AE. In ICA, we choose the parallel fast ICA algorithm and use initial whitening as the preprocessing step, and the maximum iteration step is set to be 200. In NMF, we use the projected gradient method and we use RBF kernel in KPCA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen11ab-2329330-large.gif
2014,6844831,Fig. 12.,"Box plot of Kappa coefficients of different methods on (a) KSC and (b) Pavia datasets. Numbers in the abscissa corresponding to 1) SAE-LR; 2) Linear SVM; 3) PCA RBF-SVM; and 4) RBF-SVM. We plot these boxes by doing 100 independent replications. The red line through the center of each box indicates the median value of the Kappa coefficients. The edges of boxes are the 25th and 75th percentiles. Whiskers extend to the maximum and minimum points. Abnormal outliers shown as red “
+
”s.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen12ab-2329330-large.gif
2014,6844831,Fig. 13.,Effect of principle components.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen13-2329330-large.gif
2014,6844831,Fig. 14.,Effect of depth.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen14-2329330-large.gif
2014,6844831,Fig. 15.,"Box plot of Kappa coefficients of spectral, spatial-dominated and joint classification scheme on (a) KSC and (b) Pavia datasets. Numbers in the abscissa corresponding to 1) SAE-LR on spatial-dominated information; 2) RBF-SVM on spatial-dominated information; 3) SAE-LR on joint information; 4) RBF-SVM on joint information; and 5) EMP RBF-SVM. The meanings of the indicators in the box are the same as Fig. 12.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen15ab-2329330-large.gif
2014,6844831,Fig. 16.,"Spectral (left), spatial-dominated (middle), and joint (right) classification results of the whole image on (a) KSC and (b) Pavia datasets. Results are generated with learned SAE-LR models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/6870503/6844831/chen16ab-2329330-large.gif
2014,6656802,Fig. 1.,"Directed graph
G
in which arc costs are uniformly 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6822661/6656802/senel1-227-large.gif
2014,6656802,Fig. 2.,"Examples of forests on graph
G
containing two trees.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6822661/6656802/senel2ab-227-large.gif
2014,6656802,TABLE I,"(
σ
x
,
σ
y
)
(Standard Deviations) Values for the 10-Communities Datasets, and for Two Degrees of Overlapping Between the Clusters (Small Overlapping in S1, Strong Overlapping in S2)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6822661/6656802/senel.t1-227-large.gif
2014,6656802,Fig. 3.,"Correlation results for the 3/10-communities datasets, between the “true” density and SoF density index, strength (Str), and clustering coefficient (CC). Weighted (W) and Unweighted (U) graphs are considered, constructed with k-Nearest Neighbours (k-NN) or Threshold (Th) methods. (a) 3-communities—k-NN. (b) 3-communities—Th—W. (c) 10-communities—k-NN. (d) 10-communities—Th—W. (e) 10-communities—Th—U.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6822661/6656802/senel3abcde-227-large.gif
2014,6656802,Fig. 4.,10-communities dataset (low sigma values S1) with true density (left figure) and SoF density index (right figure) superimposed. (a) True density. (b) SoF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6822661/6656802/senel4ab-227-large.gif
2014,6656802,Fig. 5.,"S-Sets datasets with SoF density index superimposed (Threshold 95, Weighted). (a)
S2
. (b)
S4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6822661/6656802/senel5ab-227-large.gif
2014,6588959,Fig. 1.,Generated triplets based on pairwise information provided by the LFW data set. The first two belong to the same individual and the third is a different individual.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-1-source-large.gif
2014,6588959,Fig. 2.,(a) ROC curves that use a single descriptor and a single classifier. (b) ROC curves that use hybrid descriptors or single classifiers and FrobMetric's curve. Each point on a curve is the average over 10 runs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-2-source-large.gif
2014,6588959,Fig. 3.,Examples of the actions from the KTH action data set [32].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-3-source-large.gif
2014,6588959,Fig. 4.,"Embedding results of different methods on the 3-D swiss-roll data set, with the neighborhood size
k=6
for all, and
σ=
10
5
for our method. (a) Isomap. (b) LLE. (c) Our method. (d) MVU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-4-source-large.gif
2014,6588959,Fig. 5.,"Embedding results of our method and MVU on the teapot data set. Top: our results with
σ=
10
10
. Bottom: MVU's results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-5-source-large.gif
2014,6588959,Fig. 6.,2-D embedding of face data by our approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-6-source-large.gif
2014,6588959,Fig. 7.,"Quality assessment of neighborhood preservation of different algorithms on 3-D swiss-roll. (a)
Q
nx
(K)
. (b)
B
nx
(K)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-7-source-large.gif
2014,6588959,Fig. 8.,"Quality assessment of neighborhood preservation of different algorithms on the teapot data set. (a)
Q
nx
(K)
. (b)
B
nx
(K)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-8-source-large.gif
2014,6588959,Fig. 9.,"Visualization results based on PSD on the 3-D swiss-roll, with the neighborhood size
k=6
for all, and
σ=
10
5
for our method. (a) Isomap. (b) LLE. (c) Our method. (d) MVU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-9-source-large.gif
2014,6588959,Fig. 10.,"Comparison of computational time on the 3-D swiss-roll data set between MVU and our fast approach. Our algorithm uses
σ=
10
2
. Our algorithm is about 15 times faster. Note that the
y
-axis is in log scale.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6709816/6588959/6588959-fig-10-source-large.gif
2014,6684281,Fig. 1.,"Illustration of construction of CBIMK between a pair of sets of 2-D local feature vectors, when the centers of clusters are considered as virtual feature vectors. (a) 2-D local feature vectors belonging to the training data of three classes, denoted by
∘
,
□
, and
∇
, are grouped into four clusters using the
k
-means clustering method. The centers of the clusters considered as
v
q
s
are denoted using the symbol
⋆
. (b) Illustration of computation of CBIMK between a pair of examples
X
m
and
X
n
belonging to the same class. The points corresponding to the local feature vectors of an example are shown in the same color. (c) Illustration of computation of CBIMK between a pair of examples
X
m
and
X
l
belonging to different classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6684281/ad1abc-2293512-large.gif
2014,6684281,Fig. 2.,"Illustration of construction of CIGMM-based IMK between a pair of sets of 2-D local feature vectors when the components of CIGMM are considered as virtual feature vectors. (a) CIGMM with four components built using the 2-D local feature vectors belonging to the training data of three classes, denoted by
∘
,
□
, and
∇
. The level curves for different components of CIGMM are also shown. (b) Illustration of computation of CIGMM-based IMK between a pair of examples
X
m
and
X
n
belonging to the same class. The points corresponding to the local feature vectors of an example are shown in the same color. (c) Illustration of computation of CIGMM-based IMK between a pair of examples
X
m
and
X
l
belonging to different classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6684281/ad2abc-2293512-large.gif
2014,6684281,Fig. 3.,"Visualization of kernel gram matrices computed between the training examples using (a) IMK that does not use any weight and (b) IMK using posterior probabilities of a particular class as weights. The numbers in the
x
-and
y
-axis of each image indicate the index of last example from each of the seven classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6855384/6684281/ad3ab-2293512-large.gif
2014,6826537,Fig. 1.,"This example illustrates the robust LDS representation of action sequences. (a) A video sequence
Y
is decomposed into two parts with robust LDS. (b) The subspace components
C
show the principal action appearance. (c) The state sequence
X
illustrates the motion dynamics over time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu1-2329301-large.gif
2014,6826537,Fig. 2.,"System diagram of the proposed method. For global dynamics, MBH is extracted in each frame and robust LDS is learned based on MBH sequences. Distance matrix is computed using a shift invariant distance metric. For local appearance, HOG is calculated for each dense curved spatio-temporal cuboid. Occurrence histogram is built in the bag-of-words framework and Chi-Squared distance is computed to measure the pairwise distance. Finally, these two distance matrices are combined in a maximum margin distance learning framework. Action classification is achieved by using a KNN classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu2-2329301-large.gif
2014,6826537,Fig. 3.,"Graphical model representation of LDS. The grey element
y
i
designates the observable feature or image. The white element
x
i
represents the unseen or latent state variable. The parameter
A
indicates the dynamic matrix, while parameter
C
indicates the subspace mapping matrix.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu3-2329301-large.gif
2014,6826537,Fig. 4.,"State sequences (
n=3
) of 10 different actions performed by one subject from the Weizmann data set. (a) bend, (b) jack, (c) jump, (d) pjump, (e) run, (f) side, (g) skip, (h) walk, (i) wave1, (j) wave2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu4-2329301-large.gif
2014,6826537,Fig. 5.,"State sequence
X
1:τ
learned with a jumping jack video from the Weizmann data set. The sequence has total 89 frames with three cycles. A robust LDS is learned with the state dimension
n=3
. The three state components are figured as blue, red and green curves, respectively. The sample images corresponding to the key periodic frames are also demonstrated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu5-2329301-large.gif
2014,6826537,Fig. 6.,"Generation comparison between robust LDS and traditional LDS. The red trajectories in (a) and (b) illustrate the state sequences learned with robust LDS (a), and traditional LDS (b) respectively, using the same walking video in each case. The green trajectories are generated by evolving the red sequences 28 steps ahead with their respective LDS, from which we see that the unstable LDS (b) cannot generate new data consistent with the given data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu6-2329301-large.gif
2014,6826537,Fig. 7.,"Comparison of subspace versus aligned distances (a), and Binet-Cauchy versus aligned kernels (b). The ‘square’ distances/kernels are computed between a model learned from a walking sequence and the set of models learned from the same sequences with different starting frames. The ‘triangle’ distances are computed in the same way between the walking sequence and shifted skipping sequences. The baseline is chosen as the maximum walk versus walk subspace angles distance. The ‘blue’ distances between different classes, say walk versus skip, which are below the baseline may cause misclassification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu7-2329301-large.gif
2014,6826537,Fig. 8.,"Illustration of raw, optical flow and MBH (
x,y
) images of two action sequences from the KTH data set (top) and the UCF sports data set (bottom), respectively. For the optical flow and MBH images, gradient/flow orientation is indicated by color (hue) and magnitude by saturation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu8-2329301-large.gif
2014,6826537,Fig. 9.,"Evaluation of robust LDS dimension
n
(a), and upper bound of temporal shift
T
(b). The performance of shift invariant and standard distance metrics is illustrated for comparison.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu9-2329301-large.gif
2014,6826537,Fig. 10.,"Confusion matrices for depicting the results of action recognition with respect to LDSs, cuboids, combined distances with class independent and class dependent weights on three action data sets. From top to bottom: Weizmann, KTH and UCF sports, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu10-2329301-large.gif
2014,6826537,Fig. 11.,"Class dependent weights of LDSs and cuboids for all the action classes. (a) Weizmann, (b) KTH, (c) UCF sports, (d) Hollywood2, (e) UCF50.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6940341/6826537/hu11-2329301-large.gif
2014,6552193,Fig. 1.,"(a) Learning a semilatent attribute space is applicable to various problem domains. (b) Representing data in terms of a semilatent attribute space partially defined by the user (solid axes), and partially learned by the model (dashed axes). A novel class (dashed circle) may be defined in terms of both user-defined and latent attributes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-1-source-large.gif
2014,6552193,Fig. 2.,Schematic of conventional (left) DAP [22] versus (right) M2LATM. Shading indicates different types of constraints placed on the variables. Symbols are explained in Section 4.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-2-source-large.gif
2014,6552193,Fig. 3.,Graphical model for M2LATM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-3-source-large.gif
2014,6552193,Fig. 4.,Schematic illustration of latent ZSL mechanism.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-4-source-large.gif
2014,6552193,Fig. 5.,(Above) Different types of attributes in visual and auditory modalities are shown in different colors. (Below) Examples from the eighth classes in the USAA data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-5-source-large.gif
2014,6552193,Fig. 6.,Exploiting multimodality: LATM versus M2LATM for USAA data set. Left: Multitask classification. Right: 0/N-shot learning shown as margin of M2LATM over LATM—positive value means increase of accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-6-source-large.gif
2014,6552193,Fig. 7.,"Examples of surprising videos: (A) birthday party with instrumental music, (B) music performance with costumes, (C) wedding ceremony with drinking glasses, and (D) an indoor parade.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-7-source-large.gif
2014,6552193,Fig. 8.,Robustness to attribute label noise in multitask classification and zero/N-shot learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-8-source-large.gif
2014,6552193,Fig. 9.,Similarity between user-defined and latent attributes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-9-source-large.gif
2014,6552193,Fig. 10.,Visualization of user-defined (circles) and corresponding latent attributes (crosses). Red circles illustrate representative words from the UD attribute (A/69); red crosses illustrate the words from the corresponding latent attribute that discovered these concepts when withheld (R/7). Blue dots illustrate interest points not related to attributes concerned.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6687161/6552193/6552193-fig-10-source-large.gif
2014,6484923,Fig. 1.,"First-order KTT condition corresponding to (a) set
O
, (b) set
S
, and (c) set
E
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-1-source-large.gif
2014,6484923,Fig. 2.,Partitioned regions,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-2-source-large.gif
2014,6484923,Fig. 3.,"Partitioned regions MO and MS and distributions of misclassified training samples, where samples in the MO region are moved using the MSGA toward to correct directions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-3-source-large.gif
2014,6484923,Fig. 4.,ROC curves for the test images with different classification models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-4-source-large.gif
2014,6484923,Fig. 5.,"(First row) Original test images. (Second row) Segmentation results of the
FM
2
. (Third row) Segmentation results of the test images using
FM
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-5-source-large.gif
2014,6484923,Fig. 6.,"Test error rates of the online trained
FM
3
in Experiment 4 at different time steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-6-source-large.gif
2014,6484923,Fig. 7.,"Test error rates of the online trained
FM
3
in Experiment 5 at different time steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6779626/6484923/6484923-fig-7-source-large.gif
2014,6705618,Fig. 1.,"Schematic diagram of our discriminative information encoding scheme. The square in green denotes the training patch; the circles in red denote patches from the same category (positive); the triangles in blue denote patches from other categories (negative). This framework aims to reduce P2CD(
q
,
N
N
p
), while elongate P2CD(
q
,
N
N
n
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6818457/6705618/6705618-fig-1-source-large.gif
2014,6705618,Fig. 2.,Illustration of our object recognition pipeline (best viewed in color).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6818457/6705618/6705618-fig-2-source-large.gif
2014,6705618,Fig. 3.,(a) Examples from STL-10 dataset. (b) Examples from four different tree species of NTU Tree-51 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6818457/6705618/6705618-fig-3-source-large.gif
2014,6705618,Fig. 4.,"Accuracy of our method on Caltech-101 versus the weight of the discriminative term. Green lines represent the accuracy results of applying the hierarchical RICA, while the red lines indicate the accuracy of our discriminative hierarchical method. (a) Influence of
η
in layer 1. (b) Influence of
η
in layer 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/6818457/6705618/6705618-fig-4-source-large.gif
2014,6579606,Fig. 1.,The system flow of the proposed online multiple kernel similarity learning scheme for visual search.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6731373/6579606/6579606-fig-1-source-large.gif
2014,6579606,Fig. 2.,Top-n precision results on “Public” and “Caltech10” data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6731373/6579606/6579606-fig-2-source-large.gif
2014,6579606,Fig. 3.,Evaluation of RatioTrain on both “Public” and “Caltech10” data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6731373/6579606/6579606-fig-3-source-large.gif
2014,6579606,Fig. 4.,"Qualitative comparison of image similarity search results on the “Caltech10” database by different algorithms. For each block, the first image is the query, and the results from the first line to the fourth line represent “OASIS-Best,” “OKS-Best,” “OMKS-U,” and “OMKS,” respectively. The category names for the queries are as follows: 1 (roulette wheel), 2 (billiard), 3 (skyscraper), 4 (bear), 5 (minotaur), 6 (laptop).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6731373/6579606/6579606-fig-4-source-large.gif
2014,6783969,Fig. 1.,The hierarchy structures of two selected subsets of the SUN data set [34] used in our experiments for hierarchical image classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen1-2315792-large.gif
2014,6783969,Fig. 2.,"Classification with taxonomies (tree loss), corresponding to the first example in Fig. 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen2-2315792-large.gif
2014,6783969,Fig. 3.,"AUC optimization on two UCI data sets. The objective values and optimization time are shown in the figure by varying boosting (or column generation) iterations. It shows that
1
-slack achieves similar objective values as
m
-slack but needs less running time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen3-2315792-large.gif
2014,6783969,Fig. 4.,"Test performance versus the number of boosting iterations of multi-class classification. StBoost-stump and StBoost-per denote our StructBoost using decision stumps and perceptrons as weak learners, respectively. The results of SSVM and SVM are shown as straight lines in the plots. The values shown in the legend are the error rates of the final iteration for each method. Our methods perform better than SSVM in most cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen4-2315792-large.gif
2014,6783969,Fig. 5.,Bounding box overlap (first row) and center location error (second row) in frames of several video sequences. Our StructBoost often achieves higher scores of box overlap and lower center location errors compared with other trackers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen5-2315792-large.gif
2014,6783969,Fig. 6.,"Some tracking examples for several video sequences: “coke”, “david”, and “walk” (best viewed on screen). The output bounding boxes of our StructBoost better overlap against the ground truth than the compared methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen6-2315792-large.gif
2014,6783969,Fig. 7.,"Some segmentation results on the Graz-02 data set (bicycle, car and person). Compared with AdaBoost, structured output learning methods (StructBoost and SSVM) present sharper segmentation boundaries, and better spatial regularization. Compared with SSVM, our StructBoost with non-linear parameter learning performs better, demonstrating more accurate foreground object boundaries and cleaner backgrounds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/6889065/6783969/shen7-2315792-large.gif
2014,6731517,Fig. 1.,Geometrical interpretation of the discrete MVN activation function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen1-2301802-large.gif
2014,6731517,Fig. 2.,Geometrical interpretation of the MVN learning rule.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen2-2301802-large.gif
2014,6731517,Fig. 3.,"Potentially nonreliable results of the learning process. The weighted sum
z
l
corresponding to the learning sample
l
was fitted too close to the desired sector
q
lower border determined by
ε
q
k
. As a result, the weighted sum
z
t
corresponding to the test sample
t
belonging to the same class
q
has fallen in the adjacent sector
q−1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen3-2301802-large.gif
2014,6731517,Fig. 4.,"Reliable results of the learning process. The weighted sum
z
l
corresponding to the learning sample
l
was fitted close to the bisector of the desired sector
q
. As a result, the weighted sum
z
t
corresponding to the test sample
t
belonging to the same class
q
has also fallen in the desired sector
q
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen4-2301802-large.gif
2014,6731517,Fig. 5.,MVN/MLMVN hard margin learning: a narrow subsector inside a desired sector is fixed to fit there the weighted sums for all learning samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen5-2301802-large.gif
2014,6731517,Fig. 6.,Modified discrete MVN error definition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen6-2301802-large.gif
2014,6731517,Fig. 7.,Learning results for the learning set corresponding to the data sample 187 of the glass identification data set obtained with MLMVN-SM. The resulting normalized weighted sums are shown.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen7-2301802-large.gif
2014,6731517,Fig. 8.,Learning results for the learning set corresponding to the data sample 187 of the glass identification data set obtained with MLMVN. The resulting normalized weighted sums are shown.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen8-2301802-large.gif
2014,6731517,Fig. 9.,Resulting weighted sums for the class 5 of the learning set corresponding to the data sample 187 of the glass identification data set obtained with MLMVN-SM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen9-2301802-large.gif
2014,6731517,Fig. 10.,Resulting weighted sums for the class 5 of the learning set corresponding to the data sample 187 of the glass identification data set obtained with MLMVN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen10-2301802-large.gif
2014,6731517,Fig. 11.,Testing result for the data sample 187 of the glass identification data set obtained with MLMVN-SM. The resulting weighted sum appears in the desired sector 5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen11-2301802-large.gif
2014,6731517,Fig. 12.,"Testing result for the data sample 187 of the glass identification data set obtained with MLMVN. The resulting weighted sum appears in the sector 0, adjacent to the desired sector 5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6879349/6731517/aizen12-2301802-large.gif
2014,6786488,Fig. 1.,Modified IEEE 13 bus standard test system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-1-source-large.gif
2014,6786488,Fig. 2.,Mean voltage signal for islanding case with near-zero mismatch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-2-source-large.gif
2014,6786488,Fig. 3.,Low-pass and high-pass filters at the decomposition and reconstruction side of the WGM1.0 filter bank.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-3-source-large.gif
2014,6786488,Fig. 4.,Steps in computing the wavelet mean voltage energy-based index.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-4-source-large.gif
2014,6786488,Fig. 5.,NEI values. (High NEI values in red and low NEI values in blue).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-5-source-large.gif
2014,6786488,Fig. 6.,NEI values in case of noise (High values in red and low values in blue).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-6-source-large.gif
2014,6786488,Fig. 7.,"Performance evaluation of SVM and ETC. (a) SVM, (b) ETC (Bagging), (c) ETC (Boosting), and (d) Recall values for SVM and ETC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/6839066/6786488/6786488-fig-7-source-large.gif
2014,6817596,Fig. 1.,A schematic overview of a drug-target interaction network. Edges between drugs and between targets represent different similarities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr1-2325031-large.gif
2014,6817596,Fig. 2.,"Propagation via triads. Similar targets tend to interact with the same drug (a), and similar drugs tend to interact with the same target (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr2-2325031-large.gif
2014,6817596,Fig. 3.,"Propagation via tetrads. Consider a pair of similar drugs and a pair of similar targets. If one of the drugs interacts with one of the targets, then the other drug may interact with the other target.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr3-2325031-large.gif
2014,6817596,Fig. 4.,Distribution variation of different similarity values between drugs and between targets. Similarities with values of zero or one are omitted in this plot.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr4-2325031-large.gif
2014,6817596,Fig. 5.,"Distribution of annotation-based similarity values for 315 drugs, where dots indicate mean similarity value between each drug and all others, and lines demonstrate standard deviation of the values. Similarities with values of zero or one are omitted in this plot. E.g., the mean of all annotation-based drug similarities with drug #200 is about 0.2 with standard deviation of 0.15.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr5-2325031-large.gif
2014,6817596,Fig. 6.,"Collective inference. Predicted interactions can be used for other inferences using target (a) or drug similarities (b), or both.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr6-2325031-large.gif
2014,6817596,Fig. 7.,"Network of drug-target interactions in the data set. Drugs are shown with blue squares and targets with red circles, where size of the node represent their degree. Similarities are not shown to simplify the graph.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr7-2325031-large.gif
2014,6817596,Fig. 8.,"Average precision of the top 130 interaction predictions for all 10 folds with
k=5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr8-2325031-large.gif
2014,6817596,Fig. 9.,Relative triad-based rule weights in models with all similarities included and models with only one similarity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr9-2325031-large.gif
2014,6817596,Fig. 10.,Collective versus non-collective average precision of the top predictions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr10-2325031-large.gif
2014,6817596,Fig. 11.,Comparing Perlman’s method with PSL’s top 130 predictions using 10-fold cross validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr11-2325031-large.gif
2014,6817596,Fig. 12.,Comparing Perlman’s method with PSL’s top 150 predictions using new interactions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/6917081/6817596/fakhr12-2325031-large.gif
2014,6615954,Fig. 1.,"Schematic illustration of the double-layered cross-validation procedure adopted in this paper. The original dataset was initially partitioned by an external fivefold cross-validation, forming the training and validation sets. The training sets from the external cross-validation were each further partitioned by a second internal cross-validation to form the partitions for SSO modeling. The validation sets from the external cross-validation were reserved for model validation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-1-source-large.gif
2014,6615954,Fig. 2.,"SSO-based under-sampling on the synthetic dataset. The decision boundaries were created by using a
kNN
(k=3)
classifier with Euclidean distance. (a) Decision boundary of
kNN
on the initial dataset that has imbalanced class distribution and five outliers. (b) Decision boundary on a dataset after SSO-based optimization. (c) Samples from the majority class were ranked by their frequency of being included in optimized sample subsets (i.e., Majority 1, Majority 2, etc.). The larger the circle, the more frequent a sample was included in the optimized sample subsets. (d) Decision boundary on the balanced dataset. The balanced dataset was formed by selecting top ranked samples from the majority class to match the number of samples in the minority class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-2-source-large.gif
2014,6615954,Fig. 3.,"Comparison of different methods for imbalanced data sampling and classification. The methods included in comparisons are SSO-PSO, SSO-GA, random under-sampling (RUS), random oversampling (ROS), and SMOTE sampling. (a)–(e) For each method, the classification accuracies were calculated from external fivefold cross-validation repeated 30 times each with a different splitting point on each of the five datasets, respectively. The box plots summarize the classification accuracies of 30 evaluation runs. (f) Classification results of each sampling method on protein localization dataset using the
k
value of 2, 5, and 10 for
k
-fold cross-validation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-3-source-large.gif
2014,6615954,Fig. 4.,SSO-based ensemble learning on the synthetic dataset. The decision boundaries were created by using a decision tree (J48) classifier. (a) Decision boundary of a decision tree on the initial dataset that has five outliers in each of the two classes. (b) Example of a horizontal decision boundary on a dataset after SSO-based sample subset selection. (c) Example of a vertical decision boundary on a dataset after SSO-based sample subset selection. (d) Example of a rectangle decision boundary on a dataset after SSO-based sample subset selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-4-source-large.gif
2014,6615954,Fig. 5.,"Sample classification comparison. The
y
-axis indicates the classification accuracy in terms of AUC and the
x
-axis indicates the number of base classifiers used to form an ensemble. For each method, the classification accuracies were calculated from a fivefold cross-validation repeated ten times each with a different data partition. The middle points of the performance curves are the averages of the classification accuracies from these ten evaluation runs and the error bars are the standard deviations. (a) Diabetes. (b) Heart disease. (c) Leukemia. (d) Liver cancer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-5-source-large.gif
2014,6615954,Fig. 6.,"Ensemble decomposition. Comparison of AUC values of each base classifiers in
SSO
E
−PSO
(denoted as blue points) with the AUC value of the majority voting of base classifiers (denoted as a red line). For each dataset, the result is divided with respect to the fold of an external fivefold cross validation. (a) Diabetes. (b) Heart disease. (c) Leukemia. (d) Liver cancer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-6-source-large.gif
2014,6615954,Fig. 7.,"Hierarchical clustering of (a) 50% most frequently selected samples in SSO procedure and (b) all samples from Leukemia dataset, using the top 100 genes filtered by BSS/WSS. The tree dendrograms are cut at the level of three branches to form three separated clusters marked by blue (first cluster of AML), red (second cluster of AML), and green (ALL cluster). Samples that are clustered incorrectly (or indistinguishable) at the level of three branches are marked in gray.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6739112/6615954/6615954-fig-7-source-large.gif
2014,6817512,FIGURE 1,"Illustration of a deep belief network architecture. This particular DBN consists of three hidden layers, each with three neurons; one input later with five neurons and one output layer also with five neurons. Any two adjacent layers can form a RBM trained with unlabeled data. The outputs of current RBM (e.g.,
h
(1)
i
in the first RBM marked in red) are the inputs of the next RBM (e.g.,
h
(2)
i
in the second RBM marked in green). The weights
W
can then be fine-tuned with labeled data after pre-training.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6817512/chen1-2325029-large.gif
2014,6817512,FIGURE 2,"Illustration of a typical convolutional neural network architecture. The input is a 2D image, which convolves with four different filters (i.e., h
(1)
i
, i
= 1
to 4), followed by a nonlinear activation, to form the four feature maps in the second layer (C1. These feature maps are down-sampled by a factor of 2 to create the feature maps in layer S1. The sequence of convolution/nonlinear activation/subsampling can be repeated many times. In this example, to form the feature maps in layer C2, we use eight different filters (i.e., h
(2)
i
,
i= 1
to 8): the first, third, fourth, and sixth feature maps in layer C2 are defined by one corresponding feature map in layer S1, each convoluting with a different filter; and the second and fifth maps in layer C2 are formed by two maps in S1 convoluting with two different filters. The last layer is an output layer to form a fully connected 1D neural network, i.e., the 2D outputs from the last subsampling later (S2 will be concatenated into one long input vector with each neuron fully connected with all the neurons in. the next layer (a hidden layer in this figure).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6817512/chen2-2325029-large.gif
2014,6817512,FIGURE 3,"An illustrative architecture of a CUDA-capable GPU with highly threaded streaming processors (SPs). In this example, the GPU has 64 stream processors (SPs) organized into four multiprocessors (MPs), each with two stream multiprocessors (SMs). Each SM has eight SPs that share control unit and instruction cache. The four MPs (building blocks) also share a global memory (e.g., graphics double data rate DRAM) that often functions as very-high-bandwidth, off-chip memory (memory bandwidth is the data exchange rate). Global memory typically has high latency and is accessible to the CPU (host). A typical processing flow includes: input data are first copied from host memory to GPU memory, followed by loading and executing GPU program; results are then sent back from GPU memory to host memory. Practically, one needs to pay careful consideration to data transfer between host and GPU memory, which may take considerable amount of time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6817512/chen3-2325029-large.gif
2014,6817512,FIGURE 4,"An illustration of the operations involved with 1D convolution and subsampling. The convolution filter's size is six. Consequently, each unit in the convolution layer is defined by six input units. Subsampling involves averaging two adjacent units in the convolution layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6817512/chen4-2325029-large.gif
2014,6817512,FIGURE 5,"DistBelief: models are partitioned into four blocks and consequently assigned to four machines [56]. Information for nodes that belong to two or more partitions is transferred between machines (e.g., the lines marked with yellow color). This model is more effective for less densely connected networks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6817512/chen5-2325029-large.gif
2014,6817512,TABLE I,Summary of recent research progress in large-scale deep learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/6705689/6817512/chen.t1-2325029-large.gif
2014,6849939,Fig. 1.,"Graphs constructed by different methods on circle data set. (a) 5NN graph. (b) 6NN graph. (c)
ϵ
-ball graph
ϵ=0.7
. (d) Graph given by [11]. (e) Graph given by [13]. (f) Proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6849939/zhang1abcdef-2300489-large.gif
2014,6849939,Fig. 2.,"Graphs constructed by different methods on two moons data set. (a) 6NN graph. (b) 7NN graph. (c)
ϵ
-ball graph,
ϵ=1.8
. (d) L1 graph. (e) Soft LLEGC graph,
α=0.1
. (f) LLPC graph,
μ=16
,
λ=3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6849939/zhang2abcdef-2300489-large.gif
2014,6849939,Fig. 3.,"Semi-supervised classification accuracy using GRF algorithm for different graph construction methods under different data sets. The horizontal axis represents the number of the randomly labeled data, and the vertical axis is the classification accuracy averaged over 100 independent runs. (a) Pima. (b) Ionosphere. (c) Sonar. (d) Wine. (e) Breast. (f) Vehicle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6849939/zhang3abcdef-2300489-large.gif
2014,6849939,Fig. 4.,"Robustness of LPGC graph to hyperparameters. (a) Robustness of LPGC graph to
λ
(
μ=16
). (b) Robustness of LPGC graph to
μ
(
λ=2
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6849939/zhang4ab-2300489-large.gif
2014,6849939,Fig. 5.,"Spareness of LPGC graph. (a) Sparseness of LPGC graph for different
λ
(
μ
=8). (b) Sparseness of LPGC graph for different
μ
(
λ
=8).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/6922172/6849939/zhang5ab-2300489-large.gif
2014,6670768,Fig. 1.,"Three kinds of inconsistencies. (a)
I
1
. (b)
I
2
. (c)
I
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6963534/6670768/6670768-fig-1-source-large.gif
2014,6670768,Fig. 2.,Covering relationships. (a) case 1. (b) case 2. (c) case 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/6963534/6670768/6670768-fig-2-source-large.gif
2014,6663664,Fig. 1.,"Normalized feature ranking weights using (a) individual scores criteria: correlation, Fisher, mRMR; and (b) combination of individual scores. Black bars correspond to the VF detection problem, whereas white bars represent the shockable rhythms detection scenario.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6740863/6663664/6663664-fig-1-source-large.gif
2014,6663664,Fig. 2.,"ROC curves calculated on the out of sample test set for the (a) VF versus nonVF problem, and (b) shockable versus nonshockable scenario.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6740863/6663664/6663664-fig-2-source-large.gif
2014,6663664,Fig. 3.,"BER metric (in
%
) analysis with respect to the number of features: mean (central line), and
95%
confidence interval (gray area).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/6740863/6663664/6663664-fig-3-source-large.gif
2015,7123635,Fig. 1.,"Basic switching principle of filamentary RRAM devices. 1R, 1T-1R configurations are also shown. Selector device can either be a diode or transistor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7729/7322313/7123635/suri1-2441112-large.gif
2015,7123635,Fig. 2.,Extracted HRS (Roff) log-normal distributions for filamentary RRAM devices listed in Table I.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7729/7322313/7123635/suri2-2441112-large.gif
2015,7123635,Fig. 3.,Basic ELM Framework and governing equations used to calculate output synaptic weights during training. ‘B’ denotes random hidden neuron biases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7729/7322313/7123635/suri3-2441112-large.gif
2015,7123635,Fig. 4.,Proposed RRAM-ELM Architecture. For diabetes diagnosis classification 8 input feature vectors and 20 hidden layer neurons were used. HRS variability of RRAM matrix was exploited for implementing both- random input weights and random neuron biases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7729/7322313/7123635/suri4-2441112-large.gif
2015,7123635,Fig. 5.,Training block schematic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7729/7322313/7123635/suri5-2441112-large.gif
2015,7123635,Fig. 6.,Sinc Function Regression results obtained using RRAM-ELM architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7729/7322313/7123635/suri6-2441112-large.gif
2015,7024132,Fig. 1.,"Network structure of the LSM. Dots and arrows represent neurons and synapses, respectively. The neurons on the left provide input spike trains to the reservoir neurons in the middle. The reservoir receives input spike trains and projects through plastic synapses to the readout neurons on the right.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li1-2388544-large.gif
2015,7024132,Fig. 2.,"Preprocessing of speech signals. The speech signal is processed by a filter representing the outer ear and middle ear followed by 77 cascaded bandpass filters modeling the cochlea. After each half-wave rectifier, the magnitude of the time-domain signal in each frequency band is compressed by an automatic control module. The resulting signal is converted to spike trains by the BSA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li2-2388544-large.gif
2015,7024132,Fig. 3.,Entire LSM system for speech recognition. Seventy-seven spike trains from the preprocessing stage are used as the inputs to the reservoir of the LSM. The BSA is implemented in each input neuron of the LSM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li3-2388544-large.gif
2015,7024132,Fig. 4.,Left: online training of a linear classifier. The orientation of the hyperplane is adjusted iteratively to separate two classes of data. Right: the linear classifier with a margin for improved generalization performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li4ab-2388544-large.gif
2015,7024132,Fig. 5.,Left: the abstract learning rule of (8) that determines potentiation or depression based upon the difference between the real and expected neuronal activities. Right: the Hebbian learning of (9) where a presynaptic spike leads to synaptic potentiation (depression) when the postsynaptic neuron is active (inactive).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li5ab-2388544-large.gif
2015,7024132,Fig. 6.,"Proposed learning rule. The eight regions shows how different combinations of
c
d
and
c
r
of the postsynaptic neuron determine the plasticity of a synapse. The four arrows show how the teacher signal drives the output neuron activity to the desired region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li6-2388544-large.gif
2015,7024132,Fig. 7.,Top left: performance of the LSM using synapses with static response. Top right: performance of the LSM using the first-order synaptic model with a time constant of 4 ms. Bottom left: performance of the LSM using the first-order synaptic model with a time constant of 8 ms. Bottom right: performance of the LSM using the second-order synaptic model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li7-2388544-large.gif
2015,7024132,Fig. 8.,"Top left: the responses of the reservoir using the static synaptic model. With the input spike trains ending at 23 ms, the reservoir shows little temporal memory. With an ascending firing frequency for the input spike trains, the top, middle, and bottom plots show that only the magnitude of the reservoir response increases with the input firing frequency. Top right: the reservoir responses with the first-order synaptic model whose time constant is 4 ms. Different from the previous case, the reservoir exhibits temporal memory whose duration increases with the firing frequency of the input spike trains. Bottom left: the reservoir responses with the first-order synaptic model whose time constant is 8 ms. Compared with the case of the first-order model with a time constant of 4 ms, temporal memory is extended in this case. Bottom right: the reservoir responses with the second-order synaptic model. The temporal memory here is longer than those of the first-order models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li8-2388544-large.gif
2015,7024132,Fig. 9.,Fading memory of the reservoir with different synaptic models averaged over 500 utterances. Fading memory is recorded starting from the end of each utterance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li9-2388544-large.gif
2015,7024132,Fig. 10.,Merging of linear synapses. The three synapses incident to a neuron are effectively merged into a single synapse.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li10-2388544-large.gif
2015,7024132,Fig. 11.,Influence of the precision of synaptic weights on the LSM performance as synaptic weight resolution varies between 5 and 10 bits. The LSM performances for the first 500 epochs of training are evaluated using the five-fold cross validation with 500 speech samples. The network performance generally improves as the number of bits increases and is saturated at a resolution of 8 bits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li11-2388544-large.gif
2015,7024132,Fig. 12.,Recognition and error rates of the LSM as functions of the size (number of neurons) of the reservoir.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li12-2388544-large.gif
2015,7024132,Fig. 13.,Classification performance of the proposed LSM on the 1590-sample dataset containing all digit utterances in TI46 speech corpus. The final classification rate reaches 92.3%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li13-2388544-large.gif
2015,7024132,Fig. 14.,"Left: the English letter classification performance as a function of the number of training epochs. The final LSM classification rate is 92.3%. Right: classification performance on each letter. The row and column indices represent the true class and output of the LSM, respectively. For example, the grayscale square with row index \$E\$ and column index \$F\$ shows the probability of letter \$E\$ to be misclassified as \$F\$ . The proposed LSM classifies most letters perfectly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7024132/li14ab-2388544-large.gif
2015,6781632,Fig. 1.,Schematic diagram of the HCCI engine setup and instrumentation (only relevant instrumentation shown).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak1-2311466-large.gif
2015,6781632,Fig. 2.,Subset of the HCCI engine experimental data showing A-PRBS inputs and engine outputs. The misfire regions are shown in dotted rectangles. The data are indexed by combustion cycles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak2-2311466-large.gif
2015,6781632,Fig. 3.,"Pressure trace of an HCCI combustion cycle showing valve events (IVO and EVC), injected FM, and SOI. The NVO can be seen as a smaller peak at eTDC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak3-2311466-large.gif
2015,6781632,Fig. 4.,Illustration showing labeling of unstable engine data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak4-2311466-large.gif
2015,6781632,Fig. 5.,Illustration showing labeling of stable engine data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak5-2311466-large.gif
2015,6781632,Fig. 6.,"Plot showing sensitivity of TPR, TNR, and total accuracy with scaling factor
f
for cost-sensitive SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak6-2311466-large.gif
2015,6781632,Fig. 7.,"Plot showing sensitivity of TPR, TNR, and total accuracy with scaling factor
f
for cost-sensitive ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak7-2311466-large.gif
2015,6781632,Fig. 8.,"Prediction results of the best ELM envelope model imposed on NMEP, CA50, and one input variable (FM) for four unseen data sets (subplots a, b, c and d). The color code indicates model prediction—green (and red) indicates the model predicting the engine state to be stable (and unstable). The dotted lines in the NMEP plot indicate misfire limit, the dotted ellipses in CA50 plot indicate high-variability instability mode, while the dotted rectangles indicate misclassified data observations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak8abcd-2311466-large.gif
2015,6781632,Fig. 9.,"Prediction results of the cost-sensitive SVM envelope model imposed on NMEP, CA50, and one input variable (FM) for four unseen data sets (subplots a, b, c and d). The color code indicates model prediction—green (and red) indicates the model predicting the engine state to be stable (and unstable). The dotted lines in the NMEP plot indicate misfire limit, the dotted ellipses in CA50 plot indicate high-variability instability mode, while the dotted rectangles indicate misclassified data observations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak9abcd-2311466-large.gif
2015,6781632,Fig. 10.,"Zoomed-in plots for two unseen data sets (subplots a and b) showing prediction results of the best ELM envelope model imposed on NMEP, CA50, and one input variable (FM). The color code indicates model prediction—green (and red) indicates the model predicting the engine state to be stable (and unstable).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6781632/janak10ab-2311466-large.gif
2015,7182735,Fig. 1.,"Multimodal generative model (top) and two generic fusion approaches to multimodal data (bottom). A cognitive phenomenon (
H
, e.g., attention, stimulus processing) influences certain aspects of modality specific neurophysiological processes, such as electrophysiological or metabolic properties. In the context of this generative model, these processes are modeled by latent variables (also called sources) and denoted by
s
x/y
. These latent variables are mapped by a modality specific transformation (
A
x/y
) to their respective sensor-space variables
(X/Y)
. Starting from the recorded data sets
X
and
Y
, it is the task of factor-model-based methods to extract estimates of the latent sources
(
s
^
x/y
)
such that features of the estimated source activity
Φ(
s
^
x
)
and
Ψ(
s
^
y
)
are informative about
H
itself, or tell us something about how exactly
H
exerts influence on
s
x/y
. In early fusion approaches, information from both modalities is already taken into account when extracting source activity from the data. In late fusion approaches, modality-specific sources are extracted without using information from the respective other modality first, and features of the estimated sources are combined thereafter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek1ab-2425807-large.gif
2015,7182735,Fig. 2.,"Illustration of the difference between extraction filters (i.e., the coefficients of backward models) and spatial activation patterns (i.e., the coefficients of forward models) by using simulated data from only one hypothetical modality. The top panel on the right-hand side shows the time courses of two hidden source components,
s
1
and
s
2
, of which
s
1
will be the signal of interest in this example and
s
2
corresponds to a noise component. These time courses are mapped to two recording channels (
x
1
and
x
2
) by means of (1), using matrix
A
x
=[
a
1
,
a
2
]
. The time courses of the data in channel space are shown in the middle panel on the right-hand side, as well as in a scatter plot on the left-hand side (
x
1
on the abscissa and
x
2
on the ordinate). The scatter plot also shows the activation patterns (columns of matrix
A
x
) as solid line vectors. Note that
a
1
is only half as long as
a
2
, which means that the noise component
s
2
is expressed much stronger in the channel data. Using
s
1
as a target variable, thevector
w
is the extraction filter of a backward model optimized by means of (20).
w
is shown in the scatter plot as a dashed line vector. Applying
w
to the data, i.e., computing
w
⊤
x(t)
, yields a reconstruction of
s
1
; see the lower panel on the right-hand side. Importantly, while
w
extracts the time course of component
s
1
, its coefficients are not to be interpreted as to how strong and with what sign
s
1
was expressed in the data. Instead, only the coefficients of
a
1
contain that information. However, an estimate of
a
1
can be derived from
w
by means of (4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek2-2425807-large.gif
2015,7182735,Fig. 3.,"Illustration of CCA for multimodal fusion, exemplary for two data sets
x
and
y
(shown in the left and right panels, respectively). Two modality-specific source spaces are assumed, each containing at least one source that is highly correlated with a corresponding source in the other modality. In this example, the time courses of
s
x,1
and
s
y,1
are correlated, while
s
x,2
and
s
y,2
are uncorrelated to all other sources. The source signals are projected to the recording channels according to (1) with modality-specific activation patterns, i.e., matrices
A
x
and
A
y
, respectively. CCA optimizes spatial filters
w
x
and
w
y
such that the correlation between the projections
w
⊤
x
x(t)
and
w
⊤
y
y(t)
are maximized.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek3-2425807-large.gif
2015,7182735,Fig. 4.,"Application example of CCA. Here an intracranially measured electrophysiology signal was fused with high-resolution fMRI. A time-frequency representation was derived from the univariate intracranial electrode signal. This, now multivariate time-frequency signal, was temporally embedded and, together with the fMRI signal subjected to CCA analysis. Shown are the resulting activation patterns for the electrode on the left and the fMRI signal on the right. The fMRI activation pattern was superimposed on an anatomical scan. See Section V-B (Applications) for interpretation of the results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek4-2425807-large.gif
2015,7182735,Fig. 5.,"Illustration of mSPoC for multimodal fusion, exemplary for two data sets
x
and
y
(shown in the left and right panels, respectively). Two modality-specific source spaces are assumed. One of the source spaces (here the
x
-source space) contains at least one oscillatory source with variable amplitude dynamics that are correlated to the time course of a corresponding source in the other source space. In this example, the amplitude modulations of
s
x,1
are correlated to the time course of
s
y,1
, while
s
x,2
(and its amplitude dynamics) as well as
s
y,2
are uncorrelated to all other sources. The source signals are projected to the recording channels according to (1) with modality-specific activation patterns, i.e., matrices
A
x
and
A
y
, respectively. mSPoC optimizes spatial filters
w
x
and
w
y
such that the correlation between the amplitude dynamics of
w
⊤
x
x(t)
and the time course of
w
⊤
y
y(t)
are maximized.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek5-2425807-large.gif
2015,7182735,Fig. 6.,"Application example of mSPoC. Bandpower modulations of EEG were fused with simultaneously measured fMRI in a motor task (transient right-hand movement). EEG was bandpass filtered to amplify oscillations in the
β
band (16–25 Hz). The left panel shows the sensor-space activation pattern of the EEG mSPoC component and the middle panel shows the fMRI activation pattern of the corresponding mSPoC component. These components were identified based on task-induced comodulation of amplitude dynamics in the EEG and the time course of BOLD dynamics in the fMRI. The right panel shows an estimate of the source-space pattern of the EEG component based on the sensor-space pattern, computed using the MUSIC algorithm [58]. See Section V-C (Applications) for further discussion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek6-2425807-large.gif
2015,7182735,Fig. 7.,"Simulation that illustrates potential problems with parameter interpretation if the assumptions of the generative model are not respected. In this EEG simulation, the task was to extract a bandpower signal that comodulates with a given target function
z
. The target signal corresponds to the true amplitude modulation of one of the simulated source components. Regression was applied to channelwise computed bandpower
ϕ(X)
and estimated
z
as
ϕ
w
=
w
⊤
ϕ(X)
. SPoC was applied to the sensor signal and estimated
z
as
ϕ
w
=ϕ(
w
⊤
X)
. Sensor-space patterns were obtained for both methods using (4). Note that
ϕ(
w
⊤
X)≠
w
⊤
ϕ(X)
because the computation of bandpower is a nonlinear operation. The left panel shows that SPoC yields better estimation of the target variable, compared to regression. More importantly, the resulting sensor-space patterns show high similarity with the pattern of the true source only in the case of SPoC (middle panel). Source localization of the sensor-space patterns using dipole fitting reveals that SPoC patterns can be well explained by dipoles that are close to the location of the true simulated dipole (right panel). The simulation were repeated with new data 100 times for each signal-to-noise ratio (SNR). The results shown were obtained on test data that was not used to train (i.e., to optimize the parameters of) the algorithms. Lines correspond to means over repetitions, and errorbars correspond to
10⋅SE
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7214335/7182735/samek7-2425807-large.gif
2015,7051329,Fig. 1.,Flowchart of OSML: an on-site implemented predictor for protein-ligand binding sites prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/7051314/7051329/7051329-fig-1-source-large.gif
2015,7051329,Fig. 2.,"Prediction efficiency comparisons between the proposed OSML and staticsvm on level i, level ii, and level III tests.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/7051314/7051329/7051329-fig-2-source-large.gif
2015,6894163,Fig. 1.,Structure of the parent-offspring progressive learning method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang1-2352594-large.gif
2015,6894163,Fig. 2.,Steps of Condition 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang2-2352594-large.gif
2015,6894163,Fig. 3.,Structure of proposed Algorithm 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang3-2352594-large.gif
2015,6894163,Fig. 4.,Region estimation for testing data points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang4-2352594-large.gif
2015,6894163,Fig. 5.,"Average testing RMSE when training I-ELM, ELM, EM-ELM, B-ELM, PC-ELM, and the proposed method on Parkin, California House, and Puma, where the
x
- and
y
-axes show the number of hidden nodes and average testing RMSE. Result on (a) Parkin, (b) California House, and (c) Puma.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang5abc-2352594-large.gif
2015,6894163,Fig. 6.,"Average testing accuracy when training ELM, EM-ELM, I-ELM, and the proposed method on Acoustic, Poker, Connect4, and A9a, where the
x
- and
y
-axes show the number of hidden nodes and average testing accuracy, respectively. Result on (a) Parkin, (b) California House, and (c) Puma.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang6abc-2352594-large.gif
2015,6894163,Fig. 7.,"2000 Samples classification results when training ELM and the proposed method on Cod-RNA, where the
x
- and
y
-axes show the first dimension value of Cod-RNA dataset and the second dimension value of cod-RNA dataset, respectively. These 2000 points are selected randomly from all 180000 testing samples. (a) True partition. (b) Estimated partition by PPLM. (c) Estimated partition by ELM. (d) Optimal partitions information. (e) Classification results by PPLM. (f) Classification results by ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang7abcdef-2352594-large.gif
2015,6894163,Fig. 8.,"Average testing accuracy when training
MultiSVM
light
(linear),
MultiSVM
light
(RBF),
MultiSVM
perf
(linear),
MultiSVM
perf
(RBF), CAPO, I-ELM, ELM, and the proposed method on A9a, W3a, and IJCNN, where the
x
- and
y
-axes show the
C
values and average testing accuracy, respectively. Result on (a) A9a, (b) W3a, and (c) IJCNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang8abc-2352594-large.gif
2015,6894163,Fig. 9.,"Time complexity when training
MultiSVM
light
(linear),
MultiSVM
light
(RBF), CAPO and the proposed method on Cod-RNA and Covapp, where the
x
- and
y
-axes show the number of training samples and CPU time (in seconds), respectively. Result on (a) Cod-RNA and (b) Covapp.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang9ab-2352594-large.gif
2015,6894163,Fig. 10.,"Performance and CPU time (in seconds) of the proposed method with different parameters
n,
i
max
,
j
max
,S,λ
. Each subfigure shows performance in the first row and corresponding CPU time in the second row. Result on (a) Bank32, (b) Puma, (c) Pole, (d) Bank32, (e) Puma, and (f) Pole.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7156182/6894163/yang10abcdef-2352594-large.gif
2015,6847217,Fig. 1.,"Basic frameworks of traditional machine learning approaches and knowledge transfer approaches. For regular machine learning approaches, the learning system can only handle the situation that testing samples and training samples are under the same distribution. On the other hand, transfer learning approaches have to deal with the data distribution mismatch problem through specific knowledge transfer methods, e.g., mining the shared patterns from data across different domains.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7086401/6847217/li1-2330900-large.gif
2015,6847217,Fig. 2.,Different ways of differentiating existing knowledge transfer approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7086401/6847217/li2-2330900-large.gif
2015,6847217,Fig. 3.,"Top row: cross-domain knowledge transfer scenario. In the target domain, the walking action performed by a single player in clean backgrounds comes from the KTH data set, while in the target domain, the walking action captured from much more complicated backgrounds with multiple players comes from the TRECVID data set. Bottom row: cross-view knowledge transfer scenario, where the target view data and the source view data are the same action captured from two different views of the IXMAS data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7086401/6847217/li3-2330900-large.gif
2015,6847217,Fig. 4.,"Knowledge transfer from multiple auxiliary domains. (a) Auxiliary domain 1. (b) Auxiliary domain 2. (c) Target domain. The two subfigures on the left denote the two different auxiliary domain data and their corresponding decision boundaries, where auxiliary domain 1 is partitioned by a horizontal line and auxiliary domain 2 is partitioned by a vertical line. By brutally combining the decision boundaries from the two auxiliary domains, ambiguous predictions will be caused in the top-left region and the bottom-right region of the target domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7086401/6847217/li4-2330900-large.gif
2015,7001711,Fig. 1.,Geometric interpretation of TWSVC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7001711/wang1-2379930-large.gif
2015,7001711,Fig. 2.,Illustration of the effectiveness of linear TWSVC with different parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7001711/wang2-2379930-large.gif
2015,7001711,Fig. 3.,"Illustration of the effectiveness of nonlinear TWSVC with different parameters on (i)–(v) Dermatology, (vi)–(x) Ecoli, and (xi)–(xv) Haberman.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7001711/wang3-2379930-large.gif
2015,6803073,Fig. 1.,Decision boundaries of the SVM (green) and a prototype-based model (blue) on a toy data set. Circles are the support vectors and crosses are the prototypes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6803073/zhang1-2315526-large.gif
2015,6803073,Fig. 2.,Performance of the PVM with different sample sizes. (a) Training time versus sample size (note that the error bars here are very small). (b) Error versus sample size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6803073/zhang2ab-2315526-large.gif
2015,6803073,Fig. 3.,Performance of the PVM at different settings. (a) Different numbers of labeled samples per class. (b) Different numbers of prototypes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6803073/zhang3ab-2315526-large.gif
2015,6803073,Fig. 4.,"Performance of PVM and the
ℓ
1
-penalized formulation at different levels of model sparsity. The
x
-axis is the ratio of nonzero coefficients in the predictive model to the sample size. (a) BCI. (b) COIL. (c)
COIL
2
. (d) Digit1. (e) USPS-2v7. (f) USPS-5v6. (g) USPS-3v8. and (h) USPS-FULL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6803073/zhang4abcdefgh-2315526-large.gif
2015,6963477,Fig. 1.,"Experimental setup: (a) Human–computer interaction between the subject and aCAMS virtual environment. (b) Four subsystems that control the air quality of the closed cabin, O2, P, CO2, and T (top-down).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang1ab-2366914-large.gif
2015,6963477,Fig. 2.,The example of a trial of data collection experiment: (a) Task-load conditions. (b) Performance variable: TIR. (c) Performance variable: DEV (here sampling interval is 5 s).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang2abc-2366914-large.gif
2015,6963477,Fig. 3.,Block diagram of the data preprocessing scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang3-2366914-large.gif
2015,6963477,Fig. 4.,"Comparison of data (F3 theta power of EEG, trial 1, subject A) smoothing results by using different methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang4-2366914-large.gif
2015,6963477,Fig. 5.,"Tenfold CV accuracy (ACC) of different data smoothing methods combined with BSVM for each subject. Here, the original, MA, LR, LS-filter, ES, AES1, and AES2 methods represent “without data smoothing,” “moving average,” “local regression,” “least-square smoothing filter,” “exponential smoothing,” “AES schemes with function 1,” and “AES schemes with function 2,” respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang5-2366914-large.gif
2015,6963477,Fig. 6.,Scheme of cross-trial and cross-subject MWL classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang6-2366914-large.gif
2015,6963477,Fig. 7.,"| r | between the 82 original features and the LPP-mapped features ( z1 and z2). The frequency bands of EEG and EOG features are marked. #81 and #82 features are two ECG-related features (i.e., HR and HRV).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang7-2366914-large.gif
2015,6963477,Fig. 8.,Average Silhouette index versus the number of clusters (k) used in k -means clustering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang8-2366914-large.gif
2015,6963477,Fig. 9.,"Classification testing accuracy (ACC): (a) Three-level MWL classification. (b) Four-level MWL classification. The results of combinations of different data smoothing (i.e., AES1, AES2, and using the original feature data) and classification approaches (i.e., BSVM and ABSVM) are compared quantitatively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang9ab-2366914-large.gif
2015,6963477,Fig. 10.,"Box–whisker plot of three-level MWL classification based on a cross-subject classifier. Six combinations of data smoothing and classification approaches are compared. Three lines in the box from top to bottom give the values of the upper quartile, the median, and the lower quartile for column data, respectively. Two additional lines at both ends of one column data indicate the whisker range of maximal to minimal values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang10-2366914-large.gif
2015,6963477,Fig. 11.,Box–whisker plot of four-level MWL classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang11-2366914-large.gif
2015,6963477,Fig. 12.,Cross-subject predictive classification results of the AES2-ABSVM scheme (subject: D): three-level classification result of (a) trial 1 and (b) trial 2 and four-level classification result of (c) trial 1 and (d) trial 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7059258/6963477/zhang12-2366914-large.gif
2015,6861448,Fig. 1.,"Performance comparison (training RMSE) with ELM,
L
2
ELM, and PolyELM for Abalone data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu1-2335212-large.gif
2015,6861448,Fig. 2.,"Performance comparison (testing RMSE) with ELM,
L
2
ELM, and PolyELM for Abalone data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu2-2335212-large.gif
2015,6861448,Fig. 3.,"Performance comparison (testing RMSE) with ELM,
L
2
ELM, and PolyELM for Abalone data set in detail.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu3-2335212-large.gif
2015,6861448,Fig. 4.,"Performance comparison (training RMSE) with ELM,
L
2
ELM, and PolyELM for Machine CPU data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu4-2335212-large.gif
2015,6861448,Fig. 5.,"Performance comparison (testing RMSE) with ELM,
L
2
ELM, and PolyELM for Machine CPU data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu5-2335212-large.gif
2015,6861448,Fig. 6.,"Performance comparison (testing RMSE) with ELM,
L
2
ELM, and PolyELM for Machine CPU data set in detail.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu6-2335212-large.gif
2015,6861448,Fig. 7.,"Performance comparison (training RMSE) with ELM,
L
2
ELM, and PolyELM for Census (house8L) data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu7-2335212-large.gif
2015,6861448,Fig. 8.,"Performance comparison (testing RMSE) with ELM,
L
2
ELM, and PolyELM for Census (house8L) data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu8-2335212-large.gif
2015,6861448,Fig. 9.,"Performance comparison (testing RMSE) with ELM,
L
2
ELM, and PolyELM for Census (house8L) data set in detail.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6861448/liu9-2335212-large.gif
2015,7093402,Fig. 1.,FE model of the induction machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou1-2364031-large.gif
2015,7093402,Fig. 2.,"Reference constraint map versus
Qr
and
δ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou2-2364031-large.gif
2015,7093402,Fig. 3.,Interpolated map with 25 points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou3-2364031-large.gif
2015,7093402,Fig. 4.,Workflow of the design method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou4-2364031-large.gif
2015,7093402,Fig. 5.,Overview of the ML algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou5-2364031-large.gif
2015,7093402,Fig. 6.,"Graph of the entropy
H
versus the probability
k
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou6-2364031-large.gif
2015,7093402,Fig. 7.,Example of a decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou7-2364031-large.gif
2015,7093402,Fig. 8.,Constraint map learned from 25 points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/7092981/7093402/arnou8-2364031-large.gif
2015,7047917,Fig. 1.,Restricted Boltzmann Machine (RBM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen1-2406889-large.gif
2015,7047917,Fig. 2.,Symmetric triangular fuzzy number.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen2-2406889-large.gif
2015,7047917,Fig. 3.,Fuzzy restricted Boltzmann Machine (FRBM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen3-2406889-large.gif
2015,7047917,Fig. 4.,"k
-step Gibbs sampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen4-2406889-large.gif
2015,7047917,Fig. 5.,BAS benchmark.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen5-2406889-large.gif
2015,7047917,Fig. 6.,Learning processes of RBM and FRBM based on BAS dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen6-2406889-large.gif
2015,7047917,Fig. 7.,BAS benchmark inpainting based on RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen7-2406889-large.gif
2015,7047917,Fig. 8.,BAS benchmark inpainting based on FRBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen8-2406889-large.gif
2015,7047917,Fig. 9.,"BAS benchmark inpainting with
20%
noises based on RBM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen9-2406889-large.gif
2015,7047917,Fig. 10.,"BAS benchmark inpainting with
20%
noises based on FRBM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen10-2406889-large.gif
2015,7047917,Fig. 11.,Learning processes of RBM and FRBM based on MNIST dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen11-2406889-large.gif
2015,7047917,Fig. 12.,MNIST samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen12-2406889-large.gif
2015,7047917,Fig. 13.,"Reconstructed MNIST samples after
20
epoch with
100
hidden units.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7047917/chen13-2406889-large.gif
2015,7061463,Fig. 1.,Active-learning guided SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin1-2413840-large.gif
2015,7061463,Fig. 2.,"(a) Hard margin SVM. Circles and squares are instances of the two classes.
f(x)=0
is the separating hyperplane. (b) Differences between soft and hard margins SVM. The dotted line is the hyperplane of hard margin SVM with margin
m
′
. The solid line is the hyperplane of soft margin SVM with larger margin
m
. The solid circle and square violate the margin
m
with slack variable
ξ
a
and
ξ
b
, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin2ab-2413840-large.gif
2015,7061463,Fig. 3.,"Version space in 2-D
W
space. (a)
x
1
–
x
3
are cutting hyperplane associated with training data.
x
1
and
x
2
are support vectors. The solid arc is the version space. (b)
w
^
∗
and version space between
x
1
and
x
2
corresponds to hard margin SVM. In soft margin SVM, hyperplane of
x
2
is shift to
x
′
2
with a vertical offset of
ξ
2
/∥w∥
. The version space is enlarged by the arc between
x
2
and
x
3
.
w
^
∗′
corresponds to the optimal classifier of soft margin SVM.
x
3
becomes the support vector as well.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin3ab-2413840-large.gif
2015,7061463,Fig. 4.,Simplified model of an LC-based dc/dc converter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin4-2413840-large.gif
2015,7061463,Fig. 5.,"Sampling procedure of active learning in the parameter space
L : [1 nH,1 uH]×C : [10 nF,10 uF]
(red star denotes positive instances, blue circle denotes negative instances, and square denotes initial training samples). (a) 20, (b) 50, (c) 100, and (d) 200 samples selected. (e) 10 000 random instances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin5abcde-2413840-large.gif
2015,7061463,Fig. 6.,"Comparison in the required number of simulations (AL, active learning; RS, random sampling) where the global accuracy is defined as the fraction of the correct predictions on 10 K random test samples (e.g., 0.1 means 10%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin6-2413840-large.gif
2015,7061463,Fig. 7.,Block diagram of PLL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin7-2413840-large.gif
2015,7061463,Fig. 8.,Timing diagram of the PD and the resulting output of the charge pump and loop filter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin8-2413840-large.gif
2015,7061463,Fig. 9.,"Results of PLL experiments (AL, active learning; RS, random sampling) where the global accuracy and the error on positive class are defined as the fraction of the correct predictions, with a test set whose size is 100 K (e.g., 0.1 means 10%). Number of simulations for (a) certain accuracy and (b) safe prediction. Runtime to achieve (c) certain accuracy and safe prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin9abcd-2413840-large.gif
2015,7061463,Fig. 10.,"Samples selected by active learning. (a) Projection of the selected samples in the continuous 3-D
(
V
1
,
V
2
,Δϕ)
space, where
V
1
and
V
2
are the initial voltages on the two capacitors, and
Δϕ
is the initial phase difference. (b) Projection of the selected samples in the
(
V
1
,
V
2
)
space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin10ab-2413840-large.gif
2015,7061463,Fig. 11.,Transistor level schematic of the three-inverter ring oscillator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin11-2413840-large.gif
2015,7061463,Fig. 12.,Functional blocks placement and sensor positions and experiment flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin12-2413840-large.gif
2015,7061463,Fig. 13.,"Results of chip thermal experiments (AL, active learning; RS, random sampling) where the global accuracy and the error on positive class are defined as the fractions of the correct predictions, using a test data set whose size is 100 K (e.g., 0.1 means 10%). Number of simulations for (a) certain accuracy and (b) safe prediction. Runtime to achieve (c) certain accuracy and (d) safe prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7208927/7061463/lin13abcd-2413840-large.gif
2015,6868201,Fig. 1.,"Instances, attributes, and class labels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf1-2344095-large.gif
2015,6868201,Fig. 2.,Typical attacks on two-way classification problems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf2-2344095-large.gif
2015,6868201,Fig. 3.,"Malicious attribute value derivation example (
g=10
,
Υ∈{
Class 1, Class 2
}
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf3-2344095-large.gif
2015,6868201,Fig. 4.,Results of attacks on the Thyroid Disease dataset for the fixed (solid line) and evolving (dashed line) cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf4-2344095-large.gif
2015,6868201,Fig. 5.,Results of attacks on the Breast Cancer dataset for the fixed (solid line) and evolving (dashed line) cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf5-2344095-large.gif
2015,6868201,Fig. 6.,Results of attacks on the Acute Inflammations dataset for the fixed (solid line) and evolving (dashed line) cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf6-2344095-large.gif
2015,6868201,Fig. 7.,Results of attacks on the Echocardiogram dataset for the fixed (solid line) and evolving (dashed line) cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf7-2344095-large.gif
2015,6868201,Fig. 8.,Results of attacks on the Molecular Biology dataset for the fixed (solid line) and evolving (dashed line) cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf8-2344095-large.gif
2015,6868201,Fig. 9.,"Attack structure for a four-way classification problem, e.g., on the Hypothyroid dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/6868201/mozaf9-2344095-large.gif
2015,6981937,Fig. 1.,"Pipeline of our proposed kinship verification approach. First, we construct a set of face samples from the LFW dataset as the prototypes and represent each face image from the kinship dataset as a combination of these prototypes in the hyperplane space. Then, we use the labeled kinship information and learn mid-level features in the hyperplane space to extract more semantic information for feature representation. Lastly, the learned hyperplane parameters are used to represent face images in both the training and testing sets as a discriminative mid-level feature for kinship verification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/6981937/lu1-2376934-large.gif
2015,6981937,Fig. 2.,"Some sample positive pairs (with kinship relation) from different face kinship databases. Face images from the first to fourth row are from the KinFaceW-I [42], KinFaceW-II [42], Cornell KinFace [16], and the University of Buffalo (UB) KinFace [50] databases, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/6981937/lu2-2376934-large.gif
2015,6981937,Fig. 3.,"Mean verification rate of our PDFL and MPDFL versus different values of
K
and
γ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/6981937/lu3-2376934-large.gif
2015,6981937,Fig. 4.,"Training time of our MPDFL versus different values of
K
and
γ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/6981937/lu4-2376934-large.gif
2015,6981937,Fig. 5.,"Mean verification rate of our PDFL and MPDFL versus different number of iterations, on the KinFaceW-I dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/6981937/lu5-2376934-large.gif
2015,6981937,Fig. 6.,"Mean verification rate of our MPDFL versus different values of
r
on different kinship face datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/6981937/lu6-2376934-large.gif
2015,6775340,Fig. 1.,"Experimental results for the Robot data set. Objective function value of each task as a function of
p
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li1-2309939-large.gif
2015,6775340,Fig. 2.,"Experimental results for the Robot data set. DCR for each task with changing
p
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li2-2309939-large.gif
2015,6775340,Fig. 3.,"Experimental results for the Robot data set. Black solid curve: the overall multiclass classification accuracy as
p
varies. Red dashed line: the accuracy obtained, when
p=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li3-2309939-large.gif
2015,6775340,TABLE I,"Comparison of Multiclass Classification Accuracy Between
p<1
,
p=1
, and
p→∞",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li.t1-2309939-large.gif
2015,6775340,TABLE II,Comparison of Multiclass Classification Accuracy Between Our Method and Four Other Models,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li.t2-2309939-large.gif
2015,6775340,TABLE III,"Comparison of Multitask Classification Accuracy Between
p<1
,
p=1
, and
p→∞",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li.t3-2309939-large.gif
2015,6775340,TABLE IV,Comparison of Multitask Classification Accuracy Between Our Method and Four Other Models,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6990695/6775340/li.t4-2309939-large.gif
2015,7018034,Fig. 1.,"Semantic gap. (a) City and countryside scenes both contain vehicle and building. (b) City and countryside scenes both contain cow and building. Each pair of images in (a) and (b) share similar objects, but their scene categories (labels) are totally different.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan1ab-2359471-large.gif
2015,7018034,Fig. 2.,Framework overview. (a) Base unit. (b) Manifold regularized deep architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan2ab-2359471-large.gif
2015,7018034,Fig. 3.,Sample images of 15 scene categories.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan3-2359471-large.gif
2015,7018034,Fig. 4.,Sample images of eight event categories.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan4-2359471-large.gif
2015,7018034,Fig. 5.,2-D visualizations of embedding result from two-layer deep architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan5-2359471-large.gif
2015,7018034,Fig. 6.,2-D visualizations of embedding result from three-layer deep architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan6-2359471-large.gif
2015,7018034,Fig. 7.,2-D visualizations of embedding result from four-layer deep architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7018034/yuan7-2359471-large.gif
2015,7138580,Fig. 1.,"(a) Raw spatial-temporal data, (b) spatial-temporal data normalized using DS equations, and (c) spatial-temporal data normalized using a MR approach. Data are shown for PD patients and aged-matched controls, with standard deviation (SD) values given (whiskers). Significant differences in gait features between PD patients and controls are indicated with one asterisk (p < 0.05), two asterisks (p < 0.01), and three asterisks (p < 0.001). In order to fit all data onto a single plot, data were scaled between 0 and 1. All data are dimensionless.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7138580/ackla1-2450232-large.gif
2015,7138580,Fig. 2.,"ROC curves for various machine learning strategies applied to (a) raw data, (b) data normalized using DS equations, and (c) data normalized using a MR approach. Data are shown for the following machine learning strategies: KFD, SVM, naïve BA, RF, and kNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7138580/ackla2-2450232-large.gif
2015,6932476,Fig. 1.,"Classification error rate (%) versus
m
2
for (a) doublet-SVM and (b) triplet-SVM with
m
1
=1
and
C=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang1ab-2361142-large.gif
2015,6932476,Fig. 2.,"Classification error rate (%) versus
m
1
(=
m
2
)
for (a) doublet-SVM and (b) triplet-SVM with
C=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang2ab-2361142-large.gif
2015,6932476,Fig. 3.,"Classification error rate (%) versus
C
for (a) doublet-SVM and (b) triplet-SVM with
m
1
=
m
2
=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang3ab-2361142-large.gif
2015,6932476,Fig. 4.,"Training time (s) of doublet-SVM, NCA, ITML, MCML, and LDML. From 1 to 10, the dataset ID represents Parkinsons, sonar, Statlog segmentation, breast tissue, ILPD, Statlog satellite, blood transfusion, SPECTF heart, cardiotocography, and letter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang4-2361142-large.gif
2015,6932476,Fig. 5.,"Training time (s) of triplet-SVM and LMNN. From 1 to 10, the dataset ID represents Parkinsons, sonar, Statlog segmentation, breast tissue, ILPD, Statlog satellite, blood transfusion, SPECTF heart, cardiotocography, and letter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang5-2361142-large.gif
2015,6932476,Fig. 6.,"Training time (s) of doublet-SVM, NCA, ITML, MCML and LDML. From 1 to 3, the dataset ID represents USPS, MNIST, and Semeion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang6-2361142-large.gif
2015,6932476,Fig. 7.,"Training time (s) of triplet-SVM and LMNN. From 1 to 3, the dataset ID represents USPS, MNIST, and Semeion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang7-2361142-large.gif
2015,6932476,Fig. 8.,"Performance comparison of the seven metric learning methods using the Bonferroni-Dunn test. Groups of methods that are not significantly different (at
p=0.05
) are connected.
CD
refers to the critical difference between the average ranks of two methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6932476/zhang8-2361142-large.gif
2015,7000606,Fig. 1.,Noncomprehensive list of ELM models. The contribution of this brief is highlighted in boldface red.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/7000606/scard1-2382094-large.gif
2015,7000606,Fig. 2.,"Architecture of a basic ELM model. For simplicity, a single input and output are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/7000606/scard2-2382094-large.gif
2015,7000606,Fig. 3.,Precision-recall curve for the Sylva dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/7000606/scard3-2382094-large.gif
2015,6913540,Fig. 1.,Cognitive model pursued in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy1-2354400-large.gif
2015,6913540,Fig. 2.,Histogram of all solutions by using an exhaustive search.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy2-2354400-large.gif
2015,6913540,Fig. 3.,"Directed graph with seven nodes and 36 edges that correspond to values in matrix
V
(13). To simplify the drawing we used bidirectional edges.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy3-2354400-large.gif
2015,6913540,Fig. 4.,Run time for different algorithms (with std-dev).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy4-2354400-large.gif
2015,6913540,Fig. 5.,Overall pain for different algorithms (with std-dev).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy5-2354400-large.gif
2015,6913540,Fig. 6.,Agent walking toward a target resource.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy6-2354400-large.gif
2015,6913540,Fig. 7.,"Total average pain comparison for standard ML, linear OML, quadratic OML, and exhaustive ML algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy7-2354400-large.gif
2015,6913540,Fig. 8.,Frequency of pains above threshold. Left: ML. Right: linear OML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7159131/6913540/jachy8-2354400-large.gif
2015,7063936,Fig. 1.,Impact of number of neighbors on model misclassification error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha1-2405759-large.gif
2015,7063936,Fig. 2.,Impacts of regularization and Gaussian parameters on model accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha2-2405759-large.gif
2015,7063936,Fig. 3.,Illustration of a single decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha3-2405759-large.gif
2015,7063936,Fig. 4.,Impact of number of trees on the misclassification error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha4-2405759-large.gif
2015,7063936,Fig. 5.,Impact of number of features on the misclassification error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha5-2405759-large.gif
2015,7063936,Fig. 6.,Feature importance for two different measures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha6-2405759-large.gif
2015,7063936,Fig. 7.,Model comparison results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7277123/7063936/rakha7-2405759-large.gif
2015,7021915,Fig. 1.,Data-driven RUL estimation strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed1-2378056-large.gif
2015,7021915,Fig. 2.,Simultaneous prediction and discrete state estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed2-2378056-large.gif
2015,7021915,Fig. 3.,Illustration of predictability measure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed3-2378056-large.gif
2015,7021915,Fig. 4.,Machine learning view of SW-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed4-2378056-large.gif
2015,7021915,Fig. 5.,Iterative model for multisteps predictions [33].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed5-2378056-large.gif
2015,7021915,Fig. 6.,Discrete state estimation from multidimensional data. (a) Learning phase. (b) Testing phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed6ab-2378056-large.gif
2015,7021915,Fig. 7.,Enhanced multivariate degradation based modeling strategy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed7-2378056-large.gif
2015,7021915,Fig. 8.,Off-line phase. (a) Learning univariate predictors. (b) Building multivariate unsupervised classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed8ab-2378056-large.gif
2015,7021915,Fig. 9.,On-line phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed9-2378056-large.gif
2015,7021915,Fig. 10.,Sensor measurement from 100 engines (training data) and life spans.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed10-2378056-large.gif
2015,7021915,Fig. 11.,RUL error interval [16].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed11-2378056-large.gif
2015,7021915,Fig. 12.,RUL estimation with all features ({F1–F8})—turbofan engine 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed12-2378056-large.gif
2015,7021915,Fig. 13.,Visualization of classes and data point membership functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed13-2378056-large.gif
2015,7021915,Fig. 14.,Dynamic FTs assignment results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed14-2378056-large.gif
2015,7021915,Fig. 15.,Actual RUL versus estimated RUL (100 tests).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed15-2378056-large.gif
2015,7021915,Fig. 16.,RUL error distribution (a) proposed approach and (b) by [16].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed16ab-2378056-large.gif
2015,7021915,Fig. 17.,Scores for 100 tests (enhanced multivariate degradation modeling).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7328787/7021915/javed17-2378056-large.gif
2015,6816040,Fig. 1.,Performance comparison whether to use RSS technology on the 2-D toy data. (a) True classes. (b) SOR (0.511 s). (c) SOR with RSS (0.476 s). (d) True classes. (e) SOR (0.0023 s). (f) SOR with RSS (0.0018 s).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6816040/tian1abcdef-2320738-large.gif
2015,6816040,Fig. 2.,"Average classification errors of SVM, PlapSVM, PLapSVM-PCG, and FLapSVM on eight real data sets.
U
: unlabeled data set.
V
: validation data set.
T
: test data set. Due to the results of PLapSVM-Newton and PLapSVM-PCG too close, we only give the result of PLapSVM-PCG. (a) G50C. (b) COIL20(B). (c) PCMAC. (d) USPST(B). (e) COIL20. (f) USPST. (g) MNIST3VS8. (h) FACEMIT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6816040/tian2abcdefgh-2320738-large.gif
2015,6816040,Fig. 3.,"Training time comparison of PLapSVM, PLapSVM-Newton, PLapSVM-PCG, and FLapSVM on eight real data sets. 1: PLapSVM. 2: PLapSVM-Newton. 3: PLapSVM-PCG. 4: FLapSVM. (a) G50C. (b) COIL20(B). (c) PCMAC. (d) USPST(B). (e) COIL20. (f) USPST. (g) MNIST3VS8. (h) FACEMIT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6816040/tian3abcdefgh-2320738-large.gif
2015,7349136,FIGURE 1.,Architecture of CC-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7349136/shukl1-2506601-large.gif
2015,7349136,FIGURE 2.,"Display of test G-mean of glass0 dataset for RWCC-ELM (a) when
λ
and L varies. (b) when L varies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7349136/shukl2ab-2506601-large.gif
2015,7181652,Fig. 1.,"CASAS smart home testbed. The floorplan of the testbed first floor is shown on the top. Motion sensors are indicated in the floorplan by red circles (example motion sensors are shown lower left), light sensors by eight-pointed stars, door sensors by green rectangles (example door sensors are shown lower right), and temperature sensors by five-pointed stars. The testbed also contains vibration sensors placed on individual items throughout the home.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7181652/cook1-2461659-large.gif
2015,7181652,Fig. 2.,Experimenter observes a participant performing activities via web cameras and logs information using the RAT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7181652/cook2-2461659-large.gif
2015,7181652,Fig. 3.,"Summary of Analysis 1 groups: box plots (left) and pdf histogram plots (right) for the cook and TUG with name generation. PD = Parkinson's Disease population group, HOA = Healthy Older Adults.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7181652/cook3-2461659-large.gif
2015,7181652,Fig. 4.,"Scatter plots of the cook, TUG with name generation, sweep and dust, and water plants activity plotting duration as a function of age with trend lines for healthy older adult participants (blue circles) and Parkinson's Disease participants (red triangles).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7181652/cook4-2461659-large.gif
2015,7067419,Fig. 1.,"Optical flow correction for head movement. (a) and (b) are two consecutive frames in a video; (a) also shows the proposed face coordinate system, where the nose tip is considered as the origin and the reference vector connects the nose tip to the midpoint between the centres of the two eyes; (c) total optical flow (
U
tot
) illustrated in blue and the head movement optical flow (
U
head
) indicated in green; (d) emotion related optical flow (
U
emo
) illustrated in red; (e)
U
tot
of mouth region; and (f)
U
emo
of mouth region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja1abcdef-2416634-large.gif
2015,7067419,Fig. 2.,"(a) Illustration of projection feature (
Proj
) for sad (left) and happy (right) emotions; and (b) illustration of rotation feature (
Rot
) for happy (left) and anger (right) emotions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja2ab-2416634-large.gif
2015,7067419,Fig. 3.,"Spatio-temporal descriptor construction. (a) Volume data is divided into a number of 3D blocks (
B
m
) and the final descriptor (
y
) is a concatenation of features from all the blocks. (b) Each block is further divided into number of 3D cells (
C
n
) and the feature vector of each block (
f
B
m
) is a concatenation of all the cell histograms within that block. (c) Weighted and un-weighted histograms are calculated for each cell based on the four spatio-temporal features and concatenated to obtain the cell histogram.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja3abc-2416634-large.gif
2015,7067419,Fig. 4.,High-level outline of the proposed ESL framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja4-2416634-large.gif
2015,7067419,Algorithm 1,Training Algorithm for Extreme Sparse Learning,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja11-2416634-large.gif
2015,7067419,Algorithm 2,Classification Algorithm for Extreme Sparse Learning,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja12-2416634-large.gif
2015,7067419,Algorithm 3,Class Specific Matching Pursuit,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja13-2416634-large.gif
2015,7067419,Fig. 5.,"Sample frames of a single subject from our own collected data. The top row shows some examples of pose variation, the middle row depicts occlusion examples, and the bottom row includes illumination variations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja5-2416634-large.gif
2015,7067419,Fig. 6.,Recognition rate of ESL as a function of the number of dictionary atoms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja6-2416634-large.gif
2015,7067419,Fig. 7.,Pose-invariant descriptor for happy and surprise emotions. (a) Features extracted from the lip segment from frontal (left) and non-frontal (right) faces with happy emotion. (b) Features extracted from the lip segment from frontal (left) and non-frontal (right) faces with surprise emotion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja7ab-2416634-large.gif
2015,7067419,Fig. 8.,"Comparison of accuracy when the different classifiers are trained on the original CK+ database and applied to data collected by us, which includes occlusion, head posed variations, and illumination changes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja8-2416634-large.gif
2015,7067419,Fig. 9.,"Sources of failure of the proposed emotion recognition system. (a) Failure of face detector to detect the whole face; (b) failure to detect all faces in an emotion sequence, leading to a scenario where the detected faces do not capture the dynamics of the emotion; (c) wrong reference point localization. The detected nose points are represented as red dots.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja9abc-2416634-large.gif
2015,7067419,Fig. 10.,Failure to estimate the correct optical flow; (a)-(b) two consequent frames; (c) estimated optical flow; (d) magnification of optical flow for nose region indicating errors in OF extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7067419/shoja10abcd-2416634-large.gif
2015,6832616,Fig. 1.,"ESN. In the typical setup, the inputs are fully connected to a randomly generated neuronal reservoir (obeying the echo-state property) with a hyperbolic tangent activation function. The outputs are also fully connected to the reservoir with weights learned via linear regression.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh1-2316291-large.gif
2015,6832616,Fig. 2.,"OESGP, which learns online from temporal sequences and produces predictive distributions. The OESGP uses a finite reservoir as a spatio-temporal kernel to compute covariances between time series.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh2-2316291-large.gif
2015,6832616,Fig. 3.,OIESGP uses a recursive kernel with ARD for multivariate time series where dimensions may have varying importance. The hyperparameters of this new kernel can be optimized in an online manner via SNGD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh3-2316291-large.gif
2015,6832616,Fig. 5.,"Adaptation and performance sensitivity to recursive kernel hyperparameters. (a) Optimal hyperparameters (in terms of RMSE) are in a valley with
l≈0.2
and
σ
−1
ρ
≈0.5
. (b) Online gradient descent minimized the error scores for the region where
σ
−1
ρ
<1
, flattening the error hill. See main text for details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh5ab-2316291-large.gif
2015,6832616,Fig. 4.,"Online hyperparameter adaptation while learning to predict the NARMA-10 system with two irrelevant input dimensions. (a) First 100 time-steps of the three inputs and output. The true output (solid blue line) is hidden and only the noise corrupted outputs (circles) are observed. Only
x
1
is relevant for generating
y
; the other two inputs are irrelevant. (b) Blue-shaded region indicates when hyperparameter optimization was performed; it automatically stopped after the convergence criteria were met. The hyperparameters change to reflect the underlying characteristics of the generating system. (c) Online adaptation led to 29% lower prediction error and a fivefold increase in the likelihood compared with the OIESGP with fixed hyperparameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh4abc-2316291-large.gif
2015,6832616,Fig. 6.,"Optimization of the hyperparameters requires gradient computations, which results in an increased CPU load. Periodic kernel matrix inversions led to the spikes after each sampling interval of 25. After convergence (at
t≈5000
), the required computational time decreased to match the OIESGP with fixed hyperparameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh6-2316291-large.gif
2015,6832616,Fig. 7.,"CPU time per iteration for testing and training. Overall, LWPR was the fastest algorithm for a majority of the problems considered. Among the online GPs, the full ARD kernel was the most expensive to train and the isotropic kernel was the cheapest. Comparing the finite and infinite online ESGPs, the OIESGP was on average cheaper than the OESGP for predictions but required longer optimization periods (and hence higher average training time).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh7-2316291-large.gif
2015,6832616,Fig. 8.,"(a) True states of the rotating dynamical system live on a unit circle but give rise to noisy observations. Two distinct sets of points, A and B, give rise to the same observation (the plane
y
t
=0
). (b) Probability of the next observation conditioned upon the current one is bimodal, violating assumptions made by typical regression methods. This bimodality occurs because of state aliasing caused by partial observations. (c) Coordinate delay embedding reconstructs the state space sufficiently well that unimodality is recovered and predictions can be performed accurately.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh8abc-2316291-large.gif
2015,6832616,Fig. 9.,"MI between the next observation (to be predicted) and the delay vector (sliding-window) grows with the delay length approaching that of the underlying state. As we would expect, a delay vector constructed using the underlying state does not increase the MI. The reservoir shows a different initial profile; a increase from three to four neurons results a sudden jump in informativeness.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh9-2316291-large.gif
2015,6832616,Fig. 10.,"Lazy-8 LbD experimental setup with the Nao humanoid robot with the collected trajectories [45]. Our goal is to learn the (shoulder pitch/roll, elbow yaw/roll) joint velocities needed to produce the figure-8s demonstrated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh10-2316291-large.gif
2015,6832616,Fig. 11.,"Joint angles and velocities for the four left arm joints. Blue lines: shoulder-pitch. Red lines: shoulder-roll. Green: elbow-yaw. Black lines: elbow-roll. For our experiments, the raw angles (left) were inputs and velocities (right) were outputs. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh11ab-2316291-large.gif
2015,6832616,Fig. 12.,"Falcon placement on the ARTY smart wheelchair with a sample demonstration using the 3-DoF Falcon haptic controller. The
x
-axis controlled the forward and backward speeds where else the
y
-axis controlled the turning velocities. The
z
-axis was not used in this paper.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh12-2316291-large.gif
2015,6832616,Fig. 13.,"Experimental area for wheelchair driving case study with recorded trajectories. Demonstrators were told to drive from S to F, passing through the waypoints (blue dots).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh13-2316291-large.gif
2015,6832616,Fig. 14.,Sample predictions for each of the Falcon axes. Black dashed lines: demonstration values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7042870/6832616/soh14-2316291-large.gif
2015,7052357,Fig. 1.,System diagram of our proposed classification framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7122954/7052357/shen1-2404809-large.gif
2015,7052357,Fig. 2.,Illustration on the proposed DTFS and DTSS models. (a) Using DTFS to select discriminative brain regions. (b) Using DTSS to select informative subjects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7122954/7052357/shen2ab-2404809-large.gif
2015,7052357,Fig. 3.,"ROC curves of different methods for MCI-C/MCI-NC classification with multimodality and single-modality data, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7122954/7052357/shen3-2404809-large.gif
2015,7052357,Fig. 4.,ROC curves of different variants of our proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7122954/7052357/shen4-2404809-large.gif
2015,7052357,Fig. 5.,Selected stable brain regions by three different methods on (top) MRI and (bottom) PET images. Note that different colors indicate different brain regions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7122954/7052357/shen5-2404809-large.gif
2015,7052357,Fig. 6,"Classification accuracy of our proposed method in multimodal case with respect to different iterations, achieved by iterative optimization algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7122954/7052357/shen6-2404809-large.gif
2015,6951423,Fig. 1.,"A general architecture of method I Ensemble. The collection of randomized recursive decision trees
{h(x,
Θ
k
),k=1…}
, where the
Θ
k
are independently, identically distributed DRTs, and each RDT provides class probability of input x.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7180481/6951423/yoo1-2369040-large.gif
2015,6951423,Fig. 2.,Model performance comparisons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7180481/6951423/yoo2-2369040-large.gif
2015,7140733,Figure 1.,Computing the output of an SLFN (ELM) model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda1-2450498-large.gif
2015,7140733,Figure 2.,A matrix form of an ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda2-2450498-large.gif
2015,7140733,Table 1.,"ELM computation and memory requirements; computations along the dimension
N
~
can be performed incrementally in L-size batches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda.t1-2450498-large.gif
2015,7140733,Figure 3.,"Mean squared error difference of predictions of SLFNs with 5 hidden neurons for Iris dataset (average over 100 runs), for different values of
s
in
W=N(0,s)
. For small
s
, outputs of sigmoid SLFN are similar to linear SLFN, and for large
s
they are similar threshold SLFN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda3-2450498-large.gif
2015,7140733,Figure 4.,"Test error of SLFNs with 25 hidden neurons on Iris dataset (averaged over 100 runs), for different values of
s
in
W=N(0,s)
. The data has 100 training and 50 test samples, balanced over the 3 classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda4-2450498-large.gif
2015,7140733,Figure 5.,"MSE difference (top) of predictions, and test error (bottom) of SLFNs with 500 hidden neurons on MNIST dataset (averaged over 10 runs), for different values of
s
in
W=N(0,s)
. The data has 60000 training and 10000 test samples. Due to high dimensionality of inputs, the optimal value of
s
differs from 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda5-2450498-large.gif
2015,7140733,Figure 6.,"MSE difference (top) of predictions, and test error (bottom) of SLFNs with 500 hidden neurons on MNIST dataset (averaged over 10 runs), for different values of
s
. With input dimensionality fix
W=N(0,s/
d
−
−
√
)
, the optimal value of
s
is around 1 even for high dimensional data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda6-2450498-large.gif
2015,7140733,Table 2.,"Meas Squared Error (bold) and runtime in seconds for the regression datasets. Results denoted by * are computed with 500 hidden neurons, as suggested by pruning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda.t2-2450498-large.gif
2015,7140733,Table 3.,Accuracy in % (bold) and runtime in seconds for the classification datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda.t3-2450498-large.gif
2015,7140733,Figure 7.,"Test errors (
a
) and runtimes (
b
) on different hardware (
c
) for large datasets, on logarithmic scale. Runtime on different hardware is shown for two datasets only for image clarity. The 4-core CPU runs at 4 GHz, 2x8-core CPU run at 2.6 GHz, a 2-core laptop CPU runs an 1.4 GHz and the GPU is GTX Titan Black (similar to Tesla K40).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda7abc-2450498-large.gif
2015,7140733,Figure 8.,"Training (
a
) and prediction (
b
) runtimes of a basic ELM for MNIST dataset with 64 neurons. ELM predictions are obtained on the same training dataset for comparable runtimes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda8ab-2450498-large.gif
2015,7140733,Figure 9.,"Training (
a
) and prediction (
b
) runtimes of a basic ELM for MNIST dataset with 4096 neurons. ELM predictions are obtained on the same training dataset for comparable runtimes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda9ab-2450498-large.gif
2015,7140733,Table 4.,"Training time of an ELM with 19,000 hidden neurons on 0,5 billion samples with 147 feature.s",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda.t4-2450498-large.gif
2015,7140733,Table 5.,"Test confusion matrix for ELM with 19,000 neurons.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda.t5-2450498-large.gif
2015,7140733,Figure 10.,"Test classification accuracy for skin and non-skin pixels. Model does not overfit with 19,000 neurons. Note the logarithmic
x
axis. (a) Test classification accuracy. (b) Zoom on skin accuracy. (c) Zoom on non-skin accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7042252/7140733/lenda10abc-2450498-large.gif
2015,6945865,Fig. 1.,"Schematic figure of the
T
d
data from sources 1 to
N
, the
T
s
data, and what training data is used in the different classifiers. Transfer classifier denotes any of the four transfer classifiers presented.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-1-source-large.gif
2015,6945865,Fig. 2.,"Learning curves showing the mean classification error and classification improvement on the test sets as a function of the number of
T
s
samples, for the six classifiers: an SVM on
T
, an SVM on
T
s
, wsvm, rsvm, tradaboost, and A-svm. (a) and (b) Average learning curves over all 56 images from the four sources, for 4 and 13 features, respectively. (c) and (d) Percentage reduction in mean classification error compared to the SVM
T
s
classifier, averaged over all 56 images, for 4 and 13 features, respectively. For TrAdaBoost and A-SVM 95%-CIs for the mean improvement were included, the CIs of SVM
T
, wsvm, and RSVM were similar to the CI of A-svm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-2-source-large.gif
2015,6945865,Fig. 3.,"Segmentations with the WSVM classifier with four features. (a) Source 1, image. (b) Source 1, manual. (c) Source 1, WSVM. (d) Source 2, image. (e) Source 2, manual. (f) Source 2, WSVM. (g) Source 3, image. (h) Source 3, manual. (i) Source 3, WSVM. (j) Source 4, image. (k) Source 4, manual. (l) Source 4, WSVM. Classifier was trained on a total of 4500
T
d
samples and 200
T
s
samples from one image from the target source, which corresponds to the right-most point of the learning curves in Fig. 2(a). Classification errors for the shown slices were (c) 8.1%, (f) 9.2%, (i) 6.9%, (l) 15.2%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-3-source-large.gif
2015,6945865,Fig. 4.,"Learning curves for wm/gm/csf segmentation, showing mean classification errors as a function of the number of
T
s
samples on 13 features for three different normalization techniques. (a) Equals the image in Fig. 2(b) and includes range matching between the 4th and 96th percentile, (b) includes range matching between the minimum and maximum value, and (c) includes the normalization of nyúl et al. [21].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-4-source-large.gif
2015,6945865,Fig. 5.,"Learning curves showing the mean classification error and mean classification improvement on the test sets as a function of the number of
T
s
images, for WML segmentation with the six classifiers: SVM on
T
, SVM on
T
s
, wsvm, rsvm, tradaboost, and A-svm. (a) and (b) Mean learning curves over all 40 images from the three sources for 6 and 33 features respectively. (c) and (d) Percentage reduction in mean classification error compared to SVM
T
s
averaged over all 40 images, for 6 and 33 features respectively. The shaded areas show 95%-CIs for the mean improvement. For (a) and (b) the CI of SVM
T
and RSVM were similar to the one of wsvm, and for (b) the CI of A-svm was similar to the one of WSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-5-source-large.gif
2015,6945865,Fig. 6.,"Examples of resulting segmentations of the RSVM classifier on 33 features after training on
T
s
samples from eight images and 4000
T
d
samples. (a) Source 1, T1. (b) PD. (c) FLAIR. (d)
P(y|x)
. (e) Segmentation. (f) Source 2, T1. (g) T2. (h) FLAIR. (i)
P(y|x)
. (j) Segmentation. (k) Source 3, T1. (l) T2. (m) FLAIR. (n)
P(y|x)
. (o) Segmentation. Figures (d), (i), (n) show the posterior outputs of the classifier, and Figures (e), (j), (o) show the final segmentation in blue, the manual segmentation in yellow, and the overlap between the two in green. The true positive rates (TPRs) and false positive rates (FPRs) for the showed slices were (e):
TPR=92%
,
FPR=14%
, (j):
TPR=47%
,
FPR=49%
, (o):
TPR=48%
,
FPR=45%
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-6-source-large.gif
2015,6945865,Fig. 7.,"Learning curves for wml/msl segmentation, showing mean classification error as a function of the number of
T
s
samples on 13 features for three different normalization techniques. (a) Equals the image in Fig. 5(b) and includes range matching between the 4th and 96th percentile, (b) includes range matching between the minimum and maximum value, and (c) includes the normalization of nyúl et al. [21].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6945865/6945865-fig-7-source-large.gif
2015,6963383,Fig. 1.,"Structure of DAELM-S algorithm with
M
target domains (
M
tasks). Solid arrow: training data from source domain
D
S
for classifier learning. Dashed arrow: labeled data from target domain
D
l
T
for classifier learning. The unlabeled data from target domain
D
u
T
are not used in learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang1-2367775-large.gif
2015,6963383,Fig. 2.,"Structure of DAELM-T algorithm with
M
target domains (
M
tasks). The solid arrow connected with
D
S
denotes the training for base classifier
β
B
, the dashed line connected with
D
u
T
denotes the tentative test of base classifier using the unlabeled data from target domain, the solid arrow connected with
D
l
T
denotes the terminal classifier learning of
β
T
, the dashed arrow connected between
β
B
, and
β
T
denotes the regularization for learning
β
T
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang2-2367775-large.gif
2015,6963383,Fig. 3.,"Response of 16 sensors before (batch 1) and after drifting (batches 2, 7, and 10) under acetone with various concentrations (i.e., 10, 50, 100, 150, 200, and 250 ppm). In total, 40 samples, including 6, 7, 7, 6, 7, and 7 samples for each concentration, respectively, are illustrated for visual inspection of the drift behavior.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang3-2367775-large.gif
2015,6963383,Fig. 4.,Principal component space of 10 batches for all data. The drift is clearly demonstrated by the different data distribution among batches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang4-2367775-large.gif
2015,6963383,Fig. 5.,Visual description of SSA Algorithm 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang5-2367775-large.gif
2015,6963383,Fig. 6.,Comparisons of different methods in Experimental Setting 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang6-2367775-large.gif
2015,6963383,Fig. 7.,Comparisons of different methods in Experimental Setting 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang7-2367775-large.gif
2015,6963383,Fig. 8.,Recognition accuracy under Setting 1 with respect to different size of guide set (labeled samples from target domain).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang8-2367775-large.gif
2015,6963383,Fig. 9.,Recognition accuracy under Setting 2 with respect to different sizes of guide set (labeled samples from target domain).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/7118796/6963383/zhang9-2367775-large.gif
2015,7270295,Fig. 1.,Images with different contextual cues evoke completely different emotions. The images are from the International Affective Picture System (IAPS) dataset [54]. (A) an image with pleasing feel. (B) an image with unsettling feel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu1ab-2479400-large.gif
2015,7270295,Fig. 2.,Overview of the proposed framework for horror image recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu2-2479400-large.gif
2015,7270295,Fig. 3.,"Contextual graph construction: (A) Contextual graph (B) Corresponding adjacency matrix
M
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu3ab-2479400-large.gif
2015,7270295,Algorithm 1,Pseudo-Code for the CMIL-FSVM Optimization Heuristics,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu10-2479400-large.gif
2015,7270295,Fig. 4.,Bag construction: (A) Original image. (B) Segmentation result. (C) Contextual graph. The 5th region is discarded because it is too small.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu4abc-2479400-large.gif
2015,7270295,Fig. 5.,"The center block (CB) of pixel
q
has 16 overlapped neighboring blocks with
w/2
overlapping pixels:
N
B
1
,N
B
2
,…N
B
16
. Each block is in size of
w×w
pixels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu5-2479400-large.gif
2015,7270295,Fig. 6.,"An example of a 4-order Tucker decomposition viewed from
1
st
Block.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu6-2479400-large.gif
2015,7270295,Fig. 7.,A horror image with the associated visual saliency map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu7-2479400-large.gif
2015,7270295,Fig. 8.,Exemplar saliency maps on horror images produced by different algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu8-2479400-large.gif
2015,7270295,Fig. 9.,Examples of horror regions extraction from the CMIL+TVS. The first row contains original images; the images in the second row are segmentation results; the images in the third row are the horror regions extracted by our CMIL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7271151/7270295/hu9abcd-2479400-large.gif
2015,6883187,Fig. 1.,"Dependence of R
p
scoring performance statistic of ML scoring models on the number of HIV protease complexes in the training set. Scoring performance in terms of all metrics (R
p
, R
s
, and RMSE) is provided in Figs. S2-S4 of the supplementary material, available online.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7080960/6883187/mahap1-2351824-large.gif
2015,6883187,Fig. 2.,"Performance of SFs in terms of R
p
as a function of BLAST sequence similarity cutoff between binding sites of proteins in training and test complexes; for R
p
, R
s
, and RMSE see Figs. S5-S12 of the supplementary material, available online.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7080960/6883187/mahap2-2351824-large.gif
2015,6883187,Fig. 3.,"Dependence of scoring accuracy of ML SFs on training set size when the training complexes are selected randomly (without replacement) from Sc and the models are tested on Cr ((a) and (c)) and random set complexes ((b) and (d)) characterized by XAR ((a) and (b)) and X features ((c) and (d)). See Figs. S13-S15 of the supplementary material, available online, for R
p
, R
s
, and RMSE measures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7080960/6883187/mahap3-2351824-large.gif
2015,6883187,Fig. 4.,"Dependence of R
p
performance statistic of ML scoring models on the number of features, with the features drawn randomly (without replacement) from a pool of X, A, and R-type features and used to train the ML models on the Sc dataset and then tested on the disjoint core set Cr. See Figures S16-S18 of the supplementary material, available online, for R
p
, R
s
, and RMSE measures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7080960/6883187/mahap4-2351824-large.gif
2015,7123640,Fig. 1.,The ground-based aircraft climb prediction problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7330243/7123640/allig1-2437452-large.gif
2015,7123640,Fig. 2.,Baseline method: the BADA prediction of the future aircraft climb.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7330243/7123640/allig2-2437452-large.gif
2015,7123640,Fig. 3.,Mass estimation (left) vs. mass prediction (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7330243/7123640/allig3ab-2437452-large.gif
2015,7123640,Fig. 4.,"Cross-validation for model assessment, with an embedded holdout validation for hyperparameter tuning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7330243/7123640/allig4-2437452-large.gif
2015,7123640,Fig. 5.,"This figure portrays the computed specific power
SP
and the observed specific energy rate
e
w
. Only one aircraft is considered here, however different masses are used to compute the specific power
SP
. According Newton's law, the specific power
SP
of the aircraft is equal to the observed specific energy rate
e
w
. The past points have a negative time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7330243/7123640/allig5-2437452-large.gif
2015,6876186,Fig. 1.,"(a) OR based on the sum-of-margins strategy.
(w,
b
1
)
and
(w,
b
2
)
are the parallel discrimination hyperplanes obtained from maximizing
2
d
1
+2
d
2
, when
⟨w,w⟩=1
. Support vectors lie on the boundaries between the neighboring categories. (b) Two cases of incremental SVOR learning. If the added sample (e.g.,
x
3
) is sandwiched between hyperplanes
(w,
b
j
−
d
j
)
and
(w,
b
j
+
d
j
)
, adjustments will be needed; otherwise, adjustments will be unnecessary for the added sample, such as
x
1
or
x
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu1ab-2342533-large.gif
2015,6876186,Fig. 2.,"Partitioning a two-class training sample set
S
j
, which is associated with the
j
th binary classification, into three independent sets by KKT-conditions. (a)
S
j
S
. (b)
S
j
E
. (c)
S
j
R
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu2abc-2342533-large.gif
2015,6876186,Fig. 3.,"RAIA. The two cases of
(
x
c
,
y
c
)
after RAIA (a) becomes a margin support vector or (b) it becomes an error support vector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu3ab-2342533-large.gif
2015,6876186,Fig. 4.,"SRA. The objective of SRA is to restore the inequality restriction (9) on the enlarged
j
c
th two-class training samples. Initial condition of
p
j
c
may be (a)
p
j
c
>2
or (b)
p
j
c
<2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu4ab-2342533-large.gif
2015,6876186,Fig. 5.,Average numbers of iterations of RAIA and SRA on the different data sets. (a) Bank. (b) Computer Activity. (c) Friedman. (d) Census. (e) Abalone. (f) Winequality-red. (g) Winequality-white. (h) Spine Image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu5abcdefgh-2342533-large.gif
2015,6876186,Fig. 6.,"Running time of SMO-SMF, SMO-EXC, IPSVM and ISVOR (in seconds) on the different data sets. (a) Bank. (b) Computer Activity. (c) Friedman. (d) Census. (e) Abalone. (f) Winequality-red. (g) Winequality-white. (h) Spine Image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu6abcdefgh-2342533-large.gif
2015,6876186,Fig. 7.,"Results of MAE and MZE, over 20 trials. The grouped boxes represent the results of PSVM, EXC, SMF, and MSMF from left to right on different data sets. The notched-boxes have lines at the lower, median, and upper quartile values. The whiskers are lines extended from each end of the box to the most extreme data value within
1.5×IQR
(Interquartile Range) of the box. Outliers are data with values beyond the ends of the whiskers, which are displayed by plus signs. (a) MAE. (b) MZE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6876186/gu7ab-2342533-large.gif
2015,7047737,Fig. 1.,Computation of extreme points in feature space using (a) convex hull algorithm with quadratic programming described in [15] and (b) sufficient condition (5).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim1ab-2377123-large.gif
2015,7047737,Fig. 2.,Computation of extreme points of complex data points in feature space using (a) convex hull algorithm with quadratic programming described in [15] and (b) sufficient condition (5).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim2ab-2377123-large.gif
2015,7047737,Fig. 3.,"Example of the gossip process (a) network connected in finite time with simple topology and (b) gossip process with join operation, where the cells with gray background indicate the convergence.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim3ab-2377123-large.gif
2015,7047737,Fig. 4.,Extreme points of the convex hull (a) general case of convex hull and (b) worst case of convex hull. The black points indicate the extreme points and the red points indicate the nonextreme points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim4ab-2377123-large.gif
2015,7047737,Fig. 5.,Comparison between (a) convex hull algorithm and (b) naive convex hull algorithm. The black points indicate the extreme points and the red points indicate the nonextreme points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim5ab-2377123-large.gif
2015,7047737,Fig. 6.,Join operation of the two naive convex hulls. The black points indicate the extreme points and the red points indicate the nonextreme points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim6-2377123-large.gif
2015,7047737,Fig. 7.,"Results of Monte Carlo simulation of join operations of naive convex hulls with varying
ε
. (a) Box plots of errors. (b) Average of exchanged data length corresponding to the margin
ε
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim7ab-2377123-large.gif
2015,7047737,Fig. 8.,"Result of the join operation of convex hulls in (a) linearly separable case and (b) linearly nonseparable case: for the positive (square) and negative (triangle) datasets obtained by node 1 (red) and node 2 (blue), the local convex hulls and SVM solutions of nodes 1 and 2 are represented as the red-dashed (node 1) and blue dash-dotted (node 2) polygons and lines, before exchanging data. Then, nodes 1 and 2 exchange the extreme point sets with each other to obtain the global (reduced) convex hull and SVM solution (the black solid polygon and line).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim8ab-2377123-large.gif
2015,7047737,Fig. 9.,Result of the centralized geometric SVM: the contour of the discriminant value is plotted over the workspace. The zero-valued contour is the discriminant function of the SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim9-2377123-large.gif
2015,7047737,Fig. 10.,Result of the distributed SVM: evolution of the contour of the discriminant value for node 1 is plotted over the workspace. The zero-valued contour is the discriminant function of the SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim10-2377123-large.gif
2015,7047737,Fig. 11.,"Agreements of the distributed SVM for nodes 5, 10, 15, 20, 25, and 30: the contour of the discriminant value is plotted over the workspace. The zero-valued contour is the discriminant function of the SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim11-2377123-large.gif
2015,7047737,Fig. 12.,"(a) Average path length. (b) Power consumption analysis for various rewiring probabilities: a logarithmic horizontal scale has been used to resolve the rapid changes in
L(p)
and
P
T
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim12ab-2377123-large.gif
2015,7047737,Fig. 13.,Environment of the indoor experiment and the results: the blue squares indicate nodes detecting a low temperature and the red squares indicate nodes detecting a high temperature. Part (a) and (b) show the results of independent trials.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim13ab-2377123-large.gif
2015,7047737,Fig. 14.,Experimental results of (a) average path length and (b) power consumption for various rewiring probabilities: a logarithmic horizontal scale has been used to resolve the rapid changes in \$L(p)\$ and \$P_{T}\$ .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7297778/7047737/kim14ab-2377123-large.gif
2015,6926746,Fig. 1.,"Indian Pines dataset. (a) RGB composite image of three bands 50, 27, and 17. (b) Ground-truth map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng1ab-2359965-large.gif
2015,6926746,Fig. 2.,"University of Pavia dataset. (a) RGB composite image of three bands 60, 30, and 2. (b) Ground-truth map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng2ab-2359965-large.gif
2015,6926746,Fig. 3.,"Salinas dataset. (a) RGB composite image of three bands 50, 30, and 20. (b) Ground-truth map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng3ab-2359965-large.gif
2015,6926746,Fig. 4.,OA versus neighborhood window width for three datasets. (a) Indian Pines. (b) University of Pavia. (c) Salinas.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng4abc-2359965-large.gif
2015,6926746,Fig. 5.,"Searching (left), testing (middle), and training time (right) of SVM-CK, ELM-CK, and KELM-CK for three datasets. (a)–(c) Indian Pines. (d)–(f) University of Pavia. (g)–(i) Salinas.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng5abcdefghi-2359965-large.gif
2015,6926746,Fig. 6.,"Classification maps for the Indian Pines dataset with 40 labeled samples per class. (a) SVM (OA
=
70.1%). (b) ELM (OA
=
64.5%). (c) KELM (OA
=
71.9%). (d) SVM-CK (OA
=
89.1%). (e) ELM-CK (OA
=
92.5%). (f) KELM-CK (OA
=
93.4%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng6abcdef-2359965-large.gif
2015,6926746,Fig. 7.,"Classification maps for the University of Pavia dataset with 40 labeled samples per class. (a) SVM (OA
=
78.5%). (b) ELM (OA
=
66.0%). (c) KELM (OA
=
77.9%). (d) SVM-CK (OA
=
91.4%). (e) ELM-CK (OA
=
91.0%). (f) KELM-CK (OA
=
93.5%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng7abcdef-2359965-large.gif
2015,6926746,Fig. 8.,"Classification maps for the Salinas dataset with 40 labeled samples per class. (a) SVM (OA
=
89.3%). (b) ELM (OA
=
88.6%). (c) KELM (OA
=
89.2%). (d) SVM-CK (OA
=
93.9%). (e) ELM-CK (OA
=
95.0%). (f) KELM-CK (OA
=
96.4%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6926746/peng8abcdef-2359965-large.gif
2015,7072521,Fig. 1.,View insufficiency assumption.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao1-2417578-large.gif
2015,7072521,Fig. 2.,Example robust estimators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao2-2417578-large.gif
2015,7072521,Fig. 3.,Reconstruction of 3-D vase model using MISL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao3-2417578-large.gif
2015,7072521,Fig. 4.,Reconstruction of 3-D chair model using MISL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao4-2417578-large.gif
2015,7072521,Fig. 5.,Reconstruction of 3-D compel model using MISL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao5-2417578-large.gif
2015,7072521,Fig. 6.,Reconstruction of s-curve using MISL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao6-2417578-large.gif
2015,7072521,Fig. 7.,Face recognition accuracies of different algorithms on different dimensional spaces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao7-2417578-large.gif
2015,7072521,Fig. 8.,Convergence curves of MISL for different dimensional latent spaces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao8-2417578-large.gif
2015,7072521,Fig. 9.,Confusion tables of different algorithms on 101 action classes and 5 action types.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao9-2417578-large.gif
2015,7072521,Fig. 10.,Category recognition accuracy of different algorithms on different dimensional spaces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7308134/7072521/tao10-2417578-large.gif
2015,7095595,Fig. 1.,Diagram of a generalized functional link network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7116638/7095595/pukis1-2426012-large.gif
2015,7095595,Fig. 2.,"GMDH network for the solution of a three-input, third-(or fourth)-order function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7116638/7095595/pukis2-2426012-large.gif
2015,7095595,Fig. 3.,"Three-input, third-order case: unique monomial terms on progressively descending and right-shifting diagonals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7116638/7095595/pukis3-2426012-large.gif
2015,7095595,Fig. 4.,"Example of the recursive Poly-Gen function for a three-input, order-2 case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7116638/7095595/pukis4-2426012-large.gif
2015,7095595,Fig. 5.,"(a) Forward-kinematics problem: input angles are mapped to Cartesian coordinates (
x
,
y
,
z
). (b) 
x
-position versus two input angles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7116638/7095595/pukis5ab-2426012-large.gif
2015,7095595,Fig. 6.,"Normalized 2-D modified Rastrigin’s function for different values of
α
: (a) 
α=1
; (b) 
α=3
; (c) 
α=5
; and (d) 
α=7
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7116638/7095595/pukis6abcd-2426012-large.gif
2015,6883191,Fig. 1.,"Comparison between Monte Carlo-estimated ERCs and our derived bounds using the Letter data set.
10
4
 σ
t
samples were used for Monte Carlo estimation. We sampled 100 data for each letter and used 9 kernel functions in multiple kernel scenario. (a) HS:
F
s
. (b) HS:
F
s,r
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6883191/li1ab-2347054-large.gif
2015,6883191,Fig. 2.,"Average classification accuracy on 20 runs for different
s
values. (a) Fixed kernel (single kernel) scenario. (b) MKL scenario.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6883191/li2ab-2347054-large.gif
2015,7001593,Fig. 1.,Hotspot or nonhotspot pattern [16].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang1-2387858-large.gif
2015,7001593,Fig. 2.,Hit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang2-2387858-large.gif
2015,7001593,Fig. 3.,Our hotspot detection framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang3-2387858-large.gif
2015,7001593,Fig. 4.,"Topological classification. (a) Core regions of four patterns
A
,
B
,
C
, and
D
. (b) Generated clusters: {
A
,
D
}, {
B
}, and {
C
}.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang4ab-2387858-large.gif
2015,7001593,Fig. 5.,Topological classification. (a)–(c) String-based classification. (d) and (e) Density-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang5abcde-2387858-large.gif
2015,7001593,Fig. 6.,From tiled pattern to MTCG [6].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang6-2387858-large.gif
2015,7001593,Fig. 7.,Critical features. (a) Internal feature. (b) External feature. (c) Diagonal feature. (d) Segment feature. (e) Nontopological feature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang7abcde-2387858-large.gif
2015,7001593,Fig. 8.,Critical feature extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang8-2387858-large.gif
2015,7001593,Fig. 9.,Training phase. (a) Multiple SVM-kernel learning. (b) Feedback kernel learning. (c) Details of our feedback kernel learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang9abc-2387858-large.gif
2015,7001593,Fig. 10.,Hotspot clip and a nonhotspot clip contain an almost identical core pattern.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang10-2387858-large.gif
2015,7001593,Fig. 11.,Layout clip extraction. (a) Polygon dissection. (b) Layout clip.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang11ab-2387858-large.gif
2015,7001593,Fig. 12.,Redundant clip removal. (a) Hotspots reported by our SVM kernel. (b) Clip merging of (a). (c) Clip reframing. (d) Clip reframing of (b). (e) Clip shifting of (d).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang12abcde-2387858-large.gif
2015,7001593,Fig. 13.,Multilayer pattern handling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang13-2387858-large.gif
2015,7001593,Fig. 14.,(a) Hotspots caused by mask misalignment. (b) Three sets of hotspot features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang14ab-2387858-large.gif
2015,7001593,Fig. 15.,Trade-offs between accuracy and false alarm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7042867/7001593/jiang15-2387858-large.gif
2015,6937189,Fig. 1.,Network structure of one hidden layer autoencoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou1-2363492-large.gif
2015,6937189,Fig. 2.,Relationship between the testing accuracy and the number of hidden nodes for MNIST dataset using the equality optimization method-based ELMs solution for very large dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou2-2363492-large.gif
2015,6937189,Fig. 3.,Block diagram showing the information flow of the first two layers of the S-ELMs learning network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou3-2363492-large.gif
2015,6937189,Fig. 4.,Fat ELM is reduced to a slim ELM using PCA dimension reduction method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou4-2363492-large.gif
2015,6937189,Fig. 5.,Lower layer ELM propagates its hidden nodes output to the upper layer ELM and functions as a part of hidden nodes in the upper layer ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou5-2363492-large.gif
2015,6937189,Fig. 6.,"In ELM autoencoder, ELM is used as the training algorithm for the network, the transpose of output weight
β
(reconstruction matrix) is used as the input weight of a normal ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou6-2363492-large.gif
2015,6937189,Fig. 7.,Network diagram of ELM AE-S-ELMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou7-2363492-large.gif
2015,6937189,Fig. 8.,"Testing accuracy of S-ELMs learning network when the number of hidden nodes is chosen as 200, 500, and 1000, respectively. The horizontal line shows the testing accuracy of a single ELM with 3000 hidden nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou8-2363492-large.gif
2015,6937189,Fig. 9.,Relationship between testing accuracy and number of total layers (iterations). (a) S-ELMs on MNIST. (b) S-ELMs on OCR. (c) ELM-AE-based S-ELMs on MNIST. (d) ELM-AE-based S-ELMs on OCR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou9abcd-2363492-large.gif
2015,6937189,Fig. 10.,"Relationship between testing accuracy and the regularization parameter
C
. (a) S-ELMs on MNIST. (b) S-ELMs on OCR. (c) ELM-AE-based S-ELMs on MNIST. (d) ELM-AE-based S-ELMs on OCR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7203181/6937189/zhou10abcd-2363492-large.gif
2015,6971210,Fig. 1.,"Pseudocode description of the four types of supervised learning used in this article: passive learning (algorithm 1), active learning based on the least certainty query strategy (algorithm 2) and on the medium certainty query strategy (algorithm 3), and co-active learning (algorithm 4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/7010069/6971210/6971210-fig-1-source-large.gif
2015,6971210,Fig. 2.,Pseudocode description of the two types of ssl used in this paper: self-training algorithm 5 and co-training algorithm 6.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/7010069/6971210/6971210-fig-2-source-large.gif
2015,6971210,Fig. 3.,"Pseudocode description of the three types of cooperative learning proposed: single-view cooperative learning (svcl), mixed-view cooperative learning (xvcl), and multi-view cooperative learning (mvcl).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/7010069/6971210/6971210-fig-3-source-large.gif
2015,6971210,Fig. 4.,"Comparison between co-training using the feature separation method based on cepstral llds, co-training using a random feature separation method, and self-training. The charts show the average uars across 20 independent runs (and respective standard deviations) vs. number of machine labeled instances for the four experiments described in this paper: (a) fau aec database with the is09 ec feature set and 200 initial supervised training instances; (b) fau aec database with the is09 ec feature set and 500 initial supervised training instances; (c) fau aec database with the is10 asc feature and 200 initial supervised training instances; and (d) the susas database with the is10 asc feature set and 100 initial training instances. (a) fau aec, feature set: is09,
l
: 200 (b) fau aec, feature set: is09,
l
: 500 (c) fau aec, feature set: is10,
l
: 200 (d) susas, feature set: is10,
l
: 100.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/7010069/6971210/6971210-fig-4-source-large.gif
2015,6971210,Fig. 5.,"Comparison between the supervised (pl, least certainty al, medium certainty al, and coal) and cooperative (al + self-training, al + co-training, and coal + co-training) learning algorithms. The performance measures shown are uars averaged across 20 independent runs of each algorithm (as well as the corresponding standard deviations) vs. The number of manually labeled instances for the fau aec with is09 ec feature set by 200 (a) or 500 (b) initial supervised training instances, as well as with the is10 asc feature set by 200 (c) initial supervised training instances, and the susas with the is10 asc feature set by 100 (d) initial training instances. (a) fau aec, feature set: is09,
l
: 200 (b) fau aec, feature set: is09,
l
: 500 (c) fau aec, feature set: is10,
l
: 200 (d) susas, feature set: is10,
l
: 100.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/7010069/6971210/6971210-fig-5-source-large.gif
2015,6899632,Fig. 1.,Machine-learning methodology for the pattern-recognition problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal1-2346243-large.gif
2015,6899632,Fig. 2.,Representation of the database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal2-2346243-large.gif
2015,6899632,Fig. 3.,"Route driven by each participant, displayed on Google Earth.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal3-2346243-large.gif
2015,6899632,Fig. 4.,"Experimental motorcycle, i.e., a Honda CBF1000, and the sensors installed on it. The accelerations and rotations are shown in the reference triad.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal4-2346243-large.gif
2015,6899632,Fig. 5.,"(Left) DFT of acceleration
a
x
. (Right) Spectrogram.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal5ab-2346243-large.gif
2015,6899632,Fig. 6.,Longitudinal acceleration recorded over time during the sequence and its corresponding labels: ST—stop; SL—straight line; LT—left turn; RT—right turn; RA—roundabout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal6-2346243-large.gif
2015,6899632,Fig. 7.,(Top) Representation of the number of samples in each class (SL: straight line; LT: left turn; RT: right turn; RA: roundabout) for each sequence. (Bottom) Number of samples in each class in the global database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7027907/6899632/attal7ab-2346243-large.gif
2015,7095607,Fig. 1.,Illustration of the simultaneously learning deep feature and learning to rank scheme. The deep hierarchical features are obtained by exploiting the deep linear feature learning algorithm on the original structure feature matrix. Each learned low-rank feature is assigned to a ranking algorithm. The two stages (feature learning and ranking) are jointly trained due to the motivation that learning features and learning rankings are correlated and able to benefit from each other.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li1-2426707-large.gif
2015,7095607,Fig. 2.,"Illustration of learning deep bases using our approach on the ORL dataset. Clearly, the bases in the first layer show the face patterns, and the bases in the other layers generally describe sparse information especially in the higher-level layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li2-2426707-large.gif
2015,7095607,Fig. 3.,"Illustration of the principle of the query-based multi-task learning. As an example, we show 10 queries from three groups. For each layer (the features in the same depth), the bagged queries are assigned to different groups (different circles shown in the figure) according to the similarity of their corresponding queries. The triangle denotes a specific ranking approach that is related to
(
w
0
+
w
r
)
r=1,2,3
. The terms
{
w
r
}
are introduced to discover the consistent information that exists in each group and
w
0
to describe the sharing information that exists among the ranking approaches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li3-2426707-large.gif
2015,7095607,Fig. 4.,"Ranking performances with different values of the feature dimensionalities. The ranking performance of DSSVM improves with increasing
c
and plateaus with a sufficiently large range of
c
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li4-2426707-large.gif
2015,7095607,Fig. 5.,"Ranking performances with different values of
L
. Clearly, the ranking performance of DSSVM keeps improving with the increase of
L
and plateaus with a sufficiently large range of
L
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li5-2426707-large.gif
2015,7095607,Fig. 6.,"The
NDCG
ranking performances with our joint approaches and the separate learning approaches. Clearly, our joint learning approaches are capable of achieving better
NDCG
values than the separate learning approaches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li6-2426707-large.gif
2015,7095607,Fig. 7.,"Precision-Recall Curve retrieval performances of different algorithms on the five datasets. Clearly, our approach obtains better precision-recall retrieval results in most cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li7-2426707-large.gif
2015,7095607,Fig. 8.,"Three retrieval examples on the NUSWIDE, Caltech256, and LabelMe datasets, respectively. The left part shows query samples while the right part displays a few retrieved images using our approach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7247809/7095607/li8-2426707-large.gif
2015,7094223,Fig. 1.,"(a) and (b) are GeoEye-1 Wuhan true-color image and ground truth reference, respectively; (c) and (d) are WorldView-2 Shenzhen true-color image and ground truth reference, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang1abcd-2420713-large.gif
2015,7094223,Fig. 2.,"Graphic examples for the four water types: rivers, ponds, lakes, and canals, as well as the ground sampling and the key words for their characteristics.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang2-2420713-large.gif
2015,7094223,Fig. 3.,Proposed water-body extraction and type classification framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang3-2420713-large.gif
2015,7094223,Fig. 4.,"Example showing urban water-body extraction based on NDWI, NDVI, and MSI. (a) Example image. (b), (c), and (d) Shows the NDWI, NDVI, and MSI feature images, respectively. (e) Water extraction result derived from the information indexes based on a set of manually selected threshold values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang4abcde-2420713-large.gif
2015,7094223,Fig. 5.,"Statistics of the information indexes for the four water types. Each box plot shows the location of the 25, 50, and 75 percentiles using horizontal lines. The two whiskers cover the 99.3 percentile of the data, and the red “
+
” is the outlier outside the two whiskers. The plots are based on the statistics of the samples derived from the GeoEye-1 Wuhan test site.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang5-2420713-large.gif
2015,7094223,Fig. 6.,"Feature distributions of the four water types at the object level. Each box plot shows the location of the 25, 50, and 75 percentiles using horizontal lines. The two whiskers cover the 99.3 percentile of the data, and the red ‘
+
’ is the outlier outside the two whiskers. The plots are based on the statistics of the samples selected from the GeoEye-1 Wuhan test site, where 8, 17, 6, 3, and 11 objects for rivers, ponds, lakes, canals, and shadow, respectively, are considered.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang6-2420713-large.gif
2015,7094223,Fig. 7.,Results of water extraction for GeoEye-1 Wuhan dataset: (a) ground truth; (b) RBF–SVM; (c) CART; and (d) RF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang7abcd-2420713-large.gif
2015,7094223,Fig. 8.,Results of water-type identification for GeoEye-1 Wuhan dataset: (a) ground truth; (b) RBF–SVM; (c) C4.5; and (d) RF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang8abcd-2420713-large.gif
2015,7094223,Fig. 9.,Results of water-body extraction for WorldView-2 Shenzhen dataset: (a) ground truth; (b) RBF–SVM; (c) C4.5; and (d) RF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang9abcd-2420713-large.gif
2015,7094223,Fig. 10.,Results of water body-type identification for WorldView-2 Shenzhen dataset: (a) ground truth; (b) RBF–SVM; (c) C4.5; and (d) RF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang10abcd-2420713-large.gif
2015,7094223,Fig. 11.,Accuracy scores (average Kappa value and standard deviation) and the training time with different number of training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang11-2420713-large.gif
2015,7094223,Fig. 12.,Average and standard deviation of the accuracies for water detection for GeoEye-1 Wuhan and WorldView-2 Shenzhen datasets when different training samples are used for machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang12-2420713-large.gif
2015,7094223,Fig. 13.,"Comparison of the accuracies between pixel-based and object-based water extractions for (a) Wuhan and (b) Shenzhen dataset, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang13ab-2420713-large.gif
2015,7094223,Fig. 14.,"Scatter plot of NDVI and NDWI, where 1000 samples per class (i.e., water and nonwater) are generated randomly from GeoEye-1 Wuhan dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7094223/huang14-2420713-large.gif
2015,7018925,Fig. 1.,Both decision tree algorithms (left) as well as rule set generators (right) generate comprehensible models that allow one to reason about the data. This example comes from the famous Titanic example and illustrates the women and children first principle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7018925/junqu1ab-2389037-large.gif
2015,7018925,Fig. 2.,Toy example: the Ripley data set was drawn from a mixture of overlapping Gaussian distributions. (a) Original Ripley data set before processing. (b) Oracle data set: the SVM decision boundary is shown by a red curved line and all valley points are marked.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7018925/junqu2ab-2389037-large.gif
2015,7018925,Fig. 5.,Problem regions as defined by (a) valley–boundary approximation and (b) valley–valley approximation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7018925/junqu5ab-2389037-large.gif
2015,7018925,Fig. 3.,"Graphical representation of the algorithm with the valley–boundary approximation when applied to the Ripley data set. (a) Finding the boundary points: the original valley points are connected to their nearest minima (the boundary points). (b) Once the boundary region is found, random points are generated (illustrated for gradient descent).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7018925/junqu3ab-2389037-large.gif
2015,7018925,Fig. 4.,Example of C4.5 and ALPA boundaries on the Ripley data set using Ripper as a rule extraction algorithm. The SVM decision boundary is shown by a curved red line. The rule set decision boundaries are drawn by straight black lines. (a) C4.5 boundary. (b) ALPA boundary.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7299729/7018925/junqu4ab-2389037-large.gif
2015,6863657,Fig. 1.,"BMI experiment of a center-out task. (a) A rhesus monkey sat in a primate chair in front of a monitor with a distance of 100 cm, using its left hand to manipulate a joystick projected onto the monitor to control the computer cursor. (b) Experimental procedure of the center-out task event across trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-1-source-large.gif
2015,6863657,Fig. 2.,"Action segmentations of the 2–d trajectory of center-out task. (a) For clarity, only one resting action is shown by blue pentagram, the four directional actions are shown by triangles and x/y position holdings are shown by different purple asterisks.
T1∼T4
represent the order of directional target appearance. (b) and (c) Corresponding action segmentations of X and Y axes, respectively. (for interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-2-source-large.gif
2015,6863657,Fig. 3.,"Learning process of the trajectory reconstructions and the action values across four data segments in day 1. (a) and (b) Trajectory reconstructions of X and Y axes by
Q(λ)
-greedy,
Q(λ)
-softmax and AGREL. Gray solid line is real trajectory, the blue dotted line is by
Q(λ)
-greedy, the green dash-dotted line is by
Q(λ)
-softmax and the red dashed line is by AGREL. (c), (d) and (e) Corresponding output values of all the possible actions of
Q(λ)
-greedy,
Q(λ)
-softmax and AGREL. (for interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-3-source-large.gif
2015,6863657,Fig. 4.,"Temporal neural patterns of (a) channel 61 for UP and DOWN directions, and (b) channel 29 for RIGHT and LEFT directions within 300 trials across the following five days. Red solid line represents the mean directional trajectory. 0 at the X axis indicates the beginning of the directional actions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-4-source-large.gif
2015,6863657,Fig. 5.,"Average CC (a), (b) and MSE (c), (d) of X and Y axes by
Q(λ)
-greedy,
Q(λ)
-softmax and AGREL across ten data segments over six days. Blue dotted, green dash-dotted and red dashed lines represent
Q(λ)
-greedy,
Q(λ)
-softmax and agrel, respectively. Error bars represent standard deviations across 50 convergent initializations. (for interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-5-source-large.gif
2015,6863657,Fig. 6.,"Average target acquisition rate across ten data segments over six days. Blue dotted, green dash-dotted and red dashed lines represent
Q(λ)
-greedy,
Q(λ)
-softmax and agrel, respectively. Error bars represent standard deviations across 50 convergent initializations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-6-source-large.gif
2015,6863657,Fig. 7.,"Statistical CC (a), (b), and MSE (c), (d) of X and Y axis by AGREL with and without weights adaptation over six data segments in the following five days. Blue and red bars represent AGREL with and without weight adaptation, respectively. Error bars represent standard deviations across 50 convergent initializations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7104187/6863657/6863657-fig-7-source-large.gif
2015,7109855,Fig. 1.,The schematic of the event-related experimental design.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-1-source-large.gif
2015,7109855,Fig. 2.,A map showing the spatial pattern of activation identifying the 8 ROIs listed in Table I.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-2-source-large.gif
2015,7109855,Fig. 3.,An illustration of the process of alignment of ROI time series with respect to the decision time point (red point).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-3-source-large.gif
2015,7109855,Fig. 4.,Flowchart of RCE-svm algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-4-source-large.gif
2015,7109855,Fig. 5.,The evolving prediction accuracy of RCE-svm with decreasing number of features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-5-source-large.gif
2015,7109855,Fig. 6.,Statistics and histogram of RCE-svm within-subject prediction accuracies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-6-source-large.gif
2015,7109855,Fig. 7.,Dynamic changes of discriminatory power in 8 ROIs (red-labeled points are top-4-ranked features).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563672/7317835/7109855/7109855-fig-7-source-large.gif
2015,6998844,Fig. 1.,Block diagram representation for tremor prediction with MWLS-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv1-2381495-large.gif
2015,6998844,Fig. 2.,Latency in the tremor compensation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv2-2381495-large.gif
2015,6998844,Fig. 3.,Offline training for MWLS-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv3-2381495-large.gif
2015,6998844,Fig. 4.,Flowchart representation of MWLS-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv4-2381495-large.gif
2015,6998844,Fig. 5.,"Grid analysis of %accuracy for various values of
C
and
σ
2
;
N
= 500. (a) Surgeon’s group. (b) Novice subject’s group.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv5ab-2381495-large.gif
2015,6998844,Fig. 6.,Selection of signal history for offline training. Computational complexity (number of operations) of MWLS-SVM for every iteration (green in color).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv6-2381495-large.gif
2015,6998844,Fig. 7.,Single-step prediction with MWLS-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv7-2381495-large.gif
2015,6998844,Fig. 8.,Performance of MWLS-SVM and LS-SVM for single-step prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv8-2381495-large.gif
2015,6998844,Fig. 9.,Multistep prediction with various induced delays for all methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv9-2381495-large.gif
2015,6998844,Fig. 10.,Multistep prediction performance in the presence of various induced delays. Novice subjects: (a) tracing task and (b) pointing task. Surgeons: (c) tracing task and (d) pointing task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv10abcd-2381495-large.gif
2015,6998844,Fig. 11.,"Scatter plots. Prediction horizon: (a) 4 ms, (b) 8 ms, (c) 16 ms, and (d) 20 ms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv11abcd-2381495-large.gif
2015,6998844,Fig. 12.,Multistep prediction in the presence of unknown delay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv12-2381495-large.gif
2015,6998844,Fig. 13.,(a) Surgeon #1 (pointing task). (b) Prediction error due to unknown delay. (c) Prediction error with MS-BMFLC-KF. (d) Prediction error with LS-SVM. (e) Prediction error with MS-AR-KF. (f) Prediction error with MWLS-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv13abcdef-2381495-large.gif
2015,6998844,Fig. 14.,Performance analysis of all methods in the presence of unknown delay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv14-2381495-large.gif
2015,6998844,Fig. 15.,Experimental procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv15-2381495-large.gif
2015,6998844,Fig. 16.,Prediction performance in presence of unknown delay (a) tracing task of surgeon #1 and (b) tracing task of novice subject #1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6998844/veluv16ab-2381495-large.gif
2015,6818375,Fig. 1.,Block diagram of the proposed rule-extraction approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7053969/6818375/pan1-2325615-large.gif
2015,6818375,Fig. 2.,Flow diagram of the experimental setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7053969/6818375/pan2-2325615-large.gif
2015,7070704,Fig. 1.,Classifier distribution for a CTB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang1-2417498-large.gif
2015,7070704,Fig. 2.,Flowchart of compressing a CTB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang2-2417498-large.gif
2015,7070704,Fig. 3.,"P
n
(i)
component,
n=0
,1,2,
i=1
,2,3,4. (a)
P
n
(i)
in HM. (b)
P
n
(i)
in early termination scheme (Shen’s scheme) [19]. (c)
P
n
(i)
in a fast scheme. (d) Proposed
P
n
(i)
component.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang3abcd-2417498-large.gif
2015,7070704,Fig. 4.,"Flowchart of the joint classifier, where 1, −1, and, U corresponds to A, B and C in Fig. 3(d).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang4-2417498-large.gif
2015,7070704,Fig. 5.,"Examples of Classification. (a) Traditional SVM. (b) Weighted SVM Classifier #1 (
W
B
>
W
A
)
. (c) Weighted SVM Classifier #2 (
W
B
<
W
A
)
. (d) Joint Classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang5abcd-2417498-large.gif
2015,7070704,Fig. 6.,Training Mode. (a) Online. (b) Offline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang6ab-2417498-large.gif
2015,7070704,Fig. 7.,Prediction accuracy for SVM classifier with different weighted factors. (a) DL 1. (b) DL 2. (c) DL 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang7abc-2417498-large.gif
2015,7070704,Fig. 8.,"1-
Q
ALL
of the joint classifier (DL 1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang8-2417498-large.gif
2015,7070704,Fig. 9.,"The ratio of
N
RD
to
N
ALL
(DL 1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang9-2417498-large.gif
2015,7070704,Fig. 10.,False Prediction Rate from DL 1 to DL 3 with Different Weighted Factors. (a) DL1. (b) DL 2. (c) DL 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang10abc-2417498-large.gif
2015,7070704,Fig. 11.,"Percentage of Split Mode
q
S,k
(i)
in DL 1 to DL 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang11-2417498-large.gif
2015,7070704,Fig. 12.,"Relation Between Average Complexity Reduction and Average BDBR/BDPSNR over Different
Δ
η
T,i
Sets. (a) Complexity reduction vs BDBR. (b) Complexity reduction vs BDPSNR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang12ab-2417498-large.gif
2015,7070704,Fig. 13.,"CU Prediction by the Proposed Algorithm (BasketballDrill,
832×480
). (a) False predicted CUs. (b) CU prediction in DL1. (c) CU prediction in DL2. (d) CU prediction in DL3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang13abcd-2417498-large.gif
2015,7070704,Fig. 14.,"CU Prediction by the Proposed Algorithm (ParkScene,
1920×1080
). (a) False predicted CUs. (b) CU prediction in DL1. (c) CU prediction in DL2. (d) CU prediction in DL3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7070895/7070704/zhang14abcd-2417498-large.gif
2015,6967781,Fig. 1.,"Nearly isotropic stack of neural tissue acquired from the ca1 hippocampus using EM microscopy. Top. This stack is made of
1065 2048×1536
slices and its resolution is 5 nm in all three directions. This represents more than three billion voxels for a volume whose largest dimension is in the order of
10 μm
. Black arrows point towards mitochondria, which we use to train our CRF-based segmentation algorithm. Bottom. 3-D rendering of the mitochondria found in a
1024×1024×1024
subvolume.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-1-source-large.gif
2015,6967781,Fig. 2.,Input image (a slice through a 3-D volume) is shown in (a). Image (b) shows the ground truth labeling of the CRF used in our method. Pink and black colors correspond to the foreground and background classes while the gray contours are the boundaries of the SLIC supervoxels. (a) Input image. (b) Ground truth for the CRF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-2-source-large.gif
2015,6967781,Fig. 3.,"Kernelized features. (a) A superpixel over-segmentation of an image. The + and • in the middle of each denotes either foreground or background. (b) In the graph used to construct the crf, each node corresponds to a superpixel and each and edge indicate adjacency in the image. In 3-D biomedical volumes, the superpixels become supervoxels and graph becomes a 3-D grid. (c) An illustration of the feature space. Each point represents a feature vector extracted from a superpixel. Because it is not linearly separable, the standard SSVM gives a poor segmentation result in (d). (e) To address this, we train a nonstructured kernel SVM on individual superpixels to obtain a set of support vectors, indicated by outlined points. (f) Kernel-transformed features
g
K,S
(
x
i
)
are obtained for each feature vector
x
i
from the kernel products of
x
i
and the support vectors. (g) Data in the
|S|
-dimensional “kernelized” feature space is linearly separable, and can be used to train a linear SSVM. (h) Improved segmentation result. (a) Superpixel segmentation (b) Resulting graph (c) Linear SSVM (d) Linear SSVM segmentation (e) Standard rbf SVM trained (f) Kerneli transform on individual superpixel (g) “kerneli” (h) “kerneli” SSVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-3-source-large.gif
2015,6967781,Fig. 4.,"Comparison of a linear svm, RBF-svm, and a linear SVM trained on the feature vectors described in Section VII-C and kernelized using the support vectors of an RBF-svm. For classification of individual samples (ignoring structure), training a linear SVM on kernelized feature vectors yields a similar performance to a standard RBF-svm. Error bars indicate standard deviation over 10 experiments on the hippocampus dataset with different samples randomly drawn from the training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-4-source-large.gif
2015,6967781,Fig. 5.,"Graphical illustration of the optimization problem posed by (34) for three random constraints shown in different colors. Two rows illustrate the two different cases explained in the main text: a) the minimum lies at the intersection itself, b) the minimum lies on the interior of one of the adjacent segments. In (a) and (b), the areas below the upper envelope are shaded in yellow. Solid lines in (a) represent the linear part of
h
R
, while those in (b) and (c) represent the full objective. Image (c) shows the upper envelope over the three constraints with the optimal
η
shown in magenta.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-5-source-large.gif
2015,6967781,Fig. 6.,Segmentation outlines obtained on one image of the photon receptor dataset using some of methods we tested overlaid in red.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-6-source-large.gif
2015,6967781,Fig. 7.,"Left. A second isotropic stack of neural tissue, similar to the one show in Fig. 1 but this time acquired from the striatum. This stack is made of
318 872×1536
slices with around six-nanometer resolution in all three directions. Right. 3-D rendering of the mitochondria found in a
450×711×318
subvolume.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-7-source-large.gif
2015,6967781,Fig. 8.,Outlines of mitochondria volumes obtained by different methods overlaid in red on a specific slice of the hippocampus dataset of Fig. 1. Our results are closer to the ground truth than the others.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-8-source-large.gif
2015,6967781,Fig. 9.,"Learning curves showing the training and test scores (jaccard index) on the hippocampus dataset as a function of the number of iterations
t
. We report results for the sampling method with and without working set in green and blue respectively. (a) Training set, EM. (b) Test set, EM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-9-source-large.gif
2015,6967781,Fig. 10.,"Learning curves showing the training and test scores, expressed in terms of the jaccard index, on the striatum dataset as a function of the number of iterations
t
. We report results for
Workingsets+inference+autostep
and
SGD+inference
in red and yellow, respectively. (a) Training set, EM. (b) Training set, EM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7097751/6967781/6967781-fig-10-source-large.gif
2015,6942154,Fig. 1.,Correlation coefficient matrix between different kernel matrixes with different scales. (a) Salinas dataset. (b) Indiana Pine dataset. (c) Pavia University dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu1abc-2362116-large.gif
2015,6942154,Fig. 2.,Ground-truth map of the Salinas dataset (16 land cover classes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu2-2362116-large.gif
2015,6942154,Fig. 3.,Ground-truth map of the Indian Pine dataset (12 land cover classes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu3-2362116-large.gif
2015,6942154,Fig. 4.,Ground-truth map of the Pavia University dataset (9 land cover classes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu4-2362116-large.gif
2015,6942154,Fig. 5.,OA in percentage of the seven methods for the AVIRIS Salinas dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu5-2362116-large.gif
2015,6942154,Fig. 6.,OA in percentage of the seven methods for the AVIRIS Indian Pine dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu6-2362116-large.gif
2015,6942154,Fig. 7.,OA in percentage of the seven methods for the ROSIS PaviaU dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu7-2362116-large.gif
2015,6942154,Fig. 8.,Weight coefficients of base kernels for three MKL methods with Indian Pine dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu8-2362116-large.gif
2015,6942154,Fig. 9.,Weight coefficients of base kernels for three MKL methods with ROSIS PaviaU dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu9-2362116-large.gif
2015,6942154,Fig. 10.,Weight coefficients of base kernels for three MKL methods with Salinas dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu10-2362116-large.gif
2015,6942154,Fig. 11.,Computation time of four compared methods. (a) Salinas dataset. (b) Indiana Pine dataset. (c) PaviaU dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/6942154/gu11abc-2362116-large.gif
2015,7214201,Fig. 1.,Photo of the custom made glove-case with an iPhone mounted on.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7214201/kosti1-2471093-large.gif
2015,7214201,Fig. 2.,"Y-axis acceleration (
α
y
(i)
) and rotational velocity (
ω
y
(i)
) spectral analyses for PD patient number three from Table I. These are signals from a Rest Right recording.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7214201/kosti2-2471093-large.gif
2015,7214201,Fig. 3.,Out-of-bag feature importance for 16 features comprising each metric for each posture. Four of them score significantly higher than all others (importance > 0.4).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7214201/kosti3-2471093-large.gif
2015,7214201,Fig. 4.,ROC curve for the classification from the minimized BagDT model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7317613/7214201/kosti4-2471093-large.gif
2015,7109937,Fig. 1.,Polyp detection results using the HOG and SVM detector [9]. False positive detections (top) and missing (bottom) frequently occur when applying the conventional detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-1-source-large.gif
2015,7109937,Fig. 2.,"(a)–(h): the diversity of polyp types: colors, textures and shapes of the polyps are different from each other. (a) Normal rectal tissue, (b) Inflammatory bowel disease (ulcerative colitis: uc), (c) Inflammatory bowel disease (crohn's disease: cd), (d) Tuberculosis (tb), (e) Rectal carcinoid, (f) Early colorectal cancer, (g) Progression of colonrectal cancer, (h) Colorectal other-related disease.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-2-source-large.gif
2015,7109937,Fig. 3.,Similar colors and textures between polyps (left) and non-polyps (right) samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-3-source-large.gif
2015,7109937,Fig. 4.,A data sampling-based boosting framework for learning a polyp detector from imbalanced datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-4-source-large.gif
2015,7109937,Fig. 5.,Polyp and non-polyp patches (left) and their HOG feature maps (right). (a) Positive training samples (polyp patches). (b) Extracted HOG features from positive training samples in (a). (c) Negative training samples (non-polyp patches). (d) Extracted HOG features from negative training samples in (c). (e) False positive samples that do not contain polyps. (f) Extracted HOG features from false positive samples in (e).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-5-source-large.gif
2015,7109937,Fig. 6.,"Weight vectors of HOG features [9] using PLS. From left to right, first learned weight vectors are shown. Each weight vector is displayed as 9 by 4 matrix. Since we use 9 orientation bins and 4 normalization factors for extracting HOG features, each row corresponds to one of the orientation bins and each column to one of the normalization factors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-6-source-large.gif
2015,7109937,Fig. 7.,Feature distribution after applying PCA (a) and PLS analysis (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-7-source-large.gif
2015,7109937,Fig. 8.,Feature distribution before (a) and after (b) up-sampling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-8-source-large.gif
2015,7109937,Fig. 9.,"Down-sampling results: (a) original sample distribution, (b) distribution after 1-NN classification (c) Tomek-linked pairs (d) distribution after removing non-polyps participating in tomek-links for 5 iterations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-9-source-large.gif
2015,7109937,Fig. 10.,Sample patches in our own datasets. We construct our own polyp datasets from 146 endoscopic videos of 141 patients.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-10-source-large.gif
2015,7109937,Fig. 11.,"(a)–(b): using different measures, precision and recall scores for an imbalanced polyp dataset are evaluated. (a) Per-window measure, (b) Per-image measure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-11-source-large.gif
2015,7109937,Fig. 12.,"For the CVC-colon dataset, ground truth and detection regions of the VHOG [29]-SVM (a6) and the proposed (b7) detectors are depicted. (a) Accurately detected polyps by both detectors. (b) Inaccurately detected polyps by the VHOG [29]-SVM (a6) detector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-12-source-large.gif
2015,7109937,Fig. 13.,"Performance evaluation for different imbalanced polyp datasets. Miss rates against FPPIs of the detectors are plotted on a log-log scale. (a) With both datasets, (b) With SET-lg, (c) With SET-sm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-13-source-large.gif
2015,7109937,Fig. 14.,"For performance analysis of the proposed methods, we plot miss rates against FPPIs on a log-log scale. (a) Multi-feature, (b) The dimension of subspaces of pls, (c) The amount of overlap area, (d) PCA vs pls, (e) Up-sampling, (f) Down-sampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-14-source-large.gif
2015,7109937,Fig. 15.,"For CVC-colon and our datasets, polyp detection results using the ESM-up-DW detector trained with SET-sm are shown. More results can be found in our supplementary material.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7310891/7109937/7109937-fig-15-source-large.gif
2015,7128681,Fig. 1.,"ε
-insensitive loss function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7182439/7128681/cao1-2439575-large.gif
2015,7128681,Fig. 2.,Flowchart of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7182439/7128681/cao2-2439575-large.gif
2015,7128681,Fig. 3.,"Study areas in Ayutthaya province, Thailand. (a) SAR image, 50-m resolution with ScanSAR narrow mode (acquired on December 4, 2011). (b) Reference data. (c) Classified map using the SVM model. (d) Classified map using the SVM-PF model from test case 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7182439/7128681/cao3abcd-2439575-large.gif
2015,7128681,Fig. 4.,"Comparison of z-scores for (a) accuracy, (b) precision, and (c) recall of the SVM and SVM-PF methods. (d) Correlation at each iteration of the PF.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7182439/7128681/cao4abcd-2439575-large.gif
2015,7128681,Fig. 5.,Comparison of error rates of SVM and SVM-PF schemes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7182439/7128681/cao5-2439575-large.gif
2015,7102732,Fig. 1.,(a) The cyclic structure of glucose with the numbers specifying bonding sites. (b) Examples of fragmentation patterns of glycans. The cross-ring ions are denoted by the covalent bonds in the ring structure specified by the numbers in gray circles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato1-2430317-large.gif
2015,7102732,Fig. 2.,"The algorithm for de novo glycan sequencing using Lagrangian relaxation.
T
is the maximum number of iterations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato2-2430317-large.gif
2015,7102732,Fig. 3.,"The algorithm for training by structured SVMs with latent variables.
κ>0
is the predefined learning rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato3-2430317-large.gif
2015,7102732,Fig. 4.,Core structures of (a) N-linked glycans and (b) O-linked glycans are indicated by the red ovals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato4-2430317-large.gif
2015,7102732,Fig. 5.,"Accuracy on the N-linked glycan dataset for all the combinations of the three techniques: Lagrangian relaxation (+L), core structure constraints (+C) and structured SVM (+S) through the F-values for the monosaccharides composition, glycosidic bonds with the linkage types, and the similarity of glycan tree structures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato5-2430317-large.gif
2015,7102732,Fig. 6.,Accuracy on the O-linked glycan dataset. See the caption of Fig. 5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato6-2430317-large.gif
2015,7102732,Fig. 7.,"The behavior of Lagrangian relaxation as the maximum number of the iteration
T
is varied. (a) The number of violated constrained over all the datasets. (b) The total elapsed time for all the datasets measured on a Macbook Pro with Intel Core i5 2.5 GHz running on Mac OS X 10.9.1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/7348917/7102732/sato7-2430317-large.gif
2015,7050264,Fig. 1.,UDAL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang1-2401595-large.gif
2015,7050264,Fig. 2.,BDAL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang2-2401595-large.gif
2015,7050264,Fig. 3.,Forward learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang3-2401595-large.gif
2015,7050264,Fig. 4.,Backward learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang4-2401595-large.gif
2015,7050264,Fig. 5.,Comparison of serial and batch modes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang5-2401595-large.gif
2015,7050264,Fig. 6.,BDAL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang6-2401595-large.gif
2015,7050264,Fig. 7.,"2-D synthetic data set.
■
: positive instance.
▲
: negative instance.
∘
: unlabeled instance. –: classifier. (a) Real data distribution and the optimal classifier. (b) Initial training data and the initial classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang7ab-2401595-large.gif
2015,7050264,Fig. 8.,"Synthetic data classification results with
N
(
N=10
, 20, 30, 40, and 50) instances selected and labeled for model update.
■
: positive instance.
▲
: negative instance.
∘
: unlabeled instance.
∙
: backward instance in BDAL. –: classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang8-2401595-large.gif
2015,7050264,Fig. 9.,Handwritten digit classification results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang9-2401595-large.gif
2015,7050264,Fig. 10.,Image classification results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang10-2401595-large.gif
2015,7050264,Fig. 11.,Patent document classification results (with 5% mislabeling).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang11-2401595-large.gif
2015,7050264,Fig. 12.,Patent document classification results (without mislabeling).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7330076/7050264/zhang12-2401595-large.gif
2015,6891215,Fig. 1.,Images used in the experiments. (a) Jeddah. (b) Riyadh. (c) Pavia University.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6899762/6891215/bazi1abc-2349538-large.gif
2015,6891215,Fig. 2.,"OA versus the number of queries obtained by the ELM-RS, the ELM-BT, the ELMRW-RS, and the ELMRW-BT for the (a) Jeddah, (b) Riyadh, and (c) Pavia University data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6899762/6891215/bazi2abc-2349538-large.gif
2015,6891215,Fig. 3.,"OA versus the number of queries obtained by the ELMRW-BT for different values of
λ
for the (a) Jeddah, (b) Riyadh, and (c) Pavia University data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/6899762/6891215/bazi3abc-2349538-large.gif
2015,7101222,Fig. 1.,Diagram of the proposed multitask deep learning framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7234958/7101222/li1-2429556-large.gif
2015,7101222,Fig. 2.,PCA example. PC 1 contains the most energy of the data but does not have any discrimination information for the “red” and “blue” classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7234958/7101222/li2-2429556-large.gif
2015,7101222,Fig. 3.,Multitask deep learning with dropout. “x” denotes a dropped unit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7234958/7101222/li3-2429556-large.gif
2015,7101222,Fig. 4.,Basic RBM model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7234958/7101222/li4-2429556-large.gif
2015,7054452,Fig. 1.,"Workflow of our proposed multimedia retrieval approach. Each data instance (e.g., an image) is represented by a set of features (i.e.,
F
1
,
F
2
, …). In the feature group discovery process, we extract the feature group assignment matrix in which the features with their assigned groups (i.e.,
G
1
,
G
2
,…
) are described. Meanwhile, the weight on each group is obtained via group coding. Subsequently, we add the weights to the features corresponding to the specific groups, and then train the modified feature vectors in the latent structural SVM framework in order to learn a ranking model. For a retrieval task, the feature vector of the query instance is weighted via feature weighting. By using the learned latent structural SVM model, the ranking list corresponding to the query is obtained.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7017605/7054452/7054452-fig-1-source-large.gif
2015,7054452,Fig. 2.,"Workflow of the group information discovery. Each row (or column) corresponds to a samplewise (or featurewise) representation. By achieving the samplewise and featurewise mappings simultaneously, the group information is discovered.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7017605/7054452/7054452-fig-2-source-large.gif
2015,7054452,Fig. 3.,"Illustration of different joint learning schemes. The red lines denote the weights
W
in the neural networks and the blue lines denote the bias value
b
. (a) shows the directly joint learning scheme while (b) displays the joint learning scheme with an auxiliary layer (represented by
p
x
i
and
p
d
j
), which makes the optimization procedure easier. (a) directly joint learning. (b) joint learning with auxiliary layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7017605/7054452/7054452-fig-3-source-large.gif
2015,7054452,Fig. 4.,One retrieval example on the NUSWIDE dataset. The left part shows the query sample while the right part displays a few retrieved images using our approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7017605/7054452/7054452-fig-4-source-large.gif
2015,7054452,Fig. 5.,"Precision-recall curve retrieval performances of different algorithms on the NUSWIDE and caltech256 datasets. Clearly, our approach obtains better precision-recall retrieval results in most cases (a) nuswide (b) caltech256.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7017605/7054452/7054452-fig-5-source-large.gif
2015,6963487,Fig. 1.,"Construct reliable twin regressors for different levels of noise automatically using RALS-TELM and AB-TELM on sinc function. (a) RALS-TELM: U[−0.1, 0.1]. (b) RALS-TELM: U[−0.2, 0.2]. (c) RALS-TELM: U[−0.5, 0.5]. (d) AB-TELM: U[−0.1, 0.1]. (e) AB-TELM: U[−0.2, 0.2]. (f) AB-TELM: U[−0.5, 0.5].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6963487/ning1abcdef-2362555-large.gif
2015,6963487,Fig. 2.,"Adaptive adjustment of
(
w
lower
,
w
upper
)
for different desired PIs using RALS-TELM on the casting problem. (a) PI: 5 °C. (b) PI: 8 °C. (c) PI: 10 °C. (d) PI: 5 °C-iteration. (e) PI: 8 °C-iteration. (f) PI: 10 °C-iteration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6963487/ning2abcdef-2362555-large.gif
2015,6963487,Fig. 3.,"Adaptive adjustment of
(
w
lower
,
w
upper
)
for different desired PIs using AB-TELM on the casting problem. (a) PI: 5 °C. (b) PI: 8 °C. (c) PI: 10 °C. (d) PI: 5 °C-iteration. (e) PI: 8 °C-iteration. (f) PI: 10 °C-iteration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6963487/ning3abcdef-2362555-large.gif
2015,6963487,Fig. 4.,Nonlinear quantile regression using AB-TELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6963487/ning4-2362555-large.gif
2015,6963487,Fig. 5.,KP analysis for the continuous casting problem. (a) RALS-TELM. (b) AB-TELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6963487/ning5ab-2362555-large.gif
2015,6963487,Fig. 6.,Training time with respect to the number of training data: an example on abalone.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6963487/ning6-2362555-large.gif
2015,6874569,Fig. 1.,"Illustration of the SVM, which is a supervised machine-learning framework for classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang1-2342153-large.gif
2015,6874569,Fig. 2.,SVM decision function and effective decision boundary in the feature space using (a) linear kernel and (b) nonlinear kernel to increase the flexibility of the effective decision boundary.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang2ab-2342153-large.gif
2015,6874569,Fig. 3.,Architecture where a small kernel of fault-protected hardware (shown shaded) enables resilient system-level performance; estimates of block gate counts from our system are shown (MCU used is MSP430 from OpenCores [26]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang3-2342153-large.gif
2015,6874569,Fig. 4.,Comparison of classification energy with MCU and with accelerator based system architecture for EEG seizure detector and ECG arrhythmia detector. Numbers are derived from [30].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang4-2342153-large.gif
2015,6874569,Fig. 5.,"Feature vectors for a seizure-detection system (shown in two-dimensions via PCA for visualization). (a) Distribution variances are only due to the application signals and are modeled by the original decision boundary. (b) Variances are also due to computational errors, making a new decision boundary necessary by training on the error-affected data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang5ab-2342153-large.gif
2015,6874569,Fig. 6.,"System for self-construction of error-aware models by estimating training labels using an auxiliary system with low duty cycle, enabled by active learning (shaded blocks are fault protected).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang6-2342153-large.gif
2015,6874569,Fig. 7.,"(a) Active-learning system, where the permanent, fault-affected system chooses data to be used for training. (b) Error-affected feature vectors selected during one iteration using a marginal-distance criterion. Note that though other feature vectors appear closer to the decision boundary, this is in fact an artifact of PCA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang7ab-2342153-large.gif
2015,6874569,Fig. 8.,"Feature vector distributions for (a) baseline case without errors and (b) case with errors, where the MI in the resulting data is degraded.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang8ab-2342153-large.gif
2015,6874569,Fig. 9.,FPGA based experimentation and demonstration flow to enable controllable scaling of fault rate as well as multiple gate-level implementations for randomized fault locations within the circuit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang9-2342153-large.gif
2015,6874569,Fig. 10.,"Logical representation of stuck-at-0/1 faults achieved by introducing multiplexers on randomly-selected outputs; the multiplexers are controllable to set the fault rate, location, and type (i.e., stuck-at-0/1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang10-2342153-large.gif
2015,6874569,Fig. 11.,FPGA test setup (with Xilinx ML509 Virtex 5 boards) for the DUT system and an Ethernet transceiver (enabling data exchange with PC).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang11-2342153-large.gif
2015,6874569,Fig. 12.,"Processors for (a) EEG-based seizure detection and (b) ECG-based cardiac arrhythmia detection used in our demonstrations (for the seizure detector, two of the shown channels are used). Both designs are implemented and synthesized from RTL, with the shaded blocks protected during fault injection (these account for \$\sim \$ 7% for the seizure detector and \$\sim \$ 31% for the cardiac arrhythmia detector).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang12ab-2342153-large.gif
2015,6874569,Fig. 13.,"Bit-error statistics of the computed feature vectors, showing the BERs for different fault levels and the RMS of the feature errors (normalized to the RMS of the true feature values) for (a) and (b) seizure-detection system and (c) and (d) arrhythmia-detection system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang13abcd-2342153-large.gif
2015,6874569,Fig. 15.,"Output histograms from the SVM classifier for seizure-detector test case. (a) Case of the baseline detector without errors. (b) Case of a detector with errors (due to twelve faults), but without DDHR. (c) Case with errors (due to twelve faults) but with DDHR. The errors initially degrade the separation between the class distributions, but DDHR restores the separation enabling classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang15abc-2342153-large.gif
2015,6874569,Fig. 14.,"Performance of the systems with respect to the fault rates, for the cases with and without DDHR (five instances of the system are tested at each fault rate). (a)–(c) Seizure detector performance. (d) and (e) Cardiac-arrhythmia detector performance (where the true-negative rate is set at 95% for all test cases). DDHR consistently restores system performance up to a fault level of 20 nodes for the seizure detector and 480 nodes for the cardiac-arrhythmia detector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang14abcde-2342153-large.gif
2015,6874569,Fig. 16.,Convergence of the error-aware model is achieved with minimal training data within an active-learning and random-learning architecture. The two cases show model convergence within (a) seizure-detection system at a fault level of four nodes and (b) arrhythmia-detection system at a fault level of thirty nodes (very similar profiles are observed at the other fault-rates tested).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang16ab-2342153-large.gif
2015,6874569,Fig. 17.,"Scatter plots of TP rate versus MI and PMI, achieved through DDHR within the (a) and (b) seizure-detection system and (c) and (d) arrhythmia-detection system (note that parameters are intentionally set for a true negative rate of 98% for the seizure detector and 95% for cardiac-arrhythmia detector to facilitate comparison across all cases). A threshold of 0.02 bits and 0.5 bits can be used, respectively, to indicate when DDHR will successfully restore system performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7164363/6874569/wang17abcd-2342153-large.gif
2015,7038151,Fig. 1.,(a) EMG acquisition setup including the textile hose with integrated dry EMG electrodes mounted on able-bodied subject and custom-made dry EMG electrode mounted on preamplifier. (b) Setup mounted on a subject with congenital limb deficiency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7150515/7038151/7038151-fig-1-source-large.gif
2015,7038151,Fig. 2.,Schematic overview of the signal processing chain. Preprocessing and feature extraction blocks are omitted in the batch training path for better readability.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7150515/7038151/7038151-fig-2-source-large.gif
2015,7038151,Fig. 3.,"Experimental paradigm. (a) Display presented to the subject during calibration phase without feedback. The larger green “target-cursor” moves along predefined trajectories—the subject is asked to follow this cursor with wrist deflections. The upcoming target location is indicated with three small green circles to minimize delays between the instruction and user-reaction. (b) Display presented to the subject during performance evaluation and adaptation phases. The subject controls the red cross with muscle contractions and tries to hit the green circle, i.e., remain within the stationary target circle for one second without leaving it.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7150515/7038151/7038151-fig-3-source-large.gif
2015,7038151,Fig. 4.,"Real-time evaluation results for the four control conditions and after adaptation with different
λ
. Blue circles and error bars show means and standard deviations across all able bodied subjects, the red squares and green diamonds indicate the performances of the two congenital subjects. The blue triangles show the mean performances for able-bodied subjects for targets with lower and higher distance to the origin separately. Panel (a)–(d) show completion rate, completion time, overshoot ratio and path efficiency. The plots in panel (e) show post-hoc pair-wise comparisons in which black fields indicate statistical significant differences between the corresponding conditions
(p<0.05)
. The best performance across all metrics is obtained after adaptation with
λ=0.995
. Completion rate and time show similar results for the upscaled condition but the poor overshoot ratio and path efficiency obtained with the upscaled model indicate a significant drop in stability. The subjects with congenital limb deficiency show the same trends as able-bodied subjects and reach a similar performance after co-adaptive real-time learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7150515/7038151/7038151-fig-4-source-large.gif
2015,7038151,Fig. 5.,"Qualitative visualization of control performance for two able-bodied subjects with relatively low (able-bodied 1) and high (able-bodied 2) initial performance and for the two subjects with congenital limb deficiency. The black curves indicate the traces of the control cursor, green circles represent successfully hit targets, red circles missed targets and orange circles targets that were entered but not hit because of insufficient dwell time. For the adaptation runs green circles represent targets that were hit before adaptation started, violet circles targets that were hit during the adaptation phase and red circles targets that were missed despite adaptation. Above the dashed line the four control conditions are presented: initial model tested in the beginning (i1) and towards the end of the session (i2), upscaled output (up) and recalibrated (re). Below, the adaptation runs with different learning constants
λ
are shown (Adapt) and next to it, the test runs for the adapted models (Test). In the initial condition (I1) some parts of the outer regions were often not accessible and remained nearly unchanged in the 2nd test (I2). Recalibration could only partly compensate for this limitation. Upscaling of the output led by definition to an extended range but made the control relatively unstable which led to many overshoots and made fine-control difficult. After adaptation with
λ=0.995
, the range became more uniform and covered almost the entire range, while stability of the control was maintained.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7150515/7038151/7038151-fig-5-source-large.gif
2015,7038151,Fig. 6.,"Adaptation of
w
1
at different adaptation speeds for the two subjects with congenital limb deficiency. Each curve represents the development of one entry of
w
1
during the adaptation process, the bold black line the norm of
w
1
and vertical gray lines separate different adaptation targets. Similar results (not shown) were obtained for
w
2
and the able-bodied subjects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7150515/7038151/7038151-fig-6-source-large.gif
2015,6949691,Fig. 1.,"Block-based representation of the agent for what concerns the hard pointwise constraints
ϕ
i
(gathered into a vector
ϕ
of constraints) and the case
μ=0
(i.e., no soft pointwise constraints are present).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6949691/sangu1-2361866-large.gif
2015,6949691,Fig. 2.,"Optimal solutions to problem LMPC for the numerical example presented in Section VIII-A. Crosses represent supervised examples associated with soft constraints expressed by the quadratic loss (15), while circles depict supervised examples associated with hard bilateral constraints of the form (21). (a) Soft constraints only; (b) mixed hard/soft constraints; (c) hard constraints only.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6949691/sangu2abc-2361866-large.gif
2015,6949691,Fig. 3.,"Optimal solutions to problem LMPC for the first numerical example presented in Section VIII-B. (a) Two negative supervised examples (crosses) are associated with soft constraints expressed by the quadratic loss (15); (b) the unsupervised example (−0,0) has been added (star), for which a hard unilateral constraint of the form (27) is imposed (drawn at
y>0
to make it better visible).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6949691/sangu3ab-2361866-large.gif
2015,6949691,Fig. 4.,"Optimal solution to problem LMPC for the second numerical example presented in Section VIII-B. Supervised examples (crosses) are associated with soft constraints, while unsupervised examples (stars) are associated with hard unilateral constraints (they are drawn at
y>0
to make them better visible).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6949691/sangu4-2361866-large.gif
2015,7024168,Fig. 1.,Graphs of the four functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7024168/sun1-2375209-large.gif
2015,7024168,Fig. 2.,MSE for learning the four functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7024168/sun2-2375209-large.gif
2015,7024168,Fig. 3.,Sparsity of SLSKR in simulations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7287875/7024168/sun3-2375209-large.gif
2015,6991537,Fig. 1.,High-level feature representation of the image patch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han1-2374218-large.gif
2015,6991537,Fig. 2.,Flowchart of WSL-based object detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han2-2374218-large.gif
2015,6991537,Fig. 3.,Learning processes for DBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han3-2374218-large.gif
2015,6991537,Fig. 4.,Illustration of saliency calculation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han4-2374218-large.gif
2015,6991537,Fig. 5.,Some examples in initial positive and negative training sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han5-2374218-large.gif
2015,6991537,Fig. 6.,"Simple illustration of model drift on GMM distribution. In (a)–(c), the distribution of positive GMM is in red color, whereas the distribution of negative GMM is in blue color. In (d), how the value changes in each iteration is shown. It is based on the iterative training of airport detector. See text for detailed explanation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han6abcd-2374218-large.gif
2015,6991537,Fig. 7.,Influence of key parameters to training example initialization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han7-2374218-large.gif
2015,6991537,Fig. 8.,Evaluation of the proposed Bayesian framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han8-2374218-large.gif
2015,6991537,Fig. 9.,"PR curves for different types of feature on three data sets. Here, DBM indicates the high-level feature learned by the proposed work.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han9-2374218-large.gif
2015,6991537,Fig. 10.,PR curves for the comparisons with the WSL method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han10-2374218-large.gif
2015,6991537,Fig. 11.,PR curves for the comparisons with the supervised learning methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han11-2374218-large.gif
2015,6991537,Fig. 12.,Some samples from the three benchmark data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7032018/6991537/han12-2374218-large.gif
2015,6826504,Fig. 1.,Outputs of DA-ELM in SinC function approximation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng1-2325594-large.gif
2015,6826504,Fig. 2.,"Average testing accuracy rate of ELM, SBLLM, and DA-ELM algorithms in letter recognition data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng2-2325594-large.gif
2015,6826504,Fig. 3.,"Average training time (second) of ELM, SBLLM, and DA-ELM algorithms in letter recognition data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng3-2325594-large.gif
2015,6826504,Fig. 4.,"Average testing accuracy rate of ELM, SBLLM, and DA-ELM algorithms in shuttle data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng4-2325594-large.gif
2015,6826504,Fig. 5.,"Average training time (second) of ELM, SBLLM, and DA-ELM algorithms in shuttle data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng5-2325594-large.gif
2015,6826504,Fig. 6.,"Average testing accuracy rate of ELM, SBLLM, and DA-ELM algorithms in image steganalysis case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng6-2325594-large.gif
2015,6826504,Fig. 7.,"Average training time (second) of ELM, SBLLM, and DA-ELM algorithms in image steganalysis case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7008577/6826504/feng7-2325594-large.gif
2015,7018950,Fig. 1.,Example of an expansion step: Replacement of a leaf node by a three-node tree. (a) Before replacement. (b) After replacement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7018950/hulle1ab-2396078-large.gif
2015,7018950,Fig. 2.,Top-down algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7018950/hulle2-2396078-large.gif
2015,7018950,Fig. 3.,"In this example, three of the six models (Box 1–3) remain in the race after this iteration. Models 4–6 can be excluded because their mean error is unlikely to become better than the one of the current best model (Box 2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7018950/hulle3-2396078-large.gif
2015,7018950,Fig. 4.,"Models compared to calculate the potential of leaf
L
(a)
M
L
(b)
M
L
Opt
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7018950/hulle4ab-2396078-large.gif
2015,7018950,Fig. 5.,"Results of the comparison between PTTD and PTTD-PH for different values of the
d
max
parameter. The upper diagram shows the training time and the lower one the predictive accuracy of the methods (including standard error bars).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7018950/hulle5-2396078-large.gif
2015,7018950,Fig. 6.,"Results of the comparison between PTTD, PTTD-R, and PTTD-fast for different training set sizes. The upper diagram shows the training time of all three variants. For better visualization, the middle one only shows PTTD-R and PTTD-fast. Finally, the lower diagram shows the predictive accuracy of the methods. All diagrams are equipped with standard error bars.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/7337513/7018950/hulle6-2396078-large.gif
2015,7041228,Fig. 1.,"Second-order face recognition datasets. (a) Yale
64×64
samples. (b) ORL
64×64
samples. (c) C07 samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7065385/7041228/yang1abc-2403235-large.gif
2015,7041228,Fig. 2.,Gait silhouette sequence for third-order gait recognition datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7065385/7041228/yang2-2403235-large.gif
2015,7041228,Fig. 3.,Third-order HIV neuroimage dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7065385/7041228/yang3-2403235-large.gif
2015,7041228,Fig. 4.,Third-order Product Image Categorization dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7065385/7041228/yang4-2403235-large.gif
2015,7041228,Fig. 5.,"The effect curves of
R
on test accuracy on (a) Yale
32×32
_3, Yale
64×64
_3, (b) ORL
32×32
_3, ORL
64×64
_3, (c) C07_3, C27_3, (d) USFGait17_
32×22×10
_3, and USFGait17_
64×44×20
_3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7065385/7041228/yang5abcd-2403235-large.gif
2015,7041228,Fig. 6.,"The effect curves of
R
on training time on (a) Yale
32×32
_3, Yale
64×64
_3, (b) ORL
32×32
_3, ORL
64×64
_3, (c) C07_3, C27_3, (d) USFGait17_
32×22×10
_3, and USFGait17_
64×44×20
_3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7065385/7041228/yang6abcd-2403235-large.gif
2015,6933939,Fig. 1.,"Simplified flowchart of machine learning techniques: (a) supervised, (b) unsupervised learning, (c) semi-supervised learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao1-2364203-large.gif
2015,6933939,Fig. 2.,Overview of the proposed fault detection and classification (FDC) model for PV systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao2-2364203-large.gif
2015,6933939,Fig. 3.,Overlapping MPPs of a PV array over a wide range of irradiance and temperature. This is difficult for FDC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao3-2364203-large.gif
2015,6933939,Fig. 4.,"V
NORM
versus
I
NORM
of a PV array over a wide range of irradiance and temperature. This is better for FDC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao4-2364203-large.gif
2015,6933939,Fig. 5.,Graph data consisting of edges and nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao5-2364203-large.gif
2015,6933939,Fig. 6.,Illustrative example of GBSSL for PV. (a) Initial labels x1 and x2 with unlabeled PV data. (b) Unlabeled data are classified by GBSSL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao6ab-2364203-large.gif
2015,6933939,Fig. 7.,Overview of the proposed GBSSL model for FDC in PV systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao7-2364203-large.gif
2015,6933939,Fig. 8.,LL faults and open-circuit faults in a simulated PV system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao8-2364203-large.gif
2015,6933939,Fig. 9.,"Illustrative example: new data points x1 and x3 will be classified as classes “NORMAL” and “OPEN,” respectively, by the proposed GBSSL model. Similarly, data points x2 and x4 will be classified as “LL.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao9-2364203-large.gif
2015,6933939,Fig. 10.,"New fault data: “OPEN3” and “OPEN4” can be identified successfully, but “LL 10%
R
f
= 0,” “LL 10%
R
f
= 10,” “LL 20%
R
f
= 0,” and “LL 20%
R
f
= 10” may be difficult to detect since they are overlapping with “NORMAL.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao10-2364203-large.gif
2015,6933939,Fig. 11.,Schematic diagram of the experimental PV system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao11-2364203-large.gif
2015,6933939,Fig. 12.,Photo of the experimental PV system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao12-2364203-large.gif
2015,6933939,Fig. 13.,Initial labels for GBSSL in experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao13-2364203-large.gif
2015,6933939,Fig. 14.,"GBSSL results of NORMAL, LL, and OPEN1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/6996060/6933939/zhao14-2364203-large.gif
2015,6926841,Fig. 1.,"Top four levels and the last level of the MLD dictionary, where the number of atoms are estimated using the MDL procedure. It comprises of geometric patterns in the first few levels and stochastic textures in the last level. Since each level has a different number of atoms, each subdictionary is padded with zero vectors, which appear as black patches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia1-2361052-large.gif
2015,6926841,Fig. 2.,Minimum MDL score of each level. The information-theoretic complexity of the subdictionaries increases with the number of levels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia2-2361052-large.gif
2015,6926841,Fig. 8.,"Demonstration of the generalization characteristics of the proposed MLD and RMLD algorithms. We plot the MSE obtained by representing patches from the test dataset, using dictionaries learned with a different number of training patches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia8-2361052-large.gif
2015,6926841,Fig. 3.,Illustration for showing the stability of cluster centroids from the stability of distortion function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia3-2361052-large.gif
2015,6926841,Fig. 4.,"Residual set
{
Ψ
¯
l,j
(β+dβ)}
, for the 1-D subspace
ψ
l,j
, lying in its orthogonal complement subspace
ψ
⊥
l,j
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia4-2361052-large.gif
2015,6926841,Fig. 5.,"Demonstration of the stability behavior of the proposed MLD learning algorithm. The minimum Frobenius norm between the difference of two dictionaries with respect to permutation of their columns and signs is shown. The second dictionary is obtained by replacing a different number of samples in the training set, used for training the original dictionary, with new data samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia5-2361052-large.gif
2015,6926841,Fig. 6.,"Choosing the number of rounds (
R
) in RMLD learning. In this demonstration, RMLD design was carried out using 100 000 samples and we observed that beyond 10, both the train MSE and the test MSE do not change significantly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia6-2361052-large.gif
2015,6926841,Fig. 7.,"Reconstruction of novel test data using MLD and RMLD for the case
T=100000
. The approximation error is plotted against the number of levels used for reconstruction with both the dictionaries.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia7-2361052-large.gif
2015,6926841,Fig. 9.,"Compressed recovery of images from random measurements (
N=8
and SNR of measurement process = 15 dB) using the different dictionaries. In each case, the PSNR of the recovered image is also shown. (a) Online OMP (24.73 dB). (b) Online
ℓ
1
(25.69 dB). (c) MLD-MulP (26.02 dB). (d) RMLD-MulP (27.41 dB).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7206790/6926841/jthia9abcd-2361052-large.gif
2015,7335566,Fig. 1.,"Considered case-study area: (a) root-mean-square deviation in
ms
−1
between ECMWF and SAR data and (b) number of available SAR-ECMWF pairs at each grid point in the given study area. An SAR-ECMWF pair is formed by SAR wind field and the temporally closest ECMWF field. The SAR-ECMWF time difference remains lower than 3 h. In each grid point, the root-mean-square deviation is calculated with all available SAR wind fields acquired from 2005 to 2010 and the temporally closest ECMWF fields. The overpass time of ENVISAT in Bergen coast sea is around 21 h30 UTC for ascending passes and around 10 h UTC for descending passes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he1ab-2496503-large.gif
2015,7335566,Fig. 2.,"Wind roses at point (N62,23°, E5,90°): this fjord point corresponds to location 4 in Fig. 5. It accounts for a inner fjord conditions with surrounding mountains. We compare ECMWF (left) and SAR (right) wind statistics. Overall, for this grid point, 520 SAR-ECMWF data pairs are available.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he2-2496503-large.gif
2015,7335566,Fig. 3.,"Local information scheme: the nine grid points in the local neighborhood defined by the yellow box around the LR grid point
q
are used to define regression variables.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he3-2496503-large.gif
2015,7335566,Fig. 4.,"Conditional entropy values (11) for a coastal HR grid point: the red square with black face shows the HR grid point
p
and the red circles indicate the nine LR grid points with the lowest conditional entropy values (11). The conditional entropy varies between 0 and 1.59 (
log
2
(3)
) in our case. High entropy corresponds to high uncertainty.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he4-2496503-large.gif
2015,7335566,Fig. 5.,"Study area and points selected for the evaluation of regression error statistics: the study area is southwest coast sea (in blue) of Bergen described in Section II. The grid points 1 to 4 account for fjord, 5 to 8 for coastal, and 9 to 12 for offshore condition evaluations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he5-2496503-large.gif
2015,7335566,Fig. 6.,"Comparison of the three types of regression variables: global information (blue circles, dashdots); local information (red x-marks, dashdot lines); and entropy-based information (black diamonds, solid lines). We report the mean regression error in
ms
−1
for different regression models: (a) MLR and (b) SVR. The study area is located to the west of Bergen in the Norwegian Sea. The data used are described in Section II. We proceed to cross-validation experiments (cf. Section III-E) to evaluate regression error statistics.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he6ab-2496503-large.gif
2015,7335566,Fig. 7.,"Influence of the number of regression variables (x-axis) on the prediction error (y-axis) at grid point 8 [(N62,07°, E5,22°) in Fig. 5]: nearest-neighbor regression (blue circles), K-nearest neighbors (green x-marks), MLR (red squares), and SVR (black diamonds). For each grid point and for each regression method, we compare two types of regressions variables, namely local information (dashdot lines) and entropy-based information (solid lines) (see Section III for details).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he7-2496503-large.gif
2015,7335566,Fig. 8.,"Mean downscaling error in
ms
−1
(y-axis) at the different HR grid points (x-axis, see Fig. 5 for their locations) for different approaches: nearest-neighbor model (NN, blue circles), K-nearest neighbor model (AN, green x-marks), MLR (red squares), and SVR (black diamonds). For each method, we compare downscaling performance using local information (dashdot lines) and entropy-based information (solid lines) (see Section III for details).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he8-2496503-large.gif
2015,7335566,Fig. 9.,"Mean eastern wind along (a) transect perpendicular to the coast (a, red line) in
ms
−1
: ECMWF (blue squares), SAR (red stars), and downscaled (black diamonds) fields. The x-axis refers to the (a) distance from the easternmost point (N61.09°, E6.50°) toward offshore. The mean eastern wind is computed at each grid point as the mean wind magnitude of all wind data with a wind direction between
−
45
∘
and 45°.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he9ab-2496503-large.gif
2015,7335566,Fig. 10.,"Distribution of the wind data (direction and speed) at point 4 (N62,23°, E5,90°) in Fig. 5: ECMWF data (left), SAR data (middle), and downscaled data (right). We compute the wind roses as in Fig. 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he10-2496503-large.gif
2015,7335566,Fig. 11.,"Wind speed scatterplots among ECMWF, SAR, and downscaled wind data for (a) grid points 4 (N62.23°, E5.90°) and (b) 8 (N62.07°, E5.22°): ECMWF versus SAR (blue circles), downscaled versus SAR (black stars). x-axis denotes SAR winds and y-axis denotes ECMWF or downscaled winds. Wind speed is in
ms
−1
. The red lines represent a perfect match. Regarding downscaled versus SAR scatterplot, we exploit the downscaled wind fields generated within the k-fold cross-validation procedure for the randomly generated validation datasets (see Section III-E for details).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he11ab-2496503-large.gif
2015,7335566,Fig. 12.,"Examples of downscaled wind fields for two dates, (a) 2009-07-18 and (b) 2006-02-05: from left to right, ECMWF field, SAR field, and downscaled field. For the first example, the ECMWF data are delivered at 12 h UTC and the SAR data are acquired at 10 h09 UTC. For the second example, the ECMWF data are delivered at 00 h UTC 2006-02-06 and the SAR data are acquired at 21 h05 UTC. There is no emulation for the white points that corresponds to oil platform locations where SAR wind measurements are erroneous. The color and the arrows indicate the wind speed in
ms
−1
and the wind direction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7377134/7335566/he12ab-2496503-large.gif
2015,6949090,Fig. 1.,"Layout of the SA and sensor placement. Sensor labels starting with “M” indicates motion sensors, “L” indicates light sensors, “D” indicates door sensors, “T” indicates temperature sensors, and “I” indicates item sensors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz1-2362529-large.gif
2015,6949090,Fig. 2.,Individual accesses the DMN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz2-2362529-large.gif
2015,6949090,Fig. 3.,Experimenter observes a participant performing everyday activities via web cameras and logs information using the RAT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz3-2362529-large.gif
2015,6949090,Fig. 4.,"Comparison of a) AUC and b) RMSE between RuLSIF Online versus Offline mode. As expected, the Online mode suffers a small decrease in performance. Error bars represent the standard error of the mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz4ab-2362529-large.gif
2015,6949090,Fig. 5.,"AUC scores for supervised learning on the scripted and unscripted datasets. AR-DT, AR-LR, AR-BOOST and AR-SVM show significant improvement over the baseline techniques of Fixed and Random in the scripted setting while only AR-DT and AR-LR outperform the baselines in the unscripted setting. Error bars represent the standard error of the mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz5-2362529-large.gif
2015,6949090,Fig. 6.,RSME scores for supervised learning on the scripted and unscripted datasets. All of the AR techniques outperform the baseline techniques of Fixed and Random. Error bars represent the standard error of the mean.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz6-2362529-large.gif
2015,6949090,Fig. 7.,"AUC scores for the unsupervised learning technique on the scripted and unscripted datasets. AR-LR is a supervised learning technique, but is included for comparison purposes. RulSIF outperforms the baseline techniques. Error bars represent the standard error of the mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz7-2362529-large.gif
2015,6949090,Fig. 8.,"RMSE scores for the unsupervised learning technique on the scripted and unscripted datasets. AR-LR is a supervised learning technique, but is included for comparison purposes. RulSIF outperforms the baseline techniques and even outperforms AR-LR in the unscripted setting. Error bars represent the standard error of the mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz8-2362529-large.gif
2015,6949090,Fig. 9.,ROC curves for both the supervised and unsupervised learning algorithms on the unscripted and scripted data. Which algorithms has the best performance depends on the desired tradeoff between the TPR and FPR. In both the scripted and unscripted settings a TPR of greater than 80% can be achieved while maintaining a FPR of less than 15%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7265128/6949090/feuz9-2362529-large.gif
2015,7069208,Fig. 1.,Examples of cut-points in different conditions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7177178/7069208/bruni1-2416727-large.gif
2015,6880767,Fig. 1.,"Prediction results of the HDL and LDL cholesterol levels using a combination of several measures based on a correlation-based variable selection technique and two machine learning algorithms (AUC: area under the receiver operating characteristics curve, HDL-NB: high-density lipoprotein cholesterol level prediction by naïve Bayes, HDL-LR: high-density lipoprotein cholesterol level prediction by logistic regression, LDL-NB: low-density lipoprotein cholesterol level prediction by naïve Bayes, LDL-LR: low-density lipoprotein cholesterol level prediction by logistic regression).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7234958/6880767/lee1-2350014-large.gif
2015,7010879,Fig. 1.,"Example of LBP binary thresholding. (a) Center pixel
t
c
and its eight circular neighbors
{
t
i
}
7
i=0
with radius
r=1
. (b) 3 × 3 sample block. (c) Binary labels of eight neighbors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li1abc-2381602-large.gif
2015,7010879,Fig. 2.,"Example of LBP versus Gabor filter. (a) Input image. (b) LBP-coded image (different intensities representing different codes). (c)–(f) Filtered images obtained by the Gabor filter with different
θ
values. (a) Input image. (b) LBP-coded image. (c) Gabor feature image,
θ=0
. (d) Gabor feature image,
θ=π/4
. (e) Gabor feature image,
θ=π/2
. (f) Gabor feature image,
θ=3π/4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li2abcdef-2381602-large.gif
2015,7010879,Fig. 3.,Implementation of LBP feature extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li3-2381602-large.gif
2015,7010879,Fig. 4.,Flowchart of the proposed classification framework. (a) Feature-level fusion. (b) Decision-level fusion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li4ab-2381602-large.gif
2015,7010879,Fig. 5.,LBP-ELM using the University of Pavia data: classification performance versus different patch sizes and numbers of selected bands.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li5-2381602-large.gif
2015,7010879,Fig. 6.,Thematic maps resulting from classification for the University of Pavia data set with nine classes. (a) Pseudocolor image. (b) Ground truth map. (c) SVM: 80.01%. (d) ELM: 79.40%. (e) Gabor-SVM: 87.37%. (f) LBP-SVM: 89.54%. (g) FF-SVM: 96.61%. (h) DF-SVM: 97.85%. (i) Gabor-ELM: 90.37%. (j) LBP-ELM: 89.49%. (k) FF-ELM: 98.11%. (l) DF-ELM: 99.25%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li6abcdefghijkl-2381602-large.gif
2015,7010879,Fig. 7.,Thematic maps resulting from classification for the Indian Pines data set with 16 classes. (a) Pseudocolor image. (b) Ground truth map. (c) SVM: 75.14%. (d) ELM: 73.72%. (e) Gabor-SVM: 86.82%. (f) LBP-SVM: 90.63%. (g) FF-SVM: 91.21%. (h) DF-SVM: 92.21%. (i) Gabor-ELM: 88.18%. (j) LBP-ELM: 92.03%. (k) FF-ELM: 92.93%. (l) DF-ELM: 93.58%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li7abcdefghijkl-2381602-large.gif
2015,7010879,Fig. 8.,Thematic maps resulting from classification for the Salinas data set with 16 classes. (a) Pseudocolor image. (b) Ground truth map. (c) SVM: 89.48%. (d) ELM: 90.33%. (e) Gabor-SVM: 91.93%. (f) LBP-SVM: 97.53%. (g) FF-SVM: 98.29%. (h) DF-SVM: 98.67%. (i) Gabor-ELM: 94.16%. (j) LBP-ELM: 98.26%. (k) FF-ELM: 99.12%. (l) DF-ELM: 99.63%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li8abcdefghijkl-2381602-large.gif
2015,7010879,Fig. 9.,Classification performance of Spec-ELM and DF-ELM with different numbers of training sample sizes for three experimental data. (a) University of Pavia. (b) Salinas. (c) Indian Pines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/7056619/7010879/li9abc-2381602-large.gif
2015,7063914,Fig. 1.,Typical scheme of a single hidden layer feedforward neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang1-2400636-large.gif
2015,7063914,Fig. 2.,Schematic flowchart of the proposed SMIR method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang2-2400636-large.gif
2015,7063914,Fig. 3.,Location of the Lake Nicaragua.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang3-2400636-large.gif
2015,7063914,Fig. 4.,Time series of the cloud coverage over Lake Nicaragua with (a) 5 and (b) 11 years composition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang4ab-2400636-large.gif
2015,7063914,Fig. 5.,Flow chart of data input/output in the case study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang5-2400636-large.gif
2015,7063914,Fig. 6.,"Candidate inputs (left) and screened training inputs (right) used for machine learning. The baseline Terra-MODIS image was acquired on January 5, 2013.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang6-2400636-large.gif
2015,7063914,Fig. 7.,"Performance of ELM and trainscg varying with the number of hidden neurons.
P
is the number of candidate images in this study. (a) CPU time. (b) rmse. (c) Correlation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang7abc-2400636-large.gif
2015,7063914,Fig. 8.,"Comparisons between spatial distribution of the predicted reflectance value with SMIR and true data value. (a) Location of the selected region for reconstruction (outlined with red rectangle,
20×20
pixels). (b) Spatial distribution of the observed MODIS-Terra reflectance data at 531 nm on January 5, 2013. (c) Simulated value with ELM-based SMIR. (d) Simulated value with trainscg-based SMIR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang8abcd-2400636-large.gif
2015,7063914,Fig. 9.,Comparisons between the observed and simulated pixel values over the selected region.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang9-2400636-large.gif
2015,7063914,Fig. 10.,Scatter plots between the observed and simulated pixel values with (a) ELM-based and (b) trainscg-based SMIR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang10ab-2400636-large.gif
2015,7063914,Fig. 11.,"Comparisons of MODIS-Terra reflectance images (at 531 nm on January 5, 2013) with small cloud cover before and after cloud removal with different training algorithms utilized by SMIR. (a) Cloudy images on January 5, 2013. (b) Reconstructed image from ELM-based SMIR. (c) Reconstructed images from trainscg-based SMIR. POC denotes percentage of clouds. Note the spectral differences shown in the outlined regions (red rectangles) in reconstructed images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang11abc-2400636-large.gif
2015,7063914,Fig. 12.,"Comparisons of MODIS-Terra reflectance images (at 531 nm on July 13, 2013) under severe cloud cover before and after cloud removal with different training algorithms utilized by SMIR. (a) Cloudy images on July 13, 2013. (b) Reconstructed image from ELM-based SMIR. (c) Reconstructed images from trainscg-based SMIR. POC denotes percentage of clouds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7161423/7063914/chang12abc-2400636-large.gif
2015,7384530,Fig. 1.,Deep learning and deep inference algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-1-source-large.gif
2015,7384530,Fig. 2.,Learning of convolutional deep belief networks. (a) Complex functions of pre-training in CDBN. (b) Semi-supervised learning. (c) Required random numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-2-source-large.gif
2015,7384530,Fig. 3.,Comparison results: pose recognition rate versus precision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-3-source-large.gif
2015,7384530,Fig. 4.,"Relation between resolution, scale, and performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-4-source-large.gif
2015,7384530,Fig. 5.,Two RNG systems. (1) local RNG system. (2) global RNG system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-5-source-large.gif
2015,7384530,Fig. 6.,Overall architecture of an implemented dl/di processor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-6-source-large.gif
2015,7384530,Fig. 7.,Deep learning core architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-7-source-large.gif
2015,7384530,Fig. 8.,Energy saving result of task-level pipeline balancing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-8-source-large.gif
2015,7384530,Fig. 9.,Deep inference core architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-9-source-large.gif
2015,7384530,Fig. 10.,Dual-layered architecture of a dl/di processor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-10-source-large.gif
2015,7384530,Fig. 11.,True random number generator architecture and its quality effect. (a) True random number generator architecture. (b) Quality effect of TRNG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-11-source-large.gif
2015,7384530,Fig. 12.,Chip measurement results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-12-source-large.gif
2015,7384530,Fig. 13.,Chip photograph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7394205/7384530/7384530-fig-13-source-large.gif
2015,6818403,Fig. 1.,"Sinc function values versus 100 uniformly spaced samples in the interval
[−10,10]
. Sparse Bayesian regression with noisy sinc data: predicted response of (a) AKORVM with
h=0.1
, (b) RVM with fixed kernel width
b=0.1
, (c) AKORVM with
h=4
, (d) RVM with fixed kernel width
b=4
, (e) AKORVM with
h=30
, and (f) RVM with fixed kernel width
b=30
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6818403/sheik1abcdef-2321134-large.gif
2015,6818403,Fig. 2.,"Convergence comparison of AKORVM (
h=1
) and the standard RVM (
b=1
) on Tipping sinc data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6818403/sheik2-2321134-large.gif
2015,6818403,Fig. 3.,Comparison of regression error rate versus kernel width initial value for AKORVM and the standard RVM on Tipping sinc data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6818403/sheik3-2321134-large.gif
2015,6818403,Fig. 4.,Comparison of the number of RVs versus kernel width initial value for AKORVM and the standard RVM on Tipping sinc data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6818403/sheik4-2321134-large.gif
2015,6818403,Fig. 5.,"Multiscale data regression using standard RVM with kernel width (a)
b=0.1
, (b)
b=0.5
, and (c)
b=1
, MKRVM with resolution (d) 0.1, (e) 0.5, and (f) 1, AKL with initial kernel width (g)
b=0.1
, (h)
b=0.5
, and (i)
b=1
, and AKORVM with the geometric mean of kernel widths (j)
h=0.1
, (k)
h=0.5
, and (l)
h=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6818403/sheik5abcdefghijkl-2321134-large.gif
2015,6818403,Fig. 6.,Kernel width values corresponding to the RVs in Fig. 5(l).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7061550/6818403/sheik6-2321134-large.gif
2015,7389806,Fig. 1.,Proposed arbitrary downsizing architecture for HEVC video.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/30/7389767/7389806/7389806-fig-1-source-large.gif
2015,7389806,Fig. 2.,The misalignment of co-located input cus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/30/7389767/7389806/7389806-fig-2-source-large.gif
2015,7389806,Fig. 3.,"Ntree selection for RF, with block size = 32. The input bit stream was encoded using qp = 32 and is downsized by a factor of 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/30/7389767/7389806/7389806-fig-3-source-large.gif
2015,7389806,Fig. 4.,Feature importance for 50 runs. The variance of the feature importance becomes smaller as the average importance reaches zero.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/30/7389767/7389806/7389806-fig-4-source-large.gif
2015,7389806,Fig. 5.,"Feature selection for transcoding the fourpeople sequence. The input video (qp = 27), is downsized by a scaling factor of 1.5. The standard deviation of important features is larger than for the noisy features, which have a standard deviation close to zero. The threshold, which is the minimum of the cart model, results in selecting the 29 most important features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/30/7389767/7389806/7389806-fig-5-source-large.gif
2015,7389806,Fig. 6.,The transcoding complexity using rf can be controlled by adjusting the threshold of the prediction confidence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/30/7389767/7389806/7389806-fig-6-source-large.gif
2015,6877713,Fig. 1.,ELM architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6877713/yap1-2341655-large.gif
2015,6877713,Fig. 2.,Framework of F-ELM that exemplifies the case of two hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6877713/yap2-2341655-large.gif
2015,6877713,Fig. 3.,CW system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7124555/6877713/yap3-2341655-large.gif
2015,7118198,Fig. 1.,(a) An example frame. (b) The silhouette obtained by pre-processing the frame image in (a).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng1ab-2441634-large.gif
2015,7118198,Fig. 2.,"Illustration for the effect of temporal information. (a) Two walk sequences performed by two different persons. (b) The original pairwise Euclidean distances
d
a
of those two walk sequences. (c) The pairwise distances
d
a
d
st
by temporal information embedding. (d) The 2D embedding results of those two walk sequences by t-SNE. (e) The 2D embedding results by ST-tSNE, our proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng2abcde-2441634-large.gif
2015,7118198,Algorithm 1,Supervised Temporal t-SNE Methods (S-tSNE and ST-tSNE),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng16-2441634-large.gif
2015,7118198,Fig. 3.,The 2D embedding results of action DatasetA by four methods. (a) SNE. (b) t-SNE. (c) S-tSNE. (d) ST-tSNE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng3abcd-2441634-large.gif
2015,7118198,Fig. 4.,The concept of LLE for linear reconstruction. (a) High-dimensional space. (b) Low-dimensional space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng4ab-2441634-large.gif
2015,7118198,Fig. 5.,The difference between LLE and LPP in constructing linear weights.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng5ab-2441634-large.gif
2015,7118198,Fig. 6.,The cosine similarity measure of coefficients between high- and low-dimensional space for LLE and LPP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng6-2441634-large.gif
2015,7118198,Fig. 7.,Some silhouette frames of action sub-sequences. (a) Action pick up from viewpoint of Camera 2 by actor alba. (b) Action set down from viewpoint of Camera 2 by actor alba. (c) Action pick up from viewpoint of Camera 4 by actor hedlena. (d) Action set down from viewpoint of Camera 4 by actor hedlena.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng7abcd-2441634-large.gif
2015,7118198,Fig. 8.,The 2D embedding results of action from Camera 4 by four methods. (a) SNE. (b) t-SNE. (c) S-tSNE. (d) ST-tSNE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng8abcd-2441634-large.gif
2015,7118198,Fig. 9.,"The influence to action recognition by parameter
τ
in S-tSNE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng9-2441634-large.gif
2015,7118198,Fig. 10.,"The influence to action recognition by neighbor parameter
k
in ST-tSNE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng10-2441634-large.gif
2015,7118198,Fig. 11.,The optimal recognition results by four dimensionality reduction methods and three incremental learning methods. The number above the bar represents the corresponding feature dimensions with the best results after dimensionality reduction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng11-2441634-large.gif
2015,7118198,Fig. 12.,"Confusion matrixes by SNE, t-SNE, S-tSNE and ST-tSNE. For each matrix, the entry at row
A
and column
B
denotes the percentage of classifying action
A
as the action
B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng12-2441634-large.gif
2015,7118198,Fig. 13.,Example silhouette frames of 10 actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng13-2441634-large.gif
2015,7118198,Fig. 14.,The recognition results on DatasetA by S-tSNE and ST-tSNE with three incremental learning methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng14ab-2441634-large.gif
2015,7118198,Fig. 15.,Example silhouette frames from the gait sequences. (a) Silhouette frames of subject LSL. (b) Silhouette frames of subject ZL. (c) Silhouette frames of subject ZJG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7116674/7118198/cheng15abc-2441634-large.gif
2015,7239600,Fig. 1.,"Pipeline of unconstrained multimodal multi-label learning. Given incomplete multimodal data (e.g., bimodal image and tag), during training, we first extract features for images and then use them to generate missing tag features. All the features are discriminatively fused into shared representations by using co-occurred labels as supervision. During testing, given only images, the model can generate the corresponding missing tag features, and then obtain the shared representations to perform the classification and retrieval tasks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-1-source-large.gif
2015,7239600,Fig. 2.,"Multimodal rbm, multimodal conditional rbm, and multi-label conditional restricted boltzmann machine (ML-crbm). Note that the ML-crbm is actually a hybrid graph containing both undirected and directed connections. For modality generation, we regard the visible variables
m
as an additional fixed input and model dependency relationships across modalities by the directed connections. (a) multimodal RBM. (b) multimodal conditional RBM. (c) ml-CRBM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-2-source-large.gif
2015,7239600,Fig. 3.,Deep multi-label conditional restricted boltzmann machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-3-source-large.gif
2015,7239600,Fig. 4.,Class-wise improvement curves for unconstrained multimodal multi-label classification on the MIR flickr and the NUS-wide datasets. (a) mir flickr. (b) nus-WIDE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-4-source-large.gif
2015,7239600,Fig. 5.,Visualization of incorrect label co-occurrence matrices on the MIR flickr dataset. (a) svm. (b) dbm. (c) ml-CRBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-5-source-large.gif
2015,7239600,Fig. 6.,Visualization of incorrect label co-occurrence matrices on the NUS-wide dataset. (a) svm. (b) dbm. (c) ml-CRBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-6-source-large.gif
2015,7239600,Fig. 7.,Visualization of incorrect label co-occurrence cubes on the MIR flickr dataset. (a) svm. (b) dbm. (c) ml-CRBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-7-source-large.gif
2015,7239600,Fig. 8.,Precision-recall curves for (unconstrained) multimodal multi-label retrieval on the MIR flickr dataset. (a) multimodal multi-label retrieval. (b) unconstrained multimodal multi-label retrieval.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-8-source-large.gif
2015,7239600,Fig. 9.,Results of unconstrained multimodal multi-label retrieval (image query) by ML-crbm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-9-source-large.gif
2015,7239600,Fig. 10.,Results of unconstrained multimodal multi-label retrieval (tag query) by ML-crbm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-10-source-large.gif
2015,7239600,Fig. 11.,Comparison of four encoding methods for class labels. (a) one-hot. (b) one-versus-all. (c) multi-hot. (d) multi-task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6046/7302100/7239600/7239600-fig-11-source-large.gif
2015,7104110,Fig. 1.,"Change in
V
TH
with Time at different
V
sd
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7298/7234980/7104110/khand1-2431436-large.gif
2015,7104110,Fig. 2.,Differential amplifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7298/7234980/7104110/khand2-2431436-large.gif
2015,7104110,Fig. 3.,Variation of gain and slew-rate with width.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7298/7234980/7104110/khand3-2431436-large.gif
2015,7104110,Fig. 4.,Variation of gain and slew-rate with time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7298/7234980/7104110/khand4-2431436-large.gif
2015,7104110,Fig. 5.,Flowchart of proposed methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7298/7234980/7104110/khand5-2431436-large.gif
2015,7104110,Fig. 6.,Correlation curve for gain and slew-Rate between SVM and HSPICE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7298/7234980/7104110/khand6-2431436-large.gif
2015,7018910,Fig. 1.,Illustration of RBM. The top layer represents the hidden units and the bottom layer represents the visible units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen1-2388577-large.gif
2015,7018910,Fig. 2.,Instance of a DBN connected with a LR layer. It has five layers: 1) one input layer; 2) three hidden layers; and 3) one output layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen2-2388577-large.gif
2015,7018910,Fig. 3.,Spectral classification using DBN-LR framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen3-2388577-large.gif
2015,7018910,Fig. 4.,Spatial classification using DBN-LR framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen4-2388577-large.gif
2015,7018910,Fig. 5.,Spectral–spatial classification using DBN-LR framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen5-2388577-large.gif
2015,7018910,Fig. 6.,"AVIRIS the Indian Pines data set. False-color composite (Band 50, 27, 17) and representing 16 land-cover classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen6-2388577-large.gif
2015,7018910,Fig. 7.,"ROSIS-3 data, Pavia, Italy. False-color composite (Band 10, 27, 46) and representing nine land-cover classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen7-2388577-large.gif
2015,7018910,Fig. 8.,"Reconstructions operated by RBMs with different numbers of hidden units for the class of Soybean-mintill in the Indian Pines data. (a) The original curve. (b)–(f) Reconstructions of (a) with 10, 50, 100, 150, and 200 hidden units, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen8abcdef-2388577-large.gif
2015,7018910,Fig. 9.,"Filter images learned by an RBM on (a) Indian Pines and (b) Pavia. Each
N
-pixel tiny rectangle stands for
N
input to hidden weights that connect each input unit to a same hidden unit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen9ab-2388577-large.gif
2015,7018910,Fig. 10.,Influence of the number of PCs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen10-2388577-large.gif
2015,7018910,Fig. 11.,Influence of depths (the spatial framework).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen11-2388577-large.gif
2015,7018910,Fig. 12.,Influence of depths (the spectral–spatial framework).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen12-2388577-large.gif
2015,7018910,Fig. 13.,Influence of the training sample size (Pavia). The ratio between training and test samples varies from 1:5 to 5:5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen13-2388577-large.gif
2015,7018910,Fig. 14.,"Spectral, spatial, and spectral–spatial classification using DBN-LR on the whole image of Indian Pines data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen14-2388577-large.gif
2015,7018910,Fig. 15.,"Spectral, spatial, and spectral–spatial classification using DBN-LR on the whole image of Pavia data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/7175093/7018910/chen15-2388577-large.gif
2015,6888473,Fig. 1.,"An overview of the unsupervised object class discovery problem. The input is the same for different types of algorithms: a set of unlabeled images. On the other hand, since different algorithms have different purposes, the outputs of the algorithms will vary according to those purposes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu1-2353617-large.gif
2015,6888473,Fig. 2.,"The pipeline of the proposed bMCL algorithm: (a) saliency-scored windows, (b) high-salience “probably positive” bags (in which we expect the object to be present), (c) low-salience “probably negative” bags (in which we expect only background to be present), (d) Bottom-up Multiple Class Learning algorithm: different colors represent positive bags that belong to different classes, and (e) object clustering and detection results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu2-2353617-large.gif
2015,6888473,Fig. 3.,(a) Localized objects from SIVAL [41]. (b) Original images from SIVAL [41].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu3-2353617-large.gif
2015,6888473,Fig. 4.,"Example of bags and instances. On the first row, red rectangles: the most salient windows as instances in the positive bag; yellow rectangles: the most salient window obtained by [21]. On the second row, green rectangles: the least salient windows from a large set of randomly sampled windows as instances in the negative bag; blue rectangles: the desired object window.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu4-2353617-large.gif
2015,6888473,Fig. 5.,"Object categorization results with varying number of clusters
K
are measured by purity. We compare bMCL with saliency detection baseline (SD) and random guess (RAND).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu5-2353617-large.gif
2015,6888473,Fig. 6.,"Illustrative categorization results of four methods in the experiments, the left from SIVAL2 [41] and the right from 3D1 [46]. From top to bottom: bMCL, M
3
IC [56], BAMIC [58], UnSL [27] and MFC [28]. In bMCL, the yellow rectangle is the localized object and the white rectangle is the most salient window given by [21]. In UnSL, the learned object key points are overlayed (red points). See Section 6.1 for detailed discussion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu6-2353617-large.gif
2015,6888473,Fig. 7.,"Object detection results for novel images: the top from SIVAL3 [41], the middle from CC [5], and the lower from 3D1 [46]. The rectangles are the localization results given by bMCL. Different colors represent the class labels returned by the algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu7-2353617-large.gif
2015,6888473,Fig. 8.,"Red rectangles: bMCL object localization results with a single object class (from top to bottom: aeroplane, cow, and motorbike) on the challenging PASCAL VOC 07.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu8-2353617-large.gif
2015,6888473,Fig. 9.,Illustrative clustering and localization results on Internet images with keywords “bean” and “bow”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu9-2353617-large.gif
2015,6888473,Fig. 10.,Object detection results for novel images returned by image search engines. The first two rows illustrate exemplar successful detection results and the last row illustrates exemplar failure cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7053986/6888473/tu10-2353617-large.gif
2015,7294615,Fig. 1.,"Representative tracking results of the proposed tracking algorithms on eight challenging image sequences (from left to right and then top to bottom: onestopmoveenter1cor, pets01human1, hunting, threepastshop2cor, football 1, horse racing, sky diving, and football 2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7194906/7294615/7294615-fig-1-source-large.gif
2015,7294615,Fig. 2.,Precision and success plots over all eight sequences (best-viewed on high-resolution display).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/7194906/7294615/7294615-fig-2-source-large.gif
2016,7348721,Fig. 1.,Comparison of envisioned and traditional implanted BMI. The envisioned system uses a machine learning coprocessor (mlcp) along with the DSP used in traditional neural implants to estimate motor intentions from neural recordings thus providing data compression. Traditional systems perform such decoding outside the implant and use bulky computers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-1-source-large.gif
2016,7348721,Fig. 2.,Algorithm. (a) The architecture of the extreme learning machine (elm) with one nonlinear hidden layer and linear output layer. (b) Use of ELM in neural decoding for classifying movement type and onset time of movement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-2-source-large.gif
2016,7348721,Fig. 3.,Training methods for ELM. T1 is the conventionally used training method to improve generalization by minimizing norm of weights as well as training error. T2 uses an additional step of sparsifying output weights to reduce the required hardware.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-3-source-large.gif
2016,7348721,Fig. 4.,(a) The diagram and (b) the timing of MLCP based neural decoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-4-source-large.gif
2016,7348721,Fig. 5.,"Sub-block circuit diagrams. (a) Input processing circuit to take moving average of incoming spikes. (b) Current-mode DAC to convert average value to input
x
for the current mirror. (c) Neuron-based CCO to implement hidden node non-linearity and convert to digital.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-5-source-large.gif
2016,7348721,Fig. 6.,"Die photo and test board. The die photo of MLCP fabricated in 0.35-
μ
m CMOS process and the portable external unit (peu) integrating MLCP with MCU and battery.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-6-source-large.gif
2016,7348721,Fig. 7.,"Jitter performance. The variation in the counter output for a fixed value of input current is observed for 100 trials and plotted as a histogram for (a) low, (b) medium and (c) high input currents. The measured jitter is
<0.1%
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-8-source-large.gif
2016,7348721,Fig. 8.,MLCP circuit blocks measurement results. (a) TDBDI feature. (b) Waveform of CCO oscillation. (c) Transfer curves of all 128 CCOs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-7-source-large.gif
2016,7348721,Fig. 9.,"DNL performance. DNL of 64 randomly selected input DAC channels show
±3
LSB performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-9-source-large.gif
2016,7348721,Fig. 10.,"The random input weights. (a) Measured mismatch map of the CCO frequencies. (b) Distribution of input weights. (c)
Δ
V
t,ij
. These values are measured by reading the output counter values when a fixed input value is given one row at a time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-10-source-large.gif
2016,7348721,Fig. 11.,Measured movement types decoding performance. (a) Decoding accuracy versus number of hidden layer nodes. (b) Decoding accuracy versus number of m1 neurons (with/without tdbdi). (c) Decoding accuracy across monkeys. (d) Decoding accuracy across 8 dies;,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-11-source-large.gif
2016,7348721,Fig. 12.,"Flow chart describing the finite state machine on DSP to calculate
G
track
from
G
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-12-source-large.gif
2016,7348721,Fig. 13.,Measured movement onset decoding results. (a) A segment of 40 channel input spike trains is shown with real-time decoding output deciding when a movement onset happens and which tpye is this onset. (b) ROC curves of onset decoding.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-13-source-large.gif
2016,7348721,Fig. 14.,Advantage of sparsity promoting training t2. The sparsity promoting method chooses best random projections and can reduce required number of hidden neurons by around 50%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-14-source-large.gif
2016,7348721,Fig. 15.,"Power breakup. Power dissipation in the MLCP is dominated by fixed analog power consumption of 360 nw compared to the power of 54 nw dissipated from
DVDD
in CCO and counter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-15-source-large.gif
2016,7348721,Fig. 16.,"Array layout. The area of the current IC is limited by the pitch of the CCO and WINCNT circuits even though the actual area of the current mirrors (0.4×0.35
μ
m2) are very small.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-16-source-large.gif
2016,7348721,Fig. 17.,"Normalization to reduce variation. Blue lines are original hidden layer output from SPICE simulation, while green dashed lines are normalized output in both (a) and (b). The input x in (a) and (b) are 8 and 10 respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/7426429/7348721/7348721-fig-17-source-large.gif
2016,7401039,Fig. 1.,"Schematic view of the classification rbm, which adds a set of label nodes to the visible layer of the standard RBM. The label nodes are connected to the input nodes through the hidden layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-1-source-large.gif
2016,7401039,Fig. 2.,"Schematic view of the convolutional rbm, which uses a convolutional weight-sharing arrangement to reduce the number of connection weights.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-2-source-large.gif
2016,7401039,Fig. 3.,"Schematic view of the convolutional classification RBM. The connection weights
U
are shared between all nodes in a feature map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-3-source-large.gif
2016,7401039,Fig. 4.,First dataset. Example from the interstitial lung disease scans. The annotation (right) shows an ROI (red) marked as micronodules.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-4-source-large.gif
2016,7401039,Fig. 5.,"Two filter banks: leung-malik (left) and schmid (middle), generated with the code from http://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html. An example of random filters (16 filters of 8 × 8 voxels) is shown right.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-5-source-large.gif
2016,7401039,Fig. 6.,"Second dataset. In the airway dataset, we extract patches at the airway centerline (green) and non-airway samples (red) close to the airway.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-6-source-large.gif
2016,7401039,Fig. 7.,"Example filters learned from the ILD dataset, with different mixtures of generative and discriminative learning (16 filters of 8 × 8 voxels).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-7-source-large.gif
2016,7401039,Fig. 8.,"Three filter sets learned from the airway data: 4, 36 or 100 filters of 16 × 16 voxels, learned with a mix of discriminative and generative learning
(β=0.01)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-8-source-large.gif
2016,7401039,Fig. 9.,"Random forest classification accuracy on the lung tissue classification dataset, for different feature representations. Large squares indicate RBM results that are not significantly different
(p<0.05)
from the best RBM result for that network configuration. Large circles indicate results that are significantly different from the RBM result at that
β
. (all significance values were computed using wilcoxon signed-rank tests comparing the per-scan classification accuracies.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-9-source-large.gif
2016,7401039,Fig. 10.,"The RBM classification accuracy on the lung tissue classification dataset, for different feature representation methods. Large squares indicate results that are not significantly different from the best result for that network configuration. (all significance values were computed using wilcoxon signed-rank tests comparing the per-scan classification accuracies.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-10-source-large.gif
2016,7401039,Fig. 11.,"Classification accuracy on the airway dataset, showing the influence of the number of hidden nodes in the RBM representation on the classification accuracy, for different mixtures of discriminative and generative learning. The graph on the left shows the classification accuracy of the classification RBM. The graph on the right shows the classification accuracy of a random forest using the RBM-learned filters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7401039/7401039-fig-11-source-large.gif
2016,7347425,Fig. 1.,Illustration of decision boundaries given by different categories of learning algorithms. (a) Data distribution. (b) Supervised learning algorithms. (c) Semi-supervised learning with clustering assumption. (d) Semi-supervised learning with manifold assumption.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang1abcd-2496157-large.gif
2016,7347425,Fig. 2.,Flowchart of SSL based distraction detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang2-2496157-large.gif
2016,7347425,Fig. 3.,Instrumented vehicle and apparatus used in the experiment. (a) Instrumented vehicle. (b) Apparatus layout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang3ab-2496157-large.gif
2016,7347425,Fig. 4.,"Feature selection results. HP stands for Head Position, HR stands for Head Rotation, GR stands for Gaze Rotation, and GT stands for Gaze Temporal. (a) Correlation coefficient. (b) Class separability. (c) Feedforward feature selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang4abc-2496157-large.gif
2016,7347425,Fig. 5.,"G
-mean averaged across all subjects using sizes of unlabeled set. The error bars indicate standard error of mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang5-2496157-large.gif
2016,7347425,Fig. 6.,"G
-mean averaged across all subjects using different sizes of labeled set. The error bars indicate standard error of mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang6-2496157-large.gif
2016,7347425,Fig. 7.,"G
-mean of SS-ELM using 60 labeled data and maximum number of unlabeled data under different regularization parameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang7-2496157-large.gif
2016,7347425,Fig. 8.,"G
-mean of SS-ELM using 60 labeled data and maximum number of unlabeled data under different graph-related parameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang8-2496157-large.gif
2016,7347425,Fig. 9.,Example of experimental setting used in case study. “DwST” stands for “Driving with Secondary Tasks.” “ND” stands for “Normal Driving.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang9-2496157-large.gif
2016,7347425,Fig. 10.,"G
-mean of SS-ELM under different regularization parameters in the case study setting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang10-2496157-large.gif
2016,7347425,Fig. 11.,"Distraction detection examples for four subjects with more than 3 “Normal Driving” sections at the beginning, middle and end. “1” corresponds to “distracted driving,” and “0” corresponds to “normal driving.” Red lines indicate the true distraction states. Blue crosses in (a), (c), (e), and (g) are the predicted state by SS-ELM. Black crosses in (b), (d), (f), and (h) are the predicted state by ELM. (a) Subject A by SS-ELM. (b) Subject A by ELM. (c) Subject B by SS-ELM. (d) Subject B by ELM. (e) Subject C by SS-ELM. (f) Subject C by ELM. (g) Subject D by SS-ELM. (h) Subject D by ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7442200/7347425/yang11abcdefgh-2496157-large.gif
2016,7078903,Fig. 1.,"Relationships between age and facial features (i.e., wrinkles, bags under-eyes, etc.). (a)–(c) indicate normal age progression characteristics; (b) has more wrinkles than (a) but less than (c). However, (d)–(f) do not obey this ordering rule. Bags under-eyes are more pronounced in (f) than in (e) but less than in (d) (green rectangular box).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang1abcdef-2416321-large.gif
2016,7078903,Fig. 2.,Framework of age estimation system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang2-2416321-large.gif
2016,7078903,Fig. 3.,Some face images and their aligned results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang3-2416321-large.gif
2016,7078903,Fig. 4.,"Process of dividing an image into sub-ROI. (a) Preprocessed image. (b) Dividing it into
5×5
sub-regions
ROI
5×5
. (c) Dividing it into
7×7
sub-regions
ROI
7×7
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang4abc-2416321-large.gif
2016,7078903,Fig. 5.,Training samples are ordered by the age labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang5-2416321-large.gif
2016,7078903,Fig. 6.,Age distribution of face images in (a) FG-NET and (b) MORPH Album 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang6ab-2416321-large.gif
2016,7078903,Fig. 7.,Comparison of CS results of different features using the OHR-SRC age estimation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang7-2416321-large.gif
2016,7078903,Fig. 8.,Comparison of CS results of different features using the OHR-SVM age estimation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang8-2416321-large.gif
2016,7078903,Fig. 9.,Comparison of CS results of different features using the OHR-raSVM+ age estimation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang9-2416321-large.gif
2016,7078903,Fig. 10.,Comparison of CS results of different DSIFT features using the OHR-raSVM+ age estimation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang10-2416321-large.gif
2016,7078903,Fig. 11.,"Comparison of CS results of
ROI
7×7
−DSIFT
features using different age estimation methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang11-2416321-large.gif
2016,7078903,Fig. 12.,"Comparison of CS results of the proposed approach and other state-of-the-art methods, namely OHRanker [30] and hierarchical framework age estimation [31].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang12-2416321-large.gif
2016,7078903,Fig. 13.,Distribution of age errors > 5 years in each age group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang13-2416321-large.gif
2016,7078903,Fig. 14.,Time cost versus parameter C during the training stage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7406788/7078903/wang14-2416321-large.gif
2016,7456223,Fig. 1.,"(a) 16QAM constellations with the corresponding labels; (b) an example of one testing data point
x
q
detected by KNN (
k=5
); (c) the testing data point
x
q
detected by DW-KNN (
k=7
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7518701/7456223/wang1abc-2555857-large.gif
2016,7456223,Fig. 2.,Numerical model of a 16QAM coherent optical transmission system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7518701/7456223/wang2-2555857-large.gif
2016,7456223,Fig. 3.,"BER as a function of (a) OSNR (
k=10
); (b) phase deviation from
π
/2 (
k=10
, OSNR = 24 dB); (c) linewidth (
k=10
, modulation depth = 2.2, phase shift = 5%); (d)
k
values (fiber length = 1600 km, launch power = 0 dBm).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7518701/7456223/wang3abcd-2555857-large.gif
2016,7456223,Fig. 4.,"BER as a function of (a) launch power (zero-dispersion link, fiber length = 1600 km), (b) transmission distance (zero-dispersion link, launch power = 0 dBm), (c) launch power (dispersion managed link, fiber length = 1200 km), and (d) launch power (dispersion unmanaged link, fiber length = 1200 km).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7518701/7456223/wang4abcd-2555857-large.gif
2016,7103337,Fig. 1.,"Proposed H-ELM learning algorithm. (a) Overall framework of H-ELM, which is divided into two phases: multilayer forward encoding followed by the original ELM-based regression. (b) Implementation of ELM-autoencoder. (c) Layout of one single layer inside the H-ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng1abc-2424995-large.gif
2016,7103337,Fig. 2.,"Testing accuracy in
(C,L)
subspace for the H-ELM and ELM. (a) Accuracy of H-ELM in terms of
L
. (b) Accuracy of ELM in terms of
L
. (c) Accuracy of H-ELM in terms of
C
. (d) Accuracy of ELM in terms of
C
. (e) Accuracy curve of H-ELM in terms of
(C,L)
. (f) Accuracy curve of ELM in terms of
(C,L)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng2abcdef-2424995-large.gif
2016,7103337,Fig. 3.,H-ELM-based car detection approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng3-2424995-large.gif
2016,7103337,Fig. 4.,Original gesture samples and difference images. (a) Cambridge gesture data set. (b) Difference images of flat/leftward gestures in Cambridge gesture data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng4ab-2424995-large.gif
2016,7103337,Fig. 5.,"H-ELM-based online tracking framework. (a) Updating with
n
-frame. (b) Tracking on
(n+1)
-frame.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng5ab-2424995-large.gif
2016,7103337,Fig. 6.,"Comparison of tracking location error using H-ELM, CT, and SDA on different data sets. (a)
David Indoor
. (b)
Trellis
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng6ab-2424995-large.gif
2016,7103337,Fig. 7.,"Tracking performance comparison of H-ELM, CT, and SDA in David Indoor and Trellis data sets. Rows 1 and 4: results of H-ELM. Rows 2 and 5: results of CT. Rows 3 and 6: results of SDA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103337/deng7-2424995-large.gif
2016,7523998,Fig. 1.,"Empirical CDF of BER. (a) Single-antenna selection for single-stream transmission:
{
n
t
,
n
r
,
n
s
,
n
d
}={8,1,1,1}
,
N=8
,
ρ
t
=0.3
,
ρ
r
=0.1
, and SNR is 15 dB. (b) Two-antenna selection for double-stream transmission:
{
n
t
,
n
r
,
n
s
,
n
d
}={6,2,2,2}
,
N=12
,
ρ
t
=0.3
,
ρ
r
=0.1
, and SNR is 20 dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/7740115/7523998/joung1ab-2594776-large.gif
2016,7437475,Fig. 1.,An exemplary satellite image of AMV [27].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park1-2535466-large.gif
2016,7437475,Fig. 2.,A standard station model for wind direction and speed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park2-2535466-large.gif
2016,7437475,Fig. 3.,A single AMV vector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park3-2535466-large.gif
2016,7437475,Fig. 4.,The target and search areas on an AMV satellite image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park4-2535466-large.gif
2016,7437475,Fig. 5.,Monthly statistics for the amount of clouds and irradiance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park5-2535466-large.gif
2016,7437475,Fig. 6.,Hourly statistics for the amount of clouds and irradiance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park6-2535466-large.gif
2016,7437475,Fig. 7.,"Finding the optimal ratio
α
∗
of the cloud speed to the wind speed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park7-2535466-large.gif
2016,7437475,Fig. 8.,"The measured and predicted amount of clouds based on the proposed SVM-based model with
t=60
minutes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park8-2535466-large.gif
2016,7437475,Fig. 9.,"Scatter plot of the measured and predicted amount of clouds based on the proposed SVM-based model with
t=60
minutes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park9-2535466-large.gif
2016,7437475,Fig. 10.,"The measured and predicted irradiance based on the proposed SVM-based model with
t=60
minutes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park10-2535466-large.gif
2016,7437475,Fig. 11.,"Scatter plot of the measured and predicted irradiance based on the proposed SVM-based model with
t=60
minutes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165391/7494741/7437475/park11-2535466-large.gif
2016,7169539,Fig. 1.,Architecture of SSFL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei1-2453359-large.gif
2016,7169539,Fig. 2.,Filtering operations on the spectral leaning stage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei2-2453359-large.gif
2016,7169539,Fig. 3.,"AWF whose size is
3×3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei3-2453359-large.gif
2016,7169539,Fig. 4.,Weighted filtering at the spatial feature learning stage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei4-2453359-large.gif
2016,7169539,Fig. 5.,Flowchart of the proposed SSN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei5-2453359-large.gif
2016,7169539,Fig. 6.,OAs of different methods with different numbers of training samples on the Aviris Indian Pines dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei6-2453359-large.gif
2016,7169539,Fig. 7.,"Box plot of the
κ
coefficients of different methods on the Aviris Indian Pines dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei7-2453359-large.gif
2016,7169539,Fig. 8.,Classification maps of the Indian Pines dataset using different methods. (a) SVM. (b) KELM. (c) PCA-KELM. (d) MH-KELM. (e) EPF. (f) MPM-LBP. (g) SADL. (h) SSN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei8abcdefgh-2453359-large.gif
2016,7169539,Fig. 9.,OAs of different methods with various numbers of training samples on the University of Pavia dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei9-2453359-large.gif
2016,7169539,Fig. 10.,"Box plot of
κ
of different methods on University of Pavia dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei10-2453359-large.gif
2016,7169539,Fig. 11.,Classification maps of the University of Pavia dataset using different methods. (a) SVM. (b) KELM. (c) PCA-KELM. (d) MH-KELM. (e) EPF. (f) MPM-LBP. (g) SADL. (h) SSN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei11abcdefgh-2453359-large.gif
2016,7169539,Fig. 12.,Effect of different depths on OAs on the Indian Pines dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei12-2453359-large.gif
2016,7169539,Fig. 13.,Verify the overfitting on the Indian Pines dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7491394/7169539/wei13-2453359-large.gif
2016,7063894,Fig. 1.,"Classification using SVM. Positive and negative samples, which belong to the class of attacked and secure measurements, are depicted by disk and star markers, respectively. Support vectors and misclassified samples are depicted by dashed circles and hexagonal markers, respectively. (a) Attack detection using the linearly separable data set. (b) Attack detection using the linearly nonseparable data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay1ab-2404803-large.gif
2016,7063894,Fig. 2.,"Results for the IEEE 57-bus system. Accuracy values of the SVE and perceptron increase, while Precision values of the
k
-NN and SLR increase as
κ/N
increases. Both Accuracy and Precision values of the SVM increase and phase transitions occur. (a) SVE. (b) Perceptron. (c) SVM with linear kernel. (d)
k
-NN. (e) SLR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay2abcde-2404803-large.gif
2016,7063894,Fig. 3.,"Experiments using the SVE for the IEEE 57-bus test system. Note that fp values increase as
κ/N
increases. (a) Performance values for Class-1. (b) Performance values for Class-2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay3ab-2404803-large.gif
2016,7063894,Fig. 4.,Performance analysis of the perceptron. (a) Results for the IEEE 57-bus. (b) Results for the IEEE 57-bus. (c) Results for the IEEE 118-bus. (d) Results for the IEEE 118-bus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay4abcd-2404803-large.gif
2016,7063894,Fig. 5.,"Since the
k
-NN is sensitive to class balance and sparsity of the data, the performance values for Class-1 increase and the values for Class-2 decrease as
κ/N
increases. Note that the performance curves intersect at the critical values
κ
∗
. (a) Precision values for the IEEE 57-bus. (b) Recall values for the IEEE 57-bus. (c) Precision values for the IEEE 118-bus. (d) Recall values for the IEEE 118-bus.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay5abcd-2404803-large.gif
2016,7063894,Fig. 6.,"Experiments using the SLR. Note that the SLR can handle the variety in the sparsity of the data as
κ/N
changes. (a) Results for the IEEE 57-bus. (b) Results for the IEEE 57-bus. (c) Results for the IEEE 118-bus. (d) Results for the IEEE 118-bus.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay6abcd-2404803-large.gif
2016,7063894,Fig. 7.,"Experiments using the SVM with linear and Gaussian kernels. Phase transitions of performance values occur at the critical point
κ
∗
. See the text for more detailed explanation. (a) Linear SVM. (b) Linear SVM. (c) Gaussian SVM. (d) Gaussian SVM. (e) Linear SVM. (f) Linear SVM. (g) Gaussian SVM. (h) Gaussian SVM. (i) Linear SVM. (j) Linear SVM. (k) Gaussian SVM. (l) Gaussian SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay7abcdefghijkl-2404803-large.gif
2016,7063894,Fig. 8.,"Sharp phase transitions are not observed in the S3VM unlike the supervised SVM, since the information obtained from unlabeled data contributes to the performance values in the computation of the learning models. (a) Results for the IEEE 57-bus. (b) Results for the IEEE 57-bus. (c) Results for the IEEE 118-bus. (d) Results for the IEEE 118-bus. (e) Results for the IEEE 57-bus. (f) Results for the IEEE 57-bus. (g) Results for the IEEE 118-bus. (h) Results for the IEEE 118-bus.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay8abcdefgh-2404803-large.gif
2016,7063894,Fig. 9.,"Experiments using Adaboost and MKL. Note that the fn values of MKL are greater than the values of Adaboost, and there are no phase transitions of the performance values of MKL compared with the supervised SVM. (a) Adaboost for the IEEE 57-bus. (b) Adaboost for the IEEE 57-bus. (c) Adaboost for the IEEE 118-bus. (d) Adaboost for the IEEE 118-bus. (e) MKL for the IEEE 57-bus. (f) MKL for the IEEE 57-bus. (g) MKL for the IEEE 118-bus. (h) MKL for the IEEE 118-bus.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay9abcdefgh-2404803-large.gif
2016,7063894,Fig. 10.,"Experiments using the OP, OPWMs, Online SVM, and SLR. Recall values of the OP are less than that of the OPWM for Class-1. Multiple phase transitions of performance values of the Online SVM are observed in the IEEE 118-bus system. (a) OP for the IEEE 57-bus. (b) OP for the IEEE 57-bus. (c) OP for the IEEE 118-bus. (d) OP for the IEEE 118-bus. (e) OPWM for the IEEE 57-bus. (f) OPWM for the IEEE 57-bus. (g) OPWM for the IEEE 118-bus. (h) OPWM for the IEEE 118-bus. (i) Online SVM for the IEEE 57-bus. (j) Online SVM for the IEEE 57-bus. (k) Online SVM for the IEEE 118-bus. (l) Online SVM for the IEEE 118-bus. (m) Online SLR for the IEEE 57-bus. (n) Online SLR for the IEEE 57-bus. (o) Online SLR for the IEEE 118-bus. (p) Online SLR for the IEEE 118-bus.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay10abcdefghijklmnop-2404803-large.gif
2016,7063894,Fig. 11.,Learning curves of online learning algorithms. (a) Observable attacks. (b) Unobservable attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7063894/ozay11ab-2404803-large.gif
2016,7337423,Fig. 1.,"Workflow of our LGL-WSVM approach as an example of seven data samples with five labels. The community-aware contextual information of the samples are exploited to regularize the label correlation. By assigning the labelwise correlation weights to the corresponding pairwise loss of the max margin classifier, the two stages are carried out in an alternating manner until the multilabel classification model with accurate label graphs are obtained via a joint optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li1-2503700-large.gif
2016,7337423,Fig. 3.,"Convergence analysis on the NUSWIDE dataset. Clearly, our approach is able to reach the convergence point.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li3-2503700-large.gif
2016,7337423,Algorithm 1,Label Graph Learning Driven Weighted SVM (LGL-WSVM),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li7-2503700-large.gif
2016,7337423,Fig. 2.,Image examples with the associated labels in the NUSWIDE dataset. The set of the labels consists of 81 words altogether.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li2-2503700-large.gif
2016,7337423,Fig. 4.,"The label correlation weights that are learned by LGL-WSVM on the NUSWIDE dataset. Clearly, most correlated label pairs are related in semantics. The label with a higher value is more related to the specific concept.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li4-2503700-large.gif
2016,7337423,Fig. 5.,"Precision-Recall Curve retrieval performances of different approaches on the two datasets. Clearly, LGL-WSVM obtains better precision-recall retrieval results in most cases. (a) NUSWIDE. (b) MediaMill.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li5ab-2503700-large.gif
2016,7337423,Fig. 6.,Results of our approach to labeling the images in the NUSWIDE dataset. Our approach is capable of producing reasonable labels and prefers to discover the highly correlated labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7331739/7337423/li6-2503700-large.gif
2016,7347331,Fig. 1.,"Exon and the regulatory instructions identified using machine learning. If an infant is homozygous in a version of the survival motor neuron gene SMN2, the result is spinal muscular atrophy, a leading cause of infant mortality. Three of the nucleotides lie within genomic instructions that a machine learning technique identified as being important for including this exon when building the protein [44].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung1-2494198-large.gif
2016,7347331,Fig. 2.,"(a) One major goal of genomic medicine is to predict phenotypes, such as disease risks, from a genotype. (b) By training models that predict how genotype defined by as a stretch of DNA sequence influences “cell variables,” such as concentrations of proteins, it hugely simplifies and modularizes the machine learning problem, and enables the exploration of therapies that target these crucial variables.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung2ab-2494198-large.gif
2016,7347331,Fig. 3.,"Simplified view of how biologists, data scientists, and medical researchers can work toward genomic medicine. Machine learning plays a central role by turning high-throughput measurements into specialized or general-purpose predictive models for what we referred to as “cell variables”—quantities that are relevant to cell function. By knowing how mutations affect disease via cell variables, diagnosticians and pharmacogeneticists can more easily find direct correlates with disease, develop treatments, and plan targeted therapies for individual patients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung3-2494198-large.gif
2016,7347331,Fig. 4.,"Gene expression consists of three high-level steps. Transcription creates an RNA molecule that is essentially a copy of the DNA in the gene being transcribed; at this stage, the RNA molecule is called precursor messenger RNA (pre-mRNA). RNA processing then modifies the pre-mRNA, which includes splicing out long stretches of sequence called introns and connecting the flanking regions called exons; at this stage, the RNA molecule is called messenger RNA (mRNA). Translation creates a protein molecule (an amino-acid chain) by reading the three-letter “codes” in the mRNA sequence. Other processes include polyadenylation, wherein adenine bases are appended to the end of the mRNA; mRNA stabilization, wherein the mRNA molecule is processed so as to make it less likely to degrade; mRNA localization, wherein the mRNA is moved to a location suitable for translation; and protein localization, wherein the protein is moved to a specific type of location in the cell.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung4-2494198-large.gif
2016,7347331,Fig. 5.,"Gene can generate different proteins by the process of alternative splicing, where genomic instructions cause an exon to sometimes be included or excluded, depending on cellular conditions, such as the cell type. Using RNA-seq, the frequency with which each of tens of thousands of exons is included in a specific cell type can be measured, and these data can be used to train a computational model that discovers the instructions that control splicing and combines them to predict splicing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung5-2494198-large.gif
2016,7347331,Fig. 6.,"DNA- and RNA-binding proteins regulate many processes in the cell, including gene transcription rates (top left) and splicing rates (top right). To determine the motifs(s) that a particular protein binds to (e.g., agata), biologists collect “specificity” data either by sequencing protein-bound fragments from a living cell (middle), or by exposing tagged proteins to synthetic fragments on a microarray (bottom).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung6-2494198-large.gif
2016,7347331,Fig. 7.,Position-frequency matrix (PFM) model of the RNA-binding protein RBFOX1 as reported by Ray et al. [35]. A PFM can be used to score each position along a target sequence. Biologists commonly visualize PFMs as a sequence logo [89].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7347331/leung7-2494198-large.gif
2016,7559748,Fig. 1.,(a) The structure of RBM. (b) The structure of CRBM with max-pooling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han1ab-2605920-large.gif
2016,7559748,Fig. 2.,The parameters involved in 2D convolution and circle convolution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han2-2605920-large.gif
2016,7559748,Fig. 3.,An illustration of circle convolution on a 3D local region.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han3-2605920-large.gif
2016,7559748,Fig. 4.,"PDD of a region centered at (a) the green vertex on the human head, and (b) the blue vertex on the human leg.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han4ab-2605920-large.gif
2016,7559748,Fig. 5.,An example of circle convolution and PDD computation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han5-2605920-large.gif
2016,7559748,Fig. 6.,The initial location ambiguity of the sector window is eliminated by FTM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han6-2605920-large.gif
2016,7559748,Fig. 7.,"The structure of CCRBM including virtual layer V, detection layer H and pooling layer P. Visible layer V1 and output layer F are also shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han7-2605920-large.gif
2016,7559748,Fig. 8.,"The PR curves obtained under different numbers of virtual words (a) with
θ
c
=
30
∘
and (b)
θ
c
=
100
∘
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han8ab-2605920-large.gif
2016,7559748,Fig. 9.,"The PR curves obtained with (a) different numbers of training samples
T
and (b) different numbers filters
K
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han9ab-2605920-large.gif
2016,7559748,Fig. 10.,"The PR curves obtained with (a) different central angle of the sector window
θ
c
and (b) with different radius of local region
R
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han10ab-2605920-large.gif
2016,7559748,Fig. 11.,PR curves under (a) the articulated subset of McGill shape benchmark; (b) the whole McGill shape benchmark.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han11ab-2605920-large.gif
2016,7559748,Fig. 13.,(a) PR curve comparison under the SHREC 2007 global shape retrieval dataset and (b) NDCG curve comparison under SHREC 2007 partial retrieval dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han13ab-2605920-large.gif
2016,7559748,Fig. 12.,"(a) The learned filters are shown briefly. Each 25-dimensional filter is shown as a 5
×
5 image. (b) PR curves comparison under SHREC 2015 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han12ab-2605920-large.gif
2016,7559748,Fig. 14.,Some examples of query shapes and the top-5 retrieved shapes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han14-2605920-large.gif
2016,7559748,Fig. 15.,Ratio of correct matches vs Ratio of geodesic error curves under (a) the Watertight dataset and (b) the SCAPE dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7564375/7559748/han15ab-2605920-large.gif
2016,7243351,Fig. 1.,Comparison between co-training and our co-labeling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7243351/xu1-2476813-large.gif
2016,7243351,Fig. 2.,"Organization of the three-layer structure for the multi-layer MKL in our co-labeling framework. The circles denote the input-output kernels, and the rectangles with different colors denote the combination coefficients at different layers, respectively. We impose different regularizers on the combination coefficients at each layer when combining them to obtain their parent node (e.g., from the blue rectangles to the green rectangles).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7243351/xu2-2476813-large.gif
2016,7549079,Fig. 1.,Performance trends based on the class imbalance ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7587442/7549079/kim1abc-2602226-large.gif
2016,7052327,Fig. 1.,Facial images depicting a person of the ORL dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif1-2401973-large.gif
2016,7052327,Fig. 2.,Facial images depicting a person of the AR dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif2-2401973-large.gif
2016,7052327,Fig. 3.,Facial images depicting a person of the Extended YALE-B dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif3-2401973-large.gif
2016,7052327,Fig. 4.,"Facial images from the COHN-KANADE dataset. From left to right: neutral, anger, disgust, fear, happy, sad, and surprise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif4-2401973-large.gif
2016,7052327,Fig. 5.,"Facial images depicting a person of the BU dataset. From left to right: neutral, anger, disgust, fear, happy, sad, and surprise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif5-2401973-large.gif
2016,7052327,Fig. 6.,"Facial images depicting a person of the JAFFE dataset. From left to right: neutral, anger, disgust, fear, happy, sad, and surprise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif6-2401973-large.gif
2016,7052327,Fig. 7.,Video frames of the Hollywood2 dataset depicting instances of all the twelve actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif7-2401973-large.gif
2016,7052327,Fig. 8.,Video frames of the Olympic Sports dataset depicting instances of all the 16 actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif8-2401973-large.gif
2016,7052327,Fig. 9.,Video frames of the Hollywood 3-D dataset depicting instances of twelve actions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif9-2401973-large.gif
2016,7052327,Fig. 10.,"Confusion matrices obtained by applying the KELM (top) and the proposed GEKELM (bottom) algorithms on the (a) BU, (b) JAFFE, and (c) KANADE facial expression recognition datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7052327/iosif10abc-2401973-large.gif
2016,7314912,Fig. 1.,"(a) Proposed method with
L
subnetwork hidden nodes. (b) In the proposed method, each subnetwork hidden node can be considered a standard SLFN; the proposed method with one subnetwork neuron can be considered a standard SLFN with
m
hidden nodes (
m
is the dimensionality of desired output t). (a) Network architecture of Algorithm 1. (b) Network architecture of Algorithm 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang1ab-2492468-large.gif
2016,7314912,Fig. 2.,"Average testing accuracy when training ELM, I-ELM, PC-ELM, EM-ELM, and EI-ELM, and the proposed method on Auto MPG, California housing, Abalone, Puma, Fried, and Machine CPU, where the
x
- and
y
-axis show the number of hidden nodes and average testing root-mean-square error (RMSE), respectively. Results on (a) Auto MPG, (b) California housing, (c) Abalone, (D) Puma, (e) Fried, and (f) Machine CPU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang2abcdef-2492468-large.gif
2016,7314912,Fig. 3.,"Average testing accuracy when training ELM, I-ELM, PC-ELM, EM-ELM, and EI-ELM, and the proposed method on Connect-4, Leukemia, Covtype, Duke, Hill Valley, and Protein, where the
x
- and
y
-axis show the number of hidden nodes and average testing accuracy, respectively. Results on (a) Connect-4, (b) Leukemia, (c) Covtype, (d) Duke, (e) Hill Valley, and (f) Protein.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang3abcdef-2492468-large.gif
2016,7314912,Fig. 4.,"Performances of ELM and the proposed method are sensitive to the
C
and the number of hidden nodes, where the
x
-,
y
-, and
z
-axis show the number of hidden nodes, the value of
C
, and average testing accuracy. (a)–(c) Proposed method with sigmoid function. (d)–(f) ELM with sigmoid function. Results on (a) DNA by the proposed method, (b) DNA by ELM, (c) protein by the proposed method, (d) protein by ELM, (e) Hill by the proposed method, and (f) Hill by ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang4abcdef-2492468-large.gif
2016,7314912,Fig. 5.,Performance of the proposed method and other methods on Caltech101 data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang5-2492468-large.gif
2016,7314912,Fig. 6.,Examples of the Caltech-101 set. (a)–(d) Top four categories where our method improves more than ELM. (e)–(h) Top four categories where ELM improves more than our proposed method. The numbers in brackets indicate the classification rate (the proposed method/ELM). (a) Saxophone (80/20). (b) Wild cat (25/0). (c) Platypus (75/50). (d) Stegosaurus (89/65). (e) Metronome (50/100). (f) Pyramid (50/85). (g) Garfield (50/75). (h) Gerenuk (50/75).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang6abcdefgh-2492468-large.gif
2016,7314912,Fig. 7.,Examples of the Caltech-256 set. (a)–(d) Top four categories where our method improves more than ELM. (e)–(h) Top four categories where ELM improves more than our proposed method. The numbers in brackets indicate the classification rate (the proposed method/ELM). (a) Grapes (58/59). (b) Video projector (58/38). (c) Toad (29/10). (d) Homer Simpson (58/40). (e) Baseball bat (8/24). (f) Megaphone (18/34). (g) Tweezer (32/47). (h) Ibis (14/28).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang7abcdefgh-2492468-large.gif
2016,7314912,Fig. 8.,"Classification performance difference per category between the proposed method and ELM. (a) In the case of the Caltech-256 set, there are 160 categories that perform better, or equal with ELM. (b) For the Caltech-101 set, there are 72 categories that perform better, or equal with ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang8ab-2492468-large.gif
2016,7314912,Fig. 9.,"Average classification percentage per category difference between the proposed method and ELM on the Scene-15 data set. The relative confusion denotes the increase (nonwhite) or decrease (white) in the classification rate of the proposed method compared to ELM. The value at column
x
and row
y
represents the difference between the proposed method and ELM in classifying images of category
y
as category
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7314912/yang9-2492468-large.gif
2016,7182768,Fig. 1.,Examples of digits 3 and 8 in the handwritten digit data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7182768/sun1-2461009-large.gif
2016,7182768,Fig. 2.,Examples of face and nonface images in the face detection data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7182768/sun2-2461009-large.gif
2016,7452591,Fig. 1.,"Block diagram of the proposed inversion-based LfD approach. Solid line depicts continuous flow of information; dashed line depicts intermittent flow of information;
y
d
is the desired output signal,
y
is the plant output,
e=
y
d
−y
is the perceived error,
u
h
is the human input, and
u
c
is the machine controller input.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas1-2545243-large.gif
2016,7452591,Fig. 2.,"Experimental setup for human-in-the-loop experiment. Solid black line depicts continuous flow of information; dashed line depicts intermittent flow of information (cycle preceding controller input update,
u
c,k
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas2-2545243-large.gif
2016,7452591,Fig. 3.,"Output trajectory of the target
y
d
and its first and second time derivatives.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas3-2545243-large.gif
2016,7452591,Fig. 4.,"Human response model
G
^
H
shown as a solid line, fitted to the frequency response
G
¯
¯
¯
¯
H
obtained experimentally (evaluation trials), shown as a dashed line with circular markers, for Subject 2. Vertical error bars indicate one standard deviation for ten trials at each frequency,
f
. The linear model
G
^
fb
is expected to fit well till the smooth-pursuit tracking limit frequency
f
∗
T
=0.5
Hz  [32] indicated by the vertical dashed line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas4ab-2545243-large.gif
2016,7452591,Fig. 5.,"Human-in-the-loop feedback-loop model
G
^
fb
shown as a solid line, for Subject 2, fitted to the frequency response
G
¯
¯
¯
¯
fb
obtained experimentally, shown as a dashed line with circular markers, for Subject 2. Vertical error bars indicate one standard deviation for ten trials at each frequency
f
. The smooth-pursuit tracking limit frequency
f
∗
T
=0.5
Hz  [32] is indicated by the vertical dashed line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas5ab-2545243-large.gif
2016,7452591,Fig. 6.,"Feedback modeling error
Δ
fb
computed using (19). (a) Magnitude error
Δ
fb,m
. (b) Phase error
Δ
fb,θ
. (c) Maximum iteration gain
ρ
∗
(circular markers), computed using (18), Lemma 1, and the chosen iteration gain
ρ
(diamond markers), according to (56) for Subject 2. A large iteration gain
ρ
is chosen in the main convergence region till frequency
f
c
=0.5
 Hz.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas6abc-2545243-large.gif
2016,7452591,Fig. 7.,"Average magnitude of the coherence squared function
C
xy
for the feedback frequency response
G
fb
, computed using (55) for Subject 2. The values are presented as the mean index at each frequency for ten trials of the model-estimation trials. Smaller coherence values,
0<
C
xy
<<1
, e.g., after frequency
f
c
=0.5
 Hz (vertical dashed line), indicate potential increase in nonlinearity and/or measurement noise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas7-2545243-large.gif
2016,7452591,Fig. 8.,"Convergence of inversion-based ILC for the target frequency
f
T
=0.125
 Hz for Subject 2. (a) System output
y
k
during human-only control (
k∈[1,10]
). (b) System output
y
k
during shared human–machine ILC (
k∈[11,20]
). (c) Tracking error
e
10
=
y
d
−
y
10
at the end of human-only control (
k=10
). (d) Tracking error
e
21
=
y
d
−
y
21
at the end of shared control (
k=21
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas8abcd-2545243-large.gif
2016,7452591,Fig. 9.,"Maximum error per iteration
e
max,k
defined in (57) for the target frequency
f
T
=0.125
 Hz for Subject 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas9-2545243-large.gif
2016,7452591,Fig. 10.,"Control effort
∥u∥
, human (circular markers), machine controller (diamond markers), as defined in  (66) for human andin   (67) for machine controller for target frequency
f
T
=0.125
 Hz for Subject 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas10-2545243-large.gif
2016,7452591,Fig. 11.,"Controller input
u
c,k
(t)
for iterations
k∈[10,20]
(dashed lines) compared with the exact inverse input
u
∗
c
(t)
(solid line), with final learned controller input in iteration
k=21
(dash-dotted line) for target frequency
f
T
=0.125
 Hz for Subject 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7511814/7452591/devas11-2545243-large.gif
2016,7446342,Fig. 1.,"Comparison of sparse code of a training image by DL or by embedding with
P
∗
in training phase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang1-2550016-large.gif
2016,7446342,Fig. 2.,"Comparison of induced sparse code of a test image by DL or by embedding with
P
∗
in testing phase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang2-2550016-large.gif
2016,7446342,Fig. 3.,"Visualization of the discriminant sparse codes
Q
over the Extended Yale-B face database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang3abcde-2550016-large.gif
2016,7446342,Fig. 4.,Averaged recognition performance vs. labeled number of spoken letters on the Isolet datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang4-2550016-large.gif
2016,7446342,Fig. 5.,Convergence behavior of our proposed JEDL approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang5-2550016-large.gif
2016,7446342,Fig. 6.,Typical sample images of ETH80 database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang6-2550016-large.gif
2016,7446342,Fig. 7.,Face image examples of AR-male (a) and AR-female and (b) face datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang7ab-2550016-large.gif
2016,7446342,Fig. 8.,Parameter sensitivity analysis under different parameter selections on the AR-male face dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang8-2550016-large.gif
2016,7446342,Fig. 9.,Performance comparisons on the AR-female face dataset with varying dictionary sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang9-2550016-large.gif
2016,7446342,Fig. 10.,Performance comparisons on the AR-male face dataset with varying dictionary sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang10-2550016-large.gif
2016,7446342,Fig. 11.,Object image examples of Caltech101 database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang11-2550016-large.gif
2016,7446342,Fig. 12.,Face image examples of CMU PIE face database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7478689/7446342/zhang12-2550016-large.gif
2016,7271021,Fig. 1.,"Schematic diagram of the multimodal infection screening system. The system acquires heart rate, respiration rate, and facial temperature readings and feeds them into a classifier, which classifies the measurement as potentially infected or healthy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7271021/yao1-2479716-large.gif
2016,7271021,Fig. 2.,"ROC plots for the thermography only classification, as well as QDA and kNN. The two exemplary facial temperature thresholds listed in Table I are indicated by thr
1
and thr
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7271021/yao2-2479716-large.gif
2016,7426845,Fig. 1.,Schematic overview of the complete proposed pipeline. The information about the object location is used to initialize a mean shape which in the second stage is deformed using learning to describe the true boundary.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-1-source-large.gif
2016,7426845,Fig. 2.,Example of a fully connected neural network with 3 layers. Every neuron in a layer is connected to all neurons in the previous layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-2-source-large.gif
2016,7426845,Fig. 3.,"Visualization of the difference between uniform/handcrafted feature patterns and self-learned, sparse, adaptive patterns.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-3-source-large.gif
2016,7426845,Fig. 4.,Schematic visualization of the marginal space deep learning framework. The black/white dots encode the sparse sampling patterns.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-4-source-large.gif
2016,7426845,Fig. 5.,Schematic visualization of the negative-sample filtering cascade used to balance the training set. The black/white dots encode the sparse sampling patterns of the shallow nets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-5-source-large.gif
2016,7426845,Fig. 6.,"Schematic visualization of the boundary deformation with SADNN. Starting from the current shape, the SADNN is aligned and applied along the normal for each point of the mesh, the boundary is deformed and projected under the current shape space. The process is iteratively repeated. The black/white dots encode the sparse patterns, learned in the cascaded shallow networks and deep boundary classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-6-source-large.gif
2016,7426845,Fig. 7.,Left: training and testing error of the SADNN in the translation stage. The performance oscillation of the sparse model is explained by the enforcement of sparsity in the dense sampling layer of the network. Right: example color-coded sparse patterns for translation (upper box) and full space (lower box). The latter representation is more compact because of the better data alignment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-7-source-large.gif
2016,7426845,Fig. 8.,"Example images showing the detection results for different patients from the test set. The detected bounding box is visualized in green and the ground truth box in yellow. The segments with the origin in the center of each box define the corresponding coordinate system. Note that as specified in the text, the 3d pose of the aortic valve (position, orientation and scale) is fully determined by the anatomy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-8-source-large.gif
2016,7426845,Fig. 9.,"Example images showing the aortic valve segmentation results for different patients from the test set, using our proposed method. Each row of images corresponds to a different patient, the left image represents the groundtruth mesh, while the right image shows the detected mesh.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7426845/7426845-fig-9-source-large.gif
2016,7471467,Fig. 1.,"ELM network structure: The hidden node parameters
(
a
i
,
b
i
)
are randomly generated.
h
i
(x)=g(
a
i
,x,
b
i
)
is the output of
i
th
hidden node for input
x
where
g
is a non-linear piecewise continuous function. This figure shows a specific additive type neuron structure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7491413/7471467/liyan1-2570569-large.gif
2016,7471467,Fig. 2.,"TAE network architecture with tied weights
β
TAE
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7491413/7471467/liyan2-2570569-large.gif
2016,7471467,Fig. 3.,"Linear and non-linear ELM-AE and SELM-AE have the same solution as the original ELM except for: 1) The target output of linear and non-linear ELM-AE and SELM-AE is the same as input
x
; 2) The hidden node parameters
g(
a
i
,
b
i
)
are made orthogonal after randomly generated for linear and non-linear ELM-AE, and the hidden node parameters
g(
a
i
,
b
i
)
are made sparse random for linear and non-linear SELM-AE.
h
i
(x)=g(
a
i
,x,
b
i
)
is the output of
i
th
hidden node for input
x
; 3) Linear activation function is used for linear ELM-AE and SELM-AE, while non-linear activation function is used for non-linear ELM-AE and SELM-AE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7491413/7471467/liyan3-2570569-large.gif
2016,7471467,Fig. 4.,"The ELM-AE (linear and non-linear), SELM-AE (linear and non-linear), PCA and NMF lower dimensional projections of IRIS dataset. (a) Output of projection
X
IRIS
β
T
AE
calculated by Equation (33). (b) Output of projection
X
IRIS
β
T
AE
calculated by Equation (33). (c) Output of projection
X
IRIS
β
T
SAE
calculated by Equation (37). (d) Output of projection
X
IRIS
β
T
SAE
calculated by Equation (37). (e) Output of projection
X
IRIS
V
T
calculated by Equation (8). (f) Output of projection
X
IRIS
W
T
calculated by Equation (11).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7491413/7471467/liyan4abcdef-2570569-large.gif
2016,7471467,Fig. 5.,"Sparsity values of ELM-AE (linear and non-linear), SELM-AE (linear and non-linear), TAE, PCA and NMF for USPS, CIFAR-10 and NORB datasets. (a) ELM-AE (linear and non-linear) and SELM-AE (linear and non-linear) is sparser than PCA for USPS dataset. (b) Large features of linear SELM-AE is sparser than linear ELM-AE, non-linear SELM-AE and non-linear ELM-AE for USPS dataset. (c) ELM-AE (linear and non-linear) and SELM-AE (linear and non-linear) is sparser than PCA for CIFAR-10 dataset. (d) Large features of linear SELM-AE is sparser than linear ELM-AE, non-linear SELM-AE and non-linear ELM-AE for CIFAR-10 dataset. (e) ELM-AE (linear and non-linear) and SELM-AE (linear and non-linear) is sparser than PCA for NORB dataset. (f) Large features of linear SELM-AE is sparser than linear ELM-AE, non-linear SELM-AE and non-linear ELM-AE for NORB dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7491413/7471467/liyan5abcdef-2570569-large.gif
2016,7405343,Fig. 1.,"Aggnet framework: (1) the multi-scale CNN model is trained from gold-standard annotations. (2) then for any incoming unlabelled image, (3) the aggnet will produce a response map which is thresholded at selected optimal operating point. (4) these few resulting positive candidates are outsourced to crowds. (5) aggnet collects back the crowd votes and jointly aggregates the ground truth and refine the CNN model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-1-source-large.gif
2016,7405343,Fig. 2.,"Aggnet architecture: the same CNN architecture is used for different scales, where
p
i
,
μ
i
,
y
j
i
represents the classifier output, the aggregated label, and the crowdvotes respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-2-source-large.gif
2016,7405343,Fig. 3.,Instructions and guidelines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-3-source-large.gif
2016,7405343,Fig. 4.,"First row shows results of one single image using multi-scale cnn, green: the true positives, orange: the false positives. Second row shows the corresponding final detection map (fdm) before thresholding. Best viewed in color. (a) 0.33-scale (b) 0.66-scale (c) Orig.-scale (d) Mulit-scale.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-4-source-large.gif
2016,7405343,Fig. 5.,"Evaluation metrics: precision, recall, and
F
1
-score of patients 9, 11 and 12. (a) Patient 9 (b) Patient 11 (c) Patient 12 (d) Overall.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-5-source-large.gif
2016,7405343,Fig. 6.,"(a) The aggregated labels of the crowdsourcing set are evaluated using the
F
1
-score metric. (b) The loss function barely changes at 3–8 epochs before starting to overfit and the gap between the validation and training curves becomes significant. The shaded area depicts the change of
τ
. (a)
F
1
-score (b) Loss function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-6-source-large.gif
2016,7405343,Fig. 7.,"ROC curves of the (a) aggregated labels using mv, GLAD and the proposed AG-noq, (b) the augmented models AM-gt, AM-mv, AM-glad and aggnet as well. (a)Aggregated labels (b) Augmented models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-7-source-large.gif
2016,7405343,Fig. 8.,"Participants analysis: accuracy and spammer scores of 100 participants. Arrows in green show some participants achieve high accuracy scores in the qualitative test, however, they are spammer. Arrows in red show very few participants who have good accuracy score as well as spammer score. Note that spammer score “0” means the participant is spammer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7405343/7405343-fig-8-source-large.gif
2016,7458793,FIGURE 1.,"Invasion game: The attacker inverts its strategy after 250 steps. The agent’s average success probability is plotted as a function of number of trials (games). A trade-off between success probability and relearning time is depicted for different
γ
values. An optimal value of
η=1
is used. The simulation was done by averaging over
10
6
agents. Adapted from [29].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni1-2556579-large.gif
2016,7458793,FIGURE 2.,"Invasion game: The attacker inverts its strategy whenever the agent’s success probability reaches 0.8. The agent’s performance is plotted as a function of number of trials on a log scale, demonstrating learning times that increase exponentially with the number of inversions. The simulation was done with a single agent, where the success probabilities were extracted directly from the agent’s base-level ECM network. Meta-parameters:
γ=0
,
η=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni2-2556579-large.gif
2016,7458793,FIGURE 3.,"Invasion game: The attacker changes its strategy every 500 steps. The agent’s average success probability is plotted as a function of number of trials, demonstrating that only one of the two set of the attacker’s strategy can be learned. Moreover, the performance of the agent, averaged over the two phases, converges to the performance of a random agent. The simulation was done by averaging over 100 agents, where for each agent the success probabilities were extracted directly from its base-level ECM network. Meta-parameters:
γ=0
,
η=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni3-2556579-large.gif
2016,7458793,FIGURE 4.,"n
-ship game: The dependence of the performance on the
η
parameter is shown for different
n
. The performance is evaluated by an average reward gained during the
10
6
-th game. The simulation was done by averaging over
10
3
agents. The
γ
parameter was set to
10
−4
. Adapted from [30].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni4-2556579-large.gif
2016,7458793,FIGURE 5.,"A schematic two-layered meta-level ECM
ξ
network, whose actions control the value of a general meta-parameter
ξ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni5-2556579-large.gif
2016,7458793,FIGURE 6.,"The glow meta-level network (ECM
η
): The specific realization employed in this work.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni6-2556579-large.gif
2016,7458793,FIGURE 7.,"The damping meta-level network (ECM
γ
): The specific realization employed in this work.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni7-2556579-large.gif
2016,7458793,FIGURE 8.,"Invasion game: The attacker inverts its strategy every
1.2×
10
5
steps. Three types of PS agents are depicted: with full meta-learning capability (in solid blue), with adjusted
γ
value but with
η
value that is chosen randomly from the ECM
η
network (in dashed red), and agents whose
γ
and
η
values are fixed to random values, each agent with its own values (in dotted gray). Top: The performances of the different agents are shown as a function of a number of trials; Middle: The average
γ
value is shown as a function of a number of trials; Bottom: The average
η
value is shown as a function of a number of trials. The simulations were done by averaging over 100 agents, where for each agent the success probabilities were extracted directly from its base-level ECM network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni8abc-2556579-large.gif
2016,7458793,FIGURE 9.,"Invasion game: The attacker inverts its strategy every
1.2×
10
5
steps as in Fig. 8. The performance of the
γ
-network of the meta-learning PS is shown as a function of a number of trials, in terms of the probability to choose rule I (see Eq. 8). The simulation was done by averaging over 100 agents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni9-2556579-large.gif
2016,7458793,FIGURE 10.,"n-ship game: The number of ships
n
increases from one to four. Each phase lasts for
3.5×
10
5
×n
trials. Two types of PS agents are depicted: with full meta-learning capability (in solid blue), and with adjusted
γ
value but with
η
value that is chosen randomly from the ECM
η
network (in dashed red). Top: The performance of the two different agents is shown as a function of a number of trials in terms of average reward. For each phase the average reward of the optimal strategy, a greedy strategy and a fully random one is plotted in dashed light blue, dotted-dashed purple, and dotted orange, respectively; Middle: The average
γ
values of the two different kinds of agents are shown as a function of a number of trials; Bottom: For the meta-learning PS agent the probability to choose each of the
10 η
-actions is plotted at the end of each phase in a different plot. Connecting lines are shown to guide the eyes. The simulations were done by averaging over 100 agents, where for each agent the average reward was extracted directly from its base-level ECM network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni10abc-2556579-large.gif
2016,7458793,FIGURE 11.,"Three setups of the grid-world task. Left: The basic grid-world as presented in Ref. [49]; Middle: Some of the walls are positioned differently; Right: The basic grid-world with a distracting small reward
λ
min
placed 12 steps from the agent. In all three setups a large reward of
λ
max
awaits the agent in 14 steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni11abc-2556579-large.gif
2016,7458793,FIGURE 12.,"Grid-world task: Two types of PS agents are depicted: with full meta-learning capability (in solid blue), and with adjusted
γ
value but with
η
value that is chosen randomly from the ECM
η
network (in dashed red). Top: The performances of the two different agents are shown as a function of a number of trials in terms of average number of steps per unit reward; Middle: The average
γ
values of the two different kinds of agents are shown as a function of a number of trials; Bottom: For the meta-learning PS agent the probability to choose
η=0.1
or
η=0.2
and the probability to choose either of the other
8 η
-actions are plotted as a function of a number of trials; The first two phases of the game last
10
6
trials, whereas the last phase lasts
5×
10
6
trials. These phases correspond to three different kinds of grid-worlds shown in Fig. 11. The simulations were done by averaging over
10
4
agents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7458793/melni12abc-2556579-large.gif
2016,7312982,Fig. 1.,"Normal gear and faulty gears, where (a) normal gear, (b) one tooth missing, and (c) one chipped tooth.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7398213/7312982/zhang1abc-2496272-large.gif
2016,7312982,Fig. 2.,"Classification performance with varying SNR, where (a) results on 0 HP, (b) results on 2 HP, and (c) results on gearbox fault dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7398213/7312982/zhang2-2496272-large.gif
2016,7312982,Fig. 3.,"Parameter sensitivity analysis under different parameter selections on (a) rolling bearing dataset (0 HP), (b) gearbox dataset, and (c) motor electrical dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7398213/7312982/zhang3-2496272-large.gif
2016,7295596,Fig. 1.,Difference and relationship among ELM and our method. (a) ELM feature mapping. (b) Our feature mapping.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang1ab-2481713-large.gif
2016,7295596,Fig. 2.,"Structure of our proposed method.
d<n
: dimension reduction,
d>n
: pexpanded dimension representation, outputs
t=x(m=n)
: unsupervised learning, outputs
t
equals desired label: supervised learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang2-2481713-large.gif
2016,7295596,Fig. 3.,"Proposed method with
c
-layer structure. In the multilayer network structure, each layer is generated by Algorithm 1 and functions as a separated feature extractor. The input weights
[(
a
^
1
f
,
b
^
1
f
),…,(
a
^
L
f
,
b
^
L
f
)]
, with respect to first layer in two-layer ELM, is a hidden layer in multilayer network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang3-2481713-large.gif
2016,7295596,Fig. 4.,"Average testing accuracy by using LPP, IsoP, LSDA, LDA, MMP, NPE and the proposed method on Caltech101, Caltech256, Indoor67, PFID61, Scene15, Flower102, Mushroom, Movement, and Olive face, where the
x
- and
y
-axis show the number of features and average testing accuracy, respectively. Results on (a) Caltech101, (b) Indoor67, (c) Caltech256, (d) PFID61, (e) Scene15, and (f) Flower102.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang4abcdef-2481713-large.gif
2016,7295596,Fig. 5.,"For each subfigure top to bottom. (a) Random samples from Yale test dataset, reconstructions produced by the 20-dimensional proposed method, reconstructions by the 20-dimensional standard PCA, and reconstructions by 20-dimensional DBN. The average testing RMSE per image for the last three rows is 0.0961, 0.1127, and 0.1761. (b) Top to bottom: random samples from Olivetti face test dataset, reconstructions produced by the 40-dimensional proposed method, reconstructions by 40-dimensional standard PCA, and reconstructions by 40-dimensional DBN. The average testing RMSE per image for the last three rows is 0.1012, 0.1270, and 0.1968. (c) Top to bottom: random samples from Car dataset, reconstructions produced by the 100-dimensional proposed method, reconstructions by 100-dimensional standard PCA, and reconstructions by 100-dimensional DBN. The average training RMSE per image for the last three rows is 0.0630, 0.2170, and 0.2370. (d) Top to bottom: random samples from Mnist dataset, reconstructions produced by the 20-dimensional proposed method, reconstructions by 20-dimensional standard PCA, and reconstructions by 20-dimensional DBN. The average training RMSE per image for the last three rows is 0.1563, 0.1586, and 0.1236.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang5abcd-2481713-large.gif
2016,7295596,Fig. 6.,"Average testing accuracy by using LPP, SAE, OLGE, LGE, DBN and the proposed method on Olivetti face, USPS, and Yale Face, where the
x
- and
y
-axis show the number of features and average testing accuracy, respectively. Results on (a) Olivetti face, (b) USPS, and (c) Yale Face.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang6abc-2481713-large.gif
2016,7295596,Fig. 9.,Compared experimental results between TLRL and MLRL on USPS and W3a.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang9-2481713-large.gif
2016,7295596,Fig. 7.,"Expanded representation learning: compared experimental results on USPS, W3a, and Covtype, where the
x
- and
y
-axis show the number of features and average testing accuracy, respectively. Results on (a) USPS, (b) w3a, and (c) IJCNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang7abc-2481713-large.gif
2016,7295596,Fig. 8.,"Sensitive performance on Codrna, Covtype, and Connect dataset. Unlike the performance of DBN, Performance of our method is not sensitive to the user-specified parameters, and good testing accuracies can be achieved. (a)–(c) Sensitive performance with our proposed method on Codrna, Covtype, and Connect. (d)–(f) Sensitive performance with DBN on Codrna, Covtype, and Connect.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7586170/7295596/yang8abcdef-2481713-large.gif
2016,7103315,Fig. 1.,(a) Sobol’. (b) Purely random sequence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7103315/cerve1ab-2424999-large.gif
2016,7358047,FIGURE 1.,"The BCM learning rule for synaptic plasticity. A sliding threshold
θ
M
depends on the average activity of the postsynaptic neuron. The green line indicates a learning curve with a relatively high average postsynaptic activity and the threshold is relatively low. The red line indicates a curve with a relatively low average activity and the threshold is relatively high. The synapse is undergoing Long-term potential (LTP) when the postsynaptic activity is higher than this threshold, and long-term depression (LTD) when it is lower.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li1-2509005-large.gif
2016,7358047,FIGURE 2.,"A basic STDP learning window (kernel) for synaptic plasticity is shown as an example. LTP happens when the presynaptic action potential precedes the postsynaptic action potential as shown at the top of the figure, while LTD happens for the reverse time order.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li2-2509005-large.gif
2016,7358047,FIGURE 3.,"An interpretive graph of neuronal intrinsic plasticity. As the input distribution changes over time, the response tuning curve of a single neuron will be adapted to match the input for producing a stable output response level and preserving the ability of efficient coding. Three different response tuning curves in the top figure match three different input distributions in the bottom figure, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li3-2509005-large.gif
2016,7358047,FIGURE 4.,A comparison of learning results of three different rate-based IP rules.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li4-2509005-large.gif
2016,7358047,FIGURE 5.,Tuning curve (spike-count rate versus constant injected current curve I ) changes with different parameter settings. (A) the slope of the tuning curve becomes steeper as rC increases; (B) the threshold current of the tuning curve increases as rR grows.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li5ab-2509005-large.gif
2016,7358047,FIGURE 6.,"The average spike-count distributions of an adapted IF neuron with three Gaussian input distributions: pdf1 for (A,B,C), pdf2 for (D,E,F), and pdf3 for (G,H,I). The window size is adjusted so that the average number of spikes per window is the same for all neurons: two spikes for (A,D,G), six spikes for (B,E,H) and ten spikes for (C,F,I) per window. All distributions are plotted in the decimal-logarithmic space so that an exponential distribution appears as a straight line. The spike-count distributions depend on the window size used to calculate the rate. The spike-count distributions with a small window size (A,D,G) are approximately exponential, and those with a large window size (B,C,E,F,H,I) are with droop-heads, as Weibull distributions. Each spike-count distribution is obtained by averaging over 5 trials with the same 10-minute stimulus. Error bars are standard errors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li6abcdefghi-2509005-large.gif
2016,7358047,FIGURE 7.,Sparse codes of excitatory neurons generated by King’s E-I Net. These sparse codes are reproduced by King’s Matlab codes. Földiak’s model and Savin’s model can also produce similar sparse codes from natural images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li7-2509005-large.gif
2016,7358047,FIGURE 8.,"An general model combining different plasticity mechanisms for sparse coding/ICA. Three important mechanisms are utilized in this general model, (1) Hebbian-like learning for feedforward connections to form cooperation between the input and output of learning units, (2) anti-Hebbian learning for horizontal connections to produce competition within learning units so as to decorrelate the responses of learning units, and (3) intrinsic plasticity of single learning units for a homeostatic sate of the whole network. As the response of a single learning unit is relatively low, the activity of the global network will be sparse with the decorrelation (unsynchronization) among different neurons introduced by anti-Hebbian learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li8-2509005-large.gif
2016,7358047,FIGURE 9.,The construction of a simple feedforward neural network trained by the synergistic learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li9-2509005-large.gif
2016,7358047,FIGURE 10.,"Learning performance of the synergies between information theoretic learning and infomax intrinsic plasticity. All these lines are 300-epoch learning curves of a 3-layer feedforward network trained with a data set of a Mackey-Glass chaotic time series (“MG”). The dashed lines denote the learning curves of the original MEE algorithm, and the solid lines denote the learning curves of the synergistic algorithm. (A) Learning curves of the quadratic information potential. (B) Learning curves of the mean squared error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li10ab-2509005-large.gif
2016,7358047,FIGURE 11.,"Relation between the training result and the number of hidden neurons of a 3-layer feedforward network. Training results are obtained after 1000 epochs for a data set of a Mackey-Glass chaotic time series (“MG”). The circle markers denote the results obtained by the MEE algorithm, and the cross markers denote the results obtained by the synergistic algorithm. (A) Results of the quadratic information potential. (B) Results of the mean squared error. The number of neuron units can be highly reduced with the infomax IP rule when the network still keeps good performance. This is an encouraging result of synergistic learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7358047/li11ab-2509005-large.gif
2016,7738452,FIGURE 1.,Sequential learning in the IoT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7738452/park1-2615643-large.gif
2016,7738452,FIGURE 2.,Effectiveness of SL for different memory sizes and device densities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7738452/park2-2615643-large.gif
2016,7738452,FIGURE 3.,Distribution of IoT devices in different CHT levels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7738452/park3-2615643-large.gif
2016,7738452,FIGURE 4.,Expected energy consumed at the cognitive hierarchy equilibrium (CHE) solution vs. CHT level.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7738452/park4-2615643-large.gif
2016,7416647,Fig. 1.,Signal processing block.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun1-2533601-large.gif
2016,7416647,Fig. 2.,d-axis current versus torque for MTPA operation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun2-2533601-large.gif
2016,7416647,Fig. 3.,Schematic of the proposed SLC scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun3-2533601-large.gif
2016,7416647,Fig. 4.,Schematic of proposed self-learning MTPA control for IPMSM drives.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun4-2533601-large.gif
2016,7416647,Fig. 5.,Variations of resultant torque and d-axis reference current from SLC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun5-2533601-large.gif
2016,7416647,Fig. 6.,Simulation result of S and d-axis current reference generation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun6-2533601-large.gif
2016,7416647,Fig. 7.,Resultant torque and the output of integrator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun7-2533601-large.gif
2016,7416647,Fig. 8.,SLC behavior after machine parameter changes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun8-2533601-large.gif
2016,7416647,Fig. 9.,"Simulation results of torque reference, resultant torque as well as q-axis current.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun9-2533601-large.gif
2016,7416647,Fig. 10.,Constant current amplitude curves and VSIC tracking performance with inaccurate q-axis current.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun10-2533601-large.gif
2016,7416647,Fig. 11.,Training performance when reference torque changes slowly.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun11-2533601-large.gif
2016,7416647,Fig. 12.,Training performance when reference torque changes rapidly.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun12-2533601-large.gif
2016,7416647,Fig. 13.,Experimental test rig.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun13-2533601-large.gif
2016,7416647,Fig. 14.,Experimental result of VSIC’s MTPA tracking performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun14-2533601-large.gif
2016,7416647,Fig. 15.,"d-axis current and estimated/measured torque response to torque command step from 20 to 35
N⋅m
then step back to 20
N⋅m
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun15-2533601-large.gif
2016,7416647,Fig. 16.,Integrator output together with the measured and estimated torque responses.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun16-2533601-large.gif
2016,7416647,Fig. 17.,Estimated/measured torque and the measured q-axis current.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun17-2533601-large.gif
2016,7416647,Fig. 18.,d-axis current and estimated/measured torque response to torque command step from 8 to 13 \${\text{N}}\cdot {\text{m}}\$ .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/7514285/7416647/sun18-2533601-large.gif
2016,7289481,Fig. 1.,Occupancy for different time slots in the band.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat1-2487047-large.gif
2016,7289481,Fig. 2.,CDFs for the eight bands between 880 and 2500 MHz.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat2-2487047-large.gif
2016,7289481,Fig. 3.,Occupancy versus spectrum frequency for (a) band 880–915 MHz and (b) band 925–960 MHz.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat3ab-2487047-large.gif
2016,7289481,Fig. 4.,Effect of different threshold levels on mean occupancy for an (a) aperiodic band (2110–2170 MHz) and a (b) periodic band (2400–2500 MHz).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat4-2487047-large.gif
2016,7289481,Fig. 5.,"Selection of optimal threshold
(γ)
and optimal splitting range
([
U
oc
,
L
oc
])
for determining the classification criteria of three days data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat5-2487047-large.gif
2016,7289481,Fig. 6.,"Performance comparison using (a)
k=55
and (b)
k=192
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat6ab-2487047-large.gif
2016,7289481,Fig. 7.,(a) Performance comparison. (b) Blocking probability.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/7569115/7289481/azmat7ab-2487047-large.gif
2016,7272072,Fig. 1.,Left: Orientation sensor on index finger for finger tapping task (top) and its corresponding model (bottom). Right: Orientation sensor on the wrist for diadochokinesis task (top) and its corresponding model (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7272072/marti1-2480242-large.gif
2016,7272072,Fig. 2.,"Example of raw angle signal (gray) and smoothing spline angle (black) signal for diadochokinesis. The frequency of each tap is obtained as the inverse of the time (t) between consecutive peaks. In the figure, the frequency of tap six was defined as the inverse of the time difference between the sixth and the fifth peaks. The amplitude of each tap is obtained as the difference in amplitude from a peak to the next valley. In the figure, the amplitude of tap eight was defined as the amplitude difference between the eighth peak and the eighth valley.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7272072/marti2-2480242-large.gif
2016,7272072,Fig. 3.,"Overview of four classification approaches. From each set of features, a subset with optimal features is constructed. The first two sets are composed of features from expert knowledge and the last two contain features obtained from PCA. Subsets 1, 2, and 4 are built using the forward-selection wrapper as the feature selection algorithm, while subset 3 includes in each iteration the PC that explains most of the variance that has not already been included. Two features are included in each subset a priori before the addition of more features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7272072/marti3-2480242-large.gif
2016,7272072,Fig. 4.,"Multiclass classification using SVM and the RBF kernel in a one-versus-all methodology using LOOCV with only two features. In this example, tasks were evaluated from 0 to 3 (four classes). In a one-versus-all methodology, every single class is evaluated against the rest of the classes (e.g., red dots correspond to the amplitude and frequency of the tasks scored as zero and white dots correspond to the amplitude and frequency of the remaining tasks in the top left figure). The different decision surfaces created using the red and white dots are illustrated with different colors. The confidence of a new sample (green dot representing the task left out by LOOCV) to belong to a certain class is represented by the color of the surface and by the scale on the right of each figure. The new sample is then classified in the class that obtained the highest confidence (score 0 in the example).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7272072/marti4-2480242-large.gif
2016,7272072,Fig. 5.,"Boxplots of combined amplitude–frequency feature for finger tapping (left), diadochokinesis (center), and toe tapping (right). On average, for the three tasks controls exhibit a significantly higher combination of amplitude and frequency than patients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7272072/marti5-2480242-large.gif
2016,7272072,Fig. 6.,"Boxplots of combined amplitude–frequency feature for finger tapping (left), diadochokinesis (center), and toe tapping (right). Below each graph, there is a visualization of the features selected for subsets 1 and 2 for each evaluator on each iteration. The incorporation of a nonrepeated feature is indicated in pale gray. On dark gray, the features that are selected by more than one classifier are indicated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7457176/7272072/marti6-2480242-large.gif
2016,7476853,Fig. 1.,Topological diagram of organizational intranet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa1-2561200-large.gif
2016,7476853,Fig. 2.,Distribution of calls based on various bandwidths.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa2-2561200-large.gif
2016,7476853,Fig. 3.,Monthwise distribution of calls in the corpus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa3-2561200-large.gif
2016,7476853,Fig. 4.,Timewise distribution of calls in the corpus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa4-2561200-large.gif
2016,7476853,Fig. 5.,Scatter plot of month versus bandwidth of call.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa5-2561200-large.gif
2016,7476853,Fig. 6.,Scatter plot of time versus bandwidth of call.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa6-2561200-large.gif
2016,7476853,Fig. 7.,Connectivity graph of source–destination video calls. (a) Nodes with low degree in the periphery. (b) Nodes with high degree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa7ab-2561200-large.gif
2016,7476853,Fig. 8.,Learning accuracy of the classifier by using various attribute sets for destination prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa8-2561200-large.gif
2016,7476853,Fig. 9.,Yearwise count of records and destination prediction accuracy with all attributes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa9-2561200-large.gif
2016,7476853,Fig. 10.,Learning accuracy of the classifiers with varying features for bandwidth prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa10-2561200-large.gif
2016,7476853,Fig. 11.,Confusion matrix for bandwidth classifier using all feature set with bandwidth values in groups of 512 kb/s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa11-2561200-large.gif
2016,7476853,Fig. 12.,% Accuracy of the classifier algorithms with specific feature set for bandwidth prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570650/7495039/7476853/goswa12-2561200-large.gif
2016,7358050,Fig. 1.,"Sample knowledge graph. Nodes represent entities, edge labels represent types of relations, and edges represent existing relationships.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke1-2483592-large.gif
2016,7358050,Fig. 2.,"Example of entity resolution in a toy knowledge graph. In this example, nodes 1 and 3 refer to the identical entity, the actor Alec Guinness. Node 2, on the other hand, refers to Arthur Guinness, the founder of the Guinness brewery. The surface name of node 2 (“A. Guinness”) alone would not be sufficient to perform a correct matching as it could refer to both Alec Guinness and Arthur Guinness. However, since links in the graph reveal the occupations of the persons, a relational approach can perform the correct matching.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke2-2483592-large.gif
2016,7358050,Fig. 3.,Tensor representation of binary relational data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke3-2483592-large.gif
2016,7358050,Fig. 4.,"RESCAL as a tensor factorization of the adjacency tensor
Y
. Figure adapted from [147].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke4-2483592-large.gif
2016,7358050,Fig. 5.,"Visualization of RESCAL and the ER-MLP model as neural networks. Here,
H
e
=
H
r
=3
and
H
a
=3
. Note that the inputs are latent features. The symbol
g
denotes the application of the function
g(⋅)
. (a) RESCAL. (b) ER-MLP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke5ab-2483592-large.gif
2016,7358050,Fig. 6.,"(a) A small KG. There are four entities (circles): three adults (
a
1
,
a
2
, and
a
3
) and one child
c
. There are two types of edges: adults may or may not be married to each other, as indicated by the red dashed edges, and the adults may or may not be parents of the child, as indicated by the blue dotted edges. (b) We add binary random variables (represented by diamonds) to each KG edge. (c) We drop the entity nodes, and add edges between the random variables that belong to the same clique potential, resulting in a standard MRF.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke6abc-2483592-large.gif
2016,7358050,Fig. 7.,Architecture of the knowledge vault.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7358050/nicke7-2483592-large.gif
2016,7122357,Fig. 1.,"Examples of decomposition of data matrix by LatLRR, adapted from [15].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin1-2436951-large.gif
2016,7122357,Fig. 5.,"Effects of parameter
α
on our two methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin5-2436951-large.gif
2016,7122357,Fig. 2.,Samples of face databases. (a) Samples of Extended YaleB. (b) Samples of the AR database. (c) Samples of the PIE database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin2abc-2436951-large.gif
2016,7122357,Fig. 3.,Examples of discriminative features extracted from the Extended YaleB database. (a) and (b) First rows are original face images and the second rows are the corresponding discriminative features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin3ab-2436951-large.gif
2016,7122357,Fig. 4.,Recognition rates on Extended YaleB under different percentages of random corruption.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin4-2436951-large.gif
2016,7122357,Fig. 6.,"Confusion matrix of ILRDFL on the Fifteen Scene Categories database. The average classification rates of each class are along the diagonal. The entry in the
i
th row and
j
column is the percentage of images from class
i
that are misidentified as class
j
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin6-2436951-large.gif
2016,7122357,Fig. 7.,Ten categories in the UCF50 database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin7-2436951-large.gif
2016,7122357,Fig. 8.,"Confusion matrix of ILRDFL on the UCF50 database. The classification rates are not shown. The color legend is drawn on the right, best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin8-2436951-large.gif
2016,7122357,Fig. 9.,Average training time (seconds) on the six databases. The training time of our methods contains the robust PCA denoising time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin9-2436951-large.gif
2016,7122357,Fig. 10.,Average training time (seconds) for fast algorithm on the three test databases. The training time of our methods contains the robust PCA denoising time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7453225/7122357/lin10-2436951-large.gif
2016,7088593,Fig. 1.,"Rotation of palm by 90°, 180°, 270°.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu1-2418275-large.gif
2016,7088593,Fig. 2.,"Transformation instance of HOG cells and bins after rotating the image. (a)–(c) Relationship of cells of blocks, pixels, and bins, respectively, between an original image (0°) and the image rotated by 180°. (d)–(f) Relationship of cells of blocks, pixels, and bins, respectively, between an original image and the image rotated by
90
∘
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu2abcdef-2418275-large.gif
2016,7088593,Fig. 3.,"Transformation functions in
β
HOG.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu3-2418275-large.gif
2016,7088593,Fig. 4.,"(a) Circles correspond to LBPs 0100011 (leftmost), 11010100, 00011001, and 11110100. All these patterns have more than two 01 transitions. (b) Circles correspond to LBPs 00110000 (leftmost), 00001100, 00000011, and 11000000. All these patterns have exactly two 0/1 transitions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu4ab-2418275-large.gif
2016,7088593,Fig. 5.,Overview of our detection and tracking system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu5-2418275-large.gif
2016,7088593,Fig. 6.,"(a) Cambridge, (b) palm, and (c) fist images from our dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu6abc-2418275-large.gif
2016,7088593,Fig. 7.,DET curves for datasets 1–3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu7-2418275-large.gif
2016,7088593,Fig. 8.,(a)–(c) Best HOG blocks for the palm posture and for the (d)–(f) fist posture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu8abcdef-2418275-large.gif
2016,7088593,Fig. 9.,"HOG+SVM versus
β
HOG+SVM for (a) Cambridge, (b) palm-our, and (c) fist-our datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu9abc-2418275-large.gif
2016,7088593,Fig. 10.,"HOG+SVM versus
β
HOG+SVM after rotation of images for (a) Cambridge, (b) palm-our, and (c) fist-our datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu10abc-2418275-large.gif
2016,7088593,Fig. 11.,"LBP versus RLBP for (a) Cambridge, (b) palm-our, and (c) fist-our datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu11abc-2418275-large.gif
2016,7088593,Fig. 12.,"Tracking the palm by mean shift (MS), particle filter (PF), and mean shift implanted particle filter (Ours).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu12-2418275-large.gif
2016,7088593,Fig. 13.,"Sampled tracking results of evaluated algorithms on short sequences. (a) Tracking results on sequence Palm1 with scale and illumination changes and complex background. (b) Tracking results on sequence Palm3 with motion blur, in-plane rotations, and scale changes. (c) Tracking results on sequence Fist2 with motion blur, in-plane rotations, and complex background. (d) Tracking results on sequence Fist5 with in-plane rotations and scale changes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu13abcd-2418275-large.gif
2016,7088593,Fig. 14.,"Sampled tracking results of evaluated algorithms on long sequences. Tracking results on sequence (a)
Long
1
and (b)
Long
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7434095/7088593/niu14ab-2418275-large.gif
2016,7352305,Fig. 1.,"OAA reduction applied to many different
k
-class classification data sets, for
k=2
. The
x
-axis is the squared-error loss of the base regressor. The
y
-axis is the
k
-classification loss of the implied classifier. The lack of any points in the upper left corner for all data sets is as predicted by analysis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7352305/beyge1-2494118-large.gif
2016,7352305,Fig. 2.,"Hard problem for OAA with linear representations. There is a single feature represented by the horizontal dimension. There are three classes 1, 2, and 3 with each example from class
i
having feature value
i
. OAA with linear regression cannot effectively distinguish class 2 from classes 1 and 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7352305/beyge2-2494118-large.gif
2016,7352305,Fig. 3.,"Mnist experimental results for OAA reduced to binary classification and OAA reduced to squared loss regression while varying training set size. The
x
-axis is the 0/1 test loss of the induced subproblems, and the
y
-axis is the 0/1 test loss on the multiclass problem. The classifiers are linear in pixels. The regression approach (a regret transform)dominates the binary approach (an error transform).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7352305/beyge3-2494118-large.gif
2016,7352305,Fig. 4.,"Training time versus evaluation accuracy for part of speech tagging (left) and named entity recognition (right). The
x
-axis is in log scale. Different points correspond to different termination criteria for training. Both figures use hyperparameters that were tuned (for accuracy) on the heldout data. (Note: lines are curved due to the log scale
x
-axis.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/7360840/7352305/beyge4ab-2494118-large.gif
2016,7479553,Fig. 1.,BMI paradigm employed in this paper for simultaneous control of multi-DOFs robot using adaptive left-right MI decoder and synergetic motor learning for peripheric joint redundancy management. The black dots indicate the targets for the subjects in the vertical plane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas1-2560532-large.gif
2016,7479553,Fig. 2.,Standard 10–20 representation of the electrodes present in an Emotiv headset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas2-2560532-large.gif
2016,7479553,Fig. 3.,"Timing diagram of a single trial to train the subject in left and right hand MI (as indicated by left and right arrow, respectively).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas3-2560532-large.gif
2016,7479553,Fig. 4.,BMI adaptive scheme employed in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas4-2560532-large.gif
2016,7479553,Fig. 5.,Block diagram of the tacit learning-based synergetic motor control paradigm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas5-2560532-large.gif
2016,7479553,Fig. 6.,Topographical plot of brain activation during −500 to 1000 ms for (a) left-hand MI and (b) right-hand MI for subject 6.0 ms marks the onset of the task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas6ab-2560532-large.gif
2016,7479553,Fig. 7.,"Average accuracies obtained over nine subjects, while changing the limiting posterior probability from 0.1 to 1.0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas7-2560532-large.gif
2016,7479553,Fig. 8.,Endpoint transition of robot with load (a) 300 g and trajectory of the shoulder-elbow-wrist joints for 300 g load and (b) 600 g and trajectory of the shoulder-elbow-wrist joints for 600 g load. (c) Phase potrait of joint angle-angular velocity for the shoulder-elbow joints at different weight conditions. (d) Energy consumption of time sequential changes in one cycle of up/down motion with 600 g load along with its FB component and FF component.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas8abcd-2560532-large.gif
2016,7479553,Fig. 9.,Subject performing the online control of multi-DOF robot in a simultaneous way by using co-adaptive BMI. The black dots indicate the targets for the subjects in the vertical plane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas9-2560532-large.gif
2016,7479553,Fig. 10.,"Trace of the movement of the robot arm during simultaneous multi-DOF robot control which is driven by the subject motor intention through co-adaptive BMI. The robot is holding (a) 300 g load, (b) 600 g load. The joint angle variance on shoulder and wrist for the heavier weight condition has become half compared to lighter weight condition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/7491400/7479553/hayas10ab-2560532-large.gif
2016,7150355,Fig. 1.,Convergence of the objective function value on (a) Haberman and (b) ionosphere data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu1ab-2401733-large.gif
2016,7150355,Fig. 2.,"Flowchart of the proposed ECGS method that uses an ensemble of multiple pairwise constraint sets. Note
N
E
is the number of pairwise constraint sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu2-2401733-large.gif
2016,7150355,Fig. 3.,Relationship of the proposed CGS method and other related methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu3-2401733-large.gif
2016,7150355,Fig. 4.,"Classification results of different supervised feature selection methods on the aYahoo data set using
K
-NN classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu4-2401733-large.gif
2016,7150355,Fig. 5.,"Classification accuracies versus different numbers of selected features achieved by seven feature selection methods on (a) Haberman, (b) ionosphere, (c) hepatitis, and (d) sonar data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu5abcd-2401733-large.gif
2016,7150355,Fig. 6.,"Classification results of our proposed methods with and without using the ensemble of pairwise constraint sets with
K
-NN classifier, while the results using SVM classifier is reported in the supplementary materials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu6-2401733-large.gif
2016,7150355,Fig. 7.,"Classification accuracies versus parameters
λ
1
,
λ
2
, and
λ
3
on the Haberman data set. (a) Fixed
λ
1
. (b) Fixed
λ
2
. (c) Fixed
λ
3
. Note that in (a)–(c), when two parameters vary, another is fixed as 0.001.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu7abc-2401733-large.gif
2016,7150355,Fig. 8.,Classification results versus the number of pairwise constraints achieved by different methods on (a) Haberman and (b) ionosphere data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7150355/liu8ab-2401733-large.gif
2016,7412749,Fig. 1.,"Deep convolutional architecture consisting of convolutional, pooling and a softmax layer(s). Input patches are extracted from multiple scales of an image. The pixel spacing of the patches is adjusted such that the feature maps at different scale levels are equally sized. Each scale level of the CSAE model is processed in isolation before all activations are integrated in the second last layer. The convolutional layers in the unsupervised parts are trained as autoencoders; In the supervised part the (pretrained) weights and bias terms are fine-tuned using softmax regression (see text for details).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7412749/7412749-fig-1-source-large.gif
2016,7412749,Fig. 2.,"Effect of varying the threshold on the posteriors
T
dense
on two performance measures of MD scoring, namely (i) the image-wise average of the dice coefficient, and (ii) the root mean squared error between the percent density (pmd) as measured by our machinery and the radiologist.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7412749/7412749-fig-2-source-large.gif
2016,7412749,Fig. 3.,"Automated MD thresholding. Depicted are (a) original image, (b) dense tissue according to expert cumulus-like threshold, and (c) dense tissue according to CSAE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7412749/7412749-fig-3-source-large.gif
2016,7412749,Fig. 4.,"(a) An autoencoder for learning the features of the convolutional layer. The input is vectorized and reconstructed by an encoder-decoder architecture. (b) Inference in a convolutional layer using a 3d convolution. The encoded units correspond to the highlighted units in output
z
(l+1)
of the convolutional layer. The weights
w
j
between input feature maps
z
(l)
and the
j
th output feature map are marked in red and initialized with the learned weights from the autoencoder. We refer to the text for details.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7463083/7412749/7412749-fig-4-source-large.gif
2016,7539280,Fig. 1.,Two frames from consumer videos (left) and YouTube videos (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang1ab-2598679-large.gif
2016,7539280,Fig. 2.,"The data distribution and decision boundaries. (a) linear classifiers learned for a three-class problem on labeled data in source domain. (b) classifiers learned on the source domain do not fit the target domain due to the change of data distribution. (c) domain adaptation with EDA by simultaneously learning new classifier and category transformation matrix. Note that the category transformation denotes the output adaptation with a matrix
Θ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang2abc-2598679-large.gif
2016,7539280,Fig. 3.,"The flowchart of the proposed cross-domain learning framework in multiple views. Specifically, for each domain, the same type of feature is first extracted. Second, the base classifier is trained on the raw feature of source data. Third, the feature mapping (random projection) is conducted on the both features of source and target data. Fourth, the EDA based domain adaptation classifier is learned. Finally, the visual categorization task with domain adaptation is done.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang3-2598679-large.gif
2016,7539280,Algorithm 1,Extreme Learning Machine Based Domain Adaptation (EDA),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang7-2598679-large.gif
2016,7539280,Algorithm 2,Multi-View EDA,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang8-2598679-large.gif
2016,7539280,Algorithm 3,Complete EDA,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang9-2598679-large.gif
2016,7539280,Fig. 4.,Recognition accuracy on Bing-Caltech data with two settings.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang4ab-2598679-large.gif
2016,7539280,Fig. 5.,"Performance variations of our EDA using different parameters
C
S
and
C
T
. (a) MAP on SIFT+ST features of YouTube & Consumer Videos data with 3 labeled target training samples per event. (b) Recognition accuracy (RA) tested on 3DA Office data for task amazon
→
dslr. (c) RA for task webcam
→
dslr.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang5abc-2598679-large.gif
2016,7539280,Fig. 6.,"Convergence of EDA for three datasets: (a, d) Consumer & YouTube Videos; (b, e) 3DA Office dataset: webcam
→
dslr; (c, f) 4DA Extended Office dataset: amazon
→
dslr.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7539280/zhang6abcdef-2598679-large.gif
2016,7087364,Fig. 1.,A support vector machine-based framework for detecting covert timing channels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres1-2423680-large.gif
2016,7087364,Fig. 2.,"K-S scores for test traffic using observation sample size of 2,000 IPDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres2-2423680-large.gif
2016,7087364,Fig. 3.,"Regularity scores for test traffic using observation sample size of 2,000 IPDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres3-2423680-large.gif
2016,7087364,Fig. 4.,"Entropy scores for test traffic using observation sample size of 2,000 IPDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres4-2423680-large.gif
2016,7087364,Fig. 5.,"Corrected Conditional Entropy (CCE) scores using observation sample size of 2,000 IPDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres5-2423680-large.gif
2016,7087364,Fig. 6.,Sensitivity test scores for the four CTC algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres6-2423680-large.gif
2016,7087364,Fig. 7.,Specificity test scores for the four CTC algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres7-2423680-large.gif
2016,7087364,Fig. 8.,Precision test scores for the four CTC algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/7430413/7087364/shres8-2423680-large.gif
2016,7335636,Fig. 1.,"Sensitivity study on BMTRL-LC(MG-MGIG): study the effect of the parameter
λ
in terms of accuracy on Cornell and ML data, respectively. We vary the
λ
in the
λ
-value set of
{
10
−2
,
10
−1
,…,
10
3
}
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7426466/7335636/li1-2502958-large.gif
2016,7335636,Fig. 2.,"Sensitivity study on BMTRL(MG-MGIG), BMTRL-LC(MG-MGIG), and several other competing methods: study the effect of the training ratio in terms of accuracy on Cornell. The
i
th coordinate on the x-axis corresponds to the training ratio
i×10%
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7426466/7335636/li2-2502958-large.gif
2016,6945857,Fig. 1.,Connect-4 board with an example 4-tuple “3-2-1-1” (see Section 3.2).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-1-source-large.gif
2016,6945857,Fig. 2.,"Options for the transfer function
g(x)
. Standard TCL uses the identity function. TCL-EXP ((9) is shown for
β=2.7
and PieceLinear is a piecewise linear function with the same endpoints as TCL-EXP and same slope at
x=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-2-source-large.gif
2016,6945857,Fig. 3.,"Former TDL [8] versus tuned TDL. The parameter settings are in Table IV. The lines without points show the exploration rate
ϵ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-3-source-large.gif
2016,6945857,Fig. 4.,"Sensitivity on the initial learning rate
α
init
for all algorithms. For algorithms specifying
β
init
(like IDBD), we use the transformation
α
init
=
e
β
init
. Parameter
α
init
is screened over a large range. Each point is the mean of 10 runs with 2 million training games. For each run we measure the success rate every 10 000 games and smooth this curve to dampen fluctuations. ‘time to learn’ is the number of games until this smoothed curve crosses the 80%-line for the first time. If a run never crosses this line, ‘time to learn’ is set to 2 millions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-4-source-large.gif
2016,6945857,Fig. 5.,"Sensitivity on the meta step size parameter
θ
for algorithms having this parameter (
μ
in the case of autostep). A large range of
θ
-values is screened. Other settings as in Fig. 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-5-source-large.gif
2016,6945857,Fig. 6.,"Final comparison of different algorithms for connect-4. Every setting is repeated 20 times with different random initialization and random exploration. The agent strength was measured every 10 000 games and each point is the mean of 20 runs. For better display of the curves, error bars are omitted and only a subset of all points is shown in the plots. The sampled points are connected by straight lines. Note that due to this procedure the visual crossing of the 80% line may differ somewhat from the exact numbers given in the second column of Table V.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-6-source-large.gif
2016,6945857,Fig. 7.,"Learning speed for different algorithms. The boxplots show the results from 20 runs. ‘time to learn’ is defined as in Fig. 4. Tcl[r]: the original TCL as in [2] using recommended weight changes,
TCL[δ]
: using the
δ
-signal instead. PL: Piecewise linear, Former TDL from [8], TDL: tuned TDL.
TCL[δ]
and tcl[r] have one and two runs, resp., which never reach the 80% success rate. ‘time to learn’ is set to 2 million games for these runs (not shown in the figure).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6945857/6945857-fig-7-source-large.gif
2016,6951334,Fig. 1.,"Screenshot of frogger game. In a normal game, the player controls the frog using a joystick or keyboard. The only controls are up, down, left, and right. The goal of the game is to maneuver the frog to the five home positions (shown at the top of the screen), without allowing the frog to be hit by one of the cars (bottom half of screen), or drown in the river (top half of screen). The player navigates the river by jumping on the logs and turtles. Each time the player maneuvers the frog to the one of the home positions, a new player-controlled frog is respawned at the bottom of the screen. Once all five home positions are filled, a new level begins and process is repeated. Pitfalls for the player to watch out for include turtles periodically diving underneath the water, and a crocodile that fills one of the home positions at random intervals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-1-source-large.gif
2016,6951334,Fig. 2.,"The perception-action-reward cycle describes the processing of an agent playing a game. The player forms an internal state representation of the gaming environment through perception. Based on the current game state the player's policy determines which action to take. After each action, or inaction, the gaming environment transitions from its current state to the next, yielding a reward. The player forms modifies the policy based on the observation of this transition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-2-source-large.gif
2016,6951334,Fig. 3.,Visualization of “holistic” feature construction. The first and second features are the x- and y-locations of frog respectively. Features 3–7 are the car sprites x-locations. Features 8–12 are the turtle and log sprite's x-locations. Features 13–17 indicate whether the corresponding home location is empty or filled.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-3-source-large.gif
2016,6951334,Fig. 4.,"Visualization of “local” feature construction. Features 1–4 and 6–9 indicate whether a sprite is in the corresponding square-shaped box. Feature 5 is the y-location of the frog. Not pictured here are features 10–14, which (like the holistic feature construction method) indicate whether the home locations are empty or filled.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-4-source-large.gif
2016,6951334,Fig. 5.,"(holistic features) plot of the minimum number of jumps to home for each episode while learning, convolved with a 5000-length moving average filter, for 5 variants of Q-learning and feature weighting. This measures how close the frog made it to its home before dying in each episode. The low-pass filtering was performed because of extremely large episode-to-episode fluctuations in the data. Each episode corresponds to one frog life.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-5-source-large.gif
2016,6951334,Fig. 6.,(holistic features) cumulative number of games won per episode while learning for five variants of Q-learning and feature-weighting. Each episode corresponds to one frog life. The top subplot shows learning over 200 000 episodes. The bottom subplot shows the same learning curves magnified over the first 60 000 episodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-6-source-large.gif
2016,6951334,Fig. 7.,(holistic features) feature weightings learned using state and Q-value data and CAML. The first and second features are the x- and y- locations of frog respectively. Features 3–7 are the car sprites x-locations. Features 8–12 are the turtle and log sprite's x-locations. Features 13–17 are indicate whether the corresponding home location is empty or filled.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-7-source-large.gif
2016,6951334,Fig. 8.,(local features) cumulative number of games won per episode while learning for four variants of Q-learning and feature-weighting. Each episode corresponds to one frog life.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-8-source-large.gif
2016,6951334,Fig. 9.,(local features) feature weightings learned using state and Q-value data and CAML. Features 1–4 and 6–9 indicate whether a sprite is in the corresponding square-shaped box. Feature 5 is the y-location of the frog. Features 10–14 indicate whether the home locations are empty or filled.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-9-source-large.gif
2016,6951334,Fig. 10.,Visualization of the weightings for local features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-10-source-large.gif
2016,6951334,Fig. 11.,"Visualization of “local” feature construction with irrelevant features. Features 1–4 and 6–12 indicate whether a sprite is in the corresponding square-shaped box. Feature 5 is the y-location of the frog. Features 10–12 are less relevant in determining the action the agent needs to take. Not pictured here are features 13–17, which indicate whether the home locations are empty or filled.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-11-source-large.gif
2016,6951334,Fig. 12.,(local + irrelevant features) feature weightings learned using state and Q-value data and CAML. Features 1–4 and 6–12 indicate whether a sprite is in the corresponding square-shaped box. Feature 5 is the y-location of the frog. Features 13–17 indicate whether the home locations are empty or filled.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-12-source-large.gif
2016,6951334,Fig. 13.,(local + irrelevant features) cumulative number of games won per episode while learning for four variants of Q-learning and feature-weighting. Each episode corresponds to one frog life.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4804728/7434093/6951334/6951334-fig-13-source-large.gif
2016,7115113,Fig. 1.,MSEs for the proposed inverse-free ELM and the standard matrix inversion-based accurate ELM. (a) Airfoil self-noise dataset. (b) Energy efficiency dataset. (c) Housing dataset. (d) Parkinson dataset. (e) Physicochemical properties of protein dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7452443/7115113/li1abcde-2434841-large.gif
2016,7115113,Fig. 2.,ROC performance of the proposed algorithm with different activation functions. (a) Diabetes dataset. (b) Leukemia dataset. (c) Adult dataset. (d) MAGIC Gamma telescope dataset. (e) Musk dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7452443/7115113/li2abcde-2434841-large.gif
2016,7115113,Fig. 3.,Sensitivity of the testing accuracy with respect to the Tikhonov regulation parameter on different datasets. (a) Scene15 dataset. (b) MNIST dataset. (c) Connect-4 dataset. (d) Caltech256 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7452443/7115113/li3abcd-2434841-large.gif
2016,7523416,Fig. 1.,"The main idea of Confidence Preserving Machine (CPM): (a) A standard single-margin classifier identifies true positive (TP), true negative (TN), false positive (FP) and false negative (FN). Data within the margin (dashed lines) consist of mostly FP and FN, producing undesired ambiguities for training a classifier. (b) The proposed confident classifiers, two hyperplanes that are not necessarily parallel, reveal easy and hard samples for preserving confident predictions in each class. (c) The proposed CPM, consisting of confident classifiers and a person-specific (PS) classifier using a quasi-semi-supervised (QSS) learning strategy, is trained to propagate predictions from confident test samples (easy test samples) to hard ones.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng1abc-2594486-large.gif
2016,7523416,Fig. 2.,Illustration of CPM on identifying AU12 from a real video. Dashed lines (light green) indicate the hard frames due to low intensities and head pose; solid lines indicate the easy samples for positive (light yellow) and negative (dark green) ones.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng2-2594486-large.gif
2016,7523416,Fig. 3.,"The proposed two-stage CPM framework: Given training videos, the confident classifiers are first trained, and then are passed to train a PS-QSS classifier, which makes the final predictions on the test video. In iterative CPM, easy test samples are selected to iteratively expand the training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng3-2594486-large.gif
2016,7523416,Algorithm 1,Train Confident Classifiers,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng12-2594486-large.gif
2016,7523416,Fig. 4.,"Illustration of two relabeling strategies. Data A and B are two synthetic data without and with noisy instances, respectively. (a)~(c) show the confident classifiers learned on the relabeled data using holistic relabeling on A, holistic relabeling on B, and localized relabeling on B, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng4-2594486-large.gif
2016,7523416,Fig. 5.,"(a) An example of
λ
with
T=5
. (b)~(d) show the effectiveness of the smoothness term
S
on AU6 on video 2F01_11, AU12 on video 2F01_09, and AU17 on video 2F01_09, respectively. The
y
-axis denotes AU occurrence (+1: presence, −1: absence).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng5abcd-2594486-large.gif
2016,7523416,Algorithm 2,Iterative Confidence Preserving Machine,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng13-2594486-large.gif
2016,7523416,Fig. 6.,"A Synthetic example of iCPM. The first column illustrates two training subjects (rectangles and circles) and a test subject (diamonds). A same color indicates the same class. The second, third, and forth column illustrates the initialization and two iterations in Alg. 2, respectively. Points in blue and red colors are easy samples, while those in green are hard ones. (This figure is best shown in color copies).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng6-2594486-large.gif
2016,7523416,Algorithm 3,Sample Selection for kCPM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng14-2594486-large.gif
2016,7523416,Fig. 7.,"An illustration of kCPM: Easy negative, easy positive, and hard samples are denoted as blue rectangles, red circles, and green triangles, respectively. (a) A standard kernel SVM trained on original samples. (b)-(c) Confident classifiers
f
+
and
f
−
trained on selected points under positive-first order and negative-first order, respectively. (d) Confident classifiers cooperatively separate easy and hard samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng7abcd-2594486-large.gif
2016,7523416,Fig. 8.,"Results of SVM, confident, CPM, iCPM, and kCPM. The values are averaged over different AUs. In each dataset, different amounts of AUs are involved: 11 in GFT, 12 in BP4D, 8 in DISFA, 7 in RU_FACS. Note that the scales in each dataset are different for display purpose.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng8-2594486-large.gif
2016,7523416,Fig. 9.,Analysis on CPM-selected hard samples in terms of AU intensities using the DISFA dataset: (Upper) the percentage of hard samples within each intensity; (lower) the percentage of each AU intensity within positive hard samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng9-2594486-large.gif
2016,7523416,Fig. 10.,"Analysis on CPM-selected hard and easy training samples in terms of head pose angle and the similarity with onset/offset frames. Triangles and rectangles indicate hard and easy samples, respectively. A number indicate an AU; “overall” indicates the average over all data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng10-2594486-large.gif
2016,7523416,Fig. 11.,A proportion of hard training samples that CPM selected from the GFT dataset. (Left) Each bar denotes an averaged portion of hard samples over 11 AUs for a subject. (Right) Each bar denotes an averaged proportion over 50 subjects for each AU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7529254/7523416/zeng11ab-2594486-large.gif
2016,7091914,Fig. 1.,Procedure for determining PIs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7091914/zio1-2418739-large.gif
2016,7091914,Fig. 2.,"Overall Pareto fronts obtained by training of the NN using winter (left) and w2011 (right) data sets with 2 and 3, and 2 and 4 input neurons, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7091914/zio2-2418739-large.gif
2016,7091914,Fig. 3.,Overall Pareto front obtained by training of the NN and the corresponding testing solutions for 1-h ahead (1-h ahead corresponds to one-step ahead in hourly measured data) wind speed prediction using w2011 data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7091914/zio3-2418739-large.gif
2016,7091914,Fig. 4.,Estimated PIs by ELM (gray solid lines) and by MOGA-NN (red dotted lines) for 1-h ahead wind speed prediction on the testing set of summer data set (black solid lines).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7514298/7091914/zio4-2418739-large.gif
2016,7091929,Fig. 1.,"Comparison of the RVMS family, SBKS, and ELM models for different sample size (without censoring). (a) MAE. (b) Sparsity. (c) Run-time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7407441/7091929/sheik1abc-2420611-large.gif
2016,7091929,Fig. 2.,"Comparison of the RVMS family, SBKS, and ELM models for different percentage of censoring. (a) MAE. (b) Sparsity. (c) Run-time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7407441/7091929/sheik2-2420611-large.gif
2016,7091929,Fig. 3.,Comparison of the RVMS and the PH model. Kaplan–Meier curves are shown for two group of patients according to whether their prognostic index is below (dashed line) or above (solid line) the median value.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7407441/7091929/sheik3-2420611-large.gif
2016,7439828,Fig. 1.,"Person re-identification remains challenging due to the drastic cross-view variations caused by illumination, occlusion, pose, etc. The images in the first row are taken from realistic surveillance systems, and those in the second and third rows come from the VIPeR and CUHK-01 datasets, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai1-2545929-large.gif
2016,7439828,Fig. 2.,"Illustration of our proposed deep ranking framework, which comprises two key components: deep joint representation learning and learning to rank. We aim to learn a deep CNN that assigns a higher similarity score to the positive pair (marked in red) than any negative pairs (marked in blue) in each ranking unit. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai2-2545929-large.gif
2016,7439828,Fig. 3.,"Architecture of our deep network. A pair of three-channel pedestrian images is first stitched, and then a
227×227
random crop is presented as the input, which is convolved with 96 different first layer filters, each of size
11×11
, using a stride of 4 in both
x
and
y
. The resulting feature maps are then passed through a rectified linear unit (ReLU; not shown in this figure), max-pooled (
3×3
regions with stride 2), and contrast normalized across the feature maps to give 96 different
27×27
feature maps. Similar operations are repeated in the second to fifth layers. The last three layers are fully connected, taking features from the top convolutional layer as the input in vector form. Finally, a similarity score for the pair is returned.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai3-2545929-large.gif
2016,7439828,Fig. 4.,Illustration of ranking unit sampling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai4-2545929-large.gif
2016,7439828,Fig. 5.,Samples of pedestrian images observed in different camera views. Each column corresponds to the same identity. (a) Samples from the VIPeR dataset [9]. (b) Samples from the CUHK-01 dataset [17]. (c) Samples from the CAVIAR4REID dataset [32].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai5abc-2545929-large.gif
2016,7439828,Fig. 6.,"Performance comparisons with state-of-the-art approaches using CMC curves on the VIPeR (
p=316
), CUHK-01 (
p=486
) and CAVIAR4REID (
p=25
) datasets. In the legends, we also report the rank-1 matching rate for each approach. Best viewed in color. (a) VIPeR dataset. (b) CUHK-01 dataset. (c) CAVIAR4REID dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai6abc-2545929-large.gif
2016,7439828,Fig. 7.,"Comparison with two deep learning based methods: DML [29] and FPNN [30]. (a) Comparison with DML on VIPeR (
p=316
); (b) Comparison with FPNN on CUHK-01 (
p=100
). Note that the gallery contains only 100 subjects, which is different from that in Figure 6(b) and Table II.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai7ab-2545929-large.gif
2016,7439828,Fig. 8.,Open-world set-based verification performance comparisons with kLFDA using TTR-FTR curves. Best viewed in color.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai8abcd-2545929-large.gif
2016,7439828,Fig. 9.,"Comparison of cross-dataset performance. Note that, different from DTRSVM, the DML and our method do not use any sample from VIPeR. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai9-2545929-large.gif
2016,7439828,Fig. 10.,"Comparison of the ranking examples of kLFDA and our method. In each row, the left-most image is the probe, and the others are the top 15 matched gallery images of kLFDA and our method. The correct match of the probe is highlighted with a red bounding box. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai10-2545929-large.gif
2016,7439828,Fig. 11.,Comparison of ranking and direct binary classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai11-2545929-large.gif
2016,7439828,Fig. 12.,"Analysis of the contribution of pre-training (abbreviated as PT in the figure). These experiments were conducted on the CUHK-01 dataset with a 1:1 positive-negative ratio with or without pre-training. (a) Shows the CMC curves, and (b) and (c) show the loss of the training and test sets with and without pre-training, respectively. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai12abc-2545929-large.gif
2016,7439828,Fig. 13.,"Comparison with FPNN on the CUHK-01 dataset (
p=100
). Note that no pre-training is used here.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai13-2545929-large.gif
2016,7439828,Fig. 14.,"Comparisons of kLFDA performance with different training data on the VIPeR and CUHK-01 datasets.
S
1
denotes the original subset for training, and
S
2
denotes the data of P2-P5 from the CUHK-02 dataset. The CMC curves of our method that pre-trains the deep network with
S
2
are also presented for comparison.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai14ab-2545929-large.gif
2016,7439828,Fig. 15.,Analysis of ranking unit sampling. The CMC curves on the VIPeR dataset (a) and CUHK-01 dataset (b) with different positive-negative ratios in the ranking units. Best viewed in color.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7435413/7439828/lai15ab-2545929-large.gif
2016,7047752,Fig. 1.,Shadow area indicates the possible region the random variable may fall into.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu1-2399420-large.gif
2016,7047752,Fig. 2.,"Positions of the WiFi AP, offline calibration points, and online testing points in the simulated field.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu2-2399420-large.gif
2016,7047752,Fig. 3.,RSS index of distribution of four APs at one position.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu3-2399420-large.gif
2016,7047752,Fig. 4.,Cumulative percentile of error distance for simulation data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu4-2399420-large.gif
2016,7047752,Fig. 5.,System architecture of our WiFi-based IPS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu5-2399420-large.gif
2016,7047752,Fig. 7.,"Positions of the WiFi APs, offline calibration points, and online testing points in the test-bed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu7-2399420-large.gif
2016,7047752,Fig. 6.,Cumulative percentile of error distance for IPS testing results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7047752/lu6-2399420-large.gif
2016,7486113,Fig. 1.,Flowchart for sparse computation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb1-2576470-large.gif
2016,7486113,Fig. 2.,"Low-dimensional data set
X∈
R
583×11
: Comparison of exact PCA, approximate-PCA and random projections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb2-2576470-large.gif
2016,7486113,Fig. 3.,"High-dimensional data set
X∈
R
41,361×33,781
: Comparison of exact PCA, approximate-PCA and random projections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb3-2576470-large.gif
2016,7486113,Fig. 4.,"Grid blocks in a
p=3
-dimensional space. Here the grid resolution
k
is set to 3. The yellow blocks are adjacent to the orange block.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb4-2576470-large.gif
2016,7486113,Fig. 5.,"Grid blocks in a
p=3
-dimensional space. Here the grid resolution
k
is set to 5. The yellow blocks are adjacent to the orange block.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb5-2576470-large.gif
2016,7486113,Fig. 6.,Partitioning of data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb6-2576470-large.gif
2016,7486113,Fig. 7.,Flowchart for the testing methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb7-2576470-large.gif
2016,7486113,Fig. 8.,"Impact of grid resolution on accuracy, density and tuning times.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb8-2576470-large.gif
2016,7486113,Fig. 9.,"Impact of grid resolution on accuracy, density and tuning times for the COV data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb9-2576470-large.gif
2016,7486113,Fig. 10.,"Impact of grid dimensionality on accuracy, F
1
-score, density and tuning times for the CAR data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb10-2576470-large.gif
2016,7486113,Fig. 11.,"Impact of the number of selected rows for approximate-PCA on accuracy, F
1
-score, density and tuning times for the SPL data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb11-2576470-large.gif
2016,7486113,Fig. 12.,Comparison with alternative sparsification approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb12-2576470-large.gif
2016,7486113,Fig. 13.,"Testing results for the data sets KDD, RLC and BOW2 using
δ
-identical grouping.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb13-2576470-large.gif
2016,7486113,Fig. 14.,Approximate-PCA versus random projections.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687317/7518705/7486113/hochb14-2576470-large.gif
2016,7051274,Fig. 1.,Illustration on image iris with different representations. Note that the left two sub-figures have different background.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu1abc-2403356-large.gif
2016,7051274,Fig. 2.,"Illustration on different types of sparsity patterns on separable sparse learning (upper row) and joint sparse learning (bottom row), respectively. Note that, the white box, the black box, and the red dot box, respectively, represent the sparse code, the dense code, and one group. (a)–(f) Codes of six samples (i.e., one column per sample), with each sample represented by four codes (i.e., four elements in one column). Moreover, the black box stands for the dense (or nonsparse) code while the white box stands for the sparse code. (a) Element-wise. (b) Group. (c) Joint element and group. (d) Row. (e) Block. (f) Row-block.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu2abcdef-2403356-large.gif
2016,7051274,Fig. 3.,"Illustration on the process of the hierarchical feature selection method by the proposed block-row regularizer. Note that, a net box and a white box, respectively, means a dense code and a sparse code. (a) View-selection. (b) Feature-selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu3ab-2403356-large.gif
2016,7051274,Algorithm 1:,Proposed Method for Solving (6),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu9-2403356-large.gif
2016,7051274,Fig. 4.,"Illustration on the comparison between the MVML learning and the single view multilabel learning on dataset MIRFlickr. Note that, the error bars shown in the curves represented standard deviation of ten iterations. (a) AP. (b) HL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu4ab-2403356-large.gif
2016,7051274,Fig. 5.,"Illustration on convergence rate of the proposed Algorithm 1 for solving the proposed objective function with fixed
λ
1
, i.e.,
λ
1
=1
. (a) MIRFlickr. (b) NUS-WIDE. (c) SCENE. (d) OBJECT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu5abcd-2403356-large.gif
2016,7051274,Fig. 6.,"Illustration on convergence rate of the proposed Algorithm 1 for solving the proposed objective function with fixed
λ
2
, i.e.,
λ
2
=1
. (a) MIRFlickr. (b) NUS-WIDE. (c) SCENE. (d) OBJECT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu6abcd-2403356-large.gif
2016,7051274,Fig. 7.,Results of AP (upper row) and HL (bottom row) with different parameters’ settings at different datasets. (a) and (e) MIRFlickr. (b) and (f) NUS-WIDE. (c) and (g) SCENE. (d) and (h) OBJECT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu7abcdefgh-2403356-large.gif
2016,7051274,Fig. 8.,Comparison on AP (left) and HL (right) on all algorithms at different datasets. Note that the error bars represent standard deviation of ten iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/7051274/zhu8ab-2403356-large.gif
2016,7482803,Fig. 1.,Receiver DSP architecture with the proposed MFI stage shown in red color.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7501626/7482803/khan1-2574800-large.gif
2016,7482803,Fig. 2.,"(a) Constellation diagrams and (b) amplitude histograms at two different OSNRs, for the three modulation formats types under consideration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7501626/7482803/khan2ab-2574800-large.gif
2016,7482803,Fig. 3.,Schematic diagram of a two hidden layers DNN. The second half of each autoencoder (called decoder) is depicted in grey color with dotted weight lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7501626/7482803/khan3-2574800-large.gif
2016,7482803,Fig. 4.,Experimental setup used for DNN-based MFI in digital coherent receivers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7501626/7482803/khan4-2574800-large.gif
2016,7482803,Fig. 5.,"Elements of DNN output vectors
v
for (a) PM QPSK, (b) PM 16-QAM, and (c) PM 64-QAM modulation formats in response to 60 bin-count vectors
x
in the testing data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7501626/7482803/khan5abc-2574800-large.gif
2016,7482803,Fig. 6.,Effect of fiber nonlinearity on the identification accuracy of proposed technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7501626/7482803/khan6-2574800-large.gif
2016,7404283,Fig. 1.,Setup for in-vivo and post-mortem acquisitions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi1-2527787-large.gif
2016,7404283,Fig. 2.,Setup for ex-vivo acquisitions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi2-2527787-large.gif
2016,7404283,Fig. 3.,Setup for data acquisition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi3-2527787-large.gif
2016,7404283,Fig. 4.,Illustration of an A-Scan signal for a cataractous lens through the ocular structures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi4-2527787-large.gif
2016,7404283,Fig. 5.,Procedure for automatic detection of ocular interfaces in pseudo-code.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi5-2527787-large.gif
2016,7404283,Fig. 6.,"Temporal features extraction from a lens echo signal. The signal in red indicates the presence of cataract. The labels
f
i
indicate features listed in Table II.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi6-2527787-large.gif
2016,7404283,Fig. 7.,Slice images of lenses with severe cataract. The arrows identify the indentation locations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi7-2527787-large.gif
2016,7404283,Fig. 8.,Wave propagation time in lenses with different cataract degrees (mean ± 1 SD).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi8-2527787-large.gif
2016,7404283,Fig. 9.,Attenuation coefficient in lenses with different cataract degrees (mean ± 1 SD).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi9-2527787-large.gif
2016,7404283,Fig. 10.,Frequency downshift in healthy and cataractous lenses (mean ± 1 SD).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi10-2527787-large.gif
2016,7404283,Fig. 11.,Correlation coefficient between the nucleus hardness and the extracted features in-vivo.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7593410/7404283/caixi11-2527787-large.gif
2016,7425158,Fig. 1.,"Three axial views, at
Z=60
mm,
Z=69
mm, and
Z=92
mm (MNI coordinates), of the WM atlas ICBM-DTI-81, showing a subset of the 50 ROIs (see Table II), overlaid on the MNI152 T1-weighted standard space. Bilateral ROIs are labeled on the right (“-R”) side only.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7505456/7425158/dicio1-2537808-large.gif
2016,7425158,Fig. 2.,"MD feature ranking for the best SVM classifier (linear kernel) using the squared modulus of ROI weights (in decreasing order) for both classification tasks: (a) patients with TMT-B ES 0 versus 1234 and (b) patients with TMT-B ES 01 versus 234. The squared modulus of ROI weights are expressed in percentage with respect to the total squared modulus of all ROIs of the corresponding SVM [35]. Squared moduli are averaged over all trained SVMs of the outer CV; standard deviations were negligible and were not displayed. Bars of highly ranked ROIs contributing to reach the 80% of the total squared modulus are depicted in dark grey, while the remaining bars are in light gray.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7505456/7425158/dicio2ab-2537808-large.gif
2016,7425158,Fig. 3.,"Anatomical localization (MNI 152 standard space) of each ROI is visible on the underlying anatomical reference for both classification tasks. Each ROI is color coded according to its importance, i.e., to the squared modulus of ROI weights, expressed in percentage with respect to the total squared modulus of all ROIs of the corresponding SVM [35]. For the sake of clarity, only ROIs that contribute to reach the 80% of the cumulative sum of squared modulus are represented (see Fig. 2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7505456/7425158/dicio3-2537808-large.gif
2016,7439855,Fig. 1.,Comparison of LDL and typical existing learning methods with numerical label indicators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7484370/7439855/geng1-2545658-large.gif
2016,7439855,Fig. 2.,"Example label distributions for single-label annotation, multi-label annotation, and the general case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7484370/7439855/geng2-2545658-large.gif
2016,7439855,Fig. 3.,Evaluation measure selection from the dendrogram (modified from [10]) for the distribution distance/similarity measures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7484370/7439855/geng3-2545658-large.gif
2016,7439855,Fig. 4.,Comparison between the ground-truth and predicted label distributions (regarded as RGB colors) on the artificial test manifold.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7484370/7439855/geng4-2545658-large.gif
2016,7372483,Fig. 1.,"The framework of our relationship induced multi-template learning (RIML) method, which consists of three main steps: 1) multi-template feature extraction, 2) relationship induced sparse feature selection, and 3) ensemble classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-1-source-large.gif
2016,7372483,Fig. 2.,Ten templates determined by the affinity propagation (AP) clustering algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-2-source-large.gif
2016,7372483,Fig. 3.,"Illustration of structural information, conveyed by (a) relationship between features of two templates (i.e., features of the
n
-th subject in the
k
1
-th and the
k
2
-th template spaces, respectively), and (b) relationship between features of two subjects in the same template (i.e., features of the
n
1
-th subject and the
n
2
-th subject in the
k
-th template space). Here, yellow denotes positive training subjects, while blue denotes negative training subjects. Different shapes (circle, triangle, and square) denote samples in three different template spaces (i.e., the
k
1
-th template, the
k
2
-th template, and the
k
-th template).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-3-source-large.gif
2016,7372483,Fig. 4.,"Distributions of classification accuracy (ACC), sensitivity (sen) and specificity (SPE) achieved by four different single-template based methods in (a) AD vs. NC classification, and (b) PMCI vs. SMCI classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-4-source-large.gif
2016,7372483,Fig. 5.,"ROC curves achieved by five ensemble-based methods using multiple templates in (a) AD vs. NC classification, and (b) PMCI vs. SMCI classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-5-source-large.gif
2016,7372483,Fig. 6.,"Accuracies of AD vs. NC classification with respect to different parameter values in the proposed RIS model. Note that, in (a)-(c), when two parameters vary, another parameter is fixed as 0.1, for convenience of display. (a)
λ
1
=0.1
. (b)
λ
2
=0.1
. (c)
λ
3
=0.1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-6-source-large.gif
2016,7372483,Fig. 7.,The diversities and mean classification errors achieved by five ensemble-based methods in four classification tasks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7480876/7372483/7372483-fig-7-source-large.gif
2016,7229360,Fig. 1.,"Architecture of our pooled features. In this example, sp-Cov are extracted from each fixed sized pooling region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen1-2474388-large.gif
2016,7229360,Fig. 2.,ROC curves of our sp-Cov features and the conventional covariance detector [3] on INRIA test images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen2-2474388-large.gif
2016,7229360,Fig. 3.,"Decision boundaries on the toy data set where each strong classifier consists of
10
weak classifiers (horizontal and vertical decision stumps). Positive and negative data are represented by
∘
and
×
, respectively. The partial AUC score in the FPR range
[0,0.2]
is also displayed. Our approach achieves the best pAUC score compared to other asymmetric classifiers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen3-2474388-large.gif
2016,7229360,Fig. 4.,"Precision-recall curves of our approach and state-of-the-art detectors (DA-PDM [60], LSVM-MDPM-sv [61], LSVM-MDPM-us [13] and mBoW [62]) on the KITTI pedestrian detection test set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen4-2474388-large.gif
2016,7229360,Fig. 5.,"ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8] , MT-DPM+Context [63], MT-DPM [63] , MultiResC+2Ped [64], ACF-Caltech [37], MOCO [65], MF+Motion+2Ped [64] , DBN-Mut [14], Roerei [34], MultiResC [66], MultiFtr+Motion [38] , ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen5-2474388-large.gif
2016,7229360,Fig. 6.,"Left: The spatial distribution of features selected by pAUCEns
T
. White pixels indicate that a large number of low-level visual features are selected in that area. These regions correspond to human head, shoulders and feet. Right: The learned linear SVM model from the BING classifier. Each pixel shows the SVM weight. Note the similarity between the learned SVM weights and SVM weights of HOG (Fig. 6b in [2]), i.e., large SVM weights are near the head and shoulder contour (
∧
-shape).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen6-2474388-large.gif
2016,7229360,Fig. 7.,"The change in the detection performance as we vary the threshold value of the BING detector (evaluated on the Caltech pedestrian test set). BING(thresh =
0
) represents the proposed two-stage pedestrian detector, in which the first stage is the BING classifier with the threshold value of zero and and the second stage is the pAUCEns
T
detector described in Section 3.3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7464929/7229360/shen7-2474388-large.gif
2016,7348689,Fig. 1.,"Comparison of (a) an entangled nonlinear manifold, (b) a flat linear manifold, and (c) a slightly disentangled manifold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu1abc-2496947-large.gif
2016,7348689,Fig. 2.,Geodesic and Euclidean distances for (a) entangled and (b) slightly disentangled manifolds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu2ab-2496947-large.gif
2016,7348689,Fig. 3.,Alignment of tangent spaces along a geodesic path.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu3-2496947-large.gif
2016,7348689,Fig. 4.,Calculating curvature from changing tangent space along the surface of a manifold.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu4-2496947-large.gif
2016,7348689,Fig. 5.,Two swiss rolls entangled within each other.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu5-2496947-large.gif
2016,7348689,Fig. 6.,"Geodesic path (in darker dots) between
S
and
D
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu6-2496947-large.gif
2016,7348689,Fig. 7.,Data set of two classes where the one in the shape of a dollar ($) contains a mixture of manifolds in itself.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu7-2496947-large.gif
2016,7348689,Fig. 8.,Images from the ISOMAP face data set with respect to the pose variables in the first two rows and lighting angle in the last row.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu8-2496947-large.gif
2016,7348689,Fig. 9.,Digit 7 (a) without a middle stick and (b) with a middle stick.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu9ab-2496947-large.gif
2016,7348689,Fig. 10.,"d
and
c
for whole class and a subclass of digit 7.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu10-2496947-large.gif
2016,7348689,Fig. 11.,"d
and
c
across the layers for the manifold of digit 4 before and after supervised backpropagation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7348689/wu11-2496947-large.gif
2016,7405265,FIGURE 1.,Cyber-physical infrastructure for a Smart City.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch1-2529562-large.gif
2016,7405265,FIGURE 2.,"(Left):
x
-coordinate signature of an anomalous (actionable) bump. (Right):
x
-coordinate signature of a flat casting (non-actionable).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch2ab-2529562-large.gif
2016,7405265,FIGURE 3.,"Top two figures: Pothole (actionable) signature and associated
Δ
-filtered signature with fitted sinusoid. Bottom two figures: Flat Casting (Non-actionable) signature and associated
Δ
-filtered signature with fitted sinusoid.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch3-2529562-large.gif
2016,7405265,FIGURE 4.,Sinusoid function fitting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch4-2529562-large.gif
2016,7405265,FIGURE 5.,"Amplitude partition of
Δ
-filtered signatures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch5-2529562-large.gif
2016,7405265,FIGURE 6.,ROC curves for the classification methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch6-2529562-large.gif
2016,7405265,FIGURE 7.,"Parameter
λ
selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch7-2529562-large.gif
2016,7405265,FIGURE 8.,"Normalized anomaly index values (
λ=0.5
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch8-2529562-large.gif
2016,7405265,FIGURE 9.,"Bump list in descending AI order with weight
λ=0.5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7405265/pasch9-2529562-large.gif
2016,7403893,Fig. 1.,"Schematic diagram of HL-ELM. The framework consists of input layer, hierarchical convolution and pooling layer, and output layer. The connections between the last pooling layer and the output layer are the ELM output weights.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7419207/7403893/lv1-2517178-large.gif
2016,7403893,Fig. 2.,Classification maps for the Indian Pines image. (a) Ground truth. (b) SVM. (c) ELM. (d) CSVM. (e) CELM. (f) HL-ELM. (g) CNN. (h) SADL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7419207/7403893/lv2abcdefgh-2517178-large.gif
2016,7403893,Fig. 3.,"Impacts on OA of HL-ELM by different factors. (a) Effect of tradeoff parameter
λ
on two data sets. (b) Effect of LRF size on Indian Pines data set. Note that
r
1
and
r
2
represent the LRF size of convolution layers C1 and C2, respectively. (c) Effect of training data size on Indian Pines data set. (d) Effect of depth (number of convolution layers) on two data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7419207/7403893/lv3abcd-2517178-large.gif
2016,7000992,Fig. 1.,Anterior segment sketch of an eye with narrow angle between iris and cornea. Inner figure representing the parameters that are used for quantifying the ACA; AOD; TISA; TIA; ARA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7369993/7000992/lin1-2387207-large.gif
2016,7000992,Fig. 2.,"AS-OCT image of an eye with the (a) iris roll mechanism, (b) exaggerated lens vault mechanism, (c) pupil block mechanism, (d) plateau iris mechanism.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7369993/7000992/lin2abcd-2387207-large.gif
2016,7000992,Fig. 3.,Block diagram of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7369993/7000992/lin3-2387207-large.gif
2016,7000992,Fig. 4.,Pseudocode of the AdaBoost algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7369993/7000992/lin4-2387207-large.gif
2016,7000992,Fig. 5.,"Comparison graph between the AdaBoost-L-score and MRMR algorithms (a) Accuracy, (b) F-measure, (c) Sensitivity, (d) Specificity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7369993/7000992/lin5abcd-2387207-large.gif
2016,7000992,Fig. 6.,Example of correlation between two features: TISA_R750 Vs ARA_R750.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7369993/7000992/lin6-2387207-large.gif
2016,7222438,Fig. 1.,"Representation of the relation and mapping between input space, feature space, and EFS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez1-2461436-large.gif
2016,7222438,Fig. 2.,"Synthethic 2-D data set representing a nonlinearly separable classification problem and their transformation to the two dominant dimensions of the EFS
E
(2)
induced by the Gaussian kernel function (linearly separable problem).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez2-2461436-large.gif
2016,7222438,Fig. 3.,Different steps for the kernel oversampling algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez3-2461436-large.gif
2016,7222438,Fig. 4.,Synthetic 2-D data sets representing nonlinearly separable classification problems and their transformation to the two dominant dimensions of the EFS induced by the Gaussian kernel function (linearly separable problem).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez4-2461436-large.gif
2016,7222438,Fig. 5.,"EFSs for the cleveland0vs4 data set associated with the original data, oversampling for different
β
values, and optimized oversampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez5-2461436-large.gif
2016,7222438,Fig. 6.,"EFSs for the led7digit02456789vs1 data set associated with the original data, oversampling for different
β
values, and optimized oversampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez6-2461436-large.gif
2016,7222438,Fig. 7.,"Histogram of the mean optimal dimensionality of the EFS for all data sets. The abscissa axis represents the mean value, over the 30 results, for the rate of the rank of the kernel matrix. The ordinate axis shows the number of data sets where this value was selected from the cross validation step.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez7-2461436-large.gif
2016,7222438,Fig. 8.,Histogram of distances between pair of patterns for different dimensionality values of the EFS. The abscissa axis represents the distance between two patterns and the ordinate axis the occurrence of each distance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez8-2461436-large.gif
2016,7222438,Fig. 9.,"Histogram of the mean values for the beta parameter used in the oversampling process. The
x
coordinate represents the different mean
β
values chosen for each data set (related to the preferential oversampling) and
y
represents the number of data sets where the value was selected from the cross validation process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7543555/7222438/perez9-2461436-large.gif
2016,7358117,Fig. 1.,"PPR
r
i
(left), reinforced probability
p(
x
i
|
θ
old
)+
r
i
(middle), and instance weight
w
i
(right) in terms of the probability
p(
x
i
|
θ
old
)
.
L1
regularization of PPRs with reinforcement meta-parameter
α=
10
3
(plain line) and
L2
regularization of PPRs with reinforcement meta-parameter
α=
10
6
(dashed line).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7358117/frena1abc-2504404-large.gif
2016,7358117,Fig. 2.,"Validation MSE to select the optimal number of neurons
m
of a standard ELM (left) and the optimal regularization constant
γ
of a regularized ELM (right) for the (a) and (b) nitrogen, (c) and (d) auto-MPG, and (e) and (f) space-GA datasets. Curves correspond to a single repetition, i.e., one meta-parameter selection. Plain lines correspond to results obtained with 0% of outliers and dashed lines correspond to results obtained with 5% of outliers. Notice the logarithmic scales.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7744634/7358117/frena2abcdef-2504404-large.gif
2016,7347438,Fig. 1.,"A cxr corresponding to a TB case with a lesion outlined in the right lung (left), and the pixel classifications produced by the MIL-based approach in [6] (middle) and its supervised equivalent (right). Notice the inaccurate detections by the former compared with those by the latter (warm colors indicate high abnormality).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-1-source-large.gif
2016,7347438,Fig. 2.,Flowchart showing the steps followed to train an improved MIL classifier by means of AL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-2-source-large.gif
2016,7347438,Fig. 3.,"Comparison of mean shift and simple thresholding for grouping the training instances into regions. From left to right: a CXR of a TB subject and its annotated lesions, the training instances targeted for relabeling (shown in red; the remaining instances are shown in green), the regions obtained after clustering those instances with mean shift and computing the convex hull, and the regions obtained by thresholding the pixel scores. Note the better lesion localization enabled by mean shift.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-3-source-large.gif
2016,7347438,Fig. 4.,Pixel-level ROC curves yielded by the approaches evaluated in the first set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-4-source-large.gif
2016,7347438,Fig. 5.,Case-level ROC curves yielded by the approaches evaluated in the first set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-5-source-large.gif
2016,7347438,Fig. 6.,PR curves yielded by the approaches evaluated in the first set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-6-source-large.gif
2016,7347438,Fig. 7.,Pixel-level ROC curves yielded by the approaches evaluated in the second set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-7-source-large.gif
2016,7347438,Fig. 8.,Case-level ROC curves yielded by the approaches evaluated in the second set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-8-source-large.gif
2016,7347438,Fig. 9.,PR curves yielded by the approaches evaluated in the second set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-9-source-large.gif
2016,7347438,Fig. 10.,"Examples of CXRs (first column) and the heat maps produced by the approaches evaluated in the first set of experiments (second to fifth columns). The top three examples correspond to TB cases (their lesions are outlined in red), whereas the bottom three examples correspond to normal cases. On the heat maps, warm colors indicate abnormality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-10-source-large.gif
2016,7347438,Fig. 11.,"Examples of CXRs (first column) and the heat maps produced by the baseline approaches evaluated in the second set of experiments (second to fifth columns). The top three examples correspond to TB cases (their lesions are outlined in red), whereas the bottom three examples correspond to normal cases. On the heat maps, warm colors indicate abnormality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-11-source-large.gif
2016,7347438,Fig. 12.,PR curves yielded by the approaches evaluated in the third set of experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7347438/7347438-fig-12-source-large.gif
2016,7308086,Fig. 1.,"Here, we illustrate the use of multiple ranking functions in the classification of an unseen sample
x
. Attribute-class associations are predefined according to human subjects’ attribute preferences (whiteness and brownness on a scale from 1 to 4) for a set of animal classes. Our multiple ranking SVM is used to learn the function
f(.)
from a training set to estimate the multiple attribute rankings from samples. The attribute associations of the test sample
x
are estimated with the multiple ranking functions, i.e.,
f
1
(x)
and
f
2
(x)
. These rankings are used to assign the sample to one of the unseen testing classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7308086/hamsi1-2477321-large.gif
2016,7308086,Fig. 2.,"Different learning strategies result in distinct rankings. (a) Fixed-margin strategy, where the margin is defined by the closest classes. In this case, the goal is to find the two hyperplanes with the common direction vector
w
and biases
b
1
and
b
2
that maximize the minimum margin. (b) Our sum of margins strategy extracts the direction
w
that maximizes the sums of margins
∑
2
i=1
(
b
i
−
a
i
)
with the four hyperplane biases of
a
1
,
b
1
,
a
2
, and
b
2
. Different colors correspond to classification regions defined by the ranking rule. The solution of fixed-margin strategy is biased by the closest class pairs of 1 and 2, and the sum of margin strategy obtains a larger margin by maximizing the sum of margins.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7308086/hamsi2ab-2477321-large.gif
2016,7308086,Fig. 3.,"Computational times required to solve the quadratic optimization problem in (13) for the three-class ranking problem shown in Fig. 2. Algorithm 1 is compared with a standard MATLAB QP solver with
{27,54,…,270}
training samples in each of the tenfold cross-validation experiments. Average and standard deviations of the computational times are plotted.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7308086/hamsi3-2477321-large.gif
2016,7308086,Fig. 4.,"Here, we illustrate the set of ranking problems that are used to compare the fixed-margin strategy with MSRSVM. Classes are sampled from three Gaussian distributions of
N(
μ
i
,4I)
, where
μ
1
=(−15,0
)
T
,
μ
2
=(0,0
)
T
, and the covariance matrices with scaled identity matrices of
4I
. Class 3 is simulated to be at multiple locations of
μ
3
=r(cos(θ),sin(θ)
)
T
, for
r={15,20,25,30}
and
θ={0,π/8,π/4}
. The circles correspond to Gaussian distributions, with
4I
as covariance matrices. Classes 1 and 2 are fixed and represented with circles of solid lines. Class 3 is set to each of the locations
(r,θ)
, which are represented by dashed circles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7308086/hamsi4-2477321-large.gif
2016,7308086,Fig. 5.,Average confusion tables for (a) DAP and (b) DAP with MSRSVM approaches. These results are obtained by averaging ten runs over independent validation sets. The indices of the rows and columns correspond to the indices of the testing classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7308086/hamsi5ab-2477321-large.gif
2016,7308086,Fig. 6.,"Three sample images of the animals in each nonempty row and column of the multiple ranking table. Rows from left to right correspond to columnwise scan of the nonempty cells of Table VI, i.e., leopard, hippopotamus, chimpanzee, humpback whale, rat, pig, Persian cat, raccoon, and giant panda.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7569114/7308086/hamsi6-2477321-large.gif
2016,7348708,Fig. 1.,"Examples of segmented melanomas: (a) melanoma
<0.76 mm
, (b) melanoma
≥0.76 mm
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7348708/7348708-fig-1-source-large.gif
2016,7348708,Fig. 2.,Sample of the color palette used for color group attribution. The colored square at the beginning of the column corresponds to the false color attributed to the pixels belonging to that color group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7348708/7348708-fig-2-source-large.gif
2016,7348708,Fig. 3.,"Example of color area identification in the melanomas shown in Fig. 1. A)
<0.76 mm
, b)
≥0.76 mm
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7348708/7348708-fig-3-source-large.gif
2016,7348708,Fig. 4.,(a) Original image (b) Pigment network detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7445273/7348708/7348708-fig-4-source-large.gif
2016,7478097,Fig. 1.,"Top view of the two-joint, six-muscle biomechanical arm model. Y-axis is anterior. Movements occur in the sagittal plane with no gravity, as sliding across a frictionless tabletop. Antagonistic muscle pairs are listed as (flexor, extensor): monoarticular shoulder muscles (A: anterior deltoid, B: posterior deltoid); monoarticular elbow muscles (C: brachialis, D: triceps brachii (short head)); and biarticular muscles (E: biceps brachii; F: triceps brachii (long head)). φ1 and φ2 are shoulder and elbow joint angles, respectively. Moment arm values: d1 = 30 cm, d2 = 50 cm. Adapted from [24] and [25] .",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7567604/7478097/jagod1-2558630-large.gif
2016,7478097,Fig. 2.,Block diagram of the RL controller system using pseudo-human rewards.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7567604/7478097/jagod2-2558630-large.gif
2016,7478097,Fig. 3.,Performance metrics for controllers trained with various forms of reward on 500 episodes of a training task set containing ten unique tasks. Plotted points show the average over ten runs. (a) Dwell-at-target success percentages. (b) Mean time to achieve the dwell state. Error bars indicate averaged standard deviations over ten runs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7567604/7478097/jagod3ab-2558630-large.gif
2016,7478097,Fig. 4.,"Effect of reward spacing on actor-critic RL controller learning performing dynamic reaching movements using a planar biomechanical arm model. (a) Dwell-at-target success (%) measures how frequently the hand reaches the target zone and remains there continuously for at least 100 ms; presence in the target zone at the final timestep is also required in order to be counted as success. The model 21.34x−1.618 + 39.53 fits the data with R2 = 0.877, adjusted R2 = 0.8691, SSE = 409.2, and RMSE = 3.633. Note that the Y-axis ranges from 0% to 100%. (b) Net learning measures the final – initial dwell-at-target success percentages over each 500-episode training session. The model 30.42x−0.9315 + 11.12 fits the data with R2 = 0.7792, adjusted R2 = 0.7649, SSE = 1465, and RMSE = 6.875.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7567604/7478097/jagod4ab-2558630-large.gif
2016,7478097,Fig. 5.,"Learning metrics as a function of the number of unique tasks in each 500-episode training set. Error bars show standard deviations over five runs per condition. (a) Dwell-at-target success (%) measures how often the hand was able to reach the target zone and dwell for at least 100 ms, and presence in the target zone at the final timestep was also required to be counted as success. (b) Net learning is defined as the final–initial dwell-at-target success percentages, and measures how much learning is achieved over each 500-episode training set. (c) Dwell change (in ms) is the difference between the time required for the final 100 episodes of each training set to reach the target, minus the time required for the initial 100 episodes of each training set to reach the target.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7567604/7478097/jagod5ab-2558630-large.gif
2016,7478097,Fig. 6.,Long-term performance of RL controllers trained using automated rewards or final-timestep pseudo-human rewards. Mean values are calculated over each set of 100 episodes. Error bars show mean standard deviations over ten runs. (a) Dwell-at-target success percentage. (b) Mean time required to achieve the dwell state.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/7567604/7478097/jagod6ab-2558630-large.gif
2016,7314894,Fig. 1.,"Reconstructed image patches by single AE (b) and SAE (e). The bright and dark colors indicate large and small reconstruction errors, respectively. (a) Training image patches, (b) Reconstructed image patches by single AE, (c) The difference between input and reconstructed image patches by AE, (d) The learned basis feature representions by AE, (e) Reconstructed image patches by AE, (f) The difference between input and reconstructed image patches by SAE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu1abcdef-2496253-large.gif
2016,7314894,Fig. 2.,Hierarchical architecture of SAE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu2-2496253-large.gif
2016,7314894,Fig. 3.,"3×3
max pooling procedure in convolutional network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu3-2496253-large.gif
2016,7314894,Fig. 4.,Importance map and the sampled image patches (denoted by the red dots) for deep learning. The color bar indicates the varying importance values for individual voxels. (a) Importance map. (b) Sampled points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu4ab-2496253-large.gif
2016,7314894,Fig. 5.,"Similarity maps of identifying the correspondence for the red-crossed point in the template (a) w.r.t. the subject (b) by handcraft features (d), (e) and the learned features by unsupervised deep learning (f). The registered subject image is shown in (c). It is clear that the inaccurate registration results might undermine the supervised feature representation learning that highly relies on the correspondences across all training images. (a) Template. (b) Subject. (c) Deformed subject. (d) By local patches. (e) By SIFT. (f) By unsupervised learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu5abcdef-2496253-large.gif
2016,7314894,Fig. 6.,Dice ratios of 56 ROIs on LONI dataset by six registration methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu6-2496253-large.gif
2016,7314894,Fig. 7.,Large structural difference around hippocampus between 1.5-T (a) and 7.0-T (b) MR images. The 1.5-T image is enlarged w.r.t. the image resolution of the 7.0-T image for convenience of visual comparison..,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu7ab-2496253-large.gif
2016,7314894,Fig. 8.,"Typical registration results on 7.0-T MR brain images by Demons, HAMMER, and H + DP. Three rows represent three different slices in the template, subject, and registered subjects. (a) Template. (b) Subject. (c) By Demons. (d) By HAMMER. (e) By H + DP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu8abcde-2496253-large.gif
2016,7314894,Fig. 9.,"Dice ratios of 56 ROIs in LONI dataset by HAMMER (blue), H + DP-LONI (red), and H + DP-ADNI (green), respectively. Note, H + DP-LONI denotes for the HAMMER registration integrating with the feature representations learned directly from LONI dataset, while H + DP-ADNI stands for applying HAMMER registration on LONI dataset but using the feature representations learned from ADNI dataset, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7493713/7314894/wu9-2496253-large.gif
2016,7439740,Fig. 1.,Machine Learning Framework to learn association between rock image features and XRF signatures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma1-2546241-large.gif
2016,7439740,Fig. 2.,Image processing steps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma2-2546241-large.gif
2016,7439740,Fig. 3.,The process of computing edge strength and orientation histogram from an image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma3-2546241-large.gif
2016,7439740,Fig. 4.,"The underground wall used for rock samples, with the indexed grid cells shown. The variations in cell geometry are partly due to the unevenness of the wall and the difficulty in manually positioning the markers. Each cell is 25 cm
×25
cm. All mineral samples and measurements were collected from wholly within each cell.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma4-2546241-large.gif
2016,7439740,Fig. 5.,"The XRF and imaging device: (a) X-ray Tube, (b) X-ray detector, and (c) A006 auto-focus digital microscope.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma5abc-2546241-large.gif
2016,7439740,Fig. 6.,Representative images of rock samples from mine surface. Some rock samples (at empty regions) were unsuitable because the samples were very small or powdery.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma6-2546241-large.gif
2016,7439740,Fig. 7.,Normalised XRF spectrum obtained from the rock sample from grid location A3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma7-2546241-large.gif
2016,7439740,Fig. 8.,"Representative color, texture, and edge orientation features from a sample image. (a) Color histogram. (b) Texture features. (c) Edge orientation histogram.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/7462310/7439740/rahma8abc-2546241-large.gif
2016,7448427,Fig. 1.,DG5 VHand glove device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7484223/7448427/pawia1-2550528-large.gif
2016,7448427,Fig. 2.,Flow chart of the algorithm developed for the purpose of gesture analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7484223/7448427/pawia2-2550528-large.gif
2016,7448427,Fig. 3.,Diagrams of cross-validations types.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7484223/7448427/pawia3-2550528-large.gif
2016,7448427,Fig. 4.,"Subsequent stages of signal processing for a sample recording: A—Raw data, B—Interpolation and resampling, C—PCA, D—Merging, E—Removing, F—Normalization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7484223/7448427/pawia4abcdef-2550528-large.gif
2016,7448427,Fig. 5.,Preprocessed data for all 22 gestures. Individual persons are shown in a different colors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7484223/7448427/pawia5-2550528-large.gif
2016,7448427,Fig. 6.,"Visualization of the
ERR(S)
for each class (22 gestures), presented for the best classifier SVM (nu-SVC), resampling equal to 20, with PCA analysis and 8 components left. The X-axis shows the gesture number.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7484223/7448427/pawia6-2550528-large.gif
2016,7587382,FIGURE 1.,Architecture of the proposed system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen1-2615520-large.gif
2016,7587382,FIGURE 2.,Foreign fiber image acquisition system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen2-2615520-large.gif
2016,7587382,FIGURE 3.,Flowchart of foreign fiber images acquisition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen3-2615520-large.gif
2016,7587382,FIGURE 4.,Sample of foreign fiber images. (a) hair (b) black plastic film (d) red cloth (d) hemp rope (e) red polypropylene (f) black feather.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen4abcdef-2615520-large.gif
2016,7587382,FIGURE 5.,Local Cuts of Sample images. (a) hair (b) black plastic film (d) red cloth (d) hemp rope (e) red polypropylene (f) black feather.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen5abcdef-2615520-large.gif
2016,7587382,FIGURE 6.,Original images. (a) plastic film (b) hair (c) hemp rope.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen6abc-2615520-large.gif
2016,7587382,FIGURE 7.,Results of image segmentation. (a) plastic film (b) hair (c) hemp rope.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen7abc-2615520-large.gif
2016,7587382,FIGURE 8.,"Training accuracy surface of KELM by two-step grid search. (a) coarse search in fold 1, (b) fine search in fold 1, (c) coarse search in fold 3, (d) fine search in fold 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen8abcd-2615520-large.gif
2016,7587382,FIGURE 9.,Relationship between the accuracy and number of hidden neurons for ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen9-2615520-large.gif
2016,7587382,FIGURE 10.,"Classification accuracy of KELM, ELM and SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen10-2615520-large.gif
2016,7587382,FIGURE 11.,Comparison of average accuracy of eight algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen11-2615520-large.gif
2016,7587382,FIGURE 12.,Curves of average accuracy of top-k feature set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7587382/chen12-2615520-large.gif
2016,7229282,Fig. 1.,Overview of the proposed approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye1-2474392-large.gif
2016,7229282,Fig. 2.,Overall flow of voltage droop-aware static timing analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye2-2474392-large.gif
2016,7229282,Fig. 3.,Illustration of an SVM regression model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye3-2474392-large.gif
2016,7229282,Fig. 4.,Block diagram of a decision function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye4-2474392-large.gif
2016,7229282,Fig. 5.,Block diagrams: linear kernel and second-degree polynomial kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye5-2474392-large.gif
2016,7229282,Fig. 6.,Illustration of correlation-based feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye6-2474392-large.gif
2016,7229282,Fig. 7.,Illustration of DFS-based mitigation method in the proposed droop-prediction framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye7-2474392-large.gif
2016,7229282,Fig. 8.,Results of over-prediction and under-prediction obtained from SVM for ITC’99 and International Work on Logic and Synthesis (IWLS)’05 benchmarks. (a) b18. (b) b19. (c) b22. (d) Reduced-instruction set computer (RISC). (e) OR1200. (f) Video graphics array (VGA).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye8abcdef-2474392-large.gif
2016,7229282,Fig. 9.,"Comparison of prediction error obtained from SVM, minMetric, maxMetric, and meanMetric with different sizes of the training set for ITC’99 and IWLS’05 benchmarks. (a) b18. (b) b19. (c) b22. (d) RISC. (e) OR1200. (f) VGA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye9abcdef-2474392-large.gif
2016,7229282,Fig. 10.,Prediction error obtained from SVM with different sizes of the training set for ITC’99 and IWLS’05 benchmarks. (a) b18. (b) b19. (c) b22. (d) RISC. (e) OR1200. (f) VGA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye10abcdef-2474392-large.gif
2016,7229282,Fig. 11.,Comparison of prediction error obtained for SVM with different lengths of the input sequence for ITC’99 and IWLS’05 benchmarks. (a) b18. (b) b19. (c) b22. (d) RISC. (e) OR1200. (f) VGA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye11abcdef-2474392-large.gif
2016,7229282,Fig. 12.,"Tradeoff between prediction accuracy, hardware overhead, and circuit response time for ITC’99 and IWLS’05 benchmarks (part 1). (a) b18. (b) b19. (c) b22.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye12abc-2474392-large.gif
2016,7229282,Fig. 13.,"Tradeoff between prediction accuracy, hardware overhead, and circuit response time for ITC’99 and IWLS’05 benchmarks (part 2). (a) RISC. (b) OR1200. (c) VGA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye13abc-2474392-large.gif
2016,7229282,Fig. 14.,Margin reduction achieved using support-vector regression (system run time = 3 days).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/7435365/7229282/ye14-2474392-large.gif
2016,7442887,Fig. 1.,"Restricted transferable dictionary learning versus relaxed transferable dictionary learning. (a) Restricted transferable dictionary learning: We learn two view-specific dictionaries
D
1
and
D
2
corresponding to the training and test views respectively. A pair of correspondence videos is denoted as
y
1
i
and
y
2
i
. The sparse codes of
y
1
i
and
y
2
i
when reconstructed from the corresponding view-specific dictionaries are the same. (b) Relaxed transferable dictionary learning: We jointly learn two view-specific dictionaries
D
1
and
D
2
and a common dictionary
D
. Each video from each view is represented by both
D
and the corresponding view-specific dictionary. The sparse representations of
y
1
i
and
y
2
i
share the same sparsity patterns (selecting the same set of dictionary atoms for reconstruction) instead of being the same.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng1ab-2548242-large.gif
2016,7442887,Fig. 2.,"An example of the ideal sparse codes
q
i
. Given a video
y
v
i
of class 2 from the
i
-th set of correspondence videos in the
v
-th view, we reconstruct it from the dictionary
D
v
=[
d
v
1
,
d
v
2
,
d
v
3
,
d
v
4
,
d
v
5
,
d
v
6
,
d
v
7
]
, where
d
v
1
,
d
v
2
,
d
v
3
are from class 1,
d
v
4
,
d
v
5
are from class 2 and
d
v
6
,
d
v
7
are from class 3. The vector
x
i
is the true sparse codes shared by the
i
-th set of correspondence videos, whereas
q
i
is the ideal sparse codes. Since
y
v
i
is from class 2, we would like to only select dictionary atoms from class 2 for reconstruction. Thus, the forth and fifth entries in
q
i
are ones, and other entries are zeros. The matrix
A
transforms the sparse codes
x
i
to approximate
q
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng2-2548242-large.gif
2016,7442887,Algorithm 1,Restricted Transferable Dictionary Learning,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng9-2548242-large.gif
2016,7442887,Algorithm 2,Relaxed Transferable Dictionary Learning,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng10-2548242-large.gif
2016,7442887,Fig. 3.,Exemplar frames from the IXMAS multi-view dataset. Each row shows one action viewed across different angles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng3-2548242-large.gif
2016,7442887,Fig. 4.,"Illustration of the benefits of the common dictionary. (a) Visualization of all dictionary atoms in
D
(green color),
D
1
(red color) and
D
2
(purple color). (b) Images titled with“Original” are original images observed from different views. Images titled with “recon1” are the reconstructed images using
D
only, while images titled with “recon2” from different rows are reconstructed using
{D,
D
1
}
,
{D,
D
2
}
and
{D,
D
1
,
D
2
}
respectively. For all the reconstructed images, only three most weighted dictionary atoms are shown. The numbers below each image denotes the weight of the dictionary atom used for reconstruction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng4ab-2548242-large.gif
2016,7442887,Fig. 5.,Performance on the IXMAS action dataset with varying dictionary size. (a) Camera0 v.s. Camera4. (b) Camera2 v.s. Camera3. (c) Camera0 v.s. Camera1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng5abc-2548242-large.gif
2016,7442887,Fig. 6.,Exemplar frames from the WVU action dataset. Each row shows one action viewed across different angles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng6-2548242-large.gif
2016,7442887,Fig. 7.,Performance on the WVU action dataset with varying dictionary size. (a) Camera0 v.s. Camera3. (b) Camera0 v.s. Camera6. (c) Camera2 v.s. Camera6.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng7abc-2548242-large.gif
2016,7442887,Fig. 8.,Exemplar frames from the MuHAVi action dataset. (a) Action class: CrawlOnKnees. (b) Action class: SmashObject.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7452455/7442887/zheng8ab-2548242-large.gif
2016,7378972,Fig. 1.,Pipeline of the establishment of our dataset with the help of a LIDAR equipped vehicle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan1-2506182-large.gif
2016,7378972,Fig. 2.,Distribution of number of snippets among categories and ranges with different image sizes (measured in image pixels). The category ID is enumerated in Fig. 3 from left to right and top to bottom. The red line shows the average.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan2ab-2506182-large.gif
2016,7378972,Fig. 3.,The sign templates of 112 categories for our dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan3-2506182-large.gif
2016,7378972,Fig. 4.,"Some normalized samples in our dataset. Each is a set of multi-view snippets. In each sample, many snippets' quality is poor, but there is at least one snippet easy to recognize.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan4-2506182-large.gif
2016,7378972,Fig. 5.,"Pipeline of our multi-view sign recognition via WSMLR. It iteratively learns a distance metric and a reliability classifier. The feature extraction is to extract the reliability feature. The positive/negative samples are the images correctly/incorrectly recognized. During testing, with the learned metric and reliability classifier, we recognize each sign sample by soft voting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan5-2506182-large.gif
2016,7378972,Fig. 6.,Comparison of 1-NN classifier with different search schemes and metrics on SIGN112. (a) and (b) demonstrate the performance with increasing categories and decreasing training samples respectively.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan6ab-2506182-large.gif
2016,7378972,Fig. 7.,"Performance of WSMLR with different scaling factor
γ
and number of nearest templates
K
on SIGN112.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan7-2506182-large.gif
2016,7378972,Fig. 8.,Scalability of WSMLR. The scalability to unseen categories is shown by the comparison of different methods on the subsets of SIGN112 with increasing categories (Fig. 8(b)). (a) Scalability to increasing categories. (b) Scalability to unseen categories.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan8ab-2506182-large.gif
2016,7378972,Fig. 9.,Comparison of one-shot learning capability between WSMLR and SVM with decreased training samples on SIGN112. (a) Accuracy. (b) Accuracy gap to WSMLR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan9ab-2506182-large.gif
2016,7378972,Fig. 10.,The performance of WSMLR* on BTSC with different normalization and search schemes compared with other methods. spHOG feature is used.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7463085/7378972/tan10-2506182-large.gif
2016,7358076,Fig. 1.,Our quadrotor acquires the trail images from a forward-looking camera; a Deep Neural Network classifies the images to determine which action will keep the robot on the trail.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust1-2509024-large.gif
2016,7358076,Fig. 2.,"Three images from our dataset. Given an image, we aim to determine the approximate direction the trail is heading with respect to the viewing direction (respectively, left, right and straight ahead).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust2-2509024-large.gif
2016,7358076,Fig. 3.,"Left: given a point,
t
⃗ 
is the direction a hiker would walk in order to follow the trail. Right: illustration of
v
⃗ 
,
α
,
β
(see text).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust3-2509024-large.gif
2016,7358076,Fig. 4.,"Left: stylized top view of the acquisition setup; Right: our hiker during an acquisition, equipped with the three head-mounted cameras.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust4-2509024-large.gif
2016,7358076,Fig. 5.,"Architecture for the DNN [17] used in our system, and representation of the maps in each layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust5-2509024-large.gif
2016,7358076,Fig. 6.,Success and failure cases. More examples are reported in supplementary material [28].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust6abcd-2509024-large.gif
2016,7358076,Fig. 7.,"Three representative frames form the cellphone video robustness test. For each frame, we report the raw network outputs for that frame (bar graph), and the motion policy (arrow) derived from such outputs averaged over the previous 10 frames (see text). The rightmost frame is acquired when looking sideways, so the trail is not visible; the DNN is then rightfully confused among TR and TL, but returns a very small value for
P(GS)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust7-2509024-large.gif
2016,7358076,Fig. 8.,"Images from preliminary field testing. Left: the Parrot ARDrone controlled by a laptop. Center, right: the standalone quadrotor with SVO and onboard processing [34].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/7419970/7358076/giust8-2509024-large.gif
2016,7374716,Fig. 1.,"Convergence of LSPI on OSIE data set. Parameter weights while learning with LSPI, for each of the six fixations in their temporal order in the scanpath. We group the cues into four groups, and plot with thicker lines the total weight of the cues in the groups.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao1-2496306-large.gif
2016,7374716,Fig. 2.,Statistics of the reward during learning on OSIE data set. (a) Average and (b) distribution of the reward during learning with LSPI.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao2ab-2496306-large.gif
2016,7374716,Fig. 5.,"Evaluation of LSPI with different feature sets and parameters on OSIE data set.
LSPI
γ=0
is with a greedy policy,
LSPI
NoCenter
is taking the center-bias channel out, and
LSPI
NoSemantics
is taking the semantic features out. IO indicates the interobserver performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao5-2496306-large.gif
2016,7374716,Fig. 3.,"Learned parameters on OSIE data set. (a) Weights of the different cues for each of the six fixations in which we divided the visual exploration. (b) We group the low-level cues by Itti-Koch saliency into one group and all the semantics into another group, and show the total weight of the cues in each group of cues. Note that the cues of the image center and the prior of the eye fixation shifts are plotted separately.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao3ab-2496306-large.gif
2016,7374716,Fig. 4.,Learned parameters on MIT data set. Weights of the different cues for each of the six fixations in which we divided the visual exploration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao4-2496306-large.gif
2016,7374716,Fig. 6.,"Evaluation of LSPI and baseline models. The LSPI model is compared with the state-of-the-art LiuICCV [8], SGC [32], and the winner-take-all heuristic [18] from several saliency maps, such as support vector machine (SVM) [15], [16], graph-based visual saliency (GBVS) [20], image signature (ImSig) [23], adaptive whitening saliency (AWS) [54], saliency using natural statistics (SUN) [21], attention based on information maximization (AIM) [22], and Itti [18]. The SVM is in solid line, because it uses the same low- and semantic-level features and center bias as LSPI. IO indicates the interobserver performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao6-2496306-large.gif
2016,7374716,Fig. 7.,Qualitative evaluation of LSPI and baselines on OSIE data set. Human ground truth and predicted visual scanpaths of the LSPI and the state-of-the-art saliency models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao7-2496306-large.gif
2016,7374716,Fig. 8.,Qualitative evaluation of LSPI and baselines on MIT data set. Human ground truth and predicted visual scanpaths of the LSPI and the state-of-the-art saliency models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7470313/7374716/zhao8-2496306-large.gif
2016,7348723,Fig. 1.,Extraction of timed automata specification from dig_latch function block. (a) IEC 61131–3 ladder diagram. (b) Extracted timed automata specification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/7447676/7348723/7348723-fig-1-source-large.gif
2016,7348723,Fig. 2.,Dhc—creation of a cycle. (a) Newly explored state found to have an existing signature. (b) States 0 and 2 with equal signatures are merged.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/7447676/7348723/7348723-fig-2-source-large.gif
2016,7348723,Fig. 3.,Trimming the TDMM learned from dig_latch (cycles not shown). (a) Untrimmed TDMM. (b) Trimmed TDMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/7447676/7348723/7348723-fig-3-source-large.gif
2016,7348723,Fig. 4.,"Valve block with auto and manual modes and no feedback (cycles removed, states manually named).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/7447676/7348723/7348723-fig-4-source-large.gif
2016,7458837,Fig. 1.,Schema of congestion areas and RSUs at intersections.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher1-2546555-large.gif
2016,7458837,Fig. 2.,"Pseudocode of the proposed
K
-means algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher2-2546555-large.gif
2016,7458837,Fig. 3.,Flowchart of the proposed congestion control strategy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher3-2546555-large.gif
2016,7458837,Fig. 4.,Variation of the packet loss ratio with the number of vehicles for ML-CC(1) and ML-CC(2).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher4-2546555-large.gif
2016,7458837,Fig. 5.,Variation of the average delay with the number of vehicles for ML-CC(1) and ML-CC(2).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher5-2546555-large.gif
2016,7458837,Fig. 6.,Impact of the number of vehicles on average delay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher6-2546555-large.gif
2016,7458837,Fig. 7.,Impact of the number of vehicles on average throughput.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher7-2546555-large.gif
2016,7458837,Fig. 8.,Impact of the number of vehicles on the (a) number of packets lost and (b) packet loss ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher8ab-2546555-large.gif
2016,7458837,Fig. 9.,Variation of the average throughput with simulation time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher9-2546555-large.gif
2016,7458837,Fig. 10.,Variation of the average delay with simulation time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher10-2546555-large.gif
2016,7458837,Fig. 11.,Variations of the collision probability with the number of vehicles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher11-2546555-large.gif
2016,7458837,Fig. 12.,Variation of packet delivery ratio with vehicle density.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7725562/7458837/taher12-2546555-large.gif
2016,7542582,Fig. 1.,SCM CU hierachitical quadtree partitioning structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm1-2597698-large.gif
2016,7542582,Fig. 2.,CTU partition decision comparison between SCM-4.0 (top) and HEVC (bottom). “ Green” blocks are coded in PLT mode and “ Magenta” blocks are coded in HEVC-Intra mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm2-2597698-large.gif
2016,7542582,Fig. 3.,Sample frames from JCT-VC SCC sequences.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm3-2597698-large.gif
2016,7542582,Fig. 4.,Proposed ML-FSCC encoding workflowCVB1 will go through both NIB and SCB branches; CVB2 will go through both Partition and Non-Partition branches. Details are summarized in Table VI.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm4-2597698-large.gif
2016,7542582,Fig. 5.,CU32 NIB-SCB classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm5-2597698-large.gif
2016,7542582,Fig. 6.,CU16 NIB-SCB classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm6-2597698-large.gif
2016,7542582,Fig. 7.,CU8 NIB-SCB classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm7-2597698-large.gif
2016,7542582,Fig. 8.,CU64 NIB NP/P-block classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm8-2597698-large.gif
2016,7542582,Fig. 9.,CU32 NIB NP/P-block classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm9-2597698-large.gif
2016,7542582,Fig. 10.,CU16 NIB NP/P-block classification decision tree,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm10-2597698-large.gif
2016,7542582,Fig. 11.,CU64 directional/non-directional classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm11-2597698-large.gif
2016,7542582,Fig. 12.,CU32 directional/non-directional classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm12-2597698-large.gif
2016,7542582,Fig. 13.,CU16 directional/non-directional classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm13-2597698-large.gif
2016,7542582,Fig. 14.,CU8 directional/non-directional classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/7776720/7542582/duanm14-2597698-large.gif
2016,7574389,Fig. 1.,"The basic idea of the DTML method. For each sample in the training sets from the source domain and the target domain, we pass it to the developed deep neural network. We enforce two constraints on the outputs of all training samples at the top of the network: 1) the inter-class variations are maximized and the intra-class variations are minimized, and 2) the distribution divergence between the source domain and the target domain at the top layer of the network is minimized.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu1-2612827-large.gif
2016,7574389,Algorithm 1,DTML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu7-2612827-large.gif
2016,7574389,Fig. 2.,"The network architecture of the autoencoder used in our methods. The
x
is an data point in the input space,
h
(M)
is the resulting representation of the
x
in the metric space, and the
x
ˆ
=
h
(2M)
is the reconstruction of the point
x
in the output space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu2-2612827-large.gif
2016,7574389,Fig. 3.,The ROC curves of several methods on LFW dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu3-2612827-large.gif
2016,7574389,Fig. 4.,Handwritten digit images are from MNIST and USPS datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu4-2612827-large.gif
2016,7574389,Fig. 5.,"The mean accuracy of the DTML, DSTML, DTML-AE and DSTML-AE methods with varying values of
β
on the LFW dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu5-2612827-large.gif
2016,7574389,Fig. 6.,"The mean accuracy of the DTML-AE and DSTML-AE methods with varying values of
θ
t
and
θ
s
(
θ
t
=
θ
s
) on the LFW dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7581012/7574389/lu6-2612827-large.gif
2016,7185347,Fig. 1.,"Classification accuracy on different parameters' setting, i.e., 
C∈[−5:5]
(upward),
λ
1
∈{
10
−5
,...,
10
−2
}
(rightward), and
λ
2
∈{
10
−5
,...,
10
−2
}
(leftward). (a) MRI (three-class). (b) PET (three-class). (c) MRI + PET (three-class). (d) MRI (four-class). (e) PET (four-class). (f) MRI + PET (four-class).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen1abcdef-2466616-large.gif
2016,7185347,Fig. 2.,"Classification accuracy (ACC) of using different number of features in four feature selection methods, on a three-class classification task (top) and a four-class classification task (bottom), respectively. Note that the horizontal axis represents different number of features selected by various feature selection methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen2-2466616-large.gif
2016,7185347,Fig. 3.,"Frequency of the selected ROIs by the proposed method with MRI+PET in a three-class classification task (top) and a four-class classification task (bottom), respectively. For example,
Frenquenc
y
22
=100
in the upper left subfigure means that the 22nd ROI was selected 100 times over 100 repeats by the proposed method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen3-2466616-large.gif
2016,7185347,Fig. 4.,Top ten selected regions in the three-class classification task with MRI/PET. (a) MRI. (b) PET.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen4ab-2466616-large.gif
2016,7185347,Fig. 5.,Top ten selected regions in the four-class classification task with MRI/PET. (a) MRI. (b) PET.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen5ab-2466616-large.gif
2016,7185347,Fig. 6.,"Accuracy changes in four methods with MRI on a three-class classification task (left) and a four-class classification task (right), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen6-2466616-large.gif
2016,7185347,Fig. 7.,"Frequency of the selected ROIs by the proposed method on a large MRI dataset in a three-class classification task (left) and a four-class classification task (right), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen7-2466616-large.gif
2016,7185347,Fig. 8.,"Classification accuracy on different parameters' setting, i.e., 
C∈[−5:5]
(upward),
λ
1
∈{
10
−5
,...,
10
−2
}
(rightward), and
λ
2
∈{
10
−5
,...,
10
−2
}
(leftward).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7412788/7185347/shen8ab-2466616-large.gif
2016,7365487,Fig. 1.,Architecture of lithium-ion battery test bench [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu1-2512237-large.gif
2016,7365487,Fig. 2.,Training data example: (a) input 1: average voltage; (b) input 2: average current; (c) input 3: power; and (d) output: SOC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu2abcd-2512237-large.gif
2016,7365487,Fig. 3.,Validation data in the first FUDS cycle: (a) input 1: average voltage; (b) input 2: average current; (c) input 3: power; and (d) output: SOC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu3abcd-2512237-large.gif
2016,7365487,Fig. 4.,Best individual.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu4-2512237-large.gif
2016,7365487,Fig. 5.,Clustering result in the input space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu5-2512237-large.gif
2016,7365487,Fig. 6.,"Model topology identified by the genetic-fuzzy clustering. “inputmf” and “outputmf” mean the membership functions of the input and output, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu6-2512237-large.gif
2016,7365487,Fig. 7.,Membership functions of input variables: (a) input 1: average voltage; (b) input 2: average current; and (c) input 3: power. Note that each input has six membership functions as labeled by the numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu7abc-2512237-large.gif
2016,7365487,Fig. 8.,Identification results of the consequent parameters: (a)–(f) correspond to rules 1–6.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu8abcdef-2512237-large.gif
2016,7365487,Fig. 9.,Response surface of the learned model in the training data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu9-2512237-large.gif
2016,7365487,Fig. 10.,Simultaneous optimization by the backpropagation learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu10-2512237-large.gif
2016,7365487,Fig. 11.,Experimental and estimated SOC in the first FUDS cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu11-2512237-large.gif
2016,7365487,Fig. 12.,SOC estimation error in the first FUDS cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu12-2512237-large.gif
2016,7365487,Fig. 13.,Experimental and estimated SOC in the seventh FUDS cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu13-2512237-large.gif
2016,7365487,Fig. 14.,SOC estimation error in the seventh FUDS cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu14-2512237-large.gif
2016,7365487,Fig. 15.,Error distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/7492220/7365487/hu15-2512237-large.gif
2016,7042326,Fig. 1.,Outline of our feature learning-based approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao1-2399172-large.gif
2016,7042326,Fig. 3.,Outline of multiscale-max Gabor filter. This figure illustrates an example of the multiscale-max Gabor filter with a fixed orientation of 45°.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao3-2399172-large.gif
2016,7042326,Fig. 2.,Illustration of the mechanism of max-pooling filter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao2-2399172-large.gif
2016,7042326,Fig. 4.,Example of input data types in the terminal set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao4-2399172-large.gif
2016,7042326,Fig. 6.,Procedure of spatio-temporal sequence representation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao6-2399172-large.gif
2016,7042326,Fig. 5.,Illustration of a possible learned-structure. All the 3D operators and pooling functions used in the structure are randomly selected through the GP evolution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao5-2399172-large.gif
2016,7042326,Fig. 7.,"Some example frames of four datasets. Images in the top row are from the KTH dataset, images in the second row are from the HMDB51 dataset, images in the third row are from the YouTube dataset, and images in the bottom row are from the Hollywood2 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao7-2399172-large.gif
2016,7042326,Fig. 8.,Near-optimal feature descriptor generated through GP on the KTH dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao8-2399172-large.gif
2016,7042326,Fig. 11.,"LISP format of the (near-)optimal feature descriptors generated through GP with nearest neighbor classifier and naive Bayes classifier, respectively in fitness function on the KTH dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao11-2399172-large.gif
2016,7042326,Fig. 12.,Evolved best-so-far values of fitness on four datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao12-2399172-large.gif
2016,7042326,Fig. 9.,LISP format of the (near-)optimal feature descriptor generated through GP on the HMDB51 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao9-2399172-large.gif
2016,7042326,Fig. 10.,LISP format of the (near-)optimal feature descriptor generated through GP on the YouTube dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao10-2399172-large.gif
2016,7042326,Fig. 13.,Near-optimal feature descriptor generated through GP on the Hollywood2 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7355418/7042326/shao13-2399172-large.gif
2016,7150415,Fig. 1.,"The correlation matrices of different methods: (a) Ground Truth, (b) STL, (c) Regularized MTL, (d) Dirty MTL, (e) Robust MTL, (f) Group MTFL, (g) FlexTClus, (h) MTRL, (i) CMTL, and (j) FCMTL. Darker color indicates higher correlation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7374781/7150415/zhou1-2452911-large.gif
2016,7150415,Fig. 2.,"The representative tasks and the corresponding assignment matrix
Z
obtained by the proposed method on the synthetic data set 2 and 3. Darker color indicates larger value.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7374781/7150415/zhou2-2452911-large.gif
2016,7150415,Fig. 3.,"The representative tasks and the corresponding assignment matrix
Z
obtained by the proposed method on the School data set by using
10
and
30
percent of the data as training data. Darker color indicates larger value. Please zoom-in the image for the best visual results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7374781/7150415/zhou3-2452911-large.gif
2016,7150415,Fig. 4.,"The representative tasks and the corresponding assignment matrix
Z
obtained by the proposed method on the MHC-I data set by using 20 and 40 percent of the data as training data. Darker color indicates larger value.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7374781/7150415/zhou4-2452911-large.gif
2016,7150415,Fig. 5.,"Illustration of the convergence of FCMTL. (a) Synthetic data set 1. (b) Synthetic data set 2. (c) Synthetic data set 3. (d) School data set with
10
percent data . (e) School data set with
30
percent data. (f) MHC-I data set with
20
percent data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7374781/7150415/zhou5-2452911-large.gif
2016,7562516,Fig. 1.,"A 2-layer SDAE with
L=4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang1-2606428-large.gif
2016,7562516,Fig. 2.,"The probabilistic graphical model for LDA,
J
is the number of documents and
D
is the number of words in a document.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang2-2606428-large.gif
2016,7562516,Fig. 3.,"The PGM for an example BDL. The red rectangle on the left indicates the perception component, and the blue rectangle on the right indicates the task-specific component. The hinge variable
Ω
h
={J}
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang3-2606428-large.gif
2016,7562516,Fig. 4.,"On the left is the graphical model of CDL. The part inside the dashed rectangle represents an SDAE. An example SDAE with
L=2
is shown. On the right is the graphical model of the degenerated CDL. The part inside the dashed rectangle represents the encoder of an SDAE. An example SDAE with
L=2
is shown on its right. Note that although
L
is still 2, the decoder of the SDAE vanishes. To prevent clutter, we omit all variables
x
l
except
x
0
and
x
L/2
in the graphical models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang4-2606428-large.gif
2016,7562516,Fig. 5.,NN representation for degenerated CDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang5-2606428-large.gif
2016,7562516,Fig. 6.,Sampling as generalized BP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang6-2606428-large.gif
2016,7562516,Fig. 7.,"Graphical model of RSDAE for
L=4
.
λ
s
is not shown here to prevent clutter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/7736162/7562516/wang7-2606428-large.gif
2016,7733110,FIGURE 1.,"Attention to machine learning has exponentially increased over the past two decades, having a stronger trend than that of medical imaging. The intersection point (in red) indicates that research efforts in medical imaging and deep learning recently became comparable (fifty-fifty). Hopefully, their combination might boost both.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang1-2624938-large.gif
2016,7733110,FIGURE 2.,Big picture of deep imaging – A full fusion of medical imaging and deep learning. A high likelihood is that the direct paths from data to features and actions may need an intermediate layer essentially equivalent to a reconstructed/processed image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang2-2624938-large.gif
2016,7733110,FIGURE 3.,Deep network for feature extraction and classification through nonlinear multi-resolution analysis (fully or locally connected).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang3-2624938-large.gif
2016,7733110,FIGURE 4.,"Past, present and future of CT image reconstruction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang4-2624938-large.gif
2016,7733110,FIGURE 5.,Low-hanging fruits by “knocking-out/down/in” computational elements in a traditional iterative reconstruction flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang5-2624938-large.gif
2016,7733110,FIGURE 6.,"Organ and body surfaces of the RPI pregnant female models at the end of 3, 6 and 9 months, respectively, for estimation of radiation doses from radiological procedures [23] (Courtesy of X. George Xu with RPI).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang6-2624938-large.gif
2016,7733110,FIGURE 7.,"Deep network capable of iterative reconstruction. The image pairs from the left to right columns are (1) two original phantoms, (2) the SART reconstruction after only 20 iterations, (3) the counterparts after 500 iterations, and (4) the deep imaging results with the corresponding 20-iteration images as the inputs, which resemble well the 500-iteration counterparts (arguably, the 4 th column looks slightly better than the 3rd column).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang7ab-2624938-large.gif
2016,7733110,FIGURE 8.,"Deep network capable of sinogram restoration. The first row lists an original image (metal in purple) and the associated metal-blocked sinogram. The second row contains the restored sinogram and the reconstructed image, which shows the potential of deep learning as a smart interpolator over the missing data region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang8-2624938-large.gif
2016,7733110,FIGURE 9.,"Deep learning based image denoising, demonstrating that deep learning could be an effective and efficient alternative to the state of the art iterative reconstruction strategy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang9-2624938-large.gif
2016,7733110,FIGURE 10.,"Fourth paradigm as presented by Dr. Jim Gray [46], and slightly renamed by the author.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7419931/7733110/wang10-2624938-large.gif
2016,7167700,Fig. 1.,"The MLQoE consists of two modules, namely, the model selection, and the performance estimation. The model selection takes as input the training set of the performance estimation loop, cross-validates it, and reports the best model. The performance estimation takes as input the dataset, partitions it into folds, estimates the performance of the best model (that the model selection outputs) in each fold and reports (as output) the mean error for the dataset. It can be easily extended to include other ML algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad1-2461216-large.gif
2016,7167700,Fig. 2.,"An illustration of (a) an SVR with one network metric, (b) an ANN with one hidden layer and weights (w and w’), and (c) a Decision Tree based on packet loss and delay.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad2-2461216-large.gif
2016,7167700,Fig. 3.,"(a) Handover scenario: user A moves to the coverage area of a different AP while (s)he participates in a VoIP call with user B, (b) Heavy UDP traffic scenario: each of the nodes D and E transmit 2 Mb/s UDP traffic towards node C, and (c) Heavy TCP traffic scenario: node C exchanges BitTorrent traffic with Internet peers (both uplink and downlink traffic).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad3-2461216-large.gif
2016,7167700,Fig. 4.,"The absolute error derived from the algorithms for the (a) dataset 1, (b) dataset 2, and (c) dataset 3 (largest dataset Table 1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad4-2461216-large.gif
2016,7167700,Fig. 5.,"The network metrics derived from the MMPC in the final prediction models for the (a) dataset 1, (b) dataset 2, and (c) dataset 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad5-2461216-large.gif
2016,7167700,Fig. 6.,Mean performance for each user with MLQoE and E-model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad6-2461216-large.gif
2016,7167700,Fig. 7.,Mean subjective opinions scores of each user for moderate degradation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad7-2461216-large.gif
2016,7167700,Fig. 8.,"(a) The mean absolute error derived from the MLQoE and the other ML algorithms for the dataset 3, (b) the accuracy of the MLQoE per QoE score, and (c) the MLQoE for different size of training and test set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad8-2461216-large.gif
2016,7167700,Fig. 9.,"The absolute error derived from the WFL, IQX and MLQoE for the dataset 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad9-2461216-large.gif
2016,7167700,Fig. 10.,"The absolute error derived from the algorithms for the (a) dataset of the conversational field study, (b) dataset of mobile video database, and (c) dataset of tablet video database. The MLQoE and SVR have the same performance in the mobile and tablet video datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/7464742/7167700/papad10-2461216-large.gif
2016,6933937,Fig. 1.,Processes of two-way learning approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/6933937/xu1-2361772-large.gif
2016,6933937,Fig. 2.,Program flow graph of learning system based on FCA in fuzzy datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/6933937/xu2-2361772-large.gif
2016,6933937,Fig. 3.,Nine kinds of membership functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7381705/6933937/xu3-2361772-large.gif
2016,7104138,Fig. 1.,Comparison of the approximation errors in terms of sparsity levels for P-LSSVM and D-LSSVM on six data sets with 2000 training samples. The approximation errors of P-LSSVM decrease sharply and those of D-LSSVM decrease only when the size of basic set nearly equals the training size. (a) USPS8. (b) SHUTTLE. (c) MINST8. (d) ADULT. (e) IJCNN. (f) VEHICLE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7104138/zhou1abcdef-2424684-large.gif
2016,7104138,Fig. 2.,"Comparison of the test accuracies with sparse solutions for P-LSSVM and D-LSSVM on six data sets with 2000 training samples. The test accuracies of P-LSSVM are higher than those of of D-LSSVM, especially P-LSSVM always achieves higher values at lower sparsity levels. (a) USPS8. (b) SHUTTLE. (c) MINST8. (d) ADULT. (e) IJCNN. (f) VEHICLE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7104138/zhou2abcdef-2424684-large.gif
2016,7104138,Fig. 3.,"Linear classification experiments. (a) P-LSSVM. (b) Original D-LSSVM. (c) Suykens’ pruning D-LSSVM. (d) Jiao’s FSA-LSSVM on D-LSSVM. (e) Standard SVM. The training samples are plotted as squares (
 □
) and triangles (
 ▽
), and the support vectors are plotted as bullets (
 ∙
) and diamonds (
 ⧫
) [in (b)–(d), their sizes are proportional to their weights]. The numbers of the support vector are, respectively, 2, 400, 93, 152, and 55 for (a)–(e), and the test errors for the corresponding algorithms are 5.4%, 5.4%, 5.6%, 6.1%, and 5.6% on 1000 test samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7104138/zhou3abcde-2424684-large.gif
2016,7104138,Fig. 4.,"Plots for comparing the convergence of P-LSSVM and D-LSSVM on related algorithms with respect to the sparsity levels. (a) USPS8 (training size 7291). (b) SHUTTLE (training size 43 500). (c) MNIST8 (training size 60 000). (d) ADULT (training size 32 561). (e) IJCNN (training size 49 990). (f) VEHICLE (training size 78 823). The plots show that all the algorithms based on P-LSSVM (PCP-LSSVM, ICP-LSSVM, RRP-LSSVM, and CSI) converge well to the ideal results, while these algorithms based on D-LSSVM (FSA-LSSVM, PFSA-LSSVM, and RRD-LSSVM) converge to the acceptable results only in respect of the easy data sets (a)–(c) and not hard data sets (d)–(f).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7104138/zhou4abcdef-2424684-large.gif
2016,7104138,Fig. 5.,"Plots for comparing the convergence of P-LSSVM and D-LSSVM with related algorithms on hard data sets with different training sizes, in term of sparsity levels. From a comparison between the plots of the upper (2000 training samples) and lower (10 000 training samples) rows, it is clear that the algorithms based on P-LSSVM are stable on any size of training data set, while those based on D-LSSVM become worse with increasing training set size. (a) ADULT (training size 2000). (b) IJCNN (training size 2000). (c) VEHICLE (training size 2000). (d) ADULT (training size 10 000). (e) IJCNN (training size 10 000). (f) VEHICLE (training size 10 000).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7434085/7104138/zhou5abcdef-2424684-large.gif
2016,7152862,Fig. 1.,"Markov chain of case 1:
Z<
λ
∗
<Z+1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang1-2446198-large.gif
2016,7152862,Fig. 2.,"Comparison of traditional method and the proposed one in terms of the equilibrium state probability with
N=20
,
η=1/2
, and
Z/N<
λ
∗
<(Z+1)/N
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang2-2446198-large.gif
2016,7152862,Fig. 3.,"Markov chain of case 2:
Z=
λ
∗
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang3-2446198-large.gif
2016,7152862,Fig. 4.,"Comparison between the traditional methods and the proposed one in terms of equilibrium state probability with
N=20
,
η=1/2
, and
λ
∗
=Z/N
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang4-2446198-large.gif
2016,7152862,Fig. 5.,"Markov chain of general algorithm case 1:
Z/N<
λ
∗
<(Z+1)/N
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang5-2446198-large.gif
2016,7152862,Fig. 6.,Stable versus unstable regions of the learning algorithm under the triple level stationary environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang6-2446198-large.gif
2016,7152862,Fig. 7.,"Markov chain of general algorithm case 2:
Z=
λ
∗
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang7-2446198-large.gif
2016,7152862,Fig. 8.,"Average absolute divergence of mean asymptotic values of the estimates with different values of
p
,
η
, and
N=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang8-2446198-large.gif
2016,7152862,Fig. 9.,"Projection view of the average absolute divergence of mean asymptotic values of the estimates with different values of
p
,
η
, and
N=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang9-2446198-large.gif
2016,7152862,Fig. 10.,"Mean values of the estimates evolving with time
n
, different values of
p
,
η=1/2
, and
N=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang10-2446198-large.gif
2016,7152862,Fig. 11.,"Mean values of the estimates evolving with time
n
, different values of
p
,
η=1/2
, and
N=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang11-2446198-large.gif
2016,7152862,Fig. 12.,"Mean values of the estimates evolving with time
n
, different values of
p>0.33
,
η=1/2
, and
N=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang12-2446198-large.gif
2016,7152862,Fig. 13.,"Mean values of the estimates evolving with time
n
, different values of
η
,
p=0.85
, and
N=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang13-2446198-large.gif
2016,7152862,Fig. 14.,"Mean values of the estimates evolving with time
n
, different values of
η
,
p=0.45
, and
N=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang14-2446198-large.gif
2016,7152862,Fig. 15.,"Mean values of the estimates evolving with time
n, 
P
stable
=0.7
, and
N=64
when
λ
∗
=0.712
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang15-2446198-large.gif
2016,7152862,Fig. 16.,"Mean values of the estimates evolving with time
n,
different values of
η
,
P
stable
=0.8
, and
N=64
when
λ
∗
=0.712
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang16-2446198-large.gif
2016,7152862,Fig. 17.,"Mean values of the estimates evolving with time
n,
different values of
η
,
P
stable
=0.9
, and
N=64
when
λ
∗
=0.712
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang17-2446198-large.gif
2016,7152862,Fig. 18.,"Mean values of the estimates evolving with time
n,
different values of
η
,
P
stable
=0.7
, and
N=64
when
λ
∗
is changing as (68) specifies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang18-2446198-large.gif
2016,7152862,Fig. 19.,"Mean values of the estimates evolving with time
n,
different values of
η
,
P
stable
=0.8
, and
N=64
when
λ
∗
is changing as (68) specifies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang19-2446198-large.gif
2016,7152862,Fig. 20.,"Mean values of the estimates evolving with time
n,
different values of
η
,
P
stable
=0.9
, and
N=64
when
λ
∗
is changing as (68) specifies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7466429/7152862/huang20-2446198-large.gif
2016,7480352,Fig. 1.,Workflow of integrative analysis of multiomic data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7558265/7480352/resso1-2574201-large.gif
2016,7480352,Fig. 2.,Distributions of: (a) raw glycomic (orange) and proteomic (cyan) datasets; (b) log-transformed data; and (c) data after log-transformation and Z-score normalization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7558265/7480352/resso2abc-2574201-large.gif
2016,7480352,Fig. 3.,"Classification accuracy at each iteration step for the top 50 features from glycomic (green), proteomic (blue), and integrated datasets (red) in the TU and GU cohorts. The optimal numbers of features (indicated by triangles) correspond to the best classification accuracy (indicated by circles). (a) TU cohort, (b) GU cohort.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7558265/7480352/resso3ab-2574201-large.gif
2016,7480352,Fig. 4.,"Classification accuracy at each iteration step for the top 50 features from proteomic (blue), glycomic (green), metabolomic (yellow), integrated proteomic and glycomic (red), and integrated proteomic, glycomic, and metabolomic (matenga) datasets in the TU and GU cohorts. (a) TU cohort, (a) GU cohort.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7558265/7480352/resso4ab-2574201-large.gif
2016,7307098,Fig. 1.,CRISP-DM Process Diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/7475979/7307098/bucza1-2494502-large.gif
2016,7307098,Fig. 2.,"Membership Functions for the Fuzzy Variable Human Body Temperature: Low, Normal, Fever, Strong Fever, and Hypothermia.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/7475979/7307098/bucza2-2494502-large.gif
2016,7307098,Fig. 3.,Example Bayesian Network for Signature Detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/7475979/7307098/bucza3-2494502-large.gif
2016,7307098,Fig. 4.,An Example Decision Tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/7475979/7307098/bucza4-2494502-large.gif
2016,7307098,Fig. 5.,An Example Hidden Markov Model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/7475979/7307098/bucza5-2494502-large.gif
2016,7359099,Fig. 1.,"Histogram of the estimated parameter vector
θ=[
θ
1
,
θ
2
]
for
M=5000
and
P=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar1-2508502-large.gif
2016,7359099,Fig. 2.,"Histogram of the estimated parameter vector
θ=[
θ
1
,
θ
2
]
for
M=500
and
P=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar2-2508502-large.gif
2016,7359099,Fig. 3.,The estimated mean of the parameters for increasing number of iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar3-2508502-large.gif
2016,7359099,Fig. 4.,"System set-up for WDM transmission system. MUX: multiplexing, ROADM: reconfigurable add drop multiplexer, DEMUX: demultiplexer,
T
N
x
:
N
th transmitter, SMF: single mode fibre,
R
0
x
: receiver for the middle channel and NL: nonlinearity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar4-2508502-large.gif
2016,7359099,Fig. 5.,"Required OSNR, to achieve SER of
10
−3
, as a function of variance of the polarization scattering process,
w
xy
, for 28 Gbaud DP-QPSK system. The XPM induced phase noise is set to zero.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar5-2508502-large.gif
2016,7359099,Fig. 6.,"Required OSNR, to achieve SER of
10
−3
, as a function of variance of the polarization scattering process,
w
xy
, for 28 Gbaud DP-16QAM system. The XPM induced phase noise is set to zero.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar6-2508502-large.gif
2016,7359099,Fig. 7.,"SER as a function OSNR for 28 Gbaud DP-QPSK in the presence of XPM induced polarization scattering and phase noise with the corresponding variances of 0.03 and 4 deg
2
, respectively",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar7-2508502-large.gif
2016,7359099,Fig. 8.,"SER as a function OSNR for 28 Gbaud DP-16QAM in the presence of XPM induced polarization scattering and phase noise with the corresponding variances of 0.01 and 1.71 deg
2
, respectively",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar8-2508502-large.gif
2016,7359099,Fig. 9.,Power spectral density of FM noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar9-2508502-large.gif
2016,7359099,Fig. 10.,BER ratio as a function of OSNR for carrier recovery employing digital PLL and rate equations based Kalman filter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar10-2508502-large.gif
2016,7359099,Fig. 11.,Experimental results: demodulated signal constellation impaired by nonlinear phase noise for 14 Gbaud DP 16-QAM after 800 km of transmission through a dispersion managed link.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar11-2508502-large.gif
2016,7359099,Fig. 12.,AIC as a function of number components of GMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar12-2508502-large.gif
2016,7359099,Fig. 13.,Contour plot of the 2-D distribution specified by equation (41) .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar13-2508502-large.gif
2016,7359099,Fig. 14.,Contour plot of the estimated 2-D distribution approximating equation (41).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar14-2508502-large.gif
2016,7359099,Fig. 15.,"Probability density function,
p(X)
of a random variable
X
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar15-2508502-large.gif
2016,7359099,Fig. 16.,"Histograms of the samples, generated by the MH algorithm, that approximate
p(X)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7425116/7359099/zibar16-2508502-large.gif
2016,7293674,Fig. 1.,WiFi RSS values measured by different mobile devices at the same location.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/7401212/7293674/xie1-2487963-large.gif
2016,7293674,Fig. 2.,Layout of the testbed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/7401212/7293674/xie2-2487963-large.gif
2016,7293674,Fig. 3.,Distribution histogram of the average standard deviations of different location fingerprints.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/7401212/7293674/xie3-2487963-large.gif
2016,7293674,Fig. 4.,Comparison of distance error distributions for different methods (KNN).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/7401212/7293674/xie4abcde-2487963-large.gif
2016,7293674,Fig. 5.,Comparison of distance error distributions for different methods (ELM and WELM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/7401212/7293674/xie5abcde-2487963-large.gif
2016,7293674,Fig. 6.,Comparison of mean localization error between different approaches under the influence of the number of APs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/7401212/7293674/xie6-2487963-large.gif
2017,7934386,Fig. 1.,SDD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu1-2708725-large.gif
2017,7934386,Fig. 2.,Back-light illumination imaging system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu2-2708725-large.gif
2017,7934386,Fig. 3.,"Curvature calculation process. (a) Edge detection. (b) Edge smooth with low frequency pass filter. (c) Slope of the tangent of the edge (normalized to [
−π
,
π
]). (d) Derivatives of curvature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu3abcd-2708725-large.gif
2017,7934386,Fig. 4.,Process for noise reduction. (a) First step: measure inclined angle with line detection. (b) Second step: rotate image to make interest area horizontal and perform edge detection. (c) Third step: extract interest area according to edge detection result. (d) Final step: apply median filter for noise reduction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu4abcd-2708725-large.gif
2017,7934386,Fig. 5.,SVM example in 2-D space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu5-2708725-large.gif
2017,7934386,Fig. 6.,Examples of image patches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu6-2708725-large.gif
2017,7934386,Fig. 7.,Illustration of a neural network. (a) Neuron. (b) Neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu7ab-2708725-large.gif
2017,7934386,Fig. 8.,Mimic of bubbles with Gaussian distribution. (a) Original picture. (b) Value of probability density function in each pixel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu8ab-2708725-large.gif
2017,7934386,Fig. 9.,Bubble recognition result of four different methods. (a) Hough transform. (b) Curvature analysis. (c) SVM. (d) DNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu9abcd-2708725-large.gif
2017,7934386,Fig. 10.,Performance of four different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/7981454/7934386/liu10-2708725-large.gif
2017,7431997,Fig. 1.,"Example multiagent network. Each agent is connected to and communicates with a set of other agents, which form its neighborhood. The network is irreducible and aperiodic, such that each node is achievable from any other node at irregular times.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7431997/vanli1-2536649-large.gif
2017,7431997,Fig. 2.,"Comparison of the algorithms for the linear regression model in (50) with mean square error loss and
ℓ
1
norm regularization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7431997/vanli2-2536649-large.gif
2017,7431997,Fig. 3.,"Comparison of the algorithms for the linear regression model in (50) with mean square error loss and
ℓ
2
2
norm regularization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7431997/vanli3-2536649-large.gif
2017,7431997,Fig. 4.,"Comparison of the algorithms for the nonlinear regression model in (52) with mean square error loss and
ℓ
2
2
norm regularization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7431997/vanli4-2536649-large.gif
2017,7431997,Fig. 5.,"Comparison of the algorithms for the dynamic linear regression model in (50) and (53) with mean square error loss and
ℓ
2
2
norm regularization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7431997/vanli5-2536649-large.gif
2017,7932863,Fig. 1.,"Evolution of deep learning from conventional Machine Intelligence and Machine Learning paradigms. Our focus, in this paper, compared to the traditional one, is on exploring how deep learning applications are disrupting network traffic control systems.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato1-2707140-large.gif
2017,7932863,Fig. 2.,"Various Machine Learning techniques exploited for solving a myriad of computer science problems. It may be noticed that deep learning techniques which are shown with
▲
labels have emerged recently with their use mainly restricted to objects recognition and have not been applied to intelligent network traffic control systems extensively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato2-2707140-large.gif
2017,7932863,Fig. 3.,"The architectures of shallow (i.e., regular), deep, convolutional, and recurrent neural networks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato3-2707140-large.gif
2017,7932863,Fig. 4.,"Simplified architectures of BM, RBM, and deep Q-networks. Note that the BMs and RBMs can be used to construct the deep BM and the DBN, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato4-2707140-large.gif
2017,7932863,Fig. 5.,The considered wireless mesh backbone network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato5-2707140-large.gif
2017,7932863,Fig. 6.,The error propagates layer by layer and the value changes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato6-2707140-large.gif
2017,7932863,Fig. 7.,"Comparison of signaling overhead, throughput, and average per-hop delay between the benchmark method (OSPF) and the new, proof-of-concept, deep learning system with different overall data packet generate rates.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8115327/7932863/kato7abc-2707140-large.gif
2017,7464324,Fig. 1.,"Experimental setup. Psychophysiological data collection from a subject: (a) experimental environment and (b) interface of the aCAMS. The output curve of the four subsystems is shown top-down, whose values at current time instant are displayed on the right. For each subsystem, the interval between two green lines defines the target range. The interval between the upper red and green lines and the interval between the lower red and green lines are transition zones. The range out of the two red lines is the error range. The values corresponding to all straight lines are given on the left and the values in black are the set-point of each subsystem, which is the average over the values corresponding to two red lines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang1-2561927-large.gif
2017,7464324,Fig. 2.,"The variation in task-load difficulty level and operator performance (subject F, session 1): (a) 8 control task-load conditions, each with different number of subsystems to be controlled manually, NOS; (b)-(d) performance variables, SIE, SIT, and ASE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang2-2561927-large.gif
2017,7464324,Fig. 3.,The framework of CTL classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang3-2561927-large.gif
2017,7464324,Fig. 4.,Performance data (the 1st and 2nd rows) and NOS parameter (the third row) for each subject.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang4-2561927-large.gif
2017,7464324,Fig. 5.,The basic structure of ensemble SVM classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang5-2561927-large.gif
2017,7464324,Fig. 6.,"The determination of the number of Gaussian components, C, in GMM based on SII for subject A-G.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang6-2561927-large.gif
2017,7464324,Fig. 7.,"The scatter plot of the 3D performance data (subject F, session 1). Five Gaussian components, i.e., the target CTL classes, were identified by using the GMM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang7-2561927-large.gif
2017,7464324,Fig. 8.,"Target CTL levels determined by using performance data (subject F, session 1): (a) Clusters derived by using GMM, (b) Merged clusters, and (c) Target CTL levels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang8-2561927-large.gif
2017,7464324,Fig. 9.,"The salient EEG features (subject F). The first two columns show 30 EEG features of session 1 and 2, respectively. The third column gives linear correlation coefficient between the EEG features and full power spectrum (1-40 Hz) of the corresponding channel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang9-2561927-large.gif
2017,7464324,Fig. 10.,"The mean and standard deviation of validating accuracy versus number of member classifiers of (a) MKSVM1, (b) MKSVM2, and (c) MKSVM3. The member classifiers are ranked in descending order of cross validation accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang10-2561927-large.gif
2017,7464324,Fig. 11.,The subject-average classification rate (on validating set) under different combinations of parameter T 1 and T3 with T2 = 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang11-2561927-large.gif
2017,7464324,Fig. 12.,"The box-whisker plot of different CTL classification methods. Three lines in the box from top to bottom give the values of the upper quartile, median, and the lower quartile for ACCs computed from subject A-G, respectively. Two additional lines at both ends indicate the whisker range of maximal to minimal values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang12-2561927-large.gif
2017,7464324,Fig. 13.,The four-class testing classification results (subject E): (a) session 1 and (b) session 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang13-2561927-large.gif
2017,7464324,Fig. 14.,The five-class testing classification results (subject F): (a) session 1 and (b) session 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8003513/7464324/zhang14-2561927-large.gif
2017,7547945,Fig. 1.,"Compatibility in natural language parsing for an input sentence
x=(the,dog,barks)
.
w
indicates how appropriate the relations
Ψ
between nodes are, which specify the grammar.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8069036/7547945/bauer1-2598721-large.gif
2017,7547945,Fig. 2.,"Factor graph representation for the MS objective with
η(G)=G
(left), with a high-order
η(G)
(middle), and an augmented objective [for a high-order
η(G)
] with auxiliary variables
L=(
l
1
,…,
l
4
)
(right). The auxiliary variables are marked blue.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8069036/7547945/bauer2abc-2598721-large.gif
2017,7547945,Fig. 3.,"Factor graph representation for the SS objective. The auxiliary variables (except
l
1
) are marked blue.
l
1
is the hub node.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8069036/7547945/bauer3-2598721-large.gif
2017,7547945,Fig. 4.,Factor graph representation (left) and an MRF representation (right) of the leftmost parse tree from Fig. 1 .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8069036/7547945/bauer4ab-2598721-large.gif
2017,7547945,Fig. 5.,Empirical comparison of run times for the loss augmented inference on a single input instance with various loss functions for the natural language parsing experiment (left) and sequence segmentation experiment (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8069036/7547945/bauer5ab-2598721-large.gif
2017,7934047,Fig. 1.,Method for automatically generating human–machine interfaces from task models. Details of the formal model's architecture can be seen in Fig. 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto1-2700630-large.gif
2017,7934047,Fig. 2.,Formal model architecture used by the teacher oracle (see Fig. 1).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto2-2700630-large.gif
2017,7934047,Fig. 3.,"(a) and (b) Visualization of the EOFM task for turning the light on and off. Activities are rounded rectangles; actions are unrounded rectangles. Preconditions and completion conditions are yellow and magenta triangles, respectively, connected to their associated activities and annotated in condition logic. Activity decompositions are downward pointing arrows annotated with decomposition operators. (c) Moore machine model of the generated human–machine interface. Each circle is a state labeled with its name (S
0
and S
1
) and the values of the corresponding system outputs. An arrow indicates a transition triggered by the action in the arrow's label. If an action does not produce a transition from a given state, then the action is not allowed in that state.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto3-2700630-large.gif
2017,7934047,Fig. 4.,"Visualization of the EOFM tasks for (a) entering money into the drink vending machine, (b) dispensing and picking up a drink from the machine, and (c) picking up change returned by the machine.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto4-2700630-large.gif
2017,7934047,Fig. 5.,Moore machine model of the generated vending machine human–machine interface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto5-2700630-large.gif
2017,7934047,Fig. 6.,Task models describing how the human should behave when interacting with the PCA pump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto6-2700630-large.gif
2017,7934047,Fig. 7.,"(a) PCA pump interface design generated from the task model shown in Fig. 6 . (b) PCA pump interface reference model from [64], presented in its original notation. Note that in (b), states have been slightly rearranged to support easy comparison with the interface in (a). States and actions have been slightly renamed for the same purpose. Recursive transitions to states (transitions that point to the same state they originated from) have been removed because they relate to automation behavior that was not of concern in this analysis. Further note that in (b), multiple expressions on a transition are assumed to be in an “and” relationship. == represents a Boolean equals and := represents a variable assignment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7934047/bolto7-2700630-large.gif
2017,7817807,Fig. 1.,"Convergence of our LLC-DL. The top line is the objective function values on (a) HP1, (b) HP2, and (c) HP3 dataset. The bottom line represents the divergence between two consecutive P on (d) HP1, (e) HP2, and (f) HP3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7904755/7817807/zhang1abcdef-2653184-large.gif
2017,7817807,Fig. 2.,"Confusion tables for the rolling bearing dataset using two signals per category for training. (a) HP1, (b) HP2, and (c) HP3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7904755/7817807/zhang2abc-2653184-large.gif
2017,7817807,Fig. 3.,Classification performance on the synthetic machine dataset under different training numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7904755/7817807/zhang3-2653184-large.gif
2017,7817807,Fig. 4.,"Classification performance with varying Var on (a) 1HP dataset, (b) 2HP dataset, and (c) 3HP dataset, where the number of training signals in each class is fixed to two for simulations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7904755/7817807/zhang4abc-2653184-large.gif
2017,7817807,Fig. 5.,"Parameter sensitivity analysis under different parameter configurations on the rolling bearing dataset (2HP): (a) effects of tuning
β
and
λ
on the performance over fixing
α=0.1
, (b) effects of tuning
α
and
λ
on the performance over fixing
β=0.1
, and (c) effects of tuning
α
and
β
on the performance over fixing
λ=0.1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/7904755/7817807/zhang5-2653184-large.gif
2017,8024036,Fig. 1.,Illustration of label clustering SSL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8168802/8024036/deng1-2748388-large.gif
2017,8024036,Fig. 2.,Typical EEG signals in Groups A-E.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8168802/8024036/deng2-2748388-large.gif
2017,8024036,Fig. 3.,"The membership functions and the possible linguistic explanation of each fuzzy subset in the antecedent of the fuzzy rules of the TSK fuzzy system. The system is obtained by the A-TL-SSL-TSK based on the WPD features. * The antecedent parameter (
c
1
1
,
δ
1
1
) of Band 1 (first dimension of the data) of the first fuzzy rule. ** A possible explanation for the fuzzy set obtained.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8168802/8024036/deng3-2748388-large.gif
2017,8024036,Fig. 4.,"An example showing the identification of epileptic patient using the fuzzy rules generated and the fuzzy system, where ‘+’ denotes the combination operation and ‘max’ denotes the operation that sets the maximal element in Y to 1 and the others to 0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8168802/8024036/deng4-2748388-large.gif
2017,7393590,Fig. 1.,"Connectivity and scale of the simulated cerebellum. Arrows and circles denote the excitatory and inhibitory synaptic connections, respectively. The number of simulated cells in each region is annotated. Stars: sites of synaptic plasticity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk1-2512838-large.gif
2017,7393590,Fig. 2.,"Cerebellum cart–pole interface. The environment transmits state and error signals to the MFs and the IO nuclei. In return, the cerebellum simulator provides real-valued output from two microzones which is applied to the cart as force in opposite directions. Arrows and circles denote the excitatory and inhibitory connections, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk2-2512838-large.gif
2017,7393590,Fig. 3.,Cerebellum simulator solves the inverted pendulum task within eight episodes. Q-learning eventually converged to the correct solution after nearly 1000 episodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk3-2512838-large.gif
2017,7393590,Fig. 4.,"Simulated autonomous vehicle control. Average reward, in the order of final performance, of a pretuned PD controller, online PD controller, the cerebellum simulator, and TEXPLORE, on the simulated autonomous vehicle acceleration and deceleration control task. PD controller was pretuned automatically using hill climbing. Results are averaged over ten trials and smoothed over a 50 episode sliding window.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk4-2512838-large.gif
2017,7393590,Fig. 5.,"Sample performance of acceleration and braking. A hill-climbing-tuned PD controller is compared against the cerebellum simulator in a task designed to test the acceleration and braking capabilities of each algorithm. At time zero, a target velocity of 10 m/s is given with a current velocity of 5 m/s. At 10 s, the target velocity 5 m/s is given. The cerebellum simulator gently approaches the target velocity yet still has oscillations. The PD controller applies the maximum allowed braking and acceleration to quickly reach the target speed. Small oscillations at the target velocity can be seen. These oscillations are likely caused by the lack of an integral term.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk5-2512838-large.gif
2017,7393590,Fig. 6.,"Dynamic robot balance. The objective of this task is to maintain stability in the face of sudden impact force. The cerebellum simulator controls the hip pitch of the robot, leaning forward in preparation for impact and returning to upright after impact. A failure to do either of these results in a fall. An augmented soccer ball with increased size and mass is used to impart sudden force. (a) Preparation. (b) Shot. (c) Impact.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk6abc-2512838-large.gif
2017,7393590,Fig. 7.,"GR weight analysis shows the effects of delayed error signals. (a) Error probability as a function of time to impact. The manual encoding delivers errors at and before impact, while the gyro and accelerometer encodings deliver delayed errors, after the impact is perceived. (b) GWM for the different Robocup error encodings. A lower/higher GWM corresponds to increased/decreased cerebellar output force when the associated MF is active. For example, in all encodings, MF 205 is active 0.5 s after impact (
x=−0.5
). The gyro and accelerometer encodings have a highly negative GWM at this point, meaning that the robot will lean forward strongly. The manual encoding has a positive GWM at this point meaning that the robot will lean backward. To succeed at this task, the cerebellum simulator must output high force at and directly before impact and low force at all other times. Of the four error encodings, only the manual encoding is capable of maintaining dynamic balance. All others output force too late or without sufficient strength. This failure is due to the delayed nature of gyroscope and accelerometer error signals. Lines were smoothed using a sliding window of size ten.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk7ab-2512838-large.gif
2017,7393590,Fig. 8.,"Cerebellum simulator has been shown to learn robust responses over a wide variety of delays
Δ
T
1
ranging from 250 ms to 1.5 s, but is inflexible in changing the delay
Δ
T
2
between the force output and the error signal. This learning window makes the cerebellum simulator suitable for control tasks featuring error signals that co-occur with high output forces. Tasks that require force output more than 100 ms prior to error signal have proven difficult or impossible to learn, precluding most typical reinforcement learning tasks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk8-2512838-large.gif
2017,7393590,Fig. 9.,"Successful cerebellar learning of all two-variable Boolean functions. Blocks of contiguous high-frequency MF firing are shown in gray. The left
y
-axis of each plot shows which MFs are active in each block. The right
y
-axis shows the average cerebellar output force in blue (forces were averaged over ten trials). Error signals are denoted by vertical red dashed lines. Learning is considered successful in each case because highest cerebellar output forces correspond with the incidence of error signals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk9-2512838-large.gif
2017,7393590,Fig. 10.,"Cerebellum simulator is capable of recreating the subtraction experiment described in [45]. In this experiment, two overlapping tones are played. The shorter tone, here, B is played for 3000 ms, while the longer one A is played for 3500 ms. Error is delivered after the end of the longer tone. Output forces show that the simulator outputs highest force just before the error signal.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk10-2512838-large.gif
2017,7393590,Fig. 11.,"Cerebellum simulator learns the XOR function when inputs are not delivered simultaneously. Dashed gray boxes indicate that MF inputs were delivered in alternating timesteps (e.g.,
A
on even timesteps and
¬B
on odd). This result indicates that the GR cells can learn functions over maximally interspersed, nonoverlapping sequences of MF input.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk11-2512838-large.gif
2017,7393590,Fig. 12.,"Discrimination between different temporal sequences of state input. Two different sequences of MF activation
B,A,C
and
A,B,C
were presented during both training and testing; only the former was followed by an error signal. Cerebellar output shows high force response to both the sequences of MF input with slightly higher response to the correct sequence, indicating that for at least these patterns of input, the cerebellum simulator was not able to strongly distinguish the first from the second.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk12-2512838-large.gif
2017,7393590,Fig. 13.,"Microzone activations for the first ten digits of the MNIST test set. From left to right, each cluster contains the activations of the microzones intended to recognize digits 0, 1, 7, and 9. True labels are given on the
x
-axis. Microzones forces are generally highest for the digit they are meant to recognize. However, some confusion may be seen in the high activations of Microzone-9 when seeing fours and sevens. Intuitively, these mistakes are reasonable as fours, nines, and sevens share common structure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk13-2512838-large.gif
2017,7393590,Fig. 14.,"Left: MNIST confusion matrix indicates that the network is highly successful at recognizing ones, fours, and sevens. The largest missed predictions were fives and eights. These incorrect predictions could result from Microzone-5 being generally less forceful than Microzone-8. Right: MNIST hit at
k
values indicate if the correct label was predicted within the top-
k
guesses.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7857118/7393590/hausk14ab-2512838-large.gif
2017,7373625,Fig. 1.,"RBF CUDA kernel implementation. The colored elements of the matrix each represent a block of threads. Arrows mean a portion of row or column, which are used in computing portion of distances.
n
F
,
n
S
, and
n
H
are number of features, number of samples, and number of hidden nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7779168/7373625/lam1-2511149-large.gif
2017,7373625,Fig. 2.,"Effect of parameter
BLOCKX
and
BLOCKY
in RBF ELM CUDA kernel. As
BLOCKY
increases toward
BLOCKX
, the training time reduces. CUDA kernel with
BLOCKX=BLOCKY=16
achieves the best performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7779168/7373625/lam2-2511149-large.gif
2017,7373625,Fig. 3.,"Speeding up RBF ELM using CUDA. The GPU algorithm is about
4×
faster than [35] and
20×
faster than CPU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7779168/7373625/lam3-2511149-large.gif
2017,7373625,Fig. 4.,Comparing multiquadric RBF ELM with sigmoid ELM in MNIST data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7779168/7373625/lam4-2511149-large.gif
2017,7546909,Fig. 1.,Illustration of restricted Boltzmann machine and deep belief network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7954079/7546909/lu1ab-2601240-large.gif
2017,7546909,Fig. 2.,Deep belief network scheme for motor imagery classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7954079/7546909/lu2-2601240-large.gif
2017,7546909,Fig. 3.,Convergence curve of the training error on session 4.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7954079/7546909/lu3-2601240-large.gif
2017,7546909,Fig. 4.,Energy change curves of the first hidden layer RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7954079/7546909/lu4-2601240-large.gif
2017,7502161,Fig. 1.,(a) Model of RBM. (b) Model of CRBM with max pooling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu1ab-2582532-large.gif
2017,7502161,Fig. 2.,Local function on (a) sphere model and (b) human model. The white point is the center of each local region.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu2ab-2582532-large.gif
2017,7502161,Fig. 3.,"Angles and the area used by discrete Laplace–Beltrami operator
Δ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu3-2582532-large.gif
2017,7502161,Fig. 4.,"LF is decomposed in the function space spanned by the sorted eigenfunctions of Laplace–Beltrami operator
Δ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu4-2582532-large.gif
2017,7502161,Fig. 5.,Overview of the learning process. (a) Input. (b) Detection nodes in lower MCRBM. (c) LFED. (d) Pooling nodes in lower MCRBM. (e) Detection nodes in upper MCRBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu5abcde-2582532-large.gif
2017,7502161,Fig. 6.,"Structure of MCRBM. The MCRBM includes a visible layer
V
, a detection layer
H
, and a pooling layer
P
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu6-2582532-large.gif
2017,7502161,Fig. 8.,"Structure of MCDBN stacked by two MCRBM with multiple filters. In each MCRBM, the group of detection nodes
H
k
and its corresponding group of pooling nodes
P
k
are shown on duplicates of the input mesh in the same color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu8-2582532-large.gif
2017,7502161,Fig. 7.,Demonstration of LSPC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu7-2582532-large.gif
2017,7502161,Fig. 9.,The structure of MCDBN with one filter. Pooling nodes (small green points) in lower MCRBM and detection nodes (big white points) in upper MCRBM are shown together on (a) a sphere mesh and (b) a human mesh.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu9ab-2582532-large.gif
2017,7502161,Fig. 10.,"Retrieval performance comparison under different values of
K
word
is shown in (a)–(c). The retrieval performance comparison under different values of
K
filter
is shown in (d)–(f). (a) BoW. (b) SS-BoF. (c) Comparsion. (d) Comparsion. (e) BoW. (f) SS-BoF.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu10abcdef-2582532-large.gif
2017,7502161,Fig. 11.,"Retrieval performance obtained from second MCRBM under different numbers of filters in (a) and (b), under different numbers of virtual words in (c) and (d). Retrieval performance obtained from third MCRBM under different numbers of filters in (e) and (f). (a) BoW. (b) Comparision. (c) BoW. (d) Comparsion. (e) BoW. (f) Comparsion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu11abcdef-2582532-large.gif
2017,7502161,Fig. 12.,"(a) and (b) 12 filters of the first MCRBM and 16 filters of the second MCRBM used in the following experiments. A filter of the first MCRBM is a
1×25
vector which is shown as a
5×5
image, while a filter of the second MCRBM is a
1×10
vector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu12ab-2582532-large.gif
2017,7502161,Fig. 13.,PR curves under (a) articulated subset of McGill shape benchmark and (b) whole McGill shape benchmark.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu13ab-2582532-large.gif
2017,7502161,Fig. 14.,(a) PR curves under the SHREC 2007 data set. (b) NDCG curves under the SHREC 2007 partial retrieval data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu14ab-2582532-large.gif
2017,7502161,Fig. 15.,"Correspondence results obtained under SCAPE data set. (a) Ratio of correct matching versus ratio of geodesic error curves. The geodesic error distribution obtained by (b) MCRBM, (c) HKS, (d) MCDBN, and (e) SIHKS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu15abcde-2582532-large.gif
2017,7502161,Fig. 16.,(a) and (b) Correspondence results obtained under Watertight data set. The probability distribution functions of geodesic errors obtained from all classes by MCRBM and MCDBN. (c) Comparison between results obtained from all classes by MCRBM and MCDBN. (d) Results compared with other methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8038919/7502161/liu16abcd-2582532-large.gif
2017,7577870,Fig. 1.,RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng1-2609437-large.gif
2017,7577870,Fig. 2.,DBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng2-2609437-large.gif
2017,7577870,Fig. 3.,PFDBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng3-2609437-large.gif
2017,7577870,Fig. 4.,Linear model of the emigration and immigration rates of BBO.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng4-2609437-large.gif
2017,7577870,Fig. 5.,Illustration of a BBO migration operation of the PFDBM learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng5-2609437-large.gif
2017,7577870,Fig. 6.,Structure of the PFDBM-based DNN for individual passenger profiling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng6-2609437-large.gif
2017,7577870,Fig. 7.,Integrated DNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng7-2609437-large.gif
2017,7577870,Fig. 8.,"Precision, recall, and fallout results of the seven models for individual passenger profiling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng8-2609437-large.gif
2017,7577870,Fig. 9.,"Precision, recall, and fallout results of the seven models on the incomplete data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng9-2609437-large.gif
2017,7577870,Fig. 10.,Classification of faking attackers of the seven models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng10-2609437-large.gif
2017,7577870,Fig. 11.,"Precision, recall, and fallout results of the five models for attacker group identification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng11-2609437-large.gif
2017,7577870,Fig. 12.,"sr
A
,
sr
A
,
sr
C
, and
sr
D
results of the five models for attacker group identification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7577870/zheng12-2609437-large.gif
2017,7973168,Fig. 1.,Schematic illustration of the constraints of similar and dissimilar pairs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo1-2725578-large.gif
2017,7973168,Algorithm 1,Algorithm of PCML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo11-2725578-large.gif
2017,7973168,Fig. 2.,Duality gap vs. number of iterations on the PenDigits database for PCML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo2-2725578-large.gif
2017,7973168,Algorithm 2,Algorithm of NCML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo12-2725578-large.gif
2017,7973168,Fig. 3.,Duality gap vs. number of iterations on the PenDigits database for NCML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo3-2725578-large.gif
2017,7973168,Fig. 4.,"Classification error rate (%) versus
C
. (a) PCML; (b) NCML.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo4ab-2725578-large.gif
2017,7973168,Fig. 5.,"Training time (s) of NCA, ITML, MCML, LDML, LMNN, DML-eig, PLML, Doublet-SVM, PCML and NCML. From 1 to 9, the Database ID represents Breast Tissue, Cardiotocography, ILPD, Letter, Parkinsons, Satellite, Segmentation, Sonar and SPECTF Heart.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo5-2725578-large.gif
2017,7973168,Fig. 6.,"Training time (s) of NCA, ITML, MCML, LDML, LMNN, DML-eig, PLML, Doublet-SVM, PCML and NCML. From 1 to 4, the Database ID represents MNIST, PenDigits, Semeion and USPS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo6-2725578-large.gif
2017,7973168,Fig. 7.,Training time (s) vs. PCA dimension on the Semeion database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo7-2725578-large.gif
2017,7973168,Fig. 8.,The ROC curves of different methods on the LFW database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo8-2725578-large.gif
2017,7973168,Fig. 9.,The CMC curves of different methods on the CUHK03 database with (a) manually labeled bounding box and (b) automatically detected bounding box.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo9ab-2725578-large.gif
2017,7973168,Fig. 10.,The CMC curves of different methods on the CUHK01 database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7983498/7973168/zuo10-2725578-large.gif
2017,7588107,Fig. 1.,"Examples of faces (first row) and their landmark faces (second row). The first three faces in each row denote eastern females, and the last three faces are from western.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang1-2607757-large.gif
2017,7588107,Fig. 2.,Examples of eastern (first five) and western (last five) dressing images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang2-2607757-large.gif
2017,7588107,Fig. 3.,Cumulative scores of facial attractiveness recognition using (a)–(c) ELM-based methods and (d)–(f) subspace-based methods. (a) and (d) Eastern. (b) and (e) Western. (c) and (f) Eastern + western.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang3abcdef-2607757-large.gif
2017,7588107,Fig. 4.,Cumulative scores of dressing attractiveness recognition using (a)–(c) ELM-based methods and (d)–(f) subspace-based methods. (a) and (d) Eastern. (b) and (e) Western. (c) and (f) Eastern + western.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang4abcdef-2607757-large.gif
2017,7588107,Fig. 5.,Cumulative scores of vocal attractiveness recognition using (a)–(c) ELM-based methods and (d)–(f) subspace-based methods. (a) and (d) Eastern. (b) and (e) Western. (c) and (f) Eastern + western.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang5abcdef-2607757-large.gif
2017,7588107,Fig. 6.,CumScore curves of all the methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang6-2607757-large.gif
2017,7588107,Fig. 7.,Sample images of one “the same” pair and one “not the same” pair from LFW.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang7-2607757-large.gif
2017,7588107,Fig. 8.,ROC and AUC analysis on LFW data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang8-2607757-large.gif
2017,7588107,Fig. 9.,Confusion matrix analysis based on E-NOSE data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang9-2607757-large.gif
2017,7588107,Fig. 10.,"Confusion matrix analysis based on
M
2
B
data (i.e., facial, dress, and vocal data). (a) Confusion matrix on the Eastern facial feature of
M
2
B
data. (b) Confusion matrix on the Western facial feature of
M
2
B
data. (c) Confusion matrix on the Eastern dress feature of
M
2
B
data. (d) Confusion matrix on the Western dress feature of
M
2
B
data. (e) Confusion matrix on the Eastern vocal feature of
M
2
B
data. (f) Confusion matrix on the Western vocal feature of
M
2
B
data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang10abcdef-2607757-large.gif
2017,7588107,Fig. 11.,"Performance variation with respect to the parameter
C=
2
p
in ELM-based methods. (a) AR with
L=300
. (b) LFW with
L=100
. (c) E-NOSE with
L=200
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang11abc-2607757-large.gif
2017,7588107,Fig. 12.,"Performance variation with respect to the number of hidden neurons
L
in ELMs. (a) AR with
C=
2
5
. (b) LFW with
C=
2
10
. (c) E-NOSE with
C=
2
20
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8113707/7588107/zhang12abc-2607757-large.gif
2017,7742965,Fig. 1.,Work flow chart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni1-2628365-large.gif
2017,7742965,Fig. 2.,Participant’s view of the Mario Bros. game.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni2-2628365-large.gif
2017,7742965,Fig. 3.,Explanations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni3-2628365-large.gif
2017,7742965,Fig. 4.,Sentiment classification decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni4-2628365-large.gif
2017,7742965,Fig. 5.,"Object-focused advice for each explanation type. Note that the responses for the free-form and survey responses are varied, while almost all of the structured responses are to jump. The poor performance of the structured responses is likely due to the increased cognitive load of that explanation format. Warnings are shown in red and underlined. P#=Participant#. JRS=JumpRightSpeed. JS=JumpSpeed. The question marks indicate the participant did not specify if shells were considered enemies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni5-2628365-large.gif
2017,7742965,Fig. 6.,Cumulative reward from survey.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni6-2628365-large.gif
2017,7742965,Fig. 7.,Reward for chasms from survey.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni7-2628365-large.gif
2017,7742965,Fig. 8.,"Visualizing object-level generalization in a policy for Goombas from survey. The color scale represents
Q
-values showing when to jump quickly to the right.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni8-2628365-large.gif
2017,7742965,Fig. 9.,"Comparing the reward of good, mediocre, adversarial, and no advice when a coin is northeast of Mario from survey.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni9-2628365-large.gif
2017,7742965,Fig. 10.,Impact of warnings on reward for coins and participant 3 from survey.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/7875186/7742965/kreni10-2628365-large.gif
2017,7801947,Fig. 1.,"Distribution of published papers that use deep learning in subareas of health informatics. Publication statistics are obtained from Google Scholar; the search phrase is defined as the subfield name with the exact phrase deep learning and at least one of medical or health appearing, e.g., “public health” “deep learning” medical OR health.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7839252/7801947/ravi1-2636665-large.gif
2017,7801947,Fig. 2.,Percentage of most used deep learning methods in health informatics. Learning method statistics are also obtained from Google Scholar by using the method name with at least one of medical or health as the search phrase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7839252/7801947/ravi2-2636665-large.gif
2017,7801947,Fig. 3.,Schematic illustration of simple NNs without deep structures. (a) Autoencoder. (b) Restricted Boltzmann machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7839252/7801947/ravi3-2636665-large.gif
2017,7801947,Fig. 4.,Basic architecture of CNN which consists in several layers of convolution and subsampling to efficiently process images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7839252/7801947/ravi4-2636665-large.gif
2017,7801947,Fig. 5.,Overview of the different inputs and applications in biomedical and health informatics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7839252/7801947/ravi5-2636665-large.gif
2017,7801947,Fig. 6.,"Data for health monitoring applications can be captured using a wide array of pervasive sensors that are worn on the body, implanted, or captured through ambient sensors, e.g., inertial motion sensors, ECG patches, smart-watches, EEG, and prosthetics.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/7839252/7801947/ravi6-2636665-large.gif
2017,7423818,Fig. 1.,Comparison of systematic frameworks. (a) The flowchart of the STL methods; (b) The flowchart of the MTL methods; (c) The flowchart of the proposed method with the shaded parts highlighting differences with respect to the state of the art.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7765151/7423818/liu1-2537337-large.gif
2017,7423818,Fig. 2.,"HC-MTL.
W
∗
denotes the optimal models and each column vector depicts the parameter of each model;
W
h
(
W
h
=
L
h
+
S
h
)
denotes the obtained models in the
hth
level;
L
h
is the low-rank part;
S
h
is the group-sparse part; the elements of the blue vector in
S
h
denote the near 0 value; each box denotes individual group of actions,
T
t
, and
T
∗
is the optimal grouping information;
⨁
denotes the column-wise vector addition in Eq. (3).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7765151/7423818/liu2-2537337-large.gif
2017,7423818,Fig. 3.,Grouping results. The samples in the first & second lines are from HMDB51 and UCF101 respectively.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7765151/7423818/liu3-2537337-large.gif
2017,7423818,Fig. 4.,Comparison by varying the iteration number.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7765151/7423818/liu4-2537337-large.gif
2017,7423818,Fig. 5.,Comparison between HC-MTL & CMTL (%).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7765151/7423818/liu5-2537337-large.gif
2017,7423818,Fig. 6.,"Performance comparison by varying
α&β
on Hollywood2(a-b), UCF Sports(c-d), YouTube(e-f), UCF50(g-h), HMDB51(i-j), UCF101(k-l) (%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7765151/7423818/liu6-2537337-large.gif
2017,7906512,Fig. 1.,Big Data characteristics with associated challenges.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli1-2696365-large.gif
2017,7906512,Fig. 2.,Variance and bias [37].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli2-2696365-large.gif
2017,7906512,Fig. 3.,Data Analytics Pipeline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli3-2696365-large.gif
2017,7906512,Fig. 4.,Manipulations for Big Data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli4-2696365-large.gif
2017,7906512,Fig. 5.,Deep Learning [101].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli5-2696365-large.gif
2017,7906512,Fig. 6.,Local Learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli6-2696365-large.gif
2017,7906512,Fig. 7.,Transfer Learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli7-2696365-large.gif
2017,7906512,Fig. 8.,Transfer Learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli8-2696365-large.gif
2017,7906512,Fig. 9.,Ensemble Learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906512/groli9-2696365-large.gif
2017,8066291,FIGURE 1.,Recurrent Neural Networks (RNNs).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8066291/chuan1-2762418-large.gif
2017,8066291,FIGURE 2.,Block diagram of proposed RNN-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8066291/chuan2-2762418-large.gif
2017,8066291,FIGURE 3.,The unfolded Recurrent Neural Network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8066291/chuan3-2762418-large.gif
2017,8066291,FIGURE 4.,The Accuracy on the KDDTest+ and KDDTest−21 datasets in the Binary Classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8066291/chuan4-2762418-large.gif
2017,8066291,FIGURE 5.,Performance of RNN-IDS and the other models in the binary classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8066291/chuan5-2762418-large.gif
2017,8066291,FIGURE 6.,Performance of RNN-IDS and the other models in the five-category classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8066291/chuan6-2762418-large.gif
2017,7867770,Figure 1.,The framework of RMLLEF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong1-2676761-large.gif
2017,7867770,Figure 2.,Effect of the preference parameter for the image dataset. (a) Hamming loss; (b) Ranking loss; (c) One-error; (d) Coverage; (e) Average Precision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong2abcde-2676761-large.gif
2017,7867770,Figure 3.,Effect of the preference parameter for the Yeast dataset. (a) Hamming loss; (b) Ranking loss; (c) One-error; (d) Coverage; (e) Average Precision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong3abcde-2676761-large.gif
2017,7867770,Figure 4.,"The performance of RMLLA on the image dataset with respect to the Linear, Polynomial, RBF, and Sigmoid kernel functions. (a) Hamming loss; (b) Ranking loss; (c) One-error; (d) Coverage; (e) Average Precision.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong4abcde-2676761-large.gif
2017,7867770,Figure 5.,"The performance of RMLLA on the Yeast dataset with respect to the Linear, Polynomial, RBF, and Sigmoid kernel functions. (a) Hamming loss; (b) Ranking loss; (c) One-error; (d) Coverage; (e) Average Precision.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong5abcde-2676761-large.gif
2017,7867770,Figure 6.,Performance comparisons between AP+MLKNN and AP+MLSVM on (a) the image dataset and (b) the Yeast dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong6ab-2676761-large.gif
2017,7867770,Figure 7.,"Performance comparisons between
k
-medoid+MLSVM and AP+MLSVM on (a) the image dataset and (b) the Yeast dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong7ab-2676761-large.gif
2017,7867770,Figure 8.,"Performance comparisons between MLSVM, MLKNN, and RMLLA (AP+MLSVM) on (a) the image dataset and (b) the Yeast dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong8ab-2676761-large.gif
2017,7867770,Figure 9.,Performance comparisons between RMLLEF and RMLLA on (a) the image dataset and (b) the Yeast dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7867770/gong9ab-2676761-large.gif
2017,7782309,Fig. 1.,Block diagram of the representation learning based hybrid framework for dysarthric speech recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s1-2638830-large.gif
2017,7782309,Fig. 2.,Scores given by ESHMMs of all classes for an utterance of ‘One’ class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s2-2638830-large.gif
2017,7782309,Fig. 3.,Scores given by ESHMMs of all classes for an utterance of ‘Alt’ class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s3-2638830-large.gif
2017,7782309,Fig. 4.,Number within brackets indicates total number of test utterances of that class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s4-2638830-large.gif
2017,7782309,Fig. 5.,WRA (in %) for words of “Very Low Intelligibility”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s5-2638830-large.gif
2017,7782309,Fig. 6.,WRA (in %) for words of “Low Intelligibility”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s6-2638830-large.gif
2017,7782309,Fig. 7.,WRA (in %) for words of “Mid Intelligibility”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s7-2638830-large.gif
2017,7782309,Fig. 8.,WRA (in %) for words of “High Intelligibility”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8026585/7782309/s8-2638830-large.gif
2017,7917366,Fig. 1.,"Top view of the biomechanical arm model. The Y-axis is anterior. Movements occur in the sagittal plane with no gravity, as if sliding across a frictionless tabletop. Antagonistic muscle pairs are listed as (flexor, extensor): monoarticular shoulder muscles: (A: anterior deltoid, B: posterior deltoid); monoarticular elbow muscles: (C: brachialis, D: triceps brachii (short head)); biarticular muscles: (E: biceps brachii; F: triceps brachii (long head)).
φ
1
and
φ
2
are shoulder and elbow joint angles, respectively. Adapted from [13] and [26]. Moment arm values: d1 = 30 cm, d2 = 50 cm [12].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod1-2700395-large.gif
2017,7917366,Fig. 2.,"Block diagram of the actor-critic reinforcement learning (RL) controller [19] with human-generated and pseudo-human rewards, to control a simulated planar human arm. TD error is temporal difference error. State variables consist of 2 current-value joint angles, 2 target-value joint angles, and 2 angular velocities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod2-2700395-large.gif
2017,7917366,Fig. 9.,"Screenshot of the arm visualization GUI used in experiments. The green dot represents the target, and the green ring indicates the target zone, which was not displayed during experiments involving human-generated rewards. For each episode, the dot representing the hand (adjacent to the wrist) had the goal of reaching and remaining at the target dot, for a variety of different tasks. Note that the text labels in this figure were not present during the testing sessions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod9-2700395-large.gif
2017,7917366,Fig. 3.,Design of 50-task video animation rating experiment. The design displayed was used for each of 10 human subjects; sample data and results from Subject 6 (representing typical reward assignment consistency behavior) are shown.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod3-2700395-large.gif
2017,7917366,Fig. 4.,"Calculation of reward consistency value for a single subject; data for Subject 6, representing typical reward assignment consistency among the 10 subjects, is shown.
μ
is mean,
σ
is standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod4-2700395-large.gif
2017,7917366,Fig. 5.,"Dwell-at-target success over the final 100 episodes of each of five 500-episode reinforcement learning (RL) controller training sessions. Learning resulting from 10 individual human subjects are shown as thin trendlines, and the mean human dwell-at-target success is shown as the thick solid blue trendline. All standard error bars represent 95% confidence intervals with averages over 10 runs of the same controller training condition in the center; the human data set (thick solid blue trendline) is averaged over all 10 human subjects. Means and confidence intervals have been offset on the x-axis between the conditions for visual clarity. Controllers trained using human rewards significantly outperformed those trained using automated rewards starting at Session 2 (
p=0.03
, FDR-adjusted
q=0.03
), and maintained this advantage over the remaining sessions. In Session 5, pairwise t-tests showed significant differences between automated rewards and both the human-generated (
p=0.0001
, FDR-adjusted
q=0.0002
) and pseudo-human rewards training conditions (
p<0.0001
, FDR-adjusted
q<0.0001
). No significant difference (
p=0.07
, FDR-adjusted
q=0.07
) was observed between the human-generated and pseudo-human rewards conditions for the final session’s dwell-at-target success values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod5-2700395-large.gif
2017,7917366,Fig. 6.,"Mean rewards over five 500-episode reinforcement learning (RL) controller training sessions. Rewards resulting from 10 individual human subjects are shown as thin trendlines, and the mean human reward value is shown as the thick solid blue trendline. All standard error bars represent 95% confidence intervals with averages over 10 runs of the same controller training condition in the center; the human data set (thick solid blue trendline) is averaged over all 10 human subjects. Means and confidence intervals have been offset on the x-axis between the conditions for visual clarity. Pseudo-human rewards are significantly more positive than human-generated rewards for all 5 sessions (Table II).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod6-2700395-large.gif
2017,7917366,Fig. 7.,"Positive:negative final-timestep reward ratio for the initial (Session 1: Panel (a)) and final (Session 5: Panel (b)) data collection sessions of reinforcement learning (RL) controller training. H1 – H10 denote reward ratios from human subjects 1 – 10; C1 – C10 denote reward ratios from computer-generated pseudo-human reward algorithm (Algorithm 2). For human reward ratios, blue dots indicate net-positive reward ratios, and red x markers indicate net-negative ratios. Black dashed line shows the division between net-positive and net-negative reward ratios.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod7ab-2700395-large.gif
2017,7917366,Fig. 8.,"Testing of reinforcement learning (RL) controllers trained using computer-generated pseudo-human and human-generated rewards. Boxplots show 10 sets of 500 tested tasks per reward condition (10 controllers trained by using pseudo-human rewards and 10 controllers trained by human subjects; 1 controller per subject). Stars indicate statistically significant differences between the two training conditions. (a) Dwell-at-target success percentages. (98.26 ± 1.18% for the human-trained controllers, 99.20 ± 0.41% for the pseudo-human-trained controllers). Kolmogorov-Smirnov analysis revealed a significant difference between the human and pseudo-human rewards conditions: pseudo-human rewards outperformed human rewards (
D=0.60
;
p=0.03
). (b) Mean time (in seconds) required to achieve the dwell state (1.01 ± 0.07 s vs. 0.95 ± 0.02 s for human-trained vs. pseudo-human reward-trained). Kolmogorov-Smirnov analysis showed a significant difference between the two cases (
D=0.60
;
p=0.03
), with the pseudo-human rewards condition achieving the dwell state more quickly. (c) Mean overshoot of target (in cm): 12.83 ± 0.92 cm for human-trained controllers vs. 11.58 ± 0.63 cm for pseudo-human reward-trained controllers. The Kolmogorov-Smirnov test showed a significant difference between the two reward conditions, with the pseudo-human rewards having smaller overshoot than human rewards (
D=0.60
;
 p=0.03
). In these plots, the red central line indicates the median; upper and lower limits of the blue boxes indicate the upper and lower quartiles, respectively; and the black horizontal lines above and below each box indicate the maximum and minimum values, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8082143/7917366/jagod8abc-2700395-large.gif
2017,8051033,FIGURE 1.,Overview of the data mining and analytics methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge1-2756872-large.gif
2017,8051033,FIGURE 2.,Machine learning methods applied in the process industry.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge2-2756872-large.gif
2017,8051033,FIGURE 3.,Illustration of the PCA method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge3-2756872-large.gif
2017,8051033,FIGURE 4.,Illustration of SVDD method [119].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge4-2756872-large.gif
2017,8051033,FIGURE 5.,Application status of unsupervised learning methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge5-2756872-large.gif
2017,8051033,FIGURE 6.,Application status of unsupervised learning methods in different aspects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge6-2756872-large.gif
2017,8051033,FIGURE 7.,Illustration of the PLS method [143].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge7-2756872-large.gif
2017,8051033,FIGURE 8.,Illustration of the FDA method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge8-2756872-large.gif
2017,8051033,FIGURE 9.,Structure of a three-layer neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge9-2756872-large.gif
2017,8051033,FIGURE 10.,Illustrations of NN for classification and regression [231].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge10-2756872-large.gif
2017,8051033,FIGURE 11.,Illustration of Gaussian process regression method for both prior and posterior distributions [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge11-2756872-large.gif
2017,8051033,FIGURE 12.,Application status of supervised learning methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge12-2756872-large.gif
2017,8051033,FIGURE 13.,Application statuses of supervised learning methods in different aspects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8051033/ge13-2756872-large.gif
2017,7549098,Fig. 1.,"α–β reference frame, the f–t reference frame, and the d–q reference frame.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang1-2602243-large.gif
2017,7549098,Fig. 2.,"Relationship between torque command and the optimal stator flux for MTPA operation based on machine parameters in Table–I. The point d is the optimal flux amplitude corresponding to
T
∗
e
and the point d’ is the interpolated flux amplitude based on points c and e , which will be detailed in Section III.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang2-2602243-large.gif
2017,7549098,Fig. 3.,Torque and voltage amplitude variations with the flux amplitude for a given current amplitude.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang3-2602243-large.gif
2017,7549098,Fig. 4.,Schematic of the proposed control scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang4-2602243-large.gif
2017,7549098,Fig. 5.,Details of the virtual signal injection compensation unit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang5-2602243-large.gif
2017,7549098,Fig. 6.,Flowchart of SLC algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang6-2602243-large.gif
2017,7549098,Fig. 7.,Responses of torque and stator flux amplitude to rapid reference torque changes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun7-2602243-large.gif
2017,7549098,Fig. 8.,Responses of reference flux amplitude and SLC outputs when reference torque changes slowly.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun8-2602243-large.gif
2017,7549098,Fig. 9.,"Responses of the reference torque, the reference flux amplitude, and the resultant torque before and after PM flux linkage change at 1000 r/min.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang9-2602243-large.gif
2017,7549098,Fig. 10.,Responses of reference torque and resultant torque when speed steps between 1000 and 3000 r/min.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun10-2602243-large.gif
2017,7549098,Fig. 11.,IPMSM test rig.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang11-2602243-large.gif
2017,7549098,Fig. 12.,Responses of resultant d-axis current and ideal MTPA d-axis current to reference torque changes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun12-2602243-large.gif
2017,7549098,Fig. 13.,MTPA quality indicator SPO and the resultant d-axis current.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/wang13-2602243-large.gif
2017,7549098,Fig. 14.,MTPA d-axis current and response of resultant d-axis current to reference torque change from 13 to 18 N·m.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun14-2602243-large.gif
2017,7549098,Fig. 15.,Measured torque in response to reference torque change.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun15-2602243-large.gif
2017,7549098,Fig. 16.,Speed and measured d-axis current during transition from the field-weakening region to the constant-torque region.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun16-2602243-large.gif
2017,7549098,Fig. 17.,"Maximum voltage amplitude, the reference voltage amplitude, and the measured d-axis current when torque reference steps from 20 to 25 N·m at 3000 r/min.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun17-2602243-large.gif
2017,7549098,Fig. 18.,Comparison between the reference torque and the measured torque when the reference torque increases from 20 to 25 N·m at 3000 r/min.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/63/7851098/7549098/sun18-2602243-large.gif
2017,7953665,Fig. 1.,"The flowchart of DDML method for face verification. Given a pair of face images
x
1
and
x
2
, we map them into the same feature space
h
(2)
1
and
h
(2)
2
by learning a set of hierarchical nonlinear transformations, where the similarity between their outputs at the top level of the network is computed to determine whether the pair is from the same person or not.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu1-2717505-large.gif
2017,7953665,Fig. 2.,Illustration of the basic idea of the proposed DDML method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu2-2717505-large.gif
2017,7953665,Algorithm 1,DDML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu7-2717505-large.gif
2017,7953665,Fig. 3.,"The flowchart of DDMML method for face verification. For a given pair of face images
x
i
and
x
j
, we first extract
K
features for each image and map them into
K
different feature subspaces
h
(2)
k,i
and
h
(2)
k,j
(
k=1,2,⋯,K
) by jointly learning
K
sets of hierarchical nonlinear transformations, where the similarity of their outputs at the most top level is adaptively combined and weighted to determine whether the given pair is from the same person or not.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu3-2717505-large.gif
2017,7953665,Algorithm 2,DDMML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu8-2717505-large.gif
2017,7953665,Fig. 4.,"ROC comparison between our methods and the state-of-the-art face verification methods on LFW under the (a) image restricted and (b) image unrestricted settings, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu4ab-2717505-large.gif
2017,7953665,Fig. 5.,ROC curves comparison of different methods on YTF under image restricted setting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu5-2717505-large.gif
2017,7953665,Fig. 6.,"Convergence curves of DDML and DDMML on the (a) LFW and (b) YTF datasets, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7953665/lu6ab-2717505-large.gif
2017,7470473,Fig. 1.,"Architecture of the ELM algorithm.
d
is the dimension of the input data and
L
denotes the number of hidden layer neurons.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao1-2558842-large.gif
2017,7470473,Fig. 2.,"(a) System architecture of the mixed-signal integrated circuit that implements the first stage of ELM; the second stage is implemented in digital domain. The digital input data are converted to current by the IGC and then multiplied by random weights
w
ij
in the current mirror array. The current is converted to digital domain by the combination of a spiking neuron and a counter. (b) Timing diagram of the ELM system where RN
_in
is a global reset, and
Data_in
,
A
, and
CLK_in
are SPI control signals to transfer input data to the IC.
NEU_EN
enables the neuron to produce spikes, while
CLK_cnt
is used to read out the counter values
C
one by one.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao2ab-2558842-large.gif
2017,7470473,Fig. 3.,"Schematic of IGC for one channel. A reference current is split according to the 10 b of input data to create
I
DAC
. The capacitor
C
ensures sufficient SNR when the current is mirrored to the
L
columns. An active current mirror is enabled to allow fast settling when
I
DAC
is small.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao3-2558842-large.gif
2017,7470473,Fig. 4.,"(a) Schematic of the neuronal oscillator circuit followed by an asynchronous counter. The neuron is enabled when control signal
NEU_EN
is high. The capacitors can be digitally reconfigured and have the following values:
C
a1
=100
fF,
C
a2
=200
fF,
C
b1
=50
fF, and
C
b2
=100
fF. (b) Oscillation waveforms at different nodes of the neuron circuit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao4ab-2558842-large.gif
2017,7470473,Fig. 5.,"(a) Neuron spiking frequency initially increases with the increase in the input current
I
z
till
I
z
=
I
flx
. It then reduces and becomes zero finally when
I
z
=
I
rst
. (b) Transfer function (solid line) of the neuron with input
I
z
and output
H
can be saturated at a predefined value of
2
b
by stopping the counter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao5ab-2558842-large.gif
2017,7470473,Fig. 6.,(a) Comparison of neuron spiking frequency between theory and simulation in SPICE show close match. (b) Simulated neuron spiking frequency with increasing input current for three different VDD. The curves saturate at higher maximum frequencies for higher VDD. Note the logarithmic scales for both plots.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao6ab-2558842-large.gif
2017,7470473,Fig. 7.,"Design space exploration. (a) Variations of
L
min
with
I
z
sat
/
I
z
max
show that the optimal value of this ratio is
≈0.75
. (b) Variations of classification accuracy with the resolution of output weight
β
showing 10 b is sufficient for accurate classification. (c) Variations of classification accuracy with the NOB of counter output
H
demonstrating that
b≈6
is enough for optimal performance. Each of the curves are averaged over 50 trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao7abc-2558842-large.gif
2017,7470473,Fig. 8.,Simplified circuit diagram of one current mirror for noise analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao8-2558842-large.gif
2017,7470473,Fig. 9.,"Tradeoffs in speed. (a) Using active current mirror for small input currents can boost the BW by
5.84×
. (b) Variations of neuron counting time (
T
neu
) and current mirror settling time (
T
cm
) reduce as maximum input current per dimension (
I
max
) is increased. Further,
T
neu
increases exponentially with increase in
b
. (c) Contours where
T
cm
is equal to
T
neu
in the space of counter dynamic range
2
b
and input dimension
d
. For increasing
d
, the total current input to a neuron
I
z
keeps on increasing, thus increasing the oscillation frequency. Hence, it can support higher dynamic range
2
b
in the same time
T
neu
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao9abc-2558842-large.gif
2017,7470473,Fig. 10.,"(a) Variation of energy per classification operation (
E
c
) with a varying maximum value of input current
I
z
max
for three different settings of VDD. (b) Same as in (a) but replacing
I
z
max
with its corresponding
T
neu
from (19).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao10ab-2558842-large.gif
2017,7470473,Fig. 11.,"Extension from a
2×3
random projection matrix to
6×6
by weight reuse technique.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao11-2558842-large.gif
2017,7470473,Fig. 12.,(a) Schematic of the peripheral circuit for hidden layer extension by shifting the input data stored in the registers and (b) its timing diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao12ab-2558842-large.gif
2017,7470473,Fig. 13.,(a) Schematic of circuit for input dimension extension by shifting and summing the output counter values and (b) its timing diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao13ab-2558842-large.gif
2017,7470473,Fig. 14.,"Die photo of the prototype chip fabricated in a 0.35-
μm
CMOS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao14-2558842-large.gif
2017,7470473,Fig. 15.,"(a) Measured transfer function of hidden layer neurons when the digital input varying from 0 to 1023 with
d=1
and
T
neu
=10
ms. (b) Surface plot showing the mismatch in weights of the
128×128
current mirror synapses. The output counter values for different neurons are plotted for
T
neu
=10
ms when
Data_in=100
is set on each input channel one by one. (c) Histogram showing the log-normal distribution of the input weights obtained from (b) for the
128×128
current mirror array.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao15abc-2558842-large.gif
2017,7470473,Fig. 16.,Regression of underlying sinc function (blue curve) based on a set of noisy samples (green dots).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao16-2558842-large.gif
2017,7470473,Fig. 17.,Comparison of hidden layer outputs for three different values of VDD in (a) conventional case and (b) normalized case. The normalization results in less variation of output due to change in VDD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao17ab-2558842-large.gif
2017,7470473,Fig. 18.,Comparison of performance when normalized and non-normalized hidden layer outputs are used for classification of (a) Australian Credit and (b) Brightdata sets from the UCI repository.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7797597/7470473/yao18ab-2558842-large.gif
2017,7913581,Fig. 1.,"The Conditional High-order Boltzmann Machine (CHBM), Latent CHBM and its factored version. (a) x, y and z are two input layers and a class label layer, respectively. The connections among the three layers are three-way multiplicative interactions, denoted by a three-order weight tensor W. (b) h is a hidden layer. h and z are fully connected by a weight matrix U. (c)
W
x
,
W
y
and
W
h
are three sets of parameters (filters) which project x, y and h into a set of hidden “factors”.
f
x
,
f
y
and
f
h
are the corresponding filter responses.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang1abc-2698918-large.gif
2017,7913581,Algorithm 1,The Generative Pretraining of Latent CHBM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang10-2698918-large.gif
2017,7913581,Fig. 2.,"The CHBM, Gated CHBM and its factored version. (b) x, y, h and z are two input layers, a hidden layer, and a class layer, respectively. The connections among x, y, h and z are four-way multiplicative interactions, represented by a four-order weight tensor W. (c)
W
x
,
W
y
,
W
h
and
W
z
are four sets of filters which project x, y, h and z to a set of latent “factors”.
f
x
,
f
y
,
f
h
and
f
z
are the corresponding filter responses.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang2abc-2698918-large.gif
2017,7913581,Fig. 3.,"The illustration of multiplicative interactions between class and latent variables. The two variables of z with different edge colors represent two relation classes. The four variables of h with different textures represent four environments. The outer product
z
h
T
represents eight subclasses with different combinations of edge colors and textures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang3-2698918-large.gif
2017,7913581,Algorithm 2,The Generative Pretraining of Gated CHBM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang11-2698918-large.gif
2017,7913581,Fig. 5.,"Samples in the
basic
,
rot
,
bg
-
img
and
bg
-
rand
datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang5-2698918-large.gif
2017,7913581,Fig. 4.,"ROC curves of invariant recognition obtained by all the compared methods on the
basic
,
rot
,
bg
-
rand
and
bg
-
img
datasets. (a)
basic
. (b)
rot
. (c)
bg
-
rand
. (d)
bg
-
img
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang4abcd-2698918-large.gif
2017,7913581,Fig. 6.,"T-SNE visualization of predicted similarities by MorphBM (a~d), CHBM (e~h), Gated CHBM (i~l) and Latent CHBM (m~p)) on the
basic
,
rot
,
bg
-
rand
and
bg
-
img
datasets. (a) MorphBM (
basic
). (b) MorphBM (
rot
). (c) MorphBM (
bg
-
rand
). (d) MorphBM (
bg
-
img
). (e) CHBM (
basic
). (f) CHBM (
rot
). (g) CHBM (
bg
-
rand
). (h) CHBM (
bg
-
img
). (i) Gated CHBM (
basic
). (j) Gated CHBM (
rot
). (k) Gated CHBM (
bg
-
rand
). (l) Gated CHBM (
bg
-
img
). (m) Latent CHBM (
basic
). (n) Latent CHBM (
rot
). (o) Latent CHBM (
bg
-
rand
). (p) Latent CHBM (
bg
-
img
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang6abcdefghijklmnop-2698918-large.gif
2017,7913581,Fig. 7.,"ROC curves of face verification by state-of-the-art models on the LFW dataset, under the protocols of restricted and unrestricted, label-free outside data. (a) Restricted, label-free outside data. (b) Unrestricted, label-free outside data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang7ab-2698918-large.gif
2017,7913581,Fig. 8.,"ROC curves of face verification by state-of-the-art methods on the LFW dataset, under the protocol of unrestricted, labeled outside data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang8-2698918-large.gif
2017,7913581,Fig. 9.,ROC curves of action similarity labeling by state-of-the-art models on the ASLAN dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7956620/7913581/wang9-2698918-large.gif
2017,8057769,FIGURE 1.,Framework of incremental and decremental ELMs based generalized inverse.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8057769/jin1-2758645-large.gif
2017,8057769,FIGURE 2.,"Learning updating curves of EI-ELM, CI-ELM, EM-ELM, N-IELM on different datasets, where different colors stand for different datasets, dash lines stand for the training results, solid lines stand for the testing results. (a) Space-ga. (b) Yaleb.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8057769/jin2ab-2758645-large.gif
2017,8057769,FIGURE 3.,"Training time of OS-ELM, S-IELM by chunk size (1–100) on Isolet, where the number of hidden nodes is 500.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8057769/jin3-2758645-large.gif
2017,8057769,FIGURE 4.,"Classification rate of OS-ELM, S-IELM by learning step on Isolet, where the number of hidden nodes is 500. In the first 500 step of initialization phase the chunk size is 1, and in the sequential learning phase the chunk size is 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8057769/jin4-2758645-large.gif
2017,8057769,FIGURE 5.,"Performance of S-DELM on Isolet and Coil-100, where the type and number of hidden nodes are sigmoid 500. On Isolet, samples of 10 classes in 26 are deleted; on Coil-100, samples of 30 classes in 100 are deleted. (a) Cpu time. (b) Classification rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8057769/jin5ab-2758645-large.gif
2017,7908958,Fig. 1.,PH-ELM overview.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li1-2690673-large.gif
2017,7908958,Fig. 2.,Proposed format. (a) RDST and BRDST. (b) SRDST.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li2ab-2690673-large.gif
2017,7908958,Fig. 3.,GFlink architecture. (a) Architecture of a work node. (b) Heterogeneous task management.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li3ab-2690673-large.gif
2017,7908958,Fig. 4.,Acceleration of CPBMM and ATHMM by GPUs. (a) GCPBMM. (b) GATHMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li4ab-2690673-large.gif
2017,7908958,Fig. 5.,Performance results overview. (a) Performance speedup of small DSTs. (b) Performance speedup of medium DSTs. (c) Average running time of large-scale DSTs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li5abc-2690673-large.gif
2017,7908958,Fig. 6.,"Performance results under different circumstances. Average running time under different: (a) data sizes, (b) numbers of slave nodes, and (c) numbers of hidden nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li6abc-2690673-large.gif
2017,7908958,Fig. 7.,Performance comparison with PELM and ELM*.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li7-2690673-large.gif
2017,7908958,Fig. 8.,Performance comparison of CPBMM and GCPBMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li8-2690673-large.gif
2017,7908958,Fig. 9.,Performance comparison of ATHMM and GATHMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li9-2690673-large.gif
2017,7908958,Fig. 10.,Performance comparison of different GPUs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7908958/li10-2690673-large.gif
2017,7390064,Fig. 1.,Learning architecture of the eT2ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7818050/7390064/prata1-2514537-large.gif
2017,7390064,Fig. 2.,Interval type-2 Gaussian activation function with uncertain means.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7818050/7390064/prata2-2514537-large.gif
2017,7390064,Fig. 3.,Flowchart of eT2ELM algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7818050/7390064/prata3-2514537-large.gif
2017,7390064,Fig. 4.,(a) Hidden node evolution in car+ problem. (b) Input variable evolution in car+ problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7818050/7390064/prata4ab-2514537-large.gif
2017,7866831,Fig. 1.,SE-INC-SVM structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang1-2667703-large.gif
2017,7866831,Fig. 2.,Equivalent normalized RBF network for neighborhood KE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang2-2667703-large.gif
2017,7866831,Fig. 3.,Flowchart of SE-INC-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang3-2667703-large.gif
2017,7866831,Fig. 4.,Training MSE of different label ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang4-2667703-large.gif
2017,7866831,Fig. 5.,Training time of different label ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang5-2667703-large.gif
2017,7866831,Fig. 6.,Test MSE of different label ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang6-2667703-large.gif
2017,7866831,Fig. 7.,"Train error and predict error with different (a)
C
and (b)
σ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang7ab-2667703-large.gif
2017,7866831,Fig. 8.,"Train error and predict error with different
ε
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang8-2667703-large.gif
2017,7866831,Fig. 9.,Penicillin fermentation process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang9-2667703-large.gif
2017,7866831,Fig. 10.,Biomass concentration estimation result. (a) No noise. (b) 3% noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang10ab-2667703-large.gif
2017,7866831,Fig. 11.,Absolute error for biomass concentration estimation. (a) No noise. (b) 3% noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang11ab-2667703-large.gif
2017,7866831,Fig. 12.,Penicillin concentration estimation result. (a) No noise. (b) 3% noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang12ab-2667703-large.gif
2017,7866831,Fig. 13.,Absolute error for penicillin concentration estimation. (a) No noise. (b) 3% noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8038140/7866831/wang13ab-2667703-large.gif
2017,7898513,Fig. 1.,Proposed approach versus existing approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba1-2693394-large.gif
2017,7898513,Fig. 2.,VCP and VSM on the load demand - voltage magnitude curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba2-2693394-large.gif
2017,7898513,Fig. 3.,The pool based active learning scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba3-2693394-large.gif
2017,7898513,Fig. 4.,"Integration of the proposed active learning approach in power systems data acquisition infrastructure, representing data flow.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba4-2693394-large.gif
2017,7898513,Fig. 5.,One-line diagram of the WECC 179-bus equivalent system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba5-2693394-large.gif
2017,7898513,Fig. 6.,"Voltage stability prediction using ANNs: a) training time per OP, (b) testing time per OP, and (c) number of labeled OPs (c) on the
x
-axis; accuracy on the y-axis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba6abc-2693394-large.gif
2017,7898513,Fig. 7.,"Voltage stability prediction using RFs: a) training time per OP, (b) testing time per OP, and (c) number of labeled OPs (c) on the x-axis; accuracy on the y-axis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba7abc-2693394-large.gif
2017,7898513,Fig. 8.,"Voltage stability prediction using SVMs: a) training time per OP, (b) testing time per OP, and (c) number of labeled OPs (c) on the x-axis; accuracy on the y-axis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/8075182/7898513/malba8abc-2693394-large.gif
2017,7752978,Fig. 1.,Damaged/Operational states of electric power grid components separated by the decision boundary.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/7951114/7752978/khoda1-2631895-large.gif
2017,7752978,Fig. 2.,The Sigmoid function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/7951114/7752978/khoda2-2631895-large.gif
2017,7752978,Fig. 3.,Proposed cost function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/7951114/7752978/khoda3-2631895-large.gif
2017,7891506,Fig. 1.,"Illustration of the performance of the pseudo adversary model, legitimate case and illegitimate case in the measure space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/7972692/7891506/wang1-2690437-large.gif
2017,7891506,Fig. 2.,Detection performance of PLA based on ELM with 2000 training data in the OFDM system with 256 sub-carrier under Rayleigh channel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/7972692/7891506/wang2abc-2690437-large.gif
2017,7976374,Fig. 1.,(a) Diagram illustrating the adaptive demodulator based on CNN; (b) Numerical model of the OAM-SK-FSO communication system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7997984/7976374/li1ab-2726139-large.gif
2017,7976374,Fig. 2.,"(a), (c) and (e) are phase perturbation caused by AT with
C
2
n
valued in
1×
10
−16
m
−2/3
,
1×
10
−15
m
−2/3
and
1×
10
−14
m
−2/3
respectively. And (b), (d) and (f) are intensity patterns of received LG beams carrying 16 kinds of OAM modes
(l=±1
, ±2, ±3, ±4, ±5, ±6, ±7, ±8, ±9, ±10, ±11, ±12,±13, ±14, ±15, ±16).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7997984/7976374/li2abcdef-2726139-large.gif
2017,7976374,Fig. 3.,"(a). Demodulating accuracy of the adaptive demodulator based on CNN for 8-OAM system
(l=±1
, ±2, ±3, ±4, ±5, ±6, ±7, ±8, ±9, ±10, ±11, ±12,±13, ±14, ±15) with 1000-
m
transmission distance; (b). Illustration of feature maps of CNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7997984/7976374/li3ab-2726139-large.gif
2017,7976374,Fig. 4.,"Demodulating accuracy of adaptive demodulators based on CNN, BP-ANN and KNN under the case of (a) 1000-
m
transmission distance and (b)
C
2
n
set to be
1×
10
−15
m
−2/3
for 4-OAM
(l=±1
, ±4, ±7, ±11), 8-OAM
(l=±1
, ±3, ±5, ±7, ±9, ±11, ±13, ±15) and 16-OAM system
(l=±1
, ±2, ±3, ±4, ±5, ±6, ±7, ±8, ±9, ±10, ±11, ±12,±13, ±14, ±15, ±16).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/7997984/7976374/li4ab-2726139-large.gif
2017,7738596,Fig. 1.,(a) Resist image with small and large spaces and (b) corresponding etch result with negative and positive etch biases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim1ab-2626304-large.gif
2017,7738596,Fig. 2.,EPC and OPC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim2-2626304-large.gif
2017,7738596,Fig. 3.,"(a) Synthetic patterns and etch bias rule table used in RB-EPC, and (b) kernels for MB-EPC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim3ab-2626304-large.gif
2017,7738596,Fig. 4.,"The proposed etch bias model: (a) each segment in a layout is parameterized, and (b) an ANN is constructed to predict etch bias from input parameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim4ab-2626304-large.gif
2017,7738596,Fig. 5.,Preparations of complicated training segments: (a) segments extracted from test layout and (b) representative segments after classifying the segments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim5ab-2626304-large.gif
2017,7738596,Fig. 6.,Parameterization of a segment using local pattern densities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim6-2626304-large.gif
2017,7738596,Fig. 7.,(a) Parameterization of a segment using optical kernel signal and (b) some optical kernels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim7ab-2626304-large.gif
2017,7738596,Fig. 8.,ANN for (a) regression and (b) classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim8ab-2626304-large.gif
2017,7738596,Fig. 9.,The EPC algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim9-2626304-large.gif
2017,7738596,Fig. 10.,"(a) Current litho pattern layout (
L
), (b) expected etch pattern (
D
), and (c) EPE measured between the expected etch pattern (
D
) and a design layout (
D
in
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim10abc-2626304-large.gif
2017,7738596,Fig. 11.,RMS error of etch bias for training (white) and test (black) patterns.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim11-2626304-large.gif
2017,7738596,Fig. 12.,Histogram of CD errors on a wafer after MB-EPC and ML-EPC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/7831499/7738596/shim12-2626304-large.gif
2017,7856983,Fig. 1.,"Bistatic and aspect angles defined relative to source, target, and receiver.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch1-2650759-large.gif
2017,7856983,Fig. 2.,"Mean radiation patterns for different cylinder rotations. The location of minima and maxima within the patterns shift with the angle
γ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch2-2650759-large.gif
2017,7856983,Fig. 3.,"Configuration for the Massachusetts Bay experiment, including source and target positions. The R/V Resolution, with the acoustic source deployed at 3-m depth, was first anchored about 100 m north of the target, then moved to approximately 100 m west of the target. Water depth at the experiment site was between 15 and 18 m.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch3-2650759-large.gif
2017,7856983,Fig. 4.,"Open-ended steel pipe used as a target during the Massachusetts Bay experiment, sitting on the deck of the R/V Resolution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch4-2650759-large.gif
2017,7856983,Fig. 5.,The AUV Unicorn being lifted from the water by crane.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch5-2650759-large.gif
2017,7856983,Fig. 6.,"Broadside sampling behavior used to collect uniform-quality bistatic acoustical scattering data from a target. The AUV maintains constant radius from the target, changing radius and going out of broadside in the direct forward-scatter direction where the source and target arrivals are difficult to distinguish in time. The behavior is configured using min and max radii (
r
min
and
r
max
), a number of radii
n
r
, and a transition region width
β
. The spiral shown here is repeated at
n
z
evenly spaced depths between minimum depth
z
min
and maximum depth
z
max
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch6-2650759-large.gif
2017,7856983,Fig. 7.,"Processing in pActiveTargetProcess used to extract target amplitudes from the array time series. The recorded data file, vehicle/target location information, and replica are used to estimate the target scattering amplitude.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch7-2650759-large.gif
2017,7856983,Fig. 8.,"Waveguide properties used in OASES-SCATT simulations. The waveguide consists of an air half-space, water layer, and fluid sand half-space approximating experiment conditions during the Massachusetts Bay experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch8-2650759-large.gif
2017,7856983,Fig. 9.,"Simulated scattering strengths in decibels for a cylinder with 45
∘
aspect versus sampling location: (a) 3-m depth; (b) 7-m depth; and (c) 11-m depth.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch9-2650759-large.gif
2017,7856983,Fig. 10.,"Feature space mapping used for model training and regression analysis. A sample at a bistatic angle
θ
is mapped to a feature number
F
based on the value of
θ
and feature width
Δθ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch10-2650759-large.gif
2017,7856983,Fig. 11.,"Sampling for (a) null, (b) first, and (c) second target aspect bistatic scattering acoustical data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch11-2650759-large.gif
2017,7856983,Fig. 12.,"Unnormalized scattering amplitude maps for 5-m depth for the two target aspects during the Massachusetts Bay experiment. The target is located at (0,0). The source is located at approximately
(−100,0)
for both target aspects. (a) Scattering amplitude map for first target aspect. (b) Scattering amplitude map for second target aspect.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch12-2650759-large.gif
2017,7856983,Fig. 13.,"Probability density function over the estimated angle for varying values of
N
for the two target aspects. The first aspect converges to a value of
γ
~
=
110
∘
, the second aspect converges to a value of
γ
~
=
36
∘
. (a) Aspect 1. (b) Aspect 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch13-2650759-large.gif
2017,7856983,Fig. 14.,"Radiation pattern for the first aspect of the steel pipe, estimated to have rotation 110
∘
, versus a simulated fluid-filled cylinder with a rotation of 110
∘
. The arrow indicates the source direction. (a) Measured scattering field data for target aspect angle 1. (b) Simulated scattering field data for target aspect angle 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch14-2650759-large.gif
2017,7856983,Fig. 15.,"Radiation pattern for the second aspect of the steel pipe, estimated to have rotation 36
∘
, versus a simulated fluid-filled cylinder with a rotation of 35
∘
. The arrow indicates the source direction. (a) Measured scattering field data for target aspect angle 2. (b) Simulated scattering field data for target aspect angle 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch15-2650759-large.gif
2017,7856983,Fig. 16.,Pearson correlations between experiment and simulation scattering amplitudes at each simulated target aspect angle. Multiple local maxima are observed for both target aspects. (a) Correlation versus aspect angle for target aspect angle 1. (b) Correlation versus aspect angle for target aspect angle 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch16-2650759-large.gif
2017,7856983,Fig. 17.,"Expected change in the target aspect minus change in the target aspect for combinations of correlation maxima. White is a large difference, and black is a zero difference. Only the
γ
1
=
108
∘
,
γ
2
=
40
∘
combination results in an error of less than 20
∘
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8063254/7856983/fisch17-2650759-large.gif
2017,7565615,Fig. 1.,"Two Internet video examples, where the same event “Rock Climbing” happened in very different time frames. The number in each frame indicates its saliency score, which describes how this keyframe is relevant to the specified event. We use this saliency information to prioritize the video shot representations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7963909/7565615/yu1-2608901-large.gif
2017,7565615,Fig. 2.,"Each input video is divided into multiple shots, and each event has a short textual description. CNN is used to extract features (Section 3.1). ImageNet concept names and skip-gram model are used to derive a probability vector (Section 3.2) and a relevance vector (Section 3.3), which are combined to yield the new semantic saliency and used for prioritizing shots in the classifier training (Section 3.4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7963909/7565615/yu2-2608901-large.gif
2017,7565615,Fig. 3.,"Qualitative analysis of the prioritization effect. A positive test video from event “Horse Riding Competition” is used as an example. The first row shows the original video shots; the second row depicts prioritized video shots, having its weight (in norm) on the bottom left and semantic saliency on the bottom right; and the third row presents the most salient concepts detected in these shots.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7963909/7565615/yu3-2608901-large.gif
2017,7565615,Fig. 4.,"Performance sensitivity w.r.t.
λ
,
γ
and initialization on the TRECVID MEDTest 2014 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/7963909/7565615/yu4-2608901-large.gif
2017,7358151,Fig. 1.,"(a) Shannon’s entropy
H(X)=−∫p(x)logp(x)dx
and Rényi’s entropy
R
α
(X)=−(1/α−1)log∫p(x
)
α
dx
. (b) QRE estimation in (6). (a)
R
α
(X)
is evaluated by setting
α
to three values:
α=1.5
,
α=2
(QRE), and
α=5
. A random variable
X
with two events is adopted in this example, where the probabilities of these two events are
p
and
1−p
, respectively. (b) QRE is estimated from three samples, in which two are fixed at 0 and 2. The QRE estimation is plotted over variation of the third sample within
[0,2]
. The Gaussian kernels
k(
x
i
,
x
j
)=exp(−(
x
i
−
x
j
)
2
/2
σ
2
)
,
i,j=1,2,3
, are adopted to generate the kernel matrix, where kernel width is set at
σ
2
=0.1
. (a) and (b) Entropy values of (5) and (6), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen1ab-2504382-large.gif
2017,7358151,Fig. 2.,"Input density function
p(x)
, the EPM
ζ(x)
, and the error
ϵ(x)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen2-2504382-large.gif
2017,7358151,Fig. 3.,Sample quantization model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen3-2504382-large.gif
2017,7358151,Fig. 4.,PQS and DQS quantization outputs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen4-2504382-large.gif
2017,7358151,Fig. 5.,Code region splitting mechanism in DQS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen5-2504382-large.gif
2017,7358151,Fig. 6.,"Comparison between PQS and DQS. (a) and (b) PQS outputs. (c) and (d) DQS outputs. The values of quantization cardinality are shown in parenthesis, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen6abcd-2504382-large.gif
2017,7358151,Fig. 7.,"Impact of quantization shrinkage threshold
δ
. A synthetic data set is generated from the Sinc function:
y(
x
i
)=sinc(
x
i
)+
v
i
,
i=1,…,1000
.
x
i
∈
R
2
is a 2-D Gaussian sequence and
v
i
denotes add-on Gaussian noise with standard deviation 0.1. The testing result is obtained from an average of 100 Monte-Carlo runs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen7-2504382-large.gif
2017,7358151,Fig. 8.,"Generalization performance versus input data density representations. A synthetic set is generated from the Sinc function:
y(
x
i
)=sinc(
x
i
)+
v
i
,
i=1,…,1000
, where
v
i
denotes a zero-mean white Gaussian noise with standard deviation of 0.1. It is noted that the samples are dense in the
x
-region
[0,2]
. (a) Examples of subset selections. (b) pdf discrepancy scores [defined by (14)]. Performance in terms of RMSE with respect to (c) selection methods, and with respect to (d) subset sizes. In (c), 10 samples are selected from the input data for each selection.
s
den1
-
s
den3
denote three selections with their highlighted regions
[−4,−2]
,
[0,2]
, and
[2,4]
, respectively.
s
uni
denotes the uniform selection and
s
rand
denotes the random selection from the
x
-region
[−4,4]
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen8abcd-2504382-large.gif
2017,7358151,Fig. 9.,"Comparison of LS-SVM, FSLS-SVM, DQLS-SVM and RSLS-SVM on a synthetic data set. (a) RMSE. (b) CPU time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7797565/7358151/chen9ab-2504382-large.gif
2017,7451273,Fig. 1.,IVDD SMO/EM optimizer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7949119/7451273/deche1-2547220-large.gif
2017,7451273,Fig. 2.,Isoprobability for the toy data set for (a) IVDD (circled import vectors) and (b) OC-SVM (circled support vectors) (for SVDD the result is similar).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7949119/7451273/deche2ab-2547220-large.gif
2017,7451273,Fig. 4.,"x
-axis: iterations number. Left side of the
y
-axis represents the cost function values and the right side represents the sparsity values in percentage. Dashed line: toy data set. Solid line: A versus All data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7949119/7451273/deche4-2547220-large.gif
2017,7451273,Fig. 3.,"Choice of
C
. Blue solid line: BER. Red dashed line: inside sample percentage. By varying
C
one can decide the radius of the sphere and thus get the desired fraction of inner/outer samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7949119/7451273/deche3-2547220-large.gif
2017,7501537,Fig. 1.,"Network and layer model in DL. (a) RBM in case of 4
×
4. (b) DBN consisting of three RBM layers and one fully connected neural network [1].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk1ab-2585675-large.gif
2017,7501537,Fig. 2.,"Demonstrated architecture on FPGA. (a) Block diagram of the RBM blocks consisting of sum-of-product, activation, and update calculations, which are connected with the shift registers. (b) Demonstration setup using a DE3 board, a touch interface for handwritten digits acquisition, and a PC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk2ab-2585675-large.gif
2017,7501537,Fig. 3.,Architectural simulation of processing speed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk3-2585675-large.gif
2017,7501537,Fig. 4.,Overall architecture flow. The memory blocks with injected errors are shown by gray boxes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk4-2585675-large.gif
2017,7501537,Fig. 5.,"Fault injection analyses based on a bit-flipping model for (a) initial hard faults, (b) random bit-flipping during RBMs learning, and (c) bit-flipping after learning of RBM. Fine-tuning starts from the seventh epoch (vertical dotted lines are a guide for the eyes).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk5abc-2585675-large.gif
2017,7501537,Fig. 6.,DBN classification accuracy versus error rate considering various error types. (a) Faults are injected into all the structural data after fine-tuning. (b) Accuracy after relearning with ten epochs of fine-tuning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk6ab-2585675-large.gif
2017,7501537,Fig. 7.,"BERs for SRAMs with varying
V
dd
values in 65-nm [10] and 90-nm [11] CMOS. Solid lines show fitted curves for the data measured using the analysis methods proposed in [10].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk7-2585675-large.gif
2017,7501537,Fig. 8.,"V
dd
dependence on BERs of SRAM made of (a) CMOS and an (b) inverter replacing the nMOS with a SONOS field-effect transistor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk8ab-2585675-large.gif
2017,7501537,Fig. 9.,"V
dd
dependence on BERs of SRAM made using (a) CMOS-Tr. and (b) SONOS-Tr. The
σ
SNM
of SONOS-Tr. after
V
th tuning is suitably lower than that of CMOS-Tr.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk9ab-2585675-large.gif
2017,7501537,Fig. 10.,"V
dd
dependence on (a) BERs of SRAM made of only CMOS or SONOS-combined CMOS. (b) Power consumption of the SRAM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/7886391/7501537/maruk10ab-2585675-large.gif
2017,7529065,Fig. 1.,An overview of the various machine learning methods applied to optical communication.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar1-2590989-large.gif
2017,7529065,Fig. 2.,"Diagram showing the experimental back-to-back setup. The power eyediagrams produced from photodetection are sampled to extract minimum and maximum variance features (A and B, respectively) for signal evaluation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar2-2590989-large.gif
2017,7529065,Fig. 3.,"Illustration of the application of the neural network for OSNR estimation. For the upper right figure, case A and B represent different sampling points in the eyediagram used to extract the variance for the back-to-back case and after 250 km of uncompensated transmission.
y:
non-linear mapping function that needs to be inferred from the training data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar3-2590989-large.gif
2017,7529065,Fig. 4.,"Classification results shown in data input space consisting of feature 5 and 8, corresponding to the mean and the absolute difference in variance respectively. Shown by triangles are classification errors preformed by a trained linear SVM. Feature space is conceptually illustrated with a probable decision boundary between two arbitary classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar4-2590989-large.gif
2017,7529065,Fig. 5.,Mean misclassification for 4-30 dB OSNR using a linear SVM classifier with 8 features and RC 0.01 pulseshaping.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar5-2590989-large.gif
2017,7529065,Fig. 6.,"Minimum variance measurements for QPSK, 8 QAM, 16 QAM and 64 QAM with RC 0.01 pulseshaping.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar6-2590989-large.gif
2017,7529065,Fig. 7.,OSNR estimation using minimum variance feature with trained NN for DP-64 QAM and RC 0.01 pulseshaping.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar7-2590989-large.gif
2017,7529065,Fig. 8.,Squared error of OSNR estimation for training and test data using DP-64 QAM and RC 0.01 pulseshaping.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar8-2590989-large.gif
2017,7529065,Fig. 9.,OSNR estimation after 250 km of dispersion uncompensated transmission.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/7869244/7529065/zibar9-2590989-large.gif
2017,7917345,Fig. 1.,Examples of the actions and typical activities for driving. (a) Examples of action (b) Examples of activity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong1-2693242-large.gif
2017,7917345,Fig. 2.,RSS for the acceleration activity with (a) high SNR and (b) low SNR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong2-2693242-large.gif
2017,7917345,Fig. 3.,"RSS and the gradient when the throttle is pressed and released, quickly for five times and then slowly for five times. (a) RSS sequence (b) Gradient series.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong3-2693242-large.gif
2017,7917345,Fig. 4.,RSS for the TP and TR actions when the throttle is located at different positions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong4-2693242-large.gif
2017,7917345,Fig. 5.,"RSS when the (a) clutch, (b) brake, and (c) throttle are pressed and then released.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong5-2693242-large.gif
2017,7917345,Fig. 6.,Illustration of the basic process of WiQ.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong6-2693242-large.gif
2017,7917345,Fig. 7.,A CNN for quality recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong7-2693242-large.gif
2017,7917345,Fig. 8.,Illustration of the fusion policy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong8-2693242-large.gif
2017,7917345,Fig. 9.,Experimental setup with a driving emulator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong9-2693242-large.gif
2017,7917345,Fig. 10.,Action recognition with high SNR and ten training instances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong10-2693242-large.gif
2017,7917345,Fig. 11.,Action recognition with high SNR and 100 training instances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong11-2693242-large.gif
2017,7917345,Fig. 12.,Action recognition with low SNR and 100 training instances.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong12-2693242-large.gif
2017,7917345,Fig. 13.,"Action recognition with multiple drivers, high SNR, and 1000 training instances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong13-2693242-large.gif
2017,7917345,Fig. 14.,Quality distribution for CP with (a) different drivers or (b) different receiver positions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong14-2693242-large.gif
2017,7917345,Fig. 15.,Body status recognition based on the quality of action with high SNR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong15-2693242-large.gif
2017,7917345,Fig. 16.,Accuracy of driver identification without fusion in the high-SNR category.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong16-2693242-large.gif
2017,7917345,Fig. 17.,Accuracy of driver identification with activity-based fusion in the high-SNR category.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong17-2693242-large.gif
2017,7917345,Fig. 18.,Accuracy of driver identification with activity-based fusion for all samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8105908/7917345/dong18-2693242-large.gif
2017,7967660,Fig. 1.,AGS with CD algorithm for the RBM model learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai1-2715171-large.gif
2017,7967660,Fig. 2.,Flow chart of the CD learning with AGS for RBM-based NN model learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai2-2715171-large.gif
2017,7967660,Fig. 3.,System architecture of the proposed RBM-P chip.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai3-2715171-large.gif
2017,7967660,Fig. 4.,Architecture of the proposed RBM core.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai4-2715171-large.gif
2017,7967660,Fig. 5.,Data layout of the NN model stored in the external memory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai5-2715171-large.gif
2017,7967660,Fig. 6.,Architecture of the proposed model reduction method. (a) Experimental results of model reduction. (b) Hardware architecture of CU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai6ab-2715171-large.gif
2017,7967660,Fig. 7.,Example of NN model reduction and decoding.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai7-2715171-large.gif
2017,7967660,Fig. 8.,Architecture of the proposed LPNB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai8-2715171-large.gif
2017,7967660,Fig. 9.,Architecture of the bias neurons generator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai9-2715171-large.gif
2017,7967660,Fig. 10.,(a) Timing diagram. (b) Experimental result of dynamic clock gating mechanism in the proposed LPNB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai10ab-2715171-large.gif
2017,7967660,Fig. 11.,(a) Region-based illustration. (b) Block diagram. (c) Timing diagram of the UDCM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai11abc-2715171-large.gif
2017,7967660,Fig. 12.,Example of inference with a convolutional NN. (a) Network translation by the proposed UDCM. (b) Experimental result of performance improvement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai12ab-2715171-large.gif
2017,7967660,Fig. 13.,(a) Architecture and (b) experimental result of the proposed ES mechanism.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai13ab-2715171-large.gif
2017,7967660,Fig. 14.,Die photograph and chip specification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai14-2715171-large.gif
2017,7967660,Fig. 15.,Measurement results and comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai15ab-2715171-large.gif
2017,7967660,Fig. 16.,(a) Performance improvement. (b) Execution time in learning and inference.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai16ab-2715171-large.gif
2017,7967660,Fig. 17.,FPGA-based platform for system evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai17-2715171-large.gif
2017,7967660,Fig. 18.,(a) ECG-based personal identification. (b) Car License plate recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4/8048205/7967660/tsai18ab-2715171-large.gif
2017,7936635,Fig. 1.,"Schematic diagram of HDSS with two components: Pervasive health decision support (PDHS) and PHDS-assisted clinical decision support system (CDSS+). Transition
i
, disease diagnosis modules, and disease-onset records are denoted by
T
i
, DDM, and DOR, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin1-2710194-large.gif
2017,7936635,Fig. 2.,"Information framework of Tier-1 PHDS. Disease onset record of disease
i
is denoted as DOR
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin2-2710194-large.gif
2017,7936635,Fig. 3.,Tier-2 information framework for clinical disease diagnosis with a parallel decision flow of a human physician.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin3-2710194-large.gif
2017,7936635,Fig. 4.,"The proposed DDM structure for disease
i
supported by the HDSS framework. Disease module for disease
i
at Tier-
j
is denoted by D
i
-T-
j
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin4-2710194-large.gif
2017,7936635,Fig. 5.,"The schematic flowchart of an automatic DDM training procedure that derives DMs from a biomedical dataset. The base learner, meta learner, performance matrix, decision maker, and pre-processor are denoted by BL, ML, PM, DM, and PP, respectively. Kappa statistic measures relative improvement over random predictors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin5-2710194-large.gif
2017,7936635,Fig. 6.,"Accuracy of the arrhythmia decision maker in the WMS tier using a single classifier, feature filtering (
+
F), ensemble method (
+
E), and both (
+
E
+
F). The 10 bar groups to the left use homogeneous ensemble methods based on AdaBoost, DECORATE, and bagger. The rightmost bar group (marked with
×
) uses a heterogeneous ensemble method based on stacker and voter. N.A.: Not applicable.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin6-2710194-large.gif
2017,7936635,Fig. 7.,The final arrhythmia DDM. N.E.: Null entry. RF: Random forest. +F: Feature filtering. BAG: Bagging. BN: Bayes network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin7-2710194-large.gif
2017,7936635,Fig. 8.,Coverage of HDSS on all the ICD-10-CM disease categories based on the generated DDMs and analysis of related work.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin8-2710194-large.gif
2017,7936635,Fig. 9.,"Storage changes with (a) increasing training instances, (b) ensemble methods, and (c) feature-filtering technique. MLM abbreviations follow Table 1. Single stands for a base learner and does not employ ensemble methods and feature filtering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687315/8207714/7936635/yin9-2710194-large.gif
2017,7944673,Fig. 1.,"The basic idea of our proposed SFDL approach to image set based face recognition, where discriminative features and dictionaries are learned simultaneously to encode the pose, illumination and expression information in face image sets, so that it is more robust to noise. In the training stage, we learn a feature projection matrix
W
and a structured dictionary
D=[
D
1
,
D
2
,⋯,
D
P
]
(one sub-dictionary per class) by using the proposed SFDL method, where
P
is the number of subjects in the training set. Given a testing face image set containing
M
image frames, we first apply the learned feature projection matrix
W
to project each sample into a feature and recognize its label by using the smallest reconstruction error corresponding to the associated sub-dictionary. Lastly, the majority voting strategy is used to classify the whole testing face image set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu1-2713940-large.gif
2017,7944673,Algorithm 1,SFDL,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu11-2713940-large.gif
2017,7944673,Fig. 2.,"The basic idea of our proposed D-SFDL approach to image set based face recognition, where multiple hierarchical non-linear transformations and class-specific dictionaries are simultaneously learned. In the training stage, we learn multiple hierarchical non-linear transformations and class-specific dictionaries by using the proposed D-SFDL method. Given a testing face image set containing
M
image frames, we first apply the learned multiple hierarchical non-linear transformations to project each sample into a feature and recognize its label by using the smallest reconstruction error corresponding to the associated sub-dictionary. Lastly, the majority voting strategy is used to classify the whole testing face image set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu2-2713940-large.gif
2017,7944673,Algorithm 2,D-SFDL,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu12-2713940-large.gif
2017,7944673,Fig. 3.,"Average recognition rate (%) of our (a) SFDL and (b) D-SFDL versus different feature dimension of the learned feature spaces on the YTC dataset, where
Dim1
and
Dim2
denote the feature dimensions of the second and third layers, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu3ab-2713940-large.gif
2017,7944673,Fig. 4.,"Average recognition rate (%) of our SFDL versus different values of
λ
1
on the YTC dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu4-2713940-large.gif
2017,7944673,Fig. 5.,"Average recognition rate (%) of our SFDL versus different values of
λ
2
on the YTC dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu5-2713940-large.gif
2017,7944673,Fig. 6.,"Average recognition rate (%) of our SFDL versus different values of
η
1
on the YTC dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu6-2713940-large.gif
2017,7944673,Fig. 7.,"Average recognition rate (%) of our SFDL versus different values of
η
2
on the YTC dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu7-2713940-large.gif
2017,7944673,Fig. 8.,"Average recognition rate (%) of our SFDL versus different values of
K
p
on the YTC dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu8-2713940-large.gif
2017,7944673,Fig. 9.,Average recognition rate (%) of our approach versus different number of iterations on the YTC dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu9-2713940-large.gif
2017,7944673,Fig. 10.,The average recognition rate (%) of different image set based face identification methods on the YTC dataset where different number of image frames are used for evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7935144/7944673/lu10-2713940-large.gif
2017,7907197,Fig. 1.,"Two existing ZSL frameworks. Both of them adopt a two-step strategy. Firstly, the test image is transformed into a intermediary (semantic/ distribution) space. Secondly, the final prediction for target classes is generated by considering the relationship between the sample and target classes in the space. (a) Embedding-space based ZSL. (b) Class-similarity based ZSL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding1ab-2696747-large.gif
2017,7907197,Fig. 2.,"Overview of the proposed sample-transfer based framework. In the training stage, we compute the transferability of each source sample for each target class via the embedding space or the class similarity. Based on the transferability and diversity, some source samples are selected for each target class and assigned by the corresponding pseudo labels. Then with the transferred samples and pseudo labels, we can train a supervised classifier to perform sample-to-class classification in one step. In the test stage, the classifier takes a target sample as the input and directly outputs the prediction on the target classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding2-2696747-large.gif
2017,7907197,Fig. 3.,Selected highly transferable images for car and dog. We use them with pseudo labels to train a car-dog classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding3-2696747-large.gif
2017,7907197,Fig. 4.,t-SNE visualization of some samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding4ab-2696747-large.gif
2017,7907197,Algorithm 1,Optimization Algorithm for Eq. (10),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding12-2696747-large.gif
2017,7907197,Algorithm 2,Zero-Shot Learning With Transferred Samples,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding13-2696747-large.gif
2017,7907197,Fig. 5.,Effect of diversity regularization with class embedding as the auxiliary information. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding5abcd-2696747-large.gif
2017,7907197,Fig. 6.,Effect of diversity regularization with class similarity as the auxiliary information. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding6abcd-2696747-large.gif
2017,7907197,Fig. 7.,Effect of robust SVM with class embedding as the auxiliary information. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding7abcd-2696747-large.gif
2017,7907197,Fig. 8.,Effect of robust SVM with class similarity as the auxiliary information. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding8abcd-2696747-large.gif
2017,7907197,Fig. 9.,"Effect of
m
s
on STZSL-I. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding9abcd-2696747-large.gif
2017,7907197,Fig. 10.,"Effect of
m
t
on STZSL-T. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding10abcd-2696747-large.gif
2017,7907197,Fig. 11.,Effect of iterative refinement on STZSL-T. (a) CIFAR10. (b) AwA. (c) aPY. (d) CUB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/7922633/7907197/ding11abcd-2696747-large.gif
2017,7811252,Fig. 1.,Main functional structure of binocular vision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/11/7938794/7811252/zhou1-2638620-large.gif
2017,7811252,Fig. 2.,High-level diagram for the proposed metric.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/11/7938794/7811252/zhou2-2638620-large.gif
2017,7811252,Fig. 3.,Binary quantification. (a) Binary quantification of magnitude response. (b) Binary quantification of phase response.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/11/7938794/7811252/zhou3ab-2638620-large.gif
2017,7811252,Fig. 4.,The KNN-based ML processes for blind 3D-IQA without distortion type classifications.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/11/7938794/7811252/zhou4-2638620-large.gif
2017,7811252,Fig. 5.,Performance using different numbers of training 3D images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/11/7938794/7811252/zhou5-2638620-large.gif
2017,7605450,Fig. 1.,(a) shows the various scenarios in which traffic signs may appear; (b) shows the appearance changes of traffic signs caused by occlusion and illumination.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg1ab-2614548-large.gif
2017,7605450,Fig. 2.,Overview of the components of our TSR framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg2-2614548-large.gif
2017,7605450,Fig. 3.,"Illustration of 10 feature channels computed during the training of three kinds of traffic signs. They consist of 6 orientation channels, 1 gradient magnitude, and 3 LUV channels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg3-2614548-large.gif
2017,7605450,Fig. 4.,"Illustration of false positives examples because of the similar appearance, including triangle, circle, and rectangle shapes. Similar color may also cause false detections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg4-2614548-large.gif
2017,7605450,Fig. 5.,(a) Statistical map of the traffic sign distribution. (b) Probability density map obtained by Parzen-window estimation. (c) The density map on y-axis. (c) The density map on x-axis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg5abcd-2614548-large.gif
2017,7605450,Fig. 6.,The process of tracking traffic signs using Kalman Filter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg6-2614548-large.gif
2017,7605450,Fig. 7.,"Example of traffic sign symmetry calculation. For an input region of interest, rotation invariant features (
F
i
and
F
j
) are first extracted and then normalized (
k
i
and
k
j
) to match their mirrors (
m
i
and
m
j
). After that, the measurement outputs the magnitude of symmetry ranging from 0 to 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg7-2614548-large.gif
2017,7605450,Fig. 8.,"Selection of the parameter
τ
experiment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg8-2614548-large.gif
2017,7605450,Fig. 9.,"Comparison between five detection methods, including the Viola-Jones+HOG detector, ACF detector, ACF detection with KF tracking, ACF detection with spatial distribution prior, and ACf detection with prior and KF tracking.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg9-2614548-large.gif
2017,7605450,Fig. 10.,(a) Detection result of a frame in TS2011. (b) False detected candidates by the ACF detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg10ab-2614548-large.gif
2017,7605450,Fig. 11.,The normalized non-overlapping area distribution of tracking by (a) KF and (b) KF +on-line detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg11ab-2614548-large.gif
2017,7605450,Fig. 12.,"The first row is the tracking only with KF, while the second shows the results of tracking with KF and on-line detection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg12-2614548-large.gif
2017,7605450,Fig. 13.,Comparison of classification results under varied fusion frame numbers and fusion strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/7956367/7605450/wangg13-2614548-large.gif
2017,7563366,Fig. 1.,"Illustration of DVP and PVP: (a) DVP during intermediate iterations. The perturbed
μ
p
participates in the (15). As a result, the output
f
p
at each iteration is a random variable, and the transmission of
f
p
is differentially private. (b) PVP during intermediate iterations. The perturbed
V
p
is a random variable. As a result, the transmission of
V
p
is differentially private.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7726079/7563366/zhang1ab-2607691-large.gif
2017,7563366,Fig. 2.,"The final iteration of both DVP and PVP. The perturbed
μ
p
participates in the (15). As a result, the output
f
∗
p
is a random variable, and the final output is differentially private.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7726079/7563366/zhang2-2607691-large.gif
2017,7563366,Fig. 3.,"Convergence of algorithms, at iteration
t=100
(before the stop time) with different values of
α
p
(t)
. DVP with
ρ=
10
−2.5
and
C
R
=1750
; PVP with
ρ=
10
−1
and
C
R
=146
; Algorithm 1 (non-private) with
ρ=
10
−10
and
C
R
=1750
. (a)
α
p
(t)=0.01
. (b)
α
p
(t)=0.1
. (c)
α
p
(t)=0.5
. (d)
α
p
(t)=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7726079/7563366/zhang3abcd-2607691-large.gif
2017,7563366,Fig. 4.,"Privacy-accuracy tradeoff.
(a)
-
(b)
: DVP, with
ω
p1
=0.02
,
ω
p2
=6
,
ω
p3
=9
,
ω
p4
=1
(before the stop time);
(c)
-
(d)
PVP with
ω
p1
=0.02
,
ω
p2
=6
,
ω
p3
=9
,
ω
p4
=1
(before the stop time).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7726079/7563366/zhang4abcd-2607691-large.gif
2017,7563366,Fig. 5.,"Privacy-accuracy tradeoff. (a): Empirical risk vs.
α
p
of final optimum output. (b): Misclassifications error rate vs.
α
p
of iteration 100.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7726079/7563366/zhang5ab-2607691-large.gif
2017,7895121,Fig. 1.,"Transmission from massive and sporadic devices to a central BS for MMV scenario, in which each slot considered as SMV format.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang1-2692776-large.gif
2017,7895121,Fig. 2.,The graphical model for SBL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang2-2692776-large.gif
2017,7895121,Fig. 3.,The graphical model for the GMM with (a) unknown activity factor; (b) known activity factor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang3-2692776-large.gif
2017,7895121,Fig. 4.,The graphical model for (a) PCSBL; (b) BSBL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang4-2692776-large.gif
2017,7895121,Fig. 5.,"Comparison of SER for MUD algorithms under
N=32
and
K=20
for BPSK when (a)
p
a
=0.2
; (b)
p
a
=0.4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang5-2692776-large.gif
2017,7895121,Fig. 6.,"Comparison of SER for MUD algorithms under
N=128
and
K=100
for BPSK when
p
a
=0.1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang6-2692776-large.gif
2017,7895121,Fig. 7.,"Comparison of SER for MUD algorithms under
N=32
and
K=20
for BPSK when (a)
p
a
=0.2
; (b)
p
a
=0.8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang7-2692776-large.gif
2017,7895121,Fig. 8.,"Comparison of SER for MUD algorithms under
N=128
and
K=100
for BPSK when (a)
p
a
=0.2
; (b)
p
a
=0.8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang8-2692776-large.gif
2017,7895121,Fig. 9.,"Comparison of SER for MUD algorithms under
N=32
and
K=20
for BPSK when SNR
=10
dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8039128/7895121/liang9-2692776-large.gif
2017,8016626,Fig. 1.,The proposed joint machine learning and game theory (MLGT) based rate control optimization framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8024124/8016626/kwong1-2745099-large.gif
2017,8016626,Fig. 2.,"CTU-level R-D relationship modeling. (a) PeopleOnStreet, CTUID=16; (b) PeopleOnStreet, CTUID=23; (c) BasketballDrill, CTUID=21; (d) BasketballDrill, CTUID=54.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8024124/8016626/kwong2abcd-2745099-large.gif
2017,8016626,Fig. 3.,R-D model prediction using the derived thresholds based on scene change measurement DOH. (a) PeopleOnStreet; (b) RaceHorsesC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8024124/8016626/kwong3ab-2745099-large.gif
2017,8016626,Algorithm 1,"Iterative Solution Search for Lagrange Multiplier
ξ",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8024124/8016626/kwong6-2745099-large.gif
2017,8016626,Fig. 4.,Buffer fullness comparisons. (a) PeopleOnStreet (target bitrate: 1400 kbps); (b) BQTerrace (target bitrate: 700 kbps); (c) BasketballDrill (target bitrate: 350 kbps); (d) RaceHorsesC (target bitrate: 350 kbps); (e) BQSquare (target bitrate: 125 kbps); (f) FourPeople (target bitrate: 300 kbps).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8024124/8016626/kwong4abcdef-2745099-large.gif
2017,8016626,Fig. 5.,Subjective visual fidelity comparisons. (a) Original frame; (b) HM16.8-RLRC; (c) TIP16-Wang; (d) TIP13-Seo; (e) Proposed MLGT. Exemplary results are listed from top to bottom: PeopleOnStreet (target bitrate: 1800 kbps); Cactus (target bitrate: 900 kbps); PartyScene (target bitrate: 550 kbps); FourPeople (target bitrate: 350 kbps).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8024124/8016626/kwong5abcde-2745099-large.gif
2017,8085130,FIGURE 1.,H-ELM architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao1-2766675-large.gif
2017,8085130,FIGURE 2.,H-ELM-based speech enhancement architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao2-2766675-large.gif
2017,8085130,FIGURE 3.,PESQ scores for ELM with different activation functions and numbers of hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao3-2766675-large.gif
2017,8085130,FIGURE 4.,"PESQ, SDI, STOI, and SSNRI average scores for ELM and H-ELM configurations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao4-2766675-large.gif
2017,8085130,FIGURE 5.,"Spectrograms of an utterance (a) clean (PESQ = 4.6439), (b) noisy (PESQ = 2.2976), (c) ELM (PESQ = 2.3018), and (d) H-ELM (PESQ = 2.5489) contaminated with babble noise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao5abcd-2766675-large.gif
2017,8085130,FIGURE 6.,"Spectrograms of an utterance (a) clean (PESQ = 4.6439), (b) noisy (PESQ = 2.4433), (c) ELM (PESQ = 2.5258), and (d) H-ELM (PESQ = 2.7345) contaminated with car noise.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao6abcd-2766675-large.gif
2017,8085130,FIGURE 7.,"PESQ score for (a) DDAE1, (b) DDAE2, (c) DDAE3, (d) DDAE4, (e) H-ELM1, (f) H-ELM2, (g) H-ELM3 and (h) H-ELM4, using different amounts of training batch samples (TS).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao7abcdefgh-2766675-large.gif
2017,8085130,FIGURE 8.,"STOI score for (a) DDAE1, (b) DDAE2, (c) DDAE3, (d) DDAE4, (e) H-ELM1, (f) H-ELM2, (g) H-ELM3 and (h) H-ELM4, using different amounts of training batch samples (TS).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8085130/tsao8abcdefgh-2766675-large.gif
2017,7585110,Fig. 1.,"Schematic representation of the leaky integrate and fire model (LIFM) of a spiking neuron. (a) Schematic representation. (b) Showing an input train of spikes (top row), the emitted output spikes (second row), and the membrane potential (from [1]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab1ab-2612890-large.gif
2017,7585110,Fig. 2.,"Schematic representation of the NeuCube-based methodology for fMRI data mapping, learning, visualization, and classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab2-2612890-large.gif
2017,7585110,Fig. 3.,"SNR index (on the
y
-axis) of top voxels (on the
x
-axis) extracted from (a) affirmative versus negative sentence fMRI data set and (b) picture versus sentence fMRI data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab3ab-2612890-large.gif
2017,7585110,Fig. 4.,Example of encoding five voxel time series captured during 8 s (16 brain images) into trains of spikes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab4-2612890-large.gif
2017,7585110,Fig. 5.,"Direct mapping of voxels into an SNNc. The dimensions of the SNNc are defined by the maximum values of
x
,
y
, and
z
voxel coordinates, which in this case study equal to
51×56×8
. In this dimensional space, 5062 voxels are mapped from the STAR/PLUS geometric voxel coordinates of a single person. The selected voxels in Fig. 3 for each case study problem are shown as input variables as circles, along with the ROI (as text in boxes) for (a) affirmative versus negative sentence fMRI data and (b) picture versus sentence fMRI data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab5ab-2612890-large.gif
2017,7585110,Fig. 6.,"Mapping voxels into SNNc using the Talairach brain template. The 5062 voxel data of one subject were first mapped into 1471 Talairach template coordinates according to [1], [33], and [38]. Then, each template coordinate is mapped into a corresponding neuron from an SNNc. The selected top informative voxels in Fig. 3 for each case study problem are used as input variables and shown as circles along with the ROI (as text in boxes) for (a) affirmative versus negative sentence fMRI data and (b) picture versus sentence fMRI data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab6ab-2612890-large.gif
2017,7585110,Fig. 7.,Voxels are mapped into SNNc using Talairach template. (a) Initial connections in an SNNc. (b) Learned connections after STDP unsupervised learning using both affirmative and negative sentence fMRI samples when 20 input voxels selected as in Fig. 3. The dense areas of connectivity evolved in the SNNc can be analyzed to understand the most active functional areas in the brain during these two tasks and how they interact dynamically.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab7ab-2612890-large.gif
2017,7585110,Fig. 8.,Voxels are mapped into SNNc using Talairach template. (a) Learned connections in an SNNc when only fMRI samples of affirmative sentences were used. (b) Learned connections in an SNNc when only fMRI samples of negative sentences were used. The initialization is the same as in Fig. 7. The dense areas of connectivity evolved in the SNNc can be analyzed to understand the difference between functional areas in the brain during each of the two tasks as dynamic interaction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab8ab-2612890-large.gif
2017,7585110,Fig. 9.,Voxels are mapped into SNNc using Talairach template. Clustering of neurons in an SNNc after unsupervised training for (a) affirmative sentence data and (b) negative sentence data. The size of a cluster indicates the importance of the input feature/voxel at the center of the cluster. This can be used for feature/voxel selection and marker identification for further studies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab9ab-2612890-large.gif
2017,7585110,Fig. 10.,Voxels are mapped into SNNc using Talairach template. (a) Connectivity of an SNNc trained on fMRI data related to seeing a picture. (b) Connectivity of an SNNc trained on fMRI data related to reading a sentence. (c) 2-D projection of the connectivity of the SNNc from Fig. 10(a). (d) 2-D projection of the SNNc from Fig. 10(b). The dense areas of connectivity evolved in the SNNc can be analyzed to understand the difference between functional areas in the brain during each of the two tasks as dynamic interaction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab10abcd-2612890-large.gif
2017,7585110,Fig. 11.,"Voxels are directly mapped into an SNNc model. Initial (A) and final (B) connectivity of an SNNc after training with four different data sets, related correspondingly to affirmative sentence, negative sentence, seeing a picture, and reading a sentence. The final connectivity is also shown as a 2-D projection (C). Positive connections are shown in blue and negative connections are shown in red.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab11abc-2612890-large.gif
2017,7585110,Fig. 12.,"Voxels are directly mapped into an SNNc model. Clustering of the neurons in a trained SNNc with (a) affirmative sentence data and (b) negative sentence data, along with their corresponding 2-D projections shown in (c) and (d), correspondingly. The size of clusters indicates the importance of the feature voxel for the task and can be used for feature/voxel selection for further studies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab12abcd-2612890-large.gif
2017,7585110,Fig. 13.,"Voxels are directly mapped into an SNNc model. Clustering of the neurons in a trained SNNc with (a) fMRI data related to seeing a picture and (b) fMRI data related to reading a sentence, along with their 2-D projections shown in (c) and (d), correspondingly. The size of the clusters indicates the importance of the features (voxels) for the task and can be used for feature/voxel selection for further studies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7585110/kasab13abcd-2612890-large.gif
2017,7335648,Fig. 1.,"RBF network structure of QAGKRL. The Gaussian kernel centers are the quantized input vectors, and the coefficient
a
k,j
is the global error-based expansive function,
g(δ)
, scaled by the learning rate,
η
. QAGKRL adopts the softmax policy as the action selection rule.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang1-2493079-large.gif
2017,7335648,Fig. 2.,"Obstacle avoidance task. (a) Rhesus monkey sat in a primate chair, using its right hand to control a joystick projected as a computer cursor on a monitor. (b) Cursor appeared randomly in one of the four blue circles labeled as 1/2/3/4, the target (big yellow circle) displayed randomly in other three potential positions, then an obstacle (a green bar), lay in the middle of the cursor and the target. In this trial, the monkey should hold the cursor limited in a red square in position 3 for a certain time period of delay, move to avoid the obstacle to reach the target 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang2ab-2493079-large.gif
2017,7335648,Fig. 3.,"Real trajectories of all the successful trials over nine scenarios. Red asterisks: start positions of all trials. Green squares: reaching target locations of all trials. Magenta circle: target area. Green bars: obstacles. Each subplot represents one scenario of obstacle avoidance. (a) S1:
1→2
. (b) S2:
1→4
. (c) S3:
2→1
. (d) S4:
2→3
. (e) S5:
3→1
. (f) S6:
3→2
. (g) S7:
3→4
. (h) S8:
4 →1
. (i) S9:
4 →3
. The other three scenarios (
1→3
,
2→4
, and
4→2
) are not shown because the number of successful trials is less than 5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang3abcdefghi-2493079-large.gif
2017,7335648,Fig. 4.,"Illustration of how to determine the output actions of the neural network. (a) Average trajectory of all the trials from position 4 to target 3 (red solid line) and the corresponding fitting curve (green solid line). Green bar: obstacle. Two blue rectangles are set at both sides of the obstacle served as virtual target areas. (b) Normalized directions distribution of Scenario:
4→3
(magenta histogram) and the tangential directions of all the trials at each time step within the nine scenarios (blue histogram). The tangential directions in Scenario:
4→3
are naturally grouped into two subsets whose peak values,
α
1
and
α
2
, are taken as the centers. All the tangential directions are pooled together and divided equally into 12 parts by the green dotted lines, and the highest local angles but larger than a threshold (0.015, the red dashed-dotted line) are picked as the output actions, so the final number of the output actions 8, that is,
θ=[0.838,1.19,1.88,2.30,3.98,4.26,4.96,5.38]
in radian, marked with the red asterisks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang4ab-2493079-large.gif
2017,7335648,Fig. 5.,"Average successful rates of QAGKRL within the last 50 trials across ten initializations with six distinctive kernel widths
h=[1.0,1.6,2.0,2.4,2.8,3.2]
, and four different quantization thresholds
ξ
U
=[3.75,9.19,11.07,12.84]
, the corresponding final kernel filter sizes are
[927,15,7,5]
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang5-2493079-large.gif
2017,7335648,Fig. 6.,"Illustration of the order mode and the random mode. In both modes, the time index (
t
1
,
t
2
,…,
t
num
, where num is the number of steps, usually 14 or 16, for the agent to complete one trial) is in the same sequence as recorded within one trial. (a) Order mode. Scenarios are played in sequence, that is S1:
1→2
, S2:
2→3
, S3:
3→1
, S4:
4→1
, and S5:
4→3
, and each includes 1000 trials grouped together. (b) Random mode. The distinctive trials are replayed with random scenarios Sx, which can be any one of the five scenarios.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang6ab-2493079-large.gif
2017,7335648,Fig. 7.,"Average learning curves within the five scenarios using threefold cross-validation across 30 random initializations by AGREL (blue solid lines) and QAGKRL (deep green lines) during the training process in the order mode and online mode. (a) Order-online mode. For clarity, error bars represent standard deviations are not shown. (b) Online mode. The corresponding light color shading areas represent respective regions with the best and the worst performance for AGREL and QAGKRL. For comparison, the magenta line represents the average learning curve of
Q(λ)
-learning (
λ=0.2
) using greedy policy, while the cyan line is
Q(λ)
-learning (
λ=0
) using the softmax policy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang7ab-2493079-large.gif
2017,7335648,Fig. 8.,"Ranges of the total successful rates within the five scenarios across 30 random initializations by AGREL (blue boxes) and QAGKRL (green boxes) using threefold cross-validation during the training process. (a) Order mode (not include the random part). (b) Online mode (right tail paired-sample
t
-test,
p=2.6965e
-4). Red plus signs: outliers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang8ab-2493079-large.gif
2017,7335648,Fig. 9.,"Ranges of the total successful rates within the five scenarios with the testing data after the parameters trained in the online mode by AGREL (blue boxes) and QAGKRL (green boxes) corresponding to Fig. 8(b). (a) Testing with the parameters fixed (the right tail paired-sample
t
-test,
p=4.2296e
-5). (b) Testing with the parameters updating (the right tail paired-sample
t
-test,
p=3.0916e
-4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang9ab-2493079-large.gif
2017,7335648,Fig. 10.,"Successfully reconstructed examples during the different training stages at (a) 10th epoch, (b) 150th epoch, and (c) 400th epoch. (d) Testing phase with all the parameters fixed after QAGKRL is trained in the online mode. Circles: initial positions of the cursor. Squares: targets. Green bar: obstacle. Green color depth: appearance order of obstacles. Solid lines: real trajectories. Dashed-dotted lines: corresponding predications.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7335648/wang10abcd-2493079-large.gif
2017,6880823,Fig. 1.,Cyber threat in the smart grid.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal1-2341597-large.gif
2017,6880823,Fig. 2.,"Attacked and safe operating modes in the
R
2
space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal2-2341597-large.gif
2017,6880823,Fig. 3.,Distributed SVM convergence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal3-2341597-large.gif
2017,6880823,Fig. 4.,"Optimal Choice for
C
and
σ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal4-2341597-large.gif
2017,6880823,Fig. 5.,Learning curve of the SVM and the anomaly detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal5-2341597-large.gif
2017,6880823,Fig. 6.,"Histogram representation of
z
tr1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal6-2341597-large.gif
2017,6880823,Fig. 7.,"Unsupervised anomaly detection with a large threshold
δ(P(z)<2e−4)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal7-2341597-large.gif
2017,6880823,Fig. 8.,"Unsupervised anomaly detection with a small threshold
δ(P(z)<2e−6)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal8-2341597-large.gif
2017,6880823,Fig. 9.,"Semisupervised anomaly detection with optimal threshold
(P(z)<2.98e−5)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/8052178/6880823/esmal9-2341597-large.gif
2017,7565530,Fig. 1.,"This figure is an illustration of Cascade SVM [10]. Different layers have to be processed sequentially, i.e., layer
i+1
can be processed after layer
i
has been finished. The tasks in the same level can be processed concurrently. If the result at the bottom layer is not good enough, the user can distribute all the support vectors (SV15 in the figure) to all the nodes and re-do the whole pass from the top layer and to the bottom layer. However, for most applications, the result will not become better after another Cascade pass. One pass is enough in most cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you1-2608823-large.gif
2017,7565530,Fig. 2.,"General Flow for CP-SVM. In the training part, different SVMs process its own dataset independently. In the classification part, different models can make the prediction independently.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you2-2608823-large.gif
2017,7565530,Fig. 3.,"This is an example of First Come First Served (FCFS) partitioning algorithm. Each figure is a distance matrix, which is referred as
dist
. For example,
dist
[i][j] is the distance between ith center and jth sample. The color of the matrix in the first figure is the original color. If
dist
[i][j] has a different color than the original one, then it means that jth sample belongs to ith center.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you3-2608823-large.gif
2017,7565530,Fig. 4.,"The figure shows that the partitioning by K-means is imbalanced while the partitioning by FCFS is balanced. Specifically, each node has exactly 20,000 samples after FCFS partitioning. The test dataset is
face
with 160,000 samples (361 features per sample). 8 nodes are used in this test.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you4-2608823-large.gif
2017,7565530,Fig. 5.,"This is an example of Balanced K-means partitioning algorithm. Each figure is a distance matrix, which is referred as
dist
. For example,
dist
[i][j] is the distance between ith center and jth sample. The color in the first figure is the original color. If
dist
[i][j] has a different color than the original one, then it means that jth sample belongs to ith center.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you5-2608823-large.gif
2017,7565530,Fig. 6.,"The figure shows that CP-SVM is load imbalanced while CA-SVM is load-balanced. The test dataset is
epsilon
with 128,000 samples (2,000 nnz per sample). Eight nodes are used in this test.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you6-2608823-large.gif
2017,7565530,Fig. 7.,"Communication Patterns of different approaches. The data is from running the 6 approaches on eight nodes with the same 5MB real-world dataset (subset of ijcnn dataset).
x
-axis is the rank of sending processors,
y
-axis is the rank of receiving processors, and
z
-axis is the volume of communication in bytes. The vertical ranges (
z
-axis) of these six sub-figures are the same. The communication pattern of BKM-SVM is similar to that of CP-SVM. The communication pattern of FCFS-SVM is similar to that of cascade without point-to-point communication.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you7-2608823-large.gif
2017,7565530,Fig. 8.,"The ratio of computation to communication. The experiment is based on a subset of ijcnn dataset. To give a fair comparison, we implemented two versions of CA-SVM. casvm1 means that we put the initial dataset on the same node, which needs communication to distribute the dataset to different nodes. casvm2 means that we put the initial dataset on different nodes, which needs no communication.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you8-2608823-large.gif
2017,7565530,Fig. 9.,"We use the 5,000 samples from the UCI covtype dataset [36] for this experiment. The kernel matrix is 5,000-by-5,000 with 458,222 nonzeroes. The first figure is the original kernel matrix, the second figure is the kernel matrix after clustering. From these figures we can observe that the kernel matrix is block-diagonal after the clustering algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/7875386/7565530/you9-2608823-large.gif
2017,7872382,Fig. 1.,Challenges in nuclear segmentation: Original H&E stained tissue images show crowded and chromatin-sparse nuclei. Otsu thresholding [9] leads to merged nuclei (under-segmentation). Marker controlled watershed segmentation [10] leads to fragmented nuclei (over-segmentation). Proposed technique detects and segments almost all nuclei well. Each segmented nucleus is shown in a separate color.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7872382/kumar1-2677499-large.gif
2017,7872382,Fig. 2.,Proposed training and inference to segment nuclei using CNN3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7872382/kumar2-2677499-large.gif
2017,7872382,Fig. 3.,Examples of sub-images taken from the test images for different organs (columns) showing challenging cases based on variation in nuclear appearance and crowding. Intermediate and final results of our segmentation process are shown in rows. Annotation boundaries of the test images were only used for evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7872382/kumar3-2677499-large.gif
2017,7872382,Fig. 4.,"Comparing segmentation methods based on deep learning on H&E stained images (top row). In the middle and bottom rows, ground truth (annotated) boundaries are red, detected are blue, and overlap between the two are yellow. Predominance of yellow (correct) boundaries can be seen with the use of proposed CNN3 in the bottom row compared to the CNN2 similar to [13] in the middle row. For easy cases where the nuclei are uniformly colored and well-separated, both methods perform equivalently as seen in the rightmost column.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7872382/kumar4-2677499-large.gif
2017,7872382,Fig. 5.,"H&E stained images with comparison of segmentation results using open-source software - cell profiler [50] and Fiji [51], – and deep learning methods CNN2 (similar to [13]), and CNN3 (proposed) – along with ground truth segmentation maps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7872382/kumar5-2677499-large.gif
2017,7872382,Fig. 6.,"The proposed metric aggregated Jaccard index (AJI) captures the better segmentation quality of (a) Kidney with respect to (b) Colon, while F1-score (F1S), mean DiceâŁ™s coefficient (DC) and mean Hausdorff distances (DC) are very similar for the two images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7872382/kumar6ab-2677499-large.gif
2017,7676308,Fig. 1.,"(a) Control system of a thin-walled workpiece mounted to a Mikron machine platform. (b) Spindle, laser sensor, and tool path of the workpiece machining system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang1ab-2620987-large.gif
2017,7676308,Fig. 2.,Architecture of the control system for the thin-walled workpieces machining platform in Fig. 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang2-2620987-large.gif
2017,7676308,Fig. 3.,Setup of a VCM with initial pushing force offset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang3-2620987-large.gif
2017,7676308,Fig. 4.,"Machining without deformation compensation, which produces an undesirable curved rough surface.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang4-2620987-large.gif
2017,7676308,Fig. 5.,"Machining deformation compensation, which improves the milling groove of Fig. 4 into a straight rough surface. Here, the displacement
y
˜
(t)=y(t)+
y
¯
¯
¯
(t)
with
y(t)
and
y
¯
¯
¯
(t)
denoting the predicted vibrations and predicted deformation of the thin-walled workpiece, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang5-2620987-large.gif
2017,7676308,Fig. 6.,"Machining with deformation compensation together with vibration feedback control, which further improves the milling groove of Fig. 5 into a desirable straight smooth surface. Here, VCM is the actuator of the feedback controller.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang6-2620987-large.gif
2017,7676308,Fig. 7.,"(a) Cutting displacement
y
˜
, predicted deformation
y
¯
¯
¯
, and prediction vibrations
y=
y
˜
−
y
¯
¯
¯
without deformation compensation or feedback control. The maximal deformation is 0.768 mm. The inner graph is the magnified vibrations and prediction errors. Here, the vibrations are detected by the laser sensor fixed on the spindle, and the prediction horizon is
H
p
=30
. It is observed that, by deformation compensation of the
z
-movement of the spindle, the milling groove is no longer a parabolic but almost a straight line. Meanwhile, the vibrations of the beginning stage (i.e.,
[0.2, 1.5]×
10
4
ms) and ending stage (i.e.,
[4.5, 5.2]×
10
4
ms) are larger than those of the middle stage (i.e.,
[1.5, 4.5]×
10
4
ms), as the formers have deeper cutting depth due to the deformation. (b) Error evolution of the predicted deformation, whose average value is 0.485 
μ
m.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang7ab-2620987-large.gif
2017,7676308,Fig. 8.,"(a) Predicted vibrations without
z
-axis deformation compensation. (b) Vibrations with online
z
-axis deformation compensation. After compensation, the maximal deformation has been reduced from 0.768 mm [see Fig. 7(a)] to 0.695 mm as the cutting force has been flattened. (c) Feedback MPC after 32 000th ms with
z
-axis compensation. (d) Control voltage of the VCM. Here, feed rate
υ=390
mm/min. It is observed that i) the vibrations become more evenly distributed with the online deformation compensation [subfigure (b)], and ii) the vibrations has been mitigated by 53% with the proposed dual-mode MPC [subfigure (c)].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang8abcd-2620987-large.gif
2017,7676308,Fig. 9.,"(a) Milling groove images without deformation compensation (upper groove) and with Bayesian learning-based compensation (lower groove). Here, the tool paths are both from 0 to 400 mm, which corresponds to the vibrations from 0 to 61 538 ms in Fig. 8(a) and (b), respectively. The comparison between the roughness without compensation
η
1
and with compensation
η
4
. It is observed that the average roughness
η
¯
¯
¯
1
=10.554μ
m and
η
¯
¯
¯
4
=8.057μ
m; and the standard deviations of
η
1
and
η
4
are
σ
1
=3.871μ
m and
σ
4
=1.609μ
m, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang9ab-2620987-large.gif
2017,7676308,Fig. 10.,(a) Milling groove image corresponding to Fig. 8(c) and (d). (b) Associated roughness along the milling groove.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang10ab-2620987-large.gif
2017,7676308,Fig. 11.,"(a) Feedback LQR control after 32 000th ms with
z
-axis compensation. (b) Control voltage of the VCM. It is found that the vibrations has been mitigated by 31% with the LQR controller.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/7855861/7676308/zhang11ab-2620987-large.gif
2017,7938698,Fig. 1.,(a) Schematic of a restricted Boltzmann machine. (b) Schematic of a deep belief network of one visible and three hidden layers (adapted from [31]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan1-2712128-large.gif
2017,7938698,Fig. 2.,Block diagram of the proposed CS scheme when using overcomplete learned dictionaries as the sparsifying transform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan2-2712128-large.gif
2017,7938698,Fig. 3.,Block diagram of the proposed CS scheme when using orthonormal bases as the sparsifying transform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan3-2712128-large.gif
2017,7938698,Fig. 4.,"Comparison of the reconstruction of synthetic signals generated with the RBM model using OMP, the RBM-OMP-like algorithm, the DBN-OMP-like algorithm, and the oracle estimator.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan4-2712128-large.gif
2017,7938698,Fig. 5.,"Comparison of the reconstruction of synthetic signals generated with the DBN model using OMP, the RBM-OMP-like algorithm, the DBN-OMP-like algorithm, and the oracle estimator.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan5-2712128-large.gif
2017,7938698,Fig. 6.,Evaluation of the MNIST validation dataset reconstruction using RBM models with different number of hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan6-2712128-large.gif
2017,7938698,Fig. 7.,Evaluation of the MNIST validation dataset reconstruction using DBN models with different number of hidden layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan7-2712128-large.gif
2017,7938698,Fig. 8.,"Evaluation of the reconstruction of images from the MNIST dataset. The OMP, BPDN, FV-OMP-like, RBM-OMP-like, DBN-OMP-like, and the oracle are employed as reconstruction algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan8-2712128-large.gif
2017,7938698,Fig. 9.,"Evaluation of the reconstruction of natural images from random projections using wavelets as the sparsifying transform. The oracle estimator, OMP, BPDN, TS-BCS, FV-OMP-like, RBM-OMP-like, and DBN-OMP-like are employed as reconstruction algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan9-2712128-large.gif
2017,7938698,Fig. 10.,"Evaluation of the reconstruction of natural images from random projections using overcomplete learned dictionaries as the sparsifying transform. Dictionary training algorithms DL1 and DL2 are employed. The OMP, BPDN, FV-OMP-like, RBM-OMP-like, and DBN-OMP-like are employed as reconstruction algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan10-2712128-large.gif
2017,7938698,Fig. 11.,"Visual evaluation of the proposed reconstruction algorithms using overcomplete dictionaries and the wavelet basis as sparsifying transforms.
M=0.2N
. First row: (a) Original image. Second row: (b-e) Reconstructed images using a wavelet basis as sparsifying transform (b) OMP reconstruction, PSNR = 17.09, (c) FV-OMP-like reconstruction, PSNR = 22.7, (d) RBM-OMP-like reconstruction, PSNR = 24.19, (e) DBN-OMP-like reconstruction, PSNR = 25.04. Third row: (f-i) Reconstructed images using an overcomplete learned dictionary as sparsifying transform, (f) OMP reconstruction, PSNR = 20.11, (g) FV-OMP-like reconstruction, PSNR = 24.28, (h) RBM-OMP-like reconstruction, PSNR = 25.82, (i) DBN-OMP-like reconstruction, PSNR = 27.1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/78/7959590/7938698/polan11-2712128-large.gif
2017,7990182,Fig. 1.,Construction of fully pretrained CNets. (a) and (b) Transfer learning with partially pretrained CNets. (c) Transfer learning with fully pretrained CNets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao1abc-2691013-large.gif
2017,7990182,Fig. 2.,Proposed transfer learning method with fully pretrained CNets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao2-2691013-large.gif
2017,7990182,Fig. 3.,MLP classifier. (a) MLP with full connection. (b) MLP with dropout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao3ab-2691013-large.gif
2017,7990182,Fig. 4.,All possible combinations of samples created by randomly cropping and mirroring trick.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao4-2691013-large.gif
2017,7990182,Fig. 5.,"UCM land-use data set. (a)–(u) Agricultural, airplane, baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection, medium residential, mobile home park, overpass, parking lot, river, runway, sparse residential, storage tanks, and tennis courts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao5abcdefghijklmnopqrstu-2691013-large.gif
2017,7990182,Fig. 6.,"Wuhan data set. (a)–(h) Dense residential, idle, industrial, medium residential, parking lot, commercial, vegetation, and water.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao6abcdefgh-2691013-large.gif
2017,7990182,Fig. 7.,Convergence of networks training process. (a) Training. (b) Testing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao7ab-2691013-large.gif
2017,7990182,Fig. 8.,Mean differences of weights contained in networks during BP process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8016687/7990182/zhao8-2691013-large.gif
2017,8013734,Fig. 1.,"The 14 low-power wireless networks deployed in the American River Hydrologic Observatory, near Sacramento, CA, USA. Each deployment site is identified by a 3-letter codename.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza1-2741468-large.gif
2017,8013734,Fig. 2.,Network architecture of the American River Hydrologic Observatory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza2-2741468-large.gif
2017,8013734,Fig. 3.,The Echo Peak deployment (marked “ECP” in Fig. 1).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza3-2741468-large.gif
2017,8013734,Fig. 4.,"Comparing the different canonical and empirical propagation models (lines) against the measurements gathered on the 2218 wireless links of the ARHO networks (cloudpoint). Canonical and empirical propagation models as detailed in Sections III-A and III-B, respectively; measurements gathered in Section II.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza4-2741468-large.gif
2017,8013734,Fig. 5.,"Distribution of errors under canonical and empirical models (top panels), compared to proposed model (bottom panel) for year-averaged RSSI data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza5ab-2741468-large.gif
2017,8013734,Fig. 6.,RSSI predictor accuracy compared to an ideal predictor (blue line) on the testing dataset of 555 RSSI measurements.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza6-2741468-large.gif
2017,8013734,Fig. 7.,Standard deviation of RSSI over all links over a one-year period.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8227024/8013734/oroza7-2741468-large.gif
2017,8023748,Fig. 1.,Diagram of the FP-ILM algorithm during its training (a) and operational (b) phases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu1-2740168-large.gif
2017,8023748,Fig. 2.,Mean Silhouette coefficient (top) and the Davies–Bouldin index (bottom) as a function of the number of ozone profile clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu2-2740168-large.gif
2017,8023748,Fig. 3.,Percentage of explained variance with respect to the number of PCs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu3-2740168-large.gif
2017,8023748,Fig. 4.,Diagram of an example of a MLP NN with two hidden layers: Interconnection (top) and neurons (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu4-2740168-large.gif
2017,8023748,Fig. 5.,Classification of ozone profile shapes in 11 clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu5-2740168-large.gif
2017,8023748,Fig. 6.,Pearson product-moment correlation coefficients of ozone profiles with respect to each cluster.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu6-2740168-large.gif
2017,8023748,Fig. 7.,Simulated UV reflectance spectra (top) and extracted nine PCs (bottom) with respect to 11 clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu7-2740168-large.gif
2017,8023748,Fig. 8.,Mean relative errors of estimated ozone profile shapes using FP-ILM with synthetic data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu8-2740168-large.gif
2017,8023748,Fig. 9.,"Comparisons of derived ozone profiles for six random cases from GOME-2 data measured on the early morning (top), the noon (middle), the night (bottom) of January 15, 2008, respectively. For each comparison, the relative differences between the two retrievals are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu9-2740168-large.gif
2017,8023748,Fig. 10.,"Comparisons of derived ozone profiles for six random cases from GOME-2 data measured on the early morning (top), the noon (middle), the night (bottom) of October 4, 2008, respectively. For each comparison, the relative differences between the two retrievals are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu10-2740168-large.gif
2017,8023748,Fig. 11.,Mean absolute differences of estimated ozone profiles between FP-ILM and RAL-OEM for each of the 11 classes. Blue area marks the corresponding standard deviation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8233431/8023748/xu11-2740168-large.gif
2017,7447818,Fig. 1.,"TLLT framework for label propagation. The labeled examples, unlabeled examples, and curriculum are represented by red, gray, and green balls, respectively. (a) Established graph
G
, in which the examples/nodes are represented by balls and the edges are denoted by blue lines. (b) Selection of curriculum examples, where the green balls are considered as simple. (c) Selected examples in (b) are propagated by the learner. The steps of Teaching-to-Learn and Leaning-to-Teach are marked with blue and black dashed boxes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao1abc-2514360-large.gif
2017,7447818,Fig. 2.,"Illustration of curvilinear search presented in Algorithm 1.
M
denotes the Stiefel manifold and
T
M
(
S
(iter)
)
denotes the tangent plane at the point
S
(iter)
. First, a searching path
−τ
AS
(iter)
in
T
M
(
S
(iter)
)
is computed, in which
−
AS
(iter)
is a valid descent direction. Second, a retraction mapping (see the red arrow) is conducted by projecting
−τ
AS
(iter)
onto the Stiefel manifold
M
, which guarantees that the projected searching curve
P
¯
(τ)
for
S
(iter+1)
is always on
M
. After finding a suitable step size
τ
, we may locate the feasible
S
(iter+1)
on the manifold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao2-2514360-large.gif
2017,7447818,Fig. 3.,"Propagation process of the methods on the DoubleMoon data set. (a) Initial state with marked labeled examples and difficult bridge point. (b) Imperfect edges during graph construction caused by the bridge point in (a). These unsuitable edges pose a difficulty for all the compared methods to achieve accurate propagation. (c)–(i) show the intermediate propagations of TLLT (Norm), TLLT (Entropy), GFHF, LGC, LNP, DLP, and GTAM. (j)–(p) Compares the results achieved by all the algorithms, which reveals that only the proposed TLLT achieves perfect classification while the other methods are misled by the ambiguous bridge point.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao3abcdefghijklmnop-2514360-large.gif
2017,7447818,Fig. 4.,"Experimental results of the compared methods on ten UCI benchmark data sets. The subfigures (a)–(j) represent Iris, Wine, Seeds, SPECTF, CNAE9, BreastCancer, BreastTissue, Haberman, Leaf, and Banknote, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao4abcdefghij-2514360-large.gif
2017,7447818,Fig. 5.,"Comparison of TLLT and other methods on three practical applications. (a) 20Newsgroups data set for text categorization. (b) USPS data set for handwritten digit recognition. (c) COIL20 data set for object recognition. The
y
-axis in each subfigure represents classification accuracy obtained by various algorithms, and the
x
-axis records the amount of initial labeled examples
l
(0)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao5abc-2514360-large.gif
2017,7447818,Fig. 6.,"Running time (unit: second) of all the methods on the ten UCI data sets and three practical data sets. (a)–(j) correspond to UCI datasets Iris, Wine, Seeds, SPECTF, CNAE9, BreastCancer, BreastTissue, Haberman, Leaf, and Banknote. (k)–(m) are practical datasets including 20Newsgroups, USPS, and COIL20, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao6abcdefghijklm-2514360-large.gif
2017,7447818,Fig. 7.,"Parametric sensitivity of TLLT. The first, second, and third rows correspond to 20Newsgroups, USPS, and COIL20 data sets, respectively. (a), (c), and (e) show the variation of accuracy with respect to the kernel width
ξ
when
α
is fixed to 1. (b), (d), and (f) evaluate the influence of the tradeoff
α
to final accuracy under
ξ=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7927806/7447818/tao7-2514360-large.gif
2017,7906545,FIGURE 1.,Smart augmentation with more than one network A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle1-2696121-large.gif
2017,7906545,FIGURE 2.,Diagram illustrating the reduced smart augmentation concept with just one network A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle2-2696121-large.gif
2017,7906545,FIGURE 3.,"The image on the left is created by a learned combination of the two images on the right. This type of image transformation helped increase the accuracy of network B. The image was not produced to be an ideal approximation of a face but instead, contains features that helped network B better generalize the concept of gender which is the task it was trained for.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle3ab-2696121-large.gif
2017,7906545,FIGURE 4.,"Arbitrarily selected images from FERET demonstrate similarities in lighting, pose, subject, background, and other photographic conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle4-2696121-large.gif
2017,7906545,FIGURE 5.,"Arbitrarily selected images from the Adience show significant variations in lighting, pose, subject, background, and other photographic conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle5-2696121-large.gif
2017,7906545,FIGURE 6.,Example images from the MIT places dataset showing two examples from each of the two classes (abbey and airport) used in our experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle6-2696121-large.gif
2017,7906545,FIGURE 7.,"Illustration of network
B
1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle7-2696121-large.gif
2017,7906545,FIGURE 9.,Diagram of simplified implementation of Smart Augmentation showing network A and network B.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle9-2696121-large.gif
2017,7906545,FIGURE 8.,"Illustration of network
A
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle8-2696121-large.gif
2017,7906545,FIGURE 10.,Diagram of our implementation of Smart Augmentation with one network A for each class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle10-2696121-large.gif
2017,7906545,FIGURE 11.,Diagram of implementation of network B without Smart Augmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle11-2696121-large.gif
2017,7906545,FIGURE 12.,"Training and validation losses for experiments 1 and 17, showing reductions in overfitting by using Smart Augmentation. The smaller difference between training loss and validation loss caused by the smart augmentation technique shows how this approach helps the network B to learn more general features for this task. To avoid confusion, we remind the reader that the loss for smart augmentation is given by
f(
L
A
,
L
B
;α,β)
. This means that the loss graphs are a combination of the losses of two networks whereas the losses without smart augmentation are only
f(
L
B
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle12-2696121-large.gif
2017,7906545,FIGURE 13.,The image on the left is a learned combination of the two images on the right as produced by network A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle13ab-2696121-large.gif
2017,7906545,FIGURE 14.,The image on the left is a learned combination of the two images on the right as produced by network A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7906545/lemle14ab-2696121-large.gif
2017,7460959,Fig. 1.,"Schematic concept of the system used to interface the semg signals to the control system embedded in the robotic hand. Preprocessing block represents the amplification, rectification and filtering performed in hardware by the otto bock EMG sensors used.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-1-source-large.gif
2017,7460959,Fig. 2.,"A: four grasps/postures used in the experiment: cylindrical, tridigital, lateral grasps, and index pointing. B: bimanual manipulation task: the subject is engaged in unscrewing the jar lid (t7 in Table I). C: 10 semg ottobock electrodes set in an elastic band. D: experimental setup consisted in a manually operated timer and 10 objects of daily use.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-2-source-large.gif
2017,7460959,Fig. 3.,"Representative bimanual task—unscrew jar lid. Acquired semg signals (in volts) versus normalized predictions (output of the ilmc), finger positions and motor currents (output of the ih2 hands, averaged across doas) during task t7. The superimposed striped rectangles delimit the times when the proper grips were performed, in order to complete the task, i.e., cylindrical grasp with the right hand, and tridigital grasp with the left hand. The graph represents the full trial with
Tc=20.12
s. Mean predictions and mean positions are relative to the dofs actuated during the current grasps, i.e., for the cylindrical grasp all the finger flexions are considered, for the tridigital grasp the three flexions of thumb, index and middle fingers are considered. In the graphs is evident the mean values of the fingers positions follow the mean values of the prediction signals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-3-source-large.gif
2017,7460959,Fig. 4.,"Completion times (tc) of expert versus naïve subjects. The first block of bars refers to the right hand tasks, the second block to the left hand tasks and the third block to the bimanual tasks. Mean ± SEM of tcs of all repetitions of the same task calculated separately for expert and naïve subjects. Completion rates (cr) values are not shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-4-source-large.gif
2017,7460959,Fig. 5.,Durations and temporal synchronization of the experimental phases per subject. Black lines denote the exact time of retraining during the experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-5-source-large.gif
2017,7460959,Fig. 6.,Required retraining sessions during the functional test per subject.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-6-source-large.gif
2017,7460959,Fig. 7.,"Results. Completion times (tc), on the y axis, and completion rates (cr, superimposed on the graphs) achieved during the experiment. Mean ± SEM (standard error of the mean, represented by the error bars) calculated across all the nine subjects. Task repetitions are showed using different color bars.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/7883987/7460959/7460959-fig-7-source-large.gif
2017,7964681,Fig. 1.,Traditional grid model (source: http://www.pserc.wisc.edu/research/ futuregrid.aspx).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya1-2722358-large.gif
2017,7964681,Fig. 2.,ESM production system architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya2-2722358-large.gif
2017,7964681,Fig. 3.,NIST IoT smart grid conceptual reference model (source: https://www.nist.gov/).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya3-2722358-large.gif
2017,7964681,Fig. 4.,Conceptual DSS technician site visit versus remote support prediction flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya4-2722358-large.gif
2017,7964681,Fig. 5.,"ESM unit number 13 693 309, normal distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya5-2722358-large.gif
2017,7964681,Fig. 6.,"ESM unit number 14 273 679, normal distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya6-2722358-large.gif
2017,7964681,Fig. 7.,BN model for ESM field technician site visits decision support.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya7-2722358-large.gif
2017,7964681,Fig. 8.,ESM network operations remote resolution decision support and total cost savings predictions with six scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8004446/7964681/sirya8-2722358-large.gif
2017,7945239,FIGURE 1.,Diagram of the non-parametric CSL approach vs. baseline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri1-2714579-large.gif
2017,7945239,FIGURE 2.,LS decision surfaces of two Gaussian classes. The blue-shaded area represents the “xor” logical operation between the two surfaces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri2-2714579-large.gif
2017,7945239,FIGURE 3.,"SVM decision surfaces of extended datasets and support vector configuration for a validation pattern with class
w
0
. Down: zoom on upper figures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri3-2714579-large.gif
2017,7945239,FIGURE 4.,Overall performance on the dataset. CSL vs baseline and relevancy. Up: 200 sample; Bottom: 500 samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri4-2714579-large.gif
2017,7945239,FIGURE 5.,Histograms of the SVM output scores under hypotheses. Up: Positive validation pattern; Down: Negative validation pattern.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri5-2714579-large.gif
2017,7945239,FIGURE 6.,"Axial example slices (# 30) of four subjects of the SPECT database. Left to right, top to bottom: NOR, AD1, AD2, AD3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri6-2714579-large.gif
2017,7945239,FIGURE 7.,Pre-selection of 20 BAs in light colors using a t-test based feature rank algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri7-2714579-large.gif
2017,7945239,FIGURE 10.,Performance of the proposed method (blue) vs the baseline (magenta) using PLS over the most relevant BAs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri10-2714579-large.gif
2017,7945239,FIGURE 11.,Configuration of the output score SVM in the PLS-based CSL approach on both hypothesis for a relevant feature. Real class: negative.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri11-2714579-large.gif
2017,7945239,FIGURE 8.,"Detail of the improvement of the proposed method (blue) vs the baseline (red) by considering the whole brain volume approach. M: miss, H: hit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri8-2714579-large.gif
2017,7945239,FIGURE 9.,PCA on the SPECT dataset. Note how the improvement subjects are located close to the decision surface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri9-2714579-large.gif
2017,7945239,FIGURE 12.,Asymptotic probability of HLS classes (above) and ambiguous generalization (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7945239/gorri12-2714579-large.gif
2017,7956172,FIGURE 1.,Distribution of personality scores on Big Five traits on Sina Weibo dataset. (a) Openness to experience. (b) Conscientiousness. (c) Extraversion. (d) Agreeableness. (e) Neuroticism.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7956172/xue1abcde-2719018-large.gif
2017,7885555,Fig. 1.,CNN feature extractor architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng1-2672643-large.gif
2017,7885555,Fig. 2.,ELM classifier architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng2-2672643-large.gif
2017,7885555,Fig. 3.,Flowchart of the proposed framework for land-use classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng3-2672643-large.gif
2017,7885555,Fig. 4.,"CNN architecture used in this letter. The boxes show the size of each feature layer and, for fully connected layers, the size of the output. Most receptive fields are
3×3
, and the max-pool layers are not displayed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng4-2672643-large.gif
2017,7885555,Fig. 5.,Class representatives of the UCM data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng5-2672643-large.gif
2017,7885555,Fig. 6.,"Performance evaluations on the classification accuracies of Bayes, KNN, SVM, and ELM with GIST, LBP, PHOG, and CNN features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng6-2672643-large.gif
2017,7885555,Fig. 7.,"Performance evaluations on the classification accuracies of Bayes, KNN, SVM, and ELM with varied proportions as training samples of the CNN feature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng7-2672643-large.gif
2017,7885555,Fig. 8.,Confusion matrix of the classification results achieved by the proposed CNN-ELM method using 70% of the CNN feature for training with 5000 hidden nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/7906640/7885555/weng8-2672643-large.gif
2017,7820190,Fig. 1.,"The model illustration of RSBLR. Left: The training set consists of pairs of queries and ranking lists, formed by data with nonlinear distribution patterns in the feature space (2-D space for simplicity). Middle: The structural learning model RSBLR learns a Bregman distance function, to effectively capture these nonlinear patterns based on the side information of ranking structures. The learned Bregman distance induces a nonlinear feature mapping to discriminate the distribution patterns (top), such that the data distributions in the mapped space are aligned with their semantic relevance relationships (bottom). Right: The learned Bregman distance function is used to produce the ranking results for test queries.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8002781/7820190/li1-2654250-large.gif
2017,7820190,Fig. 2.,"The PR curves for
s=0,10
on the three datasets. Our methods (RSBLR/SBLR) have a better overall performance than the comparative methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8002781/7820190/li2-2654250-large.gif
2017,7820190,Fig. 3.,"The ROC curves for
s=0,10
on the three datasets. Our methods (RSBLR/SBLR) have a better overall performance than the comparative methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8002781/7820190/li3-2654250-large.gif
2017,7820190,Fig. 4.,"The MAP@100 and pAUC
(0,0.3)
performances of RSBLR in the
s=0
case w.r.t.
λ∈{0,50,100,500,
10
3
}
, with fixed
C=
10
3
. The integration of
R
2
(φ)
is beneficial for the model performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8002781/7820190/li4-2654250-large.gif
2017,7820190,Fig. 5.,"Three retrieval examples for RSBLR and the best two comparative methods. The images bounded with red boxes are the matched ones. The mismatched images have different visual characteristics that are similar to that of the query in some sense. Our method RSBLR can better discriminate them in the ranking lists, due to its capacity of capturing the complicated nonlinear correlations of image features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8002781/7820190/li5-2654250-large.gif
2017,7820190,Fig. 6.,"(a) A sketch diagram of a Bregman distance function in 1-D space, with
φ(x)=exp(
1
2
x)
. The
d
B
(
x
a
,
x
b
)
is the sum of lengths of the two red dashed lines. (b) Equidistant contours of five center points (black “x”) in 2-D space, for the Mahalanobis distance with metric matrix
W=[3,−1;−1,3]
and the symmetrized Bregman distance with
φ(x)=exp(
1
20
x
T
Wx)
, respectively. Each contour has a unit distance to its corresponding center.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8002781/7820190/li6-2654250-large.gif
2017,7393573,Fig. 1.,"Three-cluster example with decision boundaries trained by different objectives. MLE: maximum likelihood estimate.
L2
: MLE with
L2
-norm. minEnt: minimum entropy with weight decay (
L2
) [15]. maxEnt: maximum entropy proposed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7393573/mille1-2514401-large.gif
2017,7393573,Fig. 2.,Supervised classification experiment. Average ROC AUC performance comparison of different feature mappings input to linear and RBF kernel SVMs on various UCI data sets. (a) Page Block. (b) Spam Base. (c) Abalone. (d) Yeast. (e) KDD’99. (f) Seismic Pump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7393573/mille2abcdef-2514401-large.gif
2017,7393573,Fig. 3.,AL experiment for different sample selection strategies. (a) Page Block. (b) Statlog Shuttle. (c) Yeast. (d) Wine Quality. (e) KDD’99. (f) Spam Base.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7393573/mille3abcdef-2514401-large.gif
2017,7393573,Fig. 4.,"AL experiment. Comparison of methods’ ROC AUC performance and average classification accuracy versus number of oracle labelings, on various data sets. (a) Page Block. (b) Statlog Shuttle. (c) Yeast. (d) Wine Quality. (e) KDD’99. (f) Spam Base.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/7879455/7393573/mille4abcdef-2514401-large.gif
2017,7444177,Fig. 1.,"Process of splitting protein amino acids sequence into smaller
k
-mers (2-mers, 3-mers, and 4-mers in this case).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7829491/7444177/zhou1-2524994-large.gif
2017,7444177,Fig. 2.,"Overall prediction accuracy rate against the reduced rank
r
of LRA for various kinds of feature representations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7829491/7444177/zhou2-2524994-large.gif
2017,7444177,Fig. 3.,"Performance comparison with six validation metrics using K-ELM classifiers (green bar) and state-of-the-art SVM classifiers (blue bar). (a) Accuracy rates. (b) Sensitivity. (c) Specificity. (d) PPV. (e) NPV. (f)
F
-score. (g) MCC. (h) AUC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7829491/7444177/zhou3abcdefgh-2524994-large.gif
2017,7444177,Fig. 4.,ROC curve for H. sapiens using different computational models for the LRA feature representation with polarity and polarizability properties. The curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7829491/7444177/zhou4-2524994-large.gif
2017,7444177,Fig. 5.,ROC curve for H. sapiens using different computational models for the LRA feature representation with hydrophobicity and van der Waals properties. The curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/7829491/7444177/zhou5-2524994-large.gif
2017,7850943,Fig. 1.,Optimization methods applied to a predefined function. (a) Multistart (function counts = 345). (b) Global search (function counts = 273). (c) Pattern search (function counts = 272). (d) Genetic algorithm (function counts = 2650). (e) BO (function counts = 100) [8].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park1abcde-2656843-large.gif
2017,7850943,Fig. 2.,"Concept of machine learning consists of training and evaluation/execution phases [9], [10].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park2-2656843-large.gif
2017,7850943,Fig. 3.,Configuration of a 3-D system for optimization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park3-2656843-large.gif
2017,7850943,Fig. 4.,Flow of electrical–thermal simulation for 3-D system design.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park4-2656843-large.gif
2017,7850943,Fig. 5.,(a) Chip layout. (b) Fabricated PCB and wire-bonded chip.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park5ab-2656843-large.gif
2017,7850943,Fig. 6.,"(a) Measured
I
–
V
profile of temperature monitoring circuits. (b)
I
–
V
profile with temperature variations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park6ab-2656843-large.gif
2017,7850943,Fig. 7.,(a) Power maps used for simulation and measurement. (b) Measured temperature profiles. (c) Simulated temperature profiles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park7abc-2656843-large.gif
2017,7850943,Fig. 8.,Black box function with multivariable for 3-D system design.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park8-2656843-large.gif
2017,7850943,Fig. 9.,Proposed flow for electrical–thermal simulation using BO. (a) electrical–thermal simulation. (b) BO.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park9ab-2656843-large.gif
2017,7850943,Fig. 10.,"Distribution plots of (a) function, (b) posterior mean, (c) posterior variance, and (d) LCB acquisition function for optimization of the 3-D system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park10abcd-2656843-large.gif
2017,7850943,Fig. 11.,Power maps used for optimization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park11-2656843-large.gif
2017,7850943,Fig. 12.,Response surface with a target value.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park12-2656843-large.gif
2017,7850943,Fig. 13.,"Optimization results for heat transfer coefficient (
X
1
)
and TIM thermal conductivity (
X
2
)
showing convergence; TIM thickness (
X
3
)
is not plotted (
N=3
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park13abcd-2656843-large.gif
2017,7850943,Fig. 14.,Optimization results with target value of \$T_{\mathrm {MAX}}\$ : 120.0 and \$T_{\mathrm {GRAD}}\$ : 25.0. (a) Found \$X\text{s}\$ and (b) temperature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park14ab-2656843-large.gif
2017,7850943,Fig. 15.,Optimization with power map II. (a) Iterations shown as a function of three parameters only. (b) Temperature distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park15ab-2656843-large.gif
2017,7850943,Fig. 16.,Optimization with power map III. (a) Iterations shown as a function of three parameters only. (b) Temperature distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park16ab-2656843-large.gif
2017,7850943,Fig. 17.,"Comparison of convergence between pattern search, nonlinear solver, and BO (a) temperature gradient and (b) thermal skew.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/7932577/7850943/park17ab-2656843-large.gif
2017,8008864,Fig. 1.,The comparison of average running time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8097365/8008864/tang1-2738643-large.gif
2017,8008864,Fig. 2.,The relationship between the annotator number and computational time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8097365/8008864/tang2-2738643-large.gif
2017,8008864,Fig. 3.,The robustness comparison of competing methods to random noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8097365/8008864/tang3-2738643-large.gif
2017,8008864,Fig. 4.,The robustness comparison of competing methods to concensus noise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8097365/8008864/tang4-2738643-large.gif
2017,8008864,Fig. 5.,The robustness comparison of competing methods to systematic noise. (a) Noisy annotators make errors on certain subsets of the data set. (b) Noisy annotators make errors following a linear bias/function. (c) Noisy annotators make errors following a nonlinear bias/function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/8097365/8008864/tang5-2738643-large.gif
2017,7953641,Fig. 1.,Instances and bags in MIL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang1-2717803-large.gif
2017,7953641,Fig. 2.,Investigation on unlabeled bags for an SVM classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang2-2717803-large.gif
2017,7953641,Fig. 3.,"Illustrative examples for (a)
k
-means clustering and (b) kernel
k
-means clustering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang3-2717803-large.gif
2017,7953641,Fig. 4.,"Computing the dissimilarity degree of instances in a bag. (a) Bag 1, (b) Bag 2, and (c) Bag 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang4-2717803-large.gif
2017,7953641,Fig. 5.,Training samples in MNIST dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang5-2717803-large.gif
2017,7953641,Fig. 6.,"Performance comparison of different learning strategies on MNIST MIL datasets. (Base-learner: mi-SVM). (a) Digit“0” (50 trials), (b) digit “1” (50 trials), (c) digit “2” (50 trials), (d) digit “3” (50 trials), (e) digit “4” (50 trials), (f) digit “5” (50 trials), (g) digit “6” (50 trials), (h) digit “7” (50 trials), (i) digit “8” (50 trials), (j) digit “9” (50 trials), (k) average result for ten digits, and (l) legend.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang6-2717803-large.gif
2017,7953641,Fig. 7.,"Sensitivity analysis of parameter
α
for (a) and (b)
SoftMax
and (c) and (d)
CombinU
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang7-2717803-large.gif
2017,7953641,Fig. 8.,Performance comparison on the multidigit MIL dataset. (Base-learner: mi-SVM.),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang8-2717803-large.gif
2017,7953641,Fig. 9.,Performance comparison of different learning strategies on Corel MIL datasets. (Base-learner: mi-SVM). (a) Elephant (50 trials). (b) Fox (50 trials). (c) Tiger (50 trials).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang9-2717803-large.gif
2017,7953641,Fig. 10.,Different bags in MNIST and Corel datasets. (a) Bags in MNIST datasets. (b) Bags in Corel datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/8124149/7953641/wang10-2717803-large.gif
2017,7953498,FIGURE 1.,Methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm1-2716978-large.gif
2017,7953498,FIGURE 2.,Outcome of the Decision Tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm2-2716978-large.gif
2017,7953498,FIGURE 3.,Q-Q Plot and Histogram for residuals with non-normalized dataset 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm3-2716978-large.gif
2017,7953498,FIGURE 4.,Q-Q Plot and Histogram for residuals with normalized dataset 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm4-2716978-large.gif
2017,7953498,FIGURE 5.,Q-Q Cook’s distance for non-normalized and normalized dataset 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm5-2716978-large.gif
2017,7953498,FIGURE 6.,Cook’s distance for non-normalized and normalized dataset 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm6-2716978-large.gif
2017,7953498,FIGURE 7.,Feature selection in dataset 1 using Adj R2 and BIC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm7-2716978-large.gif
2017,7953498,FIGURE 8.,Variances with Principal Components for dataset 1 and 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7953498/stojm8-2716978-large.gif
2017,7842619,Fig. 1.,Illustration of the eTL framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng1-2664665-large.gif
2017,7842619,Fig. 2.,Illustration of meme representation taking the form of FALCON and BP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng2-2664665-large.gif
2017,7842619,Algorithm 1:,Meme Internal Evolution Process,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng12-2664665-large.gif
2017,7842619,Algorithm 2:,Meme Selection Process,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng13-2664665-large.gif
2017,7842619,Fig. 3.,Fully general multiagent scenario with the proposed eTL paradigms. Agents employ FALCON or BP as the learning machine in their mind universes and learn the domain knowledge for performing appropriate actions given environmental states. They may also interact directly as indicated by the arrows between agents.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng3-2664665-large.gif
2017,7842619,Algorithm 4:,Basic eTL Framework,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng14-2664665-large.gif
2017,7842619,Fig. 4.,Snapshot of the MNT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng4-2664665-large.gif
2017,7842619,Fig. 5.,"SRs of agents under AE models with discount factor 0.1, 0.5, and 0.9 on completing the missions in MNTs. (a) FALCON agents. (b) BP agents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng5ab-2664665-large.gif
2017,7842619,Fig. 6.,"SRs of FALCON or BP agents under eTL, PTL, AE-AVG, and Conv. M on completing the missions in MNT. (a) FALCON agents. (b) BP agents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng6ab-2664665-large.gif
2017,7842619,Fig. 7.,"Snapshots of the FALCON agents’ navigation routes on completing 1, 500, 1000, and 2000 learning missions in MNT. The objective of each FALCON agent (denoted as a tank in the figure) in a learning mission is to arrive at the target successfully by navigating across the minefield safely and within the allocated time span. Thus a learning mission completes when all tanks reach the target, hit a mine or collide with another tank, or exceed the given time steps. All tanks and mines are randomly generated for each learning mission hence different learning missions have unique navigational routes. (a) The first learning mission (agents have zero knowledge). (b) After 500 learning missions. (c) After 1000 learning missions. (d) After 2000 learning missions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng7abcd-2664665-large.gif
2017,7842619,Fig. 8.,"SRs of FALCON and BP agents under eTL, PTL, AE-AVG, and Conv. M on completing the missions in the heterogeneous MAS. (a) FALCON agents. (b) BP agents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng8ab-2664665-large.gif
2017,7842619,Fig. 9.,"SRs of FALCON and BP agents under AE models with discount factor 0.1, 0.5, and 0.9 on completing the missions in the heterogeneous MAS. (a) FALCON agents. (b) BP agents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng9ab-2664665-large.gif
2017,7842619,Fig. 10.,Snapshot of the UT2004.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng10-2664665-large.gif
2017,7842619,Fig. 11.,KR of FALCON robots under the Conv. M and the proposed eTL fighting against the rule-based Hunters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/7990276/7842619/feng11-2664665-large.gif
2017,7933339,FIGURE 1.,"The proposed machine learning-enabled system for wearable cuff-less SBP and HR monitoring from motion artifacts-sensitive ear-ECG/PPG signals. Algorithm block coloring in b): white colored blocks are active in both training and testing phases; each blue colored block include both a supervised model learning process and a model using process, which are active in the training phase and the testing phase, respectively; each gray colored block includes an unsupervised model learning process and is active only in the training phase. Abbreviations: R/B/S represent the reference/bias/signal electrodes used for single-lead ECG signal measurement, respectively; P corresponds to the reflective PPG sensor; SVM, support vector machine; HB, heartbeat; DTW, dynamic time warping; SQI, signal quality index; PTT, pulse transit time; HR, heart rate; SBP, systolic blood pressure. Definitions of the Greek letters: refer to section II material and method => unsupervised learning of signal quality labelling and purification => algorithm 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang1ab-2707472-large.gif
2017,7933339,FIGURE 2.,"Ear-ECG/PPG Signals, with the amplitude both scaled to be between 0 and 1 for good readability (further analysis on the signal quality such as the signal strength and morphology will be given in the results section).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang2-2707472-large.gif
2017,7933339,FIGURE 3.,Pulse transit time (PTT) measured with ECG and PPG signals (This illustration of PTT is based on ear-ECG/PPG signals).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang3-2707472-large.gif
2017,7933339,FIGURE 4.,"An example of the signal segments acquired (chest-ECG, ear-PPG, and ear-ECG), showing that the weak ear-ECG has a peak-to-peak voltage only around 5% of that of the chest-ECG.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang4abcd-2707472-large.gif
2017,7933339,FIGURE 5.,Two examples of heartbeat identification results in the testing session of the subject 1. Blue dots: identified raw ECG heartbeat locations; red dots: identified raw PPG heartbeat locations; wide orange rectangles: signal periods with deliberately introduced motion artifacts due to head movements; narrow orange rectangle: signal period with missing or fake heartbeats due to severe background motion artifacts; all the weak ECG signal is continuously impacted by background motion artifacts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang5-2707472-large.gif
2017,7933339,FIGURE 6.,"K-medoids clustering-based PPG template learning in the first trial in the training session of subject 1, with K equals to 20 (only top 9 clusters are visualized here). Pink curve: medoid of each cluster; green curves: instances represented by the medoids; #=: percentage of instances in current cluster.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang6-2707472-large.gif
2017,7933339,FIGURE 7.,PPG distortion threshold learning in the first trial in the training session of subject 1. Blue line: the histogram hypotenuse; red curve: the histogram envelope; green line: the maximum perpendicular distance; green dot: the learned normalized (0-1 range) threshold.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang7ab-2707472-large.gif
2017,7933339,FIGURE 8.,"The whole SQI generation process in the testing session of subject 1, including the signals acquired, quantified degree of distortion for raw PPG heartbeats, the adaptive distortion threshold and the SQI sequence generated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang8abcdef-2707472-large.gif
2017,7933339,FIGURE 9.,Bland-Altman plot for estimated HR (HRest) and reference HR (HRref).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang9-2707472-large.gif
2017,7933339,FIGURE 10.,Bland-Altman plots for SBP model 1 and 10 based on our framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7933339/zhang10ab-2707472-large.gif
2017,7850956,Fig. 1.,"A subset of frames illustrating the amount of information present in a video. A single video can capture a subject’s face under different pose, expression, and illumination variations. While some frames can be highly useful for face recognition, others can be detrimental to performance. Images are frames from the PaSC database [2].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa1-2668221-large.gif
2017,7850956,Fig. 2.,Summarizing the performance of some of the best performing face verification algorithms on the YouTube faces database [3]. It is evident that there is a huge gap in the performance at low false accept rates as compared to performance at EER. We showcase that the proposed algorithm performs well even at a low false accept rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa2-2668221-large.gif
2017,7850956,Fig. 3.,Illustrating the steps involved in the proposed face recognition algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa3-2668221-large.gif
2017,7850956,Fig. 4.,Feature-richness distributions for two different videos. Some of the most feature-rich (values close to 1) and least feature-rich frames (values close to 0) are presented for illustration. We can see that the high fidelity frames are assigned a higher feature richness score and the poor frames which showcase artifacts such as occlusion and blur are assigned a low feature-richness score. Note that the total number of frames in the two videos is different.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa4-2668221-large.gif
2017,7850956,Fig. 5.,"Proposed deep learning architecture for facial representation: from input layer (image), two hidden layer representations are computed using SDAE encoding function. A joint representation is then obtained which combines the information from two SDAE encoding layers. Using joint representation as input, a DBM is used for computing a final feature vector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa5-2668221-large.gif
2017,7850956,Fig. 6.,"Joint learning framework: features learned from the first and second levels of autoencoder, i.e.,
f
1
and
f
2
are given as input to DBM to learn the joint representation
J
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa6-2668221-large.gif
2017,7850956,Fig. 7.,ROC curves comparing the verification performance of the proposed algorithm with existing results as reported on the YTF database webpage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa7-2668221-large.gif
2017,7850956,Fig. 8.,Summarizing the verification performance of the proposed algorithm and state-of-the-art algorithms on the YouTube Faces database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa8-2668221-large.gif
2017,7850956,Fig. 9.,"Sample frames from the PaSC database: (a) random frames, (b) frontal frames, (c) most feature-rich frames, and (d) least feature-rich frames.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa9abcd-2668221-large.gif
2017,7850956,Fig. 10.,ROC curves comparing the verification performance of the proposed algorithm with frame selection approaches on the two databases. (a) YouTube Faces database. (b) Handheld subset of PaSC database. (c) Control subset of PaSC database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/7898876/7850956/vatsa10abc-2668221-large.gif
2017,7368909,Fig. 1.,"Example of the manifold structures existing in the tracking task. The top row shows the structure of the adjacent tracked results, and the bottom row indicates the structure of the samples with different transformations near the target region in the same frame.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang1-2513659-large.gif
2017,7368909,Fig. 2.,"Two examples of the convergence curves of the normalized
∥w∥
2
and obj during the solving procedure. (a) Training samples are selected from the sequence carDark. (b) Training samples are selected from the sequence david. Both
∥w∥
2
and obj are normalized by their maximum values for better presentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang2ab-2513659-large.gif
2017,7368909,Fig. 3.,"GST tracking framework. It includes the motion model, the appearance model, and the update model. Note that the proposed GS-SVM is trained based on both the labeled and unlabeled samples. (a) Select samples from previous frames. (b) Select samples from current frame before location. (c) Select new samples from current frame after location.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang3abc-2513659-large.gif
2017,7368909,Fig. 4.,"Complete Laplacian graph is constructed based on all of the samples, including the labeled samples from frame
t−Δt
to
t−1
and the unlabeled samples from frame
t
. Note that in each frame, the regions corresponding to the rectangle
x
i
with different transformations
{
y
m
}(j=1,…,M)
are all considered as the samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang4-2513659-large.gif
2017,7368909,Fig. 5.,"Proposed hybrid update model, which includes the (a) update of the target pool and (b) update of the transformation pool.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang5ab-2513659-large.gif
2017,7368909,Fig. 6.,"Precision plots and success plots obtained by GST and the top nine trackers in the benchmark. The values in the square brackets represent the precision with
T
h
p
=20
pixels on precision plots and the AUC on success plots, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang6-2513659-large.gif
2017,7368909,Fig. 7.,Precision and success plots obtained by GST and the top nine trackers in the benchmark for the sequences with different attributes. The value in the title represents the number of sequences with corresponding attribute. (a) Plots for occlusion attribute. (b) Plots for deformation attribute. (c) Plots for illumination variation attribute. (d) Plots for scale variation attribute. (e) Plots for out-of-plane rotation attribute. (f) Plots for in-plane rotation attribute. (g) Plots for fast motion attribute. (h) Plots for background clutter attribute.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang7abcdefgh-2513659-large.gif
2017,7368909,Fig. 8.,"Precision and success plots obtained by GST with different
γ
. The value in the title represents the number of sequences with corresponding attribute. (a) Plots for the overall evaluation. (b) Plots for occlusion attribute. (c) Plots for out-of-plane rotation attribute. (d) Plots for fast motion attribute. (e) Plots for background clutter attribute. (f) Plots for illumination variation attribute.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang8abcdef-2513659-large.gif
2017,7368909,Fig. 9.,"Precision and success plots obtained by GST with different
k
. The value in the title represents the number of sequences with corresponding attribute. (a) Plots for the overall evaluation. (b) Plots for occlusion attribute. (c) Plots for out-of-plane rotation attribute. (d) Plots for fast motion attribute. (e) Plots for scale variation attribute. (f) Plots for deformation attribute.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang9abcdef-2513659-large.gif
2017,7368909,Fig. 10.,Precision and success plots obtained by GS-SVM and LapSVM. The value in the title represents the number of sequences with corresponding attribute. (a) Plots for the overall evaluation. (b) Plots for occlusion attribute. (c) Plots for out-of-plane rotation attribute. (d) Plots for fast motion attribute. (e) Plots for background clutter attribute. (f) Plots for deformation attribute.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang10abcdef-2513659-large.gif
2017,7368909,Fig. 11.,"Failure examples of GST on some representative sequences. Top to bottom: jogging2, skiing, and twinings.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/76/7938542/7368909/zhang11-2513659-large.gif
2017,7983338,FIGURE 1.,Architecture of AE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen1-2728010-large.gif
2017,7983338,FIGURE 2.,SAE training process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen2-2728010-large.gif
2017,7983338,FIGURE 3.,Architecture of the stacked SAE network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen3-2728010-large.gif
2017,7983338,FIGURE 4.,Training process of the stacked SAE network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen4-2728010-large.gif
2017,7983338,FIGURE 5.,Diagnosis flow chart of the proposed stacked SAE network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen5-2728010-large.gif
2017,7983338,FIGURE 6.,Test rig for bearing signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen6-2728010-large.gif
2017,7983338,FIGURE 7.,"Bearing signals of different health conditions in the time domain: (a) IF, (b) BF, (c) OF, (d) N.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen7abcd-2728010-large.gif
2017,7983338,FIGURE 8.,Varying curve of the FPE criterion under different health conditions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen8-2728010-large.gif
2017,7983338,FIGURE 9.,Fault classification results of the proposed stacked SAE network for bearing datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen9-2728010-large.gif
2017,7983338,FIGURE 10.,Test rig of the gearbox signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen10-2728010-large.gif
2017,7983338,FIGURE 11.,"Gearbox signals of different health conditions in the time domain: (a) SW, (b) MW, (c) BT, (d) N.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen11abcd-2728010-large.gif
2017,7983338,FIGURE 12.,Varying curve of the FPE criterion under different health conditions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen12-2728010-large.gif
2017,7983338,FIGURE 13.,Fault classification results of the proposed stacked SAE network for gearbox datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen13-2728010-large.gif
2017,7983338,FIGURE 14.,"Diagnosis accuracy of different methods with different training sample sizes: (a) bearing datasets, (b) gearbox datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen14ab-2728010-large.gif
2017,7983338,FIGURE 15.,"Features visualization: (a) raw data features for bearing, (b) AR-based features for bearing, (c) raw data features for gearbox, (d) AR-based features for gearbox.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen15abcd-2728010-large.gif
2017,7983338,FIGURE 16.,"Visualization of features mined by the proposed method: (a) bearing datasets, (b) gearbox datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7983338/shen16ab-2728010-large.gif
2017,8032490,Fig. 1.,"The main contribution of this paper is the joint analysis of the unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views with the automatically generated mass (yellow annotations) and micro-calcification (red annotations) segmentation maps. This is a holistic methodology that can classify a whole mammographic exam, with the CC and MLO views and the segmentation maps, as opposed to the classification of individual lesions, which is the mainstream approach of the field. The functionality of our methodology relies on the use of deep learning models, pre-trained with computer vision datasets [4], [5], [15], [81].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne1-2751523-large.gif
2017,8032490,Fig. 2.,"Distribution of BI-RADS (left) and negative, benign and malignant classes (right) for the cases in INbreast (blue) and DDSM (red), where a case is represented by the MLO and CC mammographic views with respective segmentation maps (MCs and masses) of a single breast scan of a patient.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne2ab-2751523-large.gif
2017,8032490,Fig. 8.,"Correct (a-c) and incorrect (d) classifications on DDSM [42] test cases using Imagenet pre-trained “JOIN 4” model with
50×
data augmentation, where the ground truth mass (white) and MC (green) detections and classifications (text below images) are shown. Notice that the manual mass and MC annotations are significantly less precise than the ones from INbreast, shown in Fig. 7.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne8abcd-2751523-large.gif
2017,8032490,Fig. 3.,"Multi-view ConvNet models using different types of merging strategies for 2-D and 3-D models. The baseline model contains
L=5
convolutional layers,
K=2
fully connected layers and one final softmax layer. The model can have 2-D or 3-D inputs (a), and be pre-trained and fine-tuned in four different manners, as depicted in (b)-(e) (i.e., JOIN 1 to JOIN 4), where for the 2-D model, there are six inputs and for the 3-D model there are two inputs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne3abcde-2751523-large.gif
2017,8032490,Fig. 7.,"Correct (a-c) and incorrect (d) classifications on INbreast [57] test cases using Imagenet pre-trained “JOIN 4” model with
50×
data augmentation, where the ground truth (GT) and the automatic (AUTO) mass (GT in white, AUTO in yellow) and MC (GT in green, AUTO in cyan) detections and classifications (text below images) are shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne7abcd-2751523-large.gif
2017,8032490,Fig. 4.,"3-Class Problem: VUS results on INbreast [57] (a-b) and DDSM [42] (c) for “JOIN 4” (pre-trained - first column) for the multi-modal (MultiView (2D) and (3D)) and individual inputs (mammographic views and segmentation maps) as a function of training data augmentation; and for all types of merging strategies (as displayed in Fig. 3) of the pre-trained and randomly initialised multi-view models using the 2-D input (“JOIN 1” to “JOIN 4” - second column). Also notice that we show the results for the semi-automated (rows a,c) and fully-automated methods (row b). The p-values show the t-test results comparing the pre-trained and randomly initialised models regarding the merging strategies and data augmentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne4abc-2751523-large.gif
2017,8032490,Fig. 5.,"2-Class Problem - lesion classification (benign vs malignant): AUC (lesion classification) results on INbreast [57] (a-b) and DDSM [42] (c) for “JOIN 4” (pre-trained - first column) for the multi-modal (MultiView (2D) and (3D)) and individual inputs (mammographic views and segmentation maps) as a function of training data augmentation; and for all types of merging strategies (as displayed in Fig. 3) of the pre-trained and randomly initialised multi-view models using the 2-D input (“JOIN 1” to “JOIN 4” - second column). Also notice that we show the results for the semi-automated (rows a,c) and fully-automated methods (rows b). The p-values show the t-test results comparing the pre-trained and randomly initialised models regarding the merging strategies and data augmentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne5abc-2751523-large.gif
2017,8032490,Fig. 6.,"2-Class problem-breast screening (negative/normal findings vs malignant): AUC (lesion classification) results on INbreast [57] (a-b) and DDSM [42] (c) for “JOIN 4” (pre-trained - first column) for the multi-modal (MultiView (2D) and (3D)) and individual inputs (mammographic views and segmentation maps) as a function of training data augmentation; and for all types of merging strategies (as displayed in Fig. 3) of the pre-trained and randomly initialised multi-view models using the 2-D input (“JOIN 1” to “JOIN 4” - second column). Also notice that we show the results for the semi-automated (rows a,c) and fully-automated methods (rows b). The p-values show the t-test results comparing the pre-trained and randomly initialised models regarding the merging strategies and data augmentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8087860/8032490/carne6abc-2751523-large.gif
2017,8015191,Fig. 1.,Schematic diagram of supervised and unsupervised methods for driving style classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang1-2736948-large.gif
2017,8015191,Fig. 2.,"(a) Hinge loss
L
1
(y,t)
and its differentiable surrogate
L
~
1
(y,t)
. (b) Hinge loss
L
2
(t)
and its differentiable surrogate
L
~
2
(t)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang2-2736948-large.gif
2017,8015191,Fig. 3.,Driving simulator for collecting driving data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang3-2736948-large.gif
2017,8015191,Fig. 4.,Road profiles designed using 3Ds-Max.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang4-2736948-large.gif
2017,8015191,Fig. 5.,Examples of (a) the rule-based scheme and (b) a few labeled data points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang5-2736948-large.gif
2017,8015191,Fig. 6.,Classification accuracy of using S3VM and SVM based on an RBF kernel and a linear kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang6-2736948-large.gif
2017,8015191,Fig. 7.,"Examples of results using SVM with a linear kernel. (a)
μ=0.755
,
P=9
. (b)
μ=0.860
,
P=290
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang7-2736948-large.gif
2017,8015191,Fig. 8.,"Examples of results using SVM with an RBF kernel. (a)
μ=0.624
,
P=9
. (b)
μ=0.772
,
P=290
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang8-2736948-large.gif
2017,8015191,Fig. 9.,"Examples of results using S3VM with a linear kernel. (a)
μ=0.762
,
P=9
. (b)
μ=0.865
,
P=290
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang9-2736948-large.gif
2017,8015191,Fig. 10.,"Examples of results using S3VM with a RBF kernel. (a)
μ=0.751
,
P=9
. (b)
μ=0.866
,
P=290
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8038150/8015191/wang10-2736948-large.gif
2017,7756418,Fig. 1.,Optimal ETFT when random parameters vary. (a) Impact of nominal processing time variation. (b) Impact of learning rate variation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/17/7827167/7756418/li1ab-2618764-large.gif
2017,7756418,Fig. 2.,Gap between the SEPT and optimal policy when random parameters vary. (a) Impact of nominal processing time variation. (b) Impact of learning rate variation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/17/7827167/7756418/li2ab-2618764-large.gif
2017,7756418,Fig. 3.,EVwPI of ETFT when random parameters vary. (a) Impact of nominal processing time variation. (b) Impact of learning rate variation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/17/7827167/7756418/li3ab-2618764-large.gif
2017,7756418,Fig. 4.,EVPI when random parameters vary. (a) ETFT. (b) Expected makespan.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/17/7827167/7756418/li4ab-2618764-large.gif
2017,7938706,Fig. 1.,"Influence of parameter selection on describing basic banana shape target data when alternating
C
and
σ
for OCELM. (a)
C=100
,
σ=10
. (b)
C=10
,
σ=10
. (c)
C=10000
,
σ=10
. (d)
C=100
,
σ=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang1abcd-2707463-large.gif
2017,7938706,Fig. 2.,"n
-round MST on a basic banana shape dataset when
n
takes 1, 2, 3, respectively. (a)
n=1
. (b)
n=2
. (c)
n=3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang2abc-2707463-large.gif
2017,7938706,Fig. 5.,Enclosing surface visualization on synthetic 2-D datasets using OCELM with MST-GEN. (a) Basic banana. (b) Spiral. (c) Sine. (d) Rectangle ring. (e) Four banana. (f) Halo.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang5abcdef-2707463-large.gif
2017,7938706,Fig. 3.,EPD (left) and repelling process (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang3ab-2707463-large.gif
2017,7938706,Fig. 4.,Generate pseudo target data for banana dataset. (a) Original banana dataset. (b) Midpoints of 2-round MST edges (in green). (c) Generated pseudo target data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang4abc-2707463-large.gif
2017,7938706,Fig. 6.,Performance comparison on UCSD pedestrian dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang6-2707463-large.gif
2017,7938706,Fig. 7.,"Sensitivity analysis of (a)
n
and (b)
m
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8026346/7938706/wang7ab-2707463-large.gif
2017,7869416,Fig. 1.,"From the IF and HE images and the vital tumour mask (box 1) obtained from a tumour tissue, the proposed methodologies must produce a high-level annotation (box 3) consisting of the number and proportion of normoxic (N), chronically hypoxic (CH), and acutely hypoxic (AH) regions. Box 2 shows the latent localisation and classification of MCSUs in the input images, where it is also necessary to detect the necrotic regions because of the inaccurate segmentation provided by the vital tumour mask. Note that in the HE image, pink regions denote vital tumour regions, red regions represent necrotic tissue, and white regions indicate missing tissue caused by the imaging process, as explained below in Sec. III; while in the IF image, red denotes micro-vessel, green represents hypoxia and blue means perfusion. This figure is better visualised electronically - please zoom in the IF/HE images in the middle box to notice the region annotations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne1-2677479-large.gif
2017,7869416,Fig. 2.,"The imaging process of an MCSU. Notice that a micro-vessel can be cut in different ways, generating visually different MCSUs. Also notice that each pixel with a strong red component in the IF stained image represents the centre pixel of an MCSU candidate, which must be clustered with other neighbouring detected MCSU candidates in order to form an MCSU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne2-2677479-large.gif
2017,7869416,Fig. 3.,"Sketch of the appearance of MCSU classes [23]. Necrotic regions (a) have a red region at the centre of the IF image, followed by black pixels around it (indicating necrotic tissue); acute hypoxia (b) also has a red centre, but immediately followed by green regions (indicating hypoxia); chronic hypoxia (c) is denoted by a red region at the centre with a blue region immediately around it (indicating perfusion), followed by a green region towards the border; and normoxic MCSUs (d) again have a red region at the centre, and a blue region around it. Moreover, normoxic, chronic and acute hypoxic MCSUs have a smooth pink appearance in the HE image (indicating vital tumour tissue), while necrotic regions have a broken red appearance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne3abcd-2677479-large.gif
2017,7869416,Fig. 5.,"The methodologies proposed in this paper receive as input the IF and HE images (a), from which micro-vessel pixels are detected and classified (first two frames in (b)). Then for the FLSSVM, the graph
G
is built and labelled using the initial graph
G
ini
in order to represent the MCSUs and form
Ψ
(.) for (7) and (18). For DCNN, a series of convolutional layers applied to the micro-vessel pixel classification images produce a final map containing the MCSUs and their classes. From the outputs of FLSSVM and DCNN, it is trivial to obtain the final annotation in (c). The naive baseline based on the majority vote of the rigid grid nodes of the set
V
, defined in Sec. IV-C, is represented in the box NAIVE, and the more sophisticated baseline based on probabilistic graphical model, but using a rigid grid, is depicted in the bos RSSVM. This figure is better visualised with an electronic reader reader - please zoom in the IF/HE images to notice the MCSU annotations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne5ab-2677479-large.gif
2017,7869416,Fig. 4.,Features used for the MCSU candidate classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne4-2677479-large.gif
2017,7869416,Fig. 6.,Inputs and outputs for the DCNN model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne6-2677479-large.gif
2017,7869416,Fig. 7.,"Bland Altman graphs of MCSU classification in terms of the numbers (left) and proportion (right) of MCSU classes for the NAIVE (1,b), RSSVM (c,d), FLSSVM (e,f) and DCNN (g,h) models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne7abcdefgh-2677479-large.gif
2017,7869416,Fig. 8.,"Results of three different test images that show a qualitative comparison of NAIVE, RSSVM, FLSSVM, and DCNN in terms of the number and percentage of MCSU classes estimated by each model compared to the manual annotation (left image in each case). This figure is better visualised electronically - please zoom in the IF images to notice the MCSU annotations (note that we only show the results on the IF image, but the input consists of the IF and HE images, as shown on the leftmost column).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7961173/7869416/carne8-2677479-large.gif
2017,7968419,FIGURE 1.,Experiment flow (a) mental stress condition and (b) control condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7968419/subha1ab-2723622-large.gif
2017,7968419,FIGURE 2.,Proposed ML framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7968419/subha2-2723622-large.gif
2017,7936433,Fig. 1.,"HSR images are with high intraclass diversity and low interclass variation. (a) Object distribution in HSR scene is very complex. For example, there are multithematic classes in an airport scene: airplanes, highways, and architectures. The airport scene is easy to be mixed with other scenes, such as freeway and buildings. (b) Interclass dissimilarity is too low for HSR scenes, such as overpass, freeway, and runway. They are too similar in visual content to separate even by human beings.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu1ab-2702596-large.gif
2017,7936433,Fig. 2.,"Procedure of the proposed method. First, we learn feature map by weighted deconvolution network. And then, the feature map is aggregated by the SPM. Finally, the HSR image scenes are classified by the SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu2-2702596-large.gif
2017,7936433,Fig. 3.,Example images associated with the 21 land-use categories in the UCMerced data set. 1: agricultural. 2: airplane. 3: baseball diamond. 4: beach. 5: buildings. 6: chaparral. 7: dense residential. 8: forest. 9: freeway. 10: golf course. 11: harbor. 12: intersection. 13: medium residential. 14: mobile home park. 15: overpass. 16: parking lot. 17: river. 18: runway. 19: sparse residential. 20: storage tanks. 21: tennis court.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu3-2702596-large.gif
2017,7936433,Fig. 4.,(a) Whole image for scene classification. (b) Example images associated with the seven land-use categories from the image. 1: residential. 2: airport. 3: meadow. 4: river. 5: ocean. 6: industrial. 7: runway.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu4ab-2702596-large.gif
2017,7936433,Fig. 5.,"Performance of the proposed method with respect to parameter
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu5-2702596-large.gif
2017,7936433,Fig. 6.,Fifteen filters learned by the weighted deconvolution model and two typical corresponding feature maps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu6-2702596-large.gif
2017,7936433,Fig. 7.,Confusion matrix with the 21 land-use categories in the UCMerced data set. 1: agricultural. 2: airplane. 3: baseball diamond. 4: beach. 5: buildings. 6: chaparral. 7: dense residential. 8: forest. 9: freeway. 10: golf course. 11: harbor. 12: intersection. 13: medium residential. 14: mobile home park. 15: overpass. 16: parking lot. 17: river. 18: runway. 19: sparse residential. 20: storage tanks. 21: tennis court.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu7-2702596-large.gif
2017,7936433,Fig. 8.,Confusion matrix with the seven land-use categories in the Sydney data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8016688/7936433/lu8-2702596-large.gif
2017,7902094,Fig. 1.,"(Left side) Representation of how passive BCI concept is used to enhance human machine interaction. In particular, the pBCI is able to quantify the actual mental state (e.g., attention, workload) of the user, even in real-time, without interfere with his/her work. Such information can then be used to change the behavior of the user interface accordingly (e.g., adaptive automation in Air Traffic Environment, right side, [5]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/7949194/7902094/arico1-2694856-large.gif
2017,8098645,Fig. 1.,Processing procedure of individual tree extraction. (a) Raw point cloud. (b) Density map of the point cloud. (c) Results of tree extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng1abc-2764938-large.gif
2017,8098645,Fig. 2.,Preprocessing for individual tree point cloud. (a) Ground point removal. (b) Noise removal. (c) Trunk direction adjustment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng2abc-2764938-large.gif
2017,8098645,Fig. 3.,Rasterization and projection of individual tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng3-2764938-large.gif
2017,8098645,Fig. 4.,Structure of DBN model. (a) RBM layers. (b) Fully connected layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng4ab-2764938-large.gif
2017,8098645,Fig. 5.,"Workflow of the proposed method. The training process is in the red dashed box, and the test process is in the green dashed box.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng5-2764938-large.gif
2017,8098645,Fig. 6.,Four tree species in data set 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng6-2764938-large.gif
2017,8098645,Fig. 8.,Eight tree species in data set 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng8-2764938-large.gif
2017,8098645,Fig. 7.,Accuracy at different number of projection images. (a) Variation curve in the training stage. (b) Variation curve in the testing stage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8165926/8098645/cheng7ab-2764938-large.gif
2017,8315313,Figure 1.,Schematic of the proposed scheme using CNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/94/8315243/8315313/8315313-fig-1-source-large.gif
2017,8315313,Figure 2.,4 Convolution kernels (out of 12) from the first layer of the network after training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/94/8315243/8315313/8315313-fig-2-source-large.gif
2017,7912315,FIGURE 1.,CNN-based multimodal disease risk prediction (CNN-MDRP) algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang1abcdefg-2694446-large.gif
2017,7912315,FIGURE 2.,Running time comparison of CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms in personal computer (PC) and data center.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang2-2694446-large.gif
2017,7912315,FIGURE 3.,"Effect of sliding window (word number) in the algorithm. (a) The corresponding accuracy of the CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms when the number of words for sliding window are 1, 3, 5, 7 and 9. (b) The corresponding recall of the CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms when the number of words for sliding window are 1, 3, 5, 7 and 9.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang3ab-2694446-large.gif
2017,7912315,FIGURE 4.,Effect of iterations on the algorithm. (a) The trend of training error rate with the iterations for CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms. (b) The trend of test accuracy with the iterations for CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang4ab-2694446-large.gif
2017,7912315,FIGURE 5.,Effect of text features on the algorithm. (a) The accuracy trend of the CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms along with the increased number of text features. (b) The recall trend of the CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms along with the increased number of features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang5ab-2694446-large.gif
2017,7912315,FIGURE 6.,The three machine learning algorithms userd in our disease prediction experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang6-2694446-large.gif
2017,7912315,FIGURE 7.,"Overall results of S-data. (a) Comparison of accuracy, precision, recall and F1-Measure under S-data for NB, KNN and DT, in which NB = naive Bayesian,
KNN=k-nearest
neighbour, and DT = decision tree. (b) ROC curves under S-data for NB, KNN and DT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang7ab-2694446-large.gif
2017,7912315,FIGURE 8.,"Overall results of S&T-data. (a) Comparison of accuracy, precision, recall and F1-measure under CNN-UDRP (T-data) and CNN-MDRP (S&T-data) algorithms. (b) ROC curves under CNN-UDRP (T-data) and CNN-MDRP (S&T-data)algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7912315/hwang8ab-2694446-large.gif
2017,8101455,FIGURE 1.,Schematic diagram of Androidetect detection system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei1-2771470-large.gif
2017,8101455,FIGURE 2.,Schematic diagram of Android malicious behavior.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei2-2771470-large.gif
2017,8101455,FIGURE 3.,Schematic diagram of application system function interception.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei3-2771470-large.gif
2017,8101455,FIGURE 4.,Schematic diagram of injection of so base and interception of behavior codes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei4-2771470-large.gif
2017,8101455,FIGURE 5.,The basic form of eigenvector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei5-2771470-large.gif
2017,8101455,FIGURE 6.,Schematic diagram of the application behavioral interception.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei6-2771470-large.gif
2017,8101455,FIGURE 7.,Schematic diagram of TPR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei7-2771470-large.gif
2017,8101455,FIGURE 8.,Schematic diagram of FPR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei8-2771470-large.gif
2017,8101455,FIGURE 9.,Schematic diagram of ACC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei9-2771470-large.gif
2017,8101455,FIGURE 10.,Schematic diagram of classification accuracy of classification algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei10-2771470-large.gif
2017,8101455,FIGURE 11.,"Schematic diagram of TPR comparison of
a
and
b
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei11-2771470-large.gif
2017,8101455,FIGURE 12.,"Schematic diagram of TPR and FPR comparison of
a
and
b
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei12-2771470-large.gif
2017,8101455,FIGURE 13.,"Schematic diagram of ACC comparison of
a
and
b
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/8101455/wei13-2771470-large.gif
2017,7967659,FIGURE 1.,Main building blocks of Sensor Node and Base Station. Four Sensor Nodes were connected to a single Base Station.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta1-2721538-large.gif
2017,7967659,FIGURE 2.,"Raw data for Set A, sensor A, B, C, D in (a), (b), (c) and (d) respectively. Each color corresponds to one of the 20 data sets collected.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta2abcd-2721538-large.gif
2017,7967659,FIGURE 3.,"Raw data for Set B, sensor A, B, C, D in (a), (b), (c) and (d) respectively. Each color corresponds to one of the 20 data sets collected.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta3abcd-2721538-large.gif
2017,7967659,FIGURE 4.,"Organization of the experiment floor and body orientation for first localization experiment (experiment A). A fridge and metallic cabinet are partially included in the designated room space, while a metallic door and an electric switch board are close to the room space. They emulate the presence of metallic and electric objects in an apartment or house.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta4-2721538-large.gif
2017,7967659,FIGURE 5.,Organization of the floor and body orientation for the second localization experiment (B).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta5-2721538-large.gif
2017,7967659,FIGURE 6.,"Offset-compensated data for Set A, sensor A, B, C, D in (a), (b), (c) and (d) respectively. Each colour corresponds to one of the data sets collected.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta6abcd-2721538-large.gif
2017,7967659,FIGURE 7.,"Offset-compensated data for Set B, sensor A, B, C, D in (a), (b), (c) and (d) respectively. Each colour corresponds to one of the data sets collected.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta7abcd-2721538-large.gif
2017,7967659,FIGURE 8.,One Random Forest confusion matrix generated by Weka for data Set A. The top row lists the correct positions and the rightmost column shows the positions determined by the algorithm. Each non-diagonal number represents the number of erroneous predictions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta8-2721538-large.gif
2017,7967659,FIGURE 9.,"Localization error (in meters) for each position for Set C: (a) Random Forest, (b) SVM, (c) k-NN (for k=1), (d) Bayes Net. Darker dots for higher errors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta9abcd-2721538-large.gif
2017,7967659,FIGURE 10.,"Training data size dependency of average accuracy (a), distance error (b), precision (c) and recall (d) for set C for best performing machine language classification algorithms in Weka collection: Bayes Net (BN), k-Nearest Neighbors (k-NN with
k=1
), Random Forest (RF), Support Vector Machine (SVM), LogitBoost (LB(RF)) and AdaBoostM1(AB(RF)) running on top of Random Forest, and AdaBoostM1(AB(C4.5)) running on top of C4.5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta10abcd-2721538-large.gif
2017,7967659,FIGURE 11.,Training data size dependency of average accuracy (a) and distance error (b) for set C for Weka collection ML classification algorithms. Starred algorithms are built on top of Random Forest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta11ab-2721538-large.gif
2017,7967659,FIGURE 12.,"Processing effort in terms of CPU time during training (a) and processing effort during inferring (b) versus accuracy for set C. Bayes Net (BN), k-Nearest Neighbors (k-NN with
k=1
), Random Forest (RF), Support Vector Machine (SVM), LogitBoost (LB(RF)) and AdaBoostM1(AB(RF)) running on top of Random Forest, and AdaBoostM1(AB(C4.5)) running on top of C4.5. Random Forest and AdaBoostM1 with C4.5 seems the best trade-off between localization processing effort and performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/7859429/7967659/binta12ab-2721538-large.gif
2018,8458184,Fig. 1.,Floor plan.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8552698/8458184/alhaj1-2869548-large.gif
2018,8458184,Fig. 2.,"Spatial correlation coefficient
C
(n)
based on CTF signatures for different environments. (a) Highly cluttered. (b) Medium cluttered. (c) Low cluttered. (d) Open space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8552698/8458184/alhaj2-2869548-large.gif
2018,8458184,Fig. 3.,"Spatial correlation coefficient
C
(n)
based on FCF signatures for different environments. (a) Highly cluttered. (b) Medium cluttered. (c) Low cluttered. (d) Open space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8552698/8458184/alhaj3-2869548-large.gif
2018,8458184,Fig. 4.,"Confusion matrix using CTF + FCF for both
k
-NN and weighted
k
-NN (
k=1
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8552698/8458184/alhaj4-2869548-large.gif
2018,8458184,Fig. 5.,"k
-NN and weighted
k
-NN as a function of
k
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8552698/8458184/alhaj5-2869548-large.gif
2018,7979555,Fig. 1.,Refinement of sequence-based predicted interactions using concurrent miRNA and mRNA expression profiles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag1-2727042-large.gif
2018,7979555,Fig. 2.,(a) Schematic of the data set used to train and test SVM models. The data consists of two groups of features: Blue columns show binding features and purple columns show weighted network features. (b) Overview of the proposed predictive modeling approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag2-2727042-large.gif
2018,7979555,Fig. 3.,Venn diagram of predicted and validated interactions downloaded from various databases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag3-2727042-large.gif
2018,7979555,Fig. 4.,"Extracting network features and assigning them to interactions obtained from union of top
n
interactions in TaLasso and GenMiR++.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag4-2727042-large.gif
2018,7979555,Fig. 5.,(a) and (b) The number of validated interactions among top-ranked identified interactions by TaLasso and GenMiR++ in TGCT and KIRC data. (c) and (d) Shared interactions identified by TaLasso and GenMiR++ in TGCT and KIRC data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag5-2727042-large.gif
2018,7979555,Fig. 6.,Correlation between network features. (a) TGCT network from ARACNE. (b) KIRC network from ARACNE. (c) TGCT network from WGCNA. (d) KIRC network from WGCNA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag6-2727042-large.gif
2018,7979555,Fig. 7.,"Cumulative proportion of variance explained. (a) Network constructed by ARACNE, TGCT data. (b) Network constructed by ARACNE, KIRC data. (c) Network constructed by WGCNA, TGCT data. (d) Network constructed by WGCNA, KIRC data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag7-2727042-large.gif
2018,7979555,Fig. 8.,Precision of individual methods as well as integrated result. (a) TGCT data and (b) KIRC data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag8-2727042-large.gif
2018,7979555,Fig. 9.,Comparison of Precisions of the proposed methods with competing approaches. (a) TGCT network from ARACNE. (b) TGCT network from WGCNA. (c) KIRC network from ARACNE. (d) KIRC network from WGCNA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag9-2727042-large.gif
2018,7979555,Fig. 10.,"Obtained AUC values of SVM classifiers. (a) Network constructed by ARACNE, TGCT data. (b) Network constructed by ARACNE, KIRC data. (c) Network constructed by WGCNA, TGCT data. (d) Network constructed by WGCNA, KIRC data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag10-2727042-large.gif
2018,7979555,Fig. 11.,Coefficients corresponding to each feature in the logistic regression model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag11-2727042-large.gif
2018,7979555,Fig. 12.,p-values corresponding to coefficients of logistic regression model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8485525/7979555/sedag12-2727042-large.gif
2018,8038860,Fig. 1.,Proposed DML framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng1-2740318-large.gif
2018,8038860,Fig. 2.,Working principle of our proposed manifold regularizer. We only present two layers for better displaying effect.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng2-2740318-large.gif
2018,8038860,Fig. 3.,"Comparison of BN strategy and our proposed manifold regularizer. (a) Distribution of original data points, (b) original fitting lines, (c) distribution of data points and fitting lines after batch normalization, and (d) distribution of data points and fitting lines after our proposed manifold regularizer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng3abcd-2740318-large.gif
2018,8038860,Fig. 4.,"Our pretraining framework by incorporating manifold
Γ
into the RBM framework.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng4-2740318-large.gif
2018,8038860,Fig. 5.,"Some sample frames in the used action data sets. (a) Sample frames from HMDB51: hand waving, drinking, sword fighting, diving, and running. (b) KTH actions data set: walking, jogging, running, boxing, and hand waving. (c) UCF sports data set: diving, kicking, walking, skateboarding, and high-bar swinging.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng5abc-2740318-large.gif
2018,8038860,Fig. 6.,Three modes of embedding in deep architectures proposed in [10]. (a) Output. (b) Internal. (c) Auxiliary.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng6abc-2740318-large.gif
2018,8038860,Fig. 7.,"Training loss comparison in 2500 iterations, and a simply model with two convolutional layers is used for computational efficiency. (a) Original model versus model with DML. (b) Comparison of different batch sizes. (c) Comparison of balance factor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8038860/weng7abc-2740318-large.gif
2018,8485329,Fig. 1.,An example of composite constellation generated from two QPSK constellations with different power ratios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8570950/8485329/cheng1-2874218-large.gif
2018,8485329,Fig. 2.,"The diagram of MLAD, including two phases: training phase and blind detection phase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8570950/8485329/cheng2-2874218-large.gif
2018,8485329,Fig. 3.,The features of QPSK and 16QAM when modulation order of target user is 64QAM on different SNRs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8570950/8485329/cheng3-2874218-large.gif
2018,8485329,Fig. 4.,Blind detection rate comparisons on different detection algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8570950/8485329/cheng4-2874218-large.gif
2018,8485329,Fig. 5.,"Throughput comparisons on different detection algorithms. Target user: 16QAM (MCS=11), Interference user: QPSK (MCS=5).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8570950/8485329/cheng5-2874218-large.gif
2018,8485329,Fig. 6.,"Throughput comparison on different detection algorithms. Target user: 64QAM, Interference user: QPSK.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8570950/8485329/cheng6-2874218-large.gif
2018,8438452,FIGURE 1.,Normalized target difference.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes1-2865135-large.gif
2018,8438452,FIGURE 2.,"Conflict level and input similarity when
τ
i
=0.25
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes2-2865135-large.gif
2018,8438452,FIGURE 3.,"Conflict level and target difference when
τ
i
=0.25
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes3-2865135-large.gif
2018,8438452,FIGURE 4.,"Conflict level
c
ij
for
σ=0.01
and
τ
i
=0.25
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes4-2865135-large.gif
2018,8438452,FIGURE 5.,Weighting function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes5-2865135-large.gif
2018,8438452,FIGURE 6.,"Conflict level
c
ij
for
σ=0.02
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes6-2865135-large.gif
2018,8438452,Fig. 7.,Effectiveness of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8438452/ledes7-2865135-large.gif
2018,8006280,Fig. 1.,Multiple vertical wheel force measurements of a train wheel by the four sensors of one measurement bar. The wheel is affected by a discrete defect that manifests itself in the measurement of the first sensor. The remaining sensors do not directly observe the defect.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm1-2720721-large.gif
2018,8006280,Fig. 2.,Diagram of one sensor on a measurement bar of the WLC. The strain gauges are attached to the side of the wheel between two sleepers and cover 28cm of vertical wheel force of the wheel rolling on the track.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm2-2720721-large.gif
2018,8006280,Fig. 3.,Picture of a serious flat spot on a train wheel of SBB (a) and the resulting idealized wheel load measurement (b). (Picture taken from Wikipedia/Bobo11),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm3ab-2720721-large.gif
2018,8006280,Fig. 4.,Signals and wavelet coefficients at different levels (C1 to C3) of a defective (right) and non-defective (left) wheel. The power in the high frequency coefficients C2-C3 reveal the defect.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm4ab-2720721-large.gif
2018,8006280,Fig. 5.,"Structure of the cyclic permutation network that automatically learns cyclic shift invariant features. The red boxes on the left represent the weight shared CNN, the coloured bars designate features learned by the CNN, the stack of colored bars are permutations of the feature vectors, the blue dots the class log-likelihoods per permutation and the green box the final class probabilities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm5-2720721-large.gif
2018,8006280,Fig. 6.,Structure of the MIL defect detection network for flat spots. The network consists of one CNN per measurement with weights shared across the networks. The defect likelihood of the whole wheel is given by the maximum defect likelihood across sensors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm6-2720721-large.gif
2018,8006280,Fig. 7.,Top layer filters (a) and features (b) learned by the 1-dimensional defect detection network for flat spots for a measurement of a defective (right) and non-defective (left) wheel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm7ab-2720721-large.gif
2018,8006280,Fig. 8.,Example of a top layer filter (a) and corresponding features of the signal of a non-defective (b) and defective (flat spot) (c) wheel learned on 2D representations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8326752/8006280/krumm8abc-2720721-large.gif
2018,8355700,Fig. 1.,"Parallel-beam architecture. Green nodes represent intermediate results in volume domain, red nodes intermediate results in projection domain. The color of arrows denotes weight sharing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl1-2833499-large.gif
2018,8355700,Fig. 2.,"Fan-beam architecture. Green nodes represent intermediate results in volume domain, red nodes intermediate results in projection domain. The color of arrows denotes weight sharing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl2-2833499-large.gif
2018,8355700,Fig. 3.,"Short scan cone-beam architecture. Green nodes represent intermediate results in volume domain, red nodes intermediate results in projection domain. The color of arrows denotes weight sharing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl3-2833499-large.gif
2018,8355700,Fig. 5.,Figure 5a shows the line along which the intensity profile (figure 5b) has been computed in red. Green: Region of interest that has been used to compute SSIM and PSNR (Tables I and II). Figure 5b depicts the intensity profiles along the line shown in Figure 5a.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl5ab-2833499-large.gif
2018,8355700,Fig. 4.,Figures 4a and 4e show examples of ground truth slices. Using only half of the projections causes loss of mass as shown in Figures 4b and 4f. Our trained model can compensate for the loss of mass. The result is shown in Figures 4d and 4h. A similar result is achieved by the heuristic compensation weights proposed by Riess et al. [26] (Figures 4c and 4g).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl4abcdefgh-2833499-large.gif
2018,8355700,Fig. 8.,"Presentation of weights: (a) weights that have been learned by our model, (b) Parker weights, (c) weights as proposed by Riess et al. [26] without Gaussian smoothing, (d) smooth compensation weights proposed by Schäfer et al. [28].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl8abcd-2833499-large.gif
2018,8355700,Fig. 7.,"To show the robustness of the learned weights to noise, we added Gaussian noise to the projection data. (a) and (c) show the reconstruction results using Parker weights while (b) and (d) show the reconstruction results using the learned weights.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl7abcd-2833499-large.gif
2018,8355700,Fig. 6.,"Exemplary results from the cross-validation: (a) Groundtruth, (b) result using the proposed method, (c) using U-net based approach, (d) using the wTV method.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8355700/wurfl6abcd-2833499-large.gif
2018,8364584,Fig. 1.,"An example of fundus images from DIARETDB1. The magenta square marks an example of
25×25
H
0
patch which contains no MA lesion while the green square marks an example of
25×25
H
1
patch which contains a MA lesion in the center.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan1-2840084-large.gif
2018,8364584,Fig. 2.,"Enlarged of H0 (non-MA) and H1 (with MA) image patches from the fundus image example in Fig. 1. Bright yellow corresponds to pixels with the highest intensities, while dark blue corresponds to pixels with the lowest intensities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan2-2840084-large.gif
2018,8364584,Fig. 3.,ROC curves of the three classifiers using rasterized raw data of DIARETDB1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan3-2840084-large.gif
2018,8364584,Fig. 4.,ROC curves of a single hidden layer NN with different number of neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan4-2840084-large.gif
2018,8364584,Fig. 5.,Original data space after normalization (on the top) and the top 50 principal component subspace (on the bottom). Yellow corresponds to high intensity while blue corresponds to low intensity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan5-2840084-large.gif
2018,8364584,Fig. 6.,"Performance comparison using the RF, NN, and SVM with different amounts of principal components included for classification on DIARETDB1. The RF and NN ROCs include the SVM performance with 100% of components for reference, and the AUC plots also include the SVM AUC with 100% of components for reference (dashed lines in (b) and (d)). As shown, the SVM performance roughly saturates with 30-40% of components. However, RF and NN performance reaches the peak using ~5% of principal components before decreasing steadily.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan6-2840084-large.gif
2018,8364584,Fig. 7.,"Examples of H0 (in dashed lines) and H1 (in solid lines) image patches, surrounding an image of the feature importance (yellow is high) learned by a RF in the center block. Brighter pixel locations in the central image correspond to more important components of the image, indicating that the center of the
25×25
image patches contains the most important information for discrimination between MAs and non-MAs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan7-2840084-large.gif
2018,8364584,Fig. 8.,"Performance comparison using the RF, NN, and SVM with different amounts of data included as selected by RF feature importance on DIARETDB1. The RF and NN ROCs include the SVM performance with 100% of components for reference, and the AUC plots also include the SVM AUC with 100% of components for reference. In contrast to PCA results, performance of all three classifiers maintains approximately maximum performance once AUC saturation occurs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan8-2840084-large.gif
2018,8364584,Fig. 9.,ROC curves of the three classifiers using rasterized raw data from ROC dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan9-2840084-large.gif
2018,8364584,Fig. 10.,"Performance comparison using the RF, NN, and SVM with different amounts of principal components included for classification on ROC dataset. The RF and NN ROCs include the SVM performance with 100% of components for reference, and the AUC plots also include the SVM AUC with 100% of components for reference (dashed lines in (b) and (d)). As shown, the SVM performance roughly saturates with 30–40% of components. However, RF and NN performance reaches the peak using 5% ~ 10% of principal components before decreasing steadily.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364584/shan10-2840084-large.gif
2018,8291154,Fig. 1.,"C
∗
s
versus
γ
¯
¯
¯
sd
for different transmission schemes and values of
N
s
with
γ
¯
¯
¯
se
=10
dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8439029/8291154/liu1-2805902-large.gif
2018,8291154,Fig. 2.,"P
∗
so
(
R
s
)
versus
γ
¯
¯
¯
sd
for different transmission schemes and values of
N
s
with
γ
¯
¯
¯
se
=10
dB and
R
s
=2
bits/Hz/s.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8439029/8291154/liu2-2805902-large.gif
2018,8010348,Fig. 1.,"The setting for monocular gaze redirection. Left–an input frame with the gaze directed below the camera. Middle–a “ground truth” frame with the gaze directed 15 degrees higher than in the input. Given an input image and the desired change in angle and direction (“15 degrees higher”) our method aims to produce an image that for human perception is as close to ground truth as possible. The result of one of the proposed systems (Section 3.4) is shown on the right. In this particular example, the computation time of the method is 5 ms on a single laptop core (excluding feature point localization). Such speed makes our system suitable for real-time use in videoconferencing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon1-2737423-large.gif
2018,8010348,Fig. 2.,Gaze redirection with our neural network-based system trained for vertical gaze redirection. The model takes an input image (middle row) and the desired redirection angle (here varying between −15 and +15 degrees) and re-synthesize the new image with the new gaze direction. Note the preservation of fine details including specular highlights in the re-synthesized images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon2-2737423-large.gif
2018,8010348,Fig. 3.,"Left–dataset collection process. Right–examples of training pairs for
15
∘
vertical redirection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon3-2737423-large.gif
2018,8010348,Fig. 4.,"Processing of a pixel (green square) at test time in an eye flow tree. The pixel is passed through an eye flow tree by applying a sequence of tests that compare the position of the pixels w.r.t., the feature points (red crosses) or compare the differences in intensity with adjacent pixels (bluish squares) with some threshold. Once a leaf is reached, this leaf defines a matching of an input pixel with other pixels in the training data. The leaf stores the map of the compatibilities between such pixels and eye flow vectors. The system then takes the optimal eye flow vector (yellow square minus green square) and uses it to copy-paste an appropriately-displaced pixel in place of the input pixel into the output image. Here, a one tree version is shown for clarity, our actual system would sum up the compatibility scores coming from several trees before making the decision about the eye flow vector to use.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon4-2737423-large.gif
2018,8010348,Fig. 5.,"The deep warp system takes an input eye region, feature points (anchors) as well as a correction angle
α
and sends them to the multi-scale neural network (see Section 3.3.1) predicting a flow field. The flow field is then applied to the input image to produce an image of a redirected eye. Finally, the output is enhanced by processing with the lightness correction neural network (see Section 3.3.3).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon5-2737423-large.gif
2018,8010348,Fig. 6.,"The architecture of the two warping modules: (process
0.5×
-scale 6(a) and process
1×
-scale 6(b)) predicting and applying pixel-flow to the input image; 6(c) represents a fully convolutional sequence of layers inside warping modules.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon6-2737423-large.gif
2018,8010348,Fig. 7.,"Visualization of three challenging redirection cases where the Lightness Correction Module helps considerably compared to the system based solely on coarse-to-fine warping (CFW), which is having difficulties with expanding the area to the left of the iris. The ‘Mask’ column shows the soft mask corresponding to parts where lightness is increased. Lightness correction fixes problems with dis-occluded eye-white, and also emphasizes the specular highlight increasing the perceived realism of the result.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon7-2737423-large.gif
2018,8010348,Fig. 8.,8(a)–The architecture of the Lightness Correction Module. The output of the lightness correction module is a weighted sum of the image created by the warping modules and the palette (which in this paper is taken to be a single white colour). The mixing weights predicted by the network are passed through the softmax activation and therefore sum to one at each pixel. The module takes the features computed by the coarse and the fine warping modules (from Fig. 8b) as input.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon8-2737423-large.gif
2018,8010348,Fig. 9.,"The output flow of warping modules (Section 3.3.1) on random samples from a training set. The coarse-to-fine model without lightness correction was trained on a task of
15
∘
redirection upwards. This data is used to train a neural network-supervised regression random forest. The right down figure is a color pattern, explaining how the direction of the flow vector is encoded with the color of the pixel. The more intense the pixel is, the longer the flow vector in this pixel is.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon9-2737423-large.gif
2018,8010348,Fig. 10.,"Ordered errors for
15
∘
vertical gaze redirection (see text for more discussion of the error metric). The best performance is shown by the full coarse-to-fine architecture with the lightness correction module.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon10-2737423-large.gif
2018,8010348,Fig. 11.,"Distribution of errors over different vertical correction angles. With the increase of the redirection angle, the more comprehensive models show the better quality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon11-2737423-large.gif
2018,8010348,Fig. 12.,"Results on a random subset of the hold-out test set. From left to right: (a) Input, (b) Eye-flow forests, (c) Neural network supervised forests, (d) Coarse-to-fine warping with the lightness correction module, (e) Coarse-to-fine warping without the lightness correction module, (f) Ground truth. The full variant (CFW + LCM) of deep warp system (e) generally performs the best.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon12-2737423-large.gif
2018,8010348,Fig. 13.,"Horizontal redirection with a model trained for both vertical and horizontal gaze redirection. For the first four rows the angle varies from
−
15
∘
to
15
∘
relative to the central (input) image. The last two rows push the redirection to extreme angles (up to
45
∘
) breaking our model down.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon13-2737423-large.gif
2018,8010348,Fig. 14.,"The results of NNSF system for the
15
∘
upwards redirection with input images of lower resolutions (the downsampling factors are shown at the top). Gaze redirection is persistent even for very low resolution images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon14-2737423-large.gif
2018,8010348,Fig. 15.,"Comparison with monocular gaze correction method from [8]. Left –the input video frame (taken from [8]). Middle –the output of our NNSF system. Right–the result of [8] . While both systems achieve convincing redirection effect, our NNSF system avoids the distortion of facial proportions (especially in the forehead and the chin regions), while also not requiring GPU to achieve real-time performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon15-2737423-large.gif
2018,8010348,Fig. 16.,"The screen-shot from the user study interface. User was instructed, that one of the four images is not real, and was asked to click on the one, which seems unnatural, spending not much time (trying not to exceed 5 seconds). In this example, the top left image is the right answer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8478843/8010348/konon16-2737423-large.gif
2018,8486814,Figure 1.,Three phases of ML product development.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/8486799/8486814/shams1-2870669-large.gif
2018,8486814,Figure 2.,Data flow diagram (DFD) for a smart engine to predict user engagement based on user search queries.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/8486799/8486814/shams2-2870669-large.gif
2018,8359287,FIGURE 1.,An example decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8359287/liu1-2836950-large.gif
2018,8359287,FIGURE 2.,An example RNN model structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8359287/liu2-2836950-large.gif
2018,8359287,FIGURE 3.,An example CNN model structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8359287/liu3-2836950-large.gif
2018,7983421,Fig. 1.,Symbiotic ramp-up cycle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse1-2717885-large.gif
2018,7983421,Fig. 2.,Assembly station process sequence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse2-2717885-large.gif
2018,7983421,Fig. 3.,Experimental set-up overview.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse3-2717885-large.gif
2018,7983421,Fig. 4.,"Q value discrepancy for every experience replay iteration for (a) alpha = 0.1, (b) different alpha values, and policy hash change for (c) alpha = 0.1 and (d) different alpha values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse4-2717885-large.gif
2018,7983421,Fig. 5.,Average accumulated reward for different alpha values after applying experience replay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse5-2717885-large.gif
2018,7983421,Fig. 6.,"Policy evaluation for random exploration in terms of (a) unfinished episodes, (b) average number of steps, (c) accumulated reward, and (d) number of unsupported states.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse6-2717885-large.gif
2018,7983421,Fig. 7.,"Policy evaluation for a greedy exploration in terms of (a) unfinished episodes, (b) average number of steps, (c) accumulated reward, and (d) number of unsupported states.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse7-2717885-large.gif
2018,7983421,Fig. 8.,"Policy evaluation for increased greedy exploration in terms of (a) unfinished episodes, (b) average number of steps, (c) accumulated reward, and (d) number of unsupported states.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse8-2717885-large.gif
2018,7983421,Fig. 9.,"Policy comparison in terms of (a) average steps and (b) unsupported states, for increasing number of ramp-up episodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse9-2717885-large.gif
2018,7983421,Fig. 10.,Comparison of different exploration strategies in terms of (a) average required steps and (b) unsupported states.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221037/8358712/7983421/lohse10-2717885-large.gif
2018,8000333,Fig. 1.,Leant adaptive weights by our (a) AELP-WL and (b) SAELP-WL on Extended YaleB face database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang1ab-2727526-large.gif
2018,8000333,Fig. 2.,"Classification results of our AELP-WL, where (a) original points without labels; (b) original partition (labeled, unlabeled, and test); (c) transductive learning result; (d) inductive learning result.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang2abcd-2727526-large.gif
2018,8000333,Fig. 3.,"Performance of our methods under various parameters, where (Top left) fix
α
to tune
β
and
γ
in AELP-WL; (Top right) fix
β
to tune
α
and
γ
in AELP-WL; (Bottom left) fix
γ
to tune
β
and
α
in AELP-WL; (Bottom right) tune
α
and
β
in our SAELP-WL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang3-2727526-large.gif
2018,8000333,Fig. 4.,"Image samples of Extended YaleB (Top left), AR (Top right), CMU PIE (Bottom left), and UMIST (Bottom right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang4-2727526-large.gif
2018,8000333,Fig. 5.,"Comparison of computational time of 1-GFHF, 2-LLGC, 3-LNP, 4-SLP, 5-CD-LNP, 6-ProjLP, 7-SparseNP, 8-LapLDA, 9-FME, 10-ELP, 11-SparseFME, 12-AELP-WL, and 13-SAELP-WL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang5-2727526-large.gif
2018,8000333,Fig. 6.,Convergence behavior of our AELP-WL and SAELP-WL on (a) AR face database and (b) Extended YaleB face database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang6ab-2727526-large.gif
2018,8000333,Fig. 7.,Classification performance of each method with varying levels of corruptions on the YaleB-UMIST face database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang7-2727526-large.gif
2018,8000333,Fig. 8.,Classification performance of each method with varying levels of corruptions on the CMU PIE face database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8000333/zhang8-2727526-large.gif
2018,8332522,Fig. 1.,OpenPLC encryption process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/8449127/8332522/alves1-2823906-large.gif
2018,8332522,Fig. 2.,External network layout of the IPS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/8449127/8332522/alves2-2823906-large.gif
2018,8332522,Fig. 3.,Ladder logic for the benchmark test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/8449127/8332522/alves3-2823906-large.gif
2018,8332522,Fig. 4.,Histogram for the benchmark test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4563995/8449127/8332522/alves4-2823906-large.gif
2018,7891559,Fig. 1.,"The graph shows the optimized annotation and validity annotation. Each row means a vector in which the elements with value 1 are shown with blue color and elements with value 0 are shown with white color. From top to bottom, the RTMTL optimized, EMVC optimized, LR optimized annotation, and the validity annotation are displayed, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891559/liang1-2690427-large.gif
2018,7891559,Fig. 2.,"FDR values computed by unoptimized, EMVC-optimized, RTMTL- optimized, and LR-optimized annotation matrices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891559/liang2-2690427-large.gif
2018,7891559,Fig. 3.,"FDR values computed by unoptimized, EMVC-optimized, RTMTL- optimized, and LR-optimized annotation matrices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891559/liang3-2690427-large.gif
2018,7891559,Fig. 4.,"FDR values computed by unoptimized, EMVC-optimized, and RTMTL- optimized annotation matrices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891559/liang4-2690427-large.gif
2018,8086220,Fig. 1.,Main prognostics approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor1-2767551-large.gif
2018,8086220,Fig. 2.,Schematic of test-rig layout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor2-2767551-large.gif
2018,8086220,Fig. 3.,"Actual and fitted SIE and RMS, Case 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor3-2767551-large.gif
2018,8086220,Fig. 4.,"Actual and fitted SIE and RMS, Case 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor4-2767551-large.gif
2018,8086220,Fig. 5.,"Actual and fitted SIE and RMS, Case 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor5-2767551-large.gif
2018,8086220,Fig. 6.,"Actual and fitted SIE and RMS, Case 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor6-2767551-large.gif
2018,8086220,Fig. 7.,Schematic of training and test processes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor7-2767551-large.gif
2018,8086220,Fig. 8.,ANN structure (two inputs RMS and SIE).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor8-2767551-large.gif
2018,8086220,Fig. 9.,Results by ANN model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor9-2767551-large.gif
2018,8086220,Fig. 10.,Results by SVMR mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor10-2767551-large.gif
2018,8086220,Fig. 11.,Results by GPR model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor11-2767551-large.gif
2018,8086220,Fig. 12.,Results of standard error with mean.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8307303/8086220/elfor12-2767551-large.gif
2018,8369054,FIGURE 1.,Proposed model of intrusion detection system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad1-2841987-large.gif
2018,8369054,FIGURE 2.,Architecture of SVM for intrusion detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad2-2841987-large.gif
2018,8369054,FIGURE 3.,Architecture of the RF for intrusion detection system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad3-2841987-large.gif
2018,8369054,FIGURE 4.,Architecture of the extreme learning machine for intrusion detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad4-2841987-large.gif
2018,8369054,FIGURE 5.,"Accuracy of SVM, RF, and ELM (80% training and 20% testing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad5-2841987-large.gif
2018,8369054,FIGURE 6.,"Precision of SVM, RF, and ELM (80% training and 20% testing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad6-2841987-large.gif
2018,8369054,FIGURE 7.,"Recall of SVM, RF, and ELM (80% training and 20% testing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad7-2841987-large.gif
2018,8369054,FIGURE 8.,"Accuracy of SVM, RF, and ELM (90% training and 10% testing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad8-2841987-large.gif
2018,8369054,FIGURE 9.,"Recall of SVM, RF, and ELM (90% training and 10% testing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad9-2841987-large.gif
2018,8369054,FIGURE 10.,"Recall of SVM, RF, and ELM (90% training and 10% testing).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8369054/ahmad10-2841987-large.gif
2018,7801860,Fig. 1.,"Architecture of ML-ELM. (a) ELM-AE outputs the transformation
γ
(1)
for representation learning. (b) Learning process of new input representation
x
(2)
is calculated by
g(
x
(1)
⋅(
γ
(1)
)
T
)
, where
g
is an activation function. (c)
x
(2)
is used as input to ELM-AE for another representation learning. (d) After the representation learning procedure is done, the final data representation
x
final
is used to calculate the output weight
β
for classification, where
c
is the number of target classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8301440/7801860/vong1abcd-2636834-large.gif
2018,7801860,Fig. 2.,"Architecture of the
i
th KELM-AE, in which the hidden layer becomes a kernel matrix
Ω
(i)
detailed in (12).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8301440/7801860/vong2-2636834-large.gif
2018,8486945,FIGURE 1.,Variable importance for the sideswipe collision type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8486945/li1-2874979-large.gif
2018,8486945,FIGURE 2.,Variable importance for ramp length.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8486945/li2-2874979-large.gif
2018,8486945,FIGURE 3.,Variable importance for rear-end collision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8486945/li3-2874979-large.gif
2018,8354788,FIGURE 1.,Experimental Testbed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos1-2833107-large.gif
2018,8354788,FIGURE 2.,Methodology for cwnd prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos2-2833107-large.gif
2018,8354788,FIGURE 3.,Methodology for TCP Variant prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos3-2833107-large.gif
2018,8354788,FIGURE 4.,Outstanding bytes calculated from the intermediate monitor using tcptrace [28] before applying convolutional filtering vs. the actual cwnd from the sender.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos4-2833107-large.gif
2018,8354788,FIGURE 5.,Initial prediction of TCP cwnd versus the actual cwnd before applying the convolutional filtering technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos5-2833107-large.gif
2018,8354788,FIGURE 6.,Final TCP cwnd prediction with different configurations of network emulation parameters for TCP CUBIC [15] and TCP Reno [19] after optimizing the initial cwnd prediction accuracy with convolution filtering technique in an emulated network. (a) CUBIC final predicted cwnd- Configuration C1. (b) CUBIC final predicted cwnd- Configuration C2. (c) CUBIC final predicted cwnd- Configuration C3. (d) CUBIC final predicted cwnd- Configuration C4. (e) Reno final predicted cwnd- Configuration C1. (f) Reno final predicted cwnd- Configuration C2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos6abcdef-2833107-large.gif
2018,8354788,FIGURE 7.,"TCP cwnd prediction of TCP CUBIC [15], TCP BIC [39] and TCP Reno [19] from a realistic scenario on different zones of Google Cloud platform (East coast USA (North Carolina) and Northeast Asia (Tokyo, Japan) sites). (a) CUBIC final predicted cwnd, USA site. (b) CUBIC final predicted cwnd, Northeast Asia site. (c) BIC final predicted cwnd, USA site. (d) BIC final predicted cwnd, Northeast Asia site. (e) Reno final predicted cwnd, USA site. (f) Reno final predicted cwnd, Northeast Asia site.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos7abcdef-2833107-large.gif
2018,8354788,FIGURE 8.,"TCP cwnd prediction of TCP CUBIC [15], TCP BIC [39] and TCP Reno [19] from a combined scenario setting. (a) CUBIC final predicted cwnd, combined scenario. (b) BIC final predicted cwnd, Northeast Asia site. (c) Reno final predicted cwnd, USA site.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos8abc-2833107-large.gif
2018,8354788,FIGURE 9.,Realistic scenario setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos9-2833107-large.gif
2018,8354788,FIGURE 10.,Combined scenario setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8354788/hagos10-2833107-large.gif
2018,8290925,FIGURE 1.,The taxonomy of security threats towards machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8290925/liu1-2805680-large.gif
2018,8290925,FIGURE 2.,Illustration of poisoning attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8290925/liu2-2805680-large.gif
2018,8290925,FIGURE 3.,Illustration of defensive techniques of machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8290925/liu3-2805680-large.gif
2018,8290925,FIGURE 4.,Typical workflows of two different defensive mechanisms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8290925/liu4-2805680-large.gif
2018,8290925,FIGURE 5.,Security assessment considering data distributions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8290925/liu5-2805680-large.gif
2018,8417405,FIGURE 1.,Overview of e-learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba1-2851790-large.gif
2018,8417405,FIGURE 2.,Challenges in e-learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba2-2851790-large.gif
2018,8417405,FIGURE 3.,VAK learning styles model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba3-2851790-large.gif
2018,8417405,FIGURE 4.,Kolb’s learning styles model [29].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba4-2851790-large.gif
2018,8417405,FIGURE 5.,Felder-Soloman learning style model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba5-2851790-large.gif
2018,8417405,FIGURE 6.,Different machine learning categories and algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba6-2851790-large.gif
2018,8417405,FIGURE 7.,RL model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba7-2851790-large.gif
2018,8417405,FIGURE 8.,Different data analytics categories and algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba8-2851790-large.gif
2018,8417405,FIGURE 9.,Qualitative data analysis process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba9-2851790-large.gif
2018,8417405,FIGURE 10.,Personalization in e-learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8417405/mouba10-2851790-large.gif
2018,8277160,Fig. 1.,"Possible representation of the DL, RL, and deep RL frameworks for biological applications. (a)–(f) Popular DL architectures. (g) Schematic of the learning framework as a part of AI. Broadly, AI can be thought to have evolved parallelly in two main directions—ES and ML. ES takes expert decisions from given factual data using rule-based inferences. ML extracts features from data mainly through statistical modeling and provides predictive output when applied to unknown data. DL, being a subdivision of ML, extracts more abstract features from a larger set of training data mostly in a hierarchical fashion resembling the working principle of our brain. The other subdivision, RL, provides a software agent that gathers experience based on interactions with the environment through some actions and aims to maximize the cumulative performance. (h) Possible applications of AI to biological data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8277160/mahmu1abcdefgh-2790388-large.gif
2018,8277160,Fig. 2.,"Performance comparison of representative DL techniques when applied to Omics data in (a) predicting splice junction, (b) CPIs, (c) secondary/tertiary structures of proteins, (d) analyzing GE data and classifying and detecting cancers from them, (e) predicting DNA- and RNA-sequence specificity (details about DREAM5 can be found in [201] and [202]), (f) RNA binding proteins, and (g) micro-RNA precursors. Ray et al.’s method [199].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8277160/mahmu2abcdefg-2790388-large.gif
2018,8277160,Fig. 3.,"Performance comparison of some DL and conventional ML techniques when applied to bioimaging application domain. (a) Performances in classifying EMIs for cell compartments, cell cycles, and cells. (b) Performances in analyzing images to automatically annotate features and detect mitosis and cell nuclei.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8277160/mahmu3ab-2790388-large.gif
2018,8277160,Fig. 4.,"Performance comparison of representative DL techniques when applied to medical imaging. (a) Performance of image segmentation techniques in segmenting tumors (BT: brain tumor), and different organ parts (ON: optic nerve, GL: gland, LV: left ventricle of heart, BP: blood pool, and MC: myocardium). (b) Image denoising techniques to improve image quality during the presence of GN, PN, SPN, and SN. (c) Detecting anomalies and diseases in mammograms. (d) Classification and detection of AD and MCI, along with healthy controls (NC). (e) Performance of prominent techniques for LNC, organ classification, brain tumor detection, colon polyp detection, and chemotherapy response detection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8277160/mahmu4abcde-2790388-large.gif
2018,8277160,Fig. 5.,"Accuracy comparison of DL and conventional ML techniques when applied to BMI signals. (a) Performance comparison in detecting motor imagery, recognizing emotion and cognitive states (ER), and detecting anomaly (AnD) from EEG signals. (b) Accuracies of MD from EMG signals. (c) Accuracies of ECG signal classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8277160/mahmu5abc-2790388-large.gif
2018,8328842,Fig. 1.,Circuit structure of a BR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak1-2821267-large.gif
2018,8328842,Fig. 2.,Oscillation and convergence states of a BR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak2abcd-2821267-large.gif
2018,8328842,Fig. 3.,Circuit structure of a BR-PUF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak3-2821267-large.gif
2018,8328842,Fig. 4.,Delay propagation of inverters in BR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak4ab-2821267-large.gif
2018,8328842,Fig. 5.,Distribution of the relative errors in the convergence time with and without the approximation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak5-2821267-large.gif
2018,8328842,Fig. 6.,"Schematic of the proposed CF-PUF (
Nmod2=1
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak6-2821267-large.gif
2018,8328842,Fig. 7.,Prediction accuracy by SVM attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak7-2821267-large.gif
2018,8328842,Fig. 8.,Prediction accuracy by ensemble ML techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak8-2821267-large.gif
2018,8328842,Fig. 9.,Reproducibility under temperature and voltage changes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/8352911/8328842/tanak9-2821267-large.gif
2018,8267239,Fig. 1.,"We design a sickness predictor to estimate the nauseogenicity of virtual content. In designing the features for the predictor, we draw on insights from the simulator sickness literature; we also test a theory of sickness that alters the role of object depth as a feature, and we run an experiment to verify the nature of this interaction with depth. Given the result, we choose as features various summary statistics based on the interaction between depth and motion speeds in a time-varying VR video. (a) Shown above is the left viewport for a single frame of one of the videos in our dataset. We calculate (b) Disparities and (c) Optical flow vectors for each pixel as measures of the depth and motion, respectively. (d) We represent each pixel in a 3d vector space with axes of disparity, horizontal velocity, and vertical velocity. This representation is parameterized and binned in various ways to find the best predictor for the video's sickness rating. (e) The sickness rating meter displays the sickness rating in a user-friendly way.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8315156/8267239/24tvcg04-wetzstein-2793560-fig-1-source-large.gif
2018,8267239,Fig. 2.,"We selected a set of 19 videos with widely varying scene content to create the dataset; next, we asked 96 users to each watch a single video from this set and answer the kennedy simulator sickness questionnaire. The kennedy SSQ scores provide the basis for our ground truth sickness ratings, which are shown above (dots). A selection of videos across the spectrum of sickness have been highlighted (larger black dots).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8315156/8267239/24tvcg04-wetzstein-2793560-fig-2-source-large.gif
2018,8267239,Fig. 3.,"(top) For the random dot kinematogram, the user is placed in the center of two independently rotating concentric spherical shells (as labeled) of equal width, containing dots subtending a constant visual angle. (bottom) The naturalistic scenes experienced by users. The near cluster of asteroids in the space scene has been brightened relative to the background for illustration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8315156/8267239/24tvcg04-wetzstein-2793560-fig-3-source-large.gif
2018,8267239,Fig. 4.,"The scatterplots show the vection and sickness ratings of each user in the three scenes, with motion condition indicated. The average ratings for each motion condition across all scenes is shown in the bar graph. As expected, the foreground motion condition has low ratings. The other two are similar in the relationship between vection and sickness, suggesting that nearby reference frames do not reduce sickness.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8315156/8267239/24tvcg04-wetzstein-2793560-fig-4-source-large.gif
2018,8267239,Fig. 5.,"An illustration of feature calculation for a single frame. (a) For the first two sets of features, we'll consider only a single component of the motion at a time, in this case, horizontal speed. This gives us a 2D space which the pixels occupy, which is then divided into nine regions. (b) The first set of features are the percent of pixels within each division of (a), giving an approximation of the fraction of the visual field which that disparity-speed range represents. (c) The second set of features only groups the points (a) by disparity and uses the mean velocity in that disparity range. (d) The last set of features operates directly on the 3d representation of the pixels. They use pca to extract the normal
n
, mean
m
, and explained variances
σ
2
1
and
σ
2
2
to capture relative interactions and spread of the data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8315156/8267239/24tvcg04-wetzstein-2793560-fig-5-source-large.gif
2018,8267239,Fig. 6.,"The predicted sickness ratings for a selection of videos in our dataset (each point of sickness is 4° on the meter). The first 11 videos are from the training set (Table 2 in order, excluding dotsall, dotsinner, glowingdance, and spacevisit) and the last four videos (outlined in purple) are the test set (their order, left to right, corresponds to their ordering in Table 3). In general, the predicted ratings cluster near the mid-to-high range of the true sickness ratings.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8315156/8267239/24tvcg04-wetzstein-2793560-fig-6-source-large.gif
2018,7829370,Fig. 1.,"Comparison of multi-scale filters learnt via MSCSC and CSC from (a) GBM dataset, where each tissue image is decomposed into two spectra (channels) corresponding to nuclei and extracellular matrix (ECM) for filter learning; and (b) A synthetic image, consisting of four distinct binarized shapes (
★
,
■
,
∙
,
▲
) at two different scales (
13×13
and
27×27
). It is clear that, through joint learning via MSCSC, the filters at smaller scale (i.e.,
13×13
) mainly captures lower-level features/small objects (e.g., edges in GBM dataset and small shapes in synthetic image), while the filters at larger scale (i.e.,
27×27
) are more responsible for higher-level features/large objects (e.g., complex pattern in ECM of GBM dataset and large shapes in synthetic image). However, filters learnt separately per scale via CSC do not have such scale-specificity, and present a mixture of low-/high-level features at both scales, which might lead to feature redundancy across scales. It is also worth to mention that, for GBM dataset, the difference in scale-specificity becomes more distinct for filters learnt from ECM, since compared with nuclear chromatin, ECM sees much more complex patterns, which CSC fails to capture.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang1-2656884-large.gif
2018,7829370,Fig. 2.,The proposed multi-scale multi-spectral feature extraction framework. CoD: Color decomposition; Abs: Absolute value rectification; LCN : Local contrast normalization; MP: Max-pooling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang2-2656884-large.gif
2018,7829370,Fig. 3.,Examples from GBM and KIRC datasets. Note that the phenotypic signatures are highly diverse in each column.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang3-2656884-large.gif
2018,7829370,Fig. 4.,"Experimental revisit on color decomposition, where, by default, MultiScale-CSCSPM operated on decomposed spectra corresponding to the nuclear chromatin and the extracellular matrix respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang4-2656884-large.gif
2018,7829370,Fig. 5.,"Experimental revisit on max-pooling, where, by default, MultiScale-CSCSPM utilized the max-pooling strategy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang5-2656884-large.gif
2018,7829370,Fig. 6.,"Experimental revisit on absolute value rectification, where, by default, MultiScale-CSCSPM employed absolute value rectification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang6-2656884-large.gif
2018,7829370,Fig. 7.,"Comparison with other related multi-scale/deep learning methods, where the best performances of each method/strategy on GBM and KIRC datasets were reported with 160 and 280 training images per category, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang7-2656884-large.gif
2018,7829370,Fig. 8.,Examples: First column: DCIS model; Second column: ERBB2+; Third column: Triple negative.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang8-2656884-large.gif
2018,7829370,Fig. 9.,Performance of different methods for the classification of subtypes in breast cancer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang9-2656884-large.gif
2018,7829370,Fig. 10.,"Illustration of generality and specificity of features/knowledge derived by MSCSC across domains. It is worth to mention that the filter banks at different scales were jointly learned in an unsupervised fashion with clear scale-specificity: The filters at smaller scale mainly captures lower-level features, while the filters at larger scale are more responsible for higher-level features. Such an scale-specificity not only help reduce the feature redundancy across scales, but also serves as the basis for transfer learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang10-2656884-large.gif
2018,7829370,Fig. 11.,"Evaluation of sharable knowledge derived by MSCSC in the tissue histology domain, where KIRCTransfer-MultiScale-CSCSPM is the direct application of pre-trained model from KIRC to GBM dataset during feature extraction, and GBMTransfer-MultiScale-CSCSPM is the direct application of pre-trained model from GBM to KIRC dataset during feature extraction. It is clear that the information derived by MSCSC independently from GBM and KIRC datasets are directly transferable to each other, which further confirms the insight indicated in Fig. 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang11-2656884-large.gif
2018,7829370,Fig. 12.,Evaluation of sharable knowledge derived by MSCSC from human tissue histology for the differentiation of mouse breast tumor morphology between radiation-induced cancer and spontaneous cancer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang12-2656884-large.gif
2018,7829370,Fig. 13.,"Experimental evaluation on the transfer learning capability of MSCSC with various transfer/fine-tuning levels, where the filter banks with two different scales were pre-trained on GBM dataset. GBMTransfer-Multiscale-CSCSPM-ft2nd: Partial-fine-tuning, where filter bank with smaller scale was fixed and filter bank with larger scale was re-trained/fine-tuned; GBMTransfer-Multiscale-CSCSPM: Non-fine-tuning, where pre-trained filter banks at both scales were directly applied without tuning; GBMTransfer-Multiscale-CSCSPM-ft1st2nd: All-fine-tuning, where pre-trained filter banks at both scales were re-trained/fine-tuned; Multiscale-CSCSPM: Learning-from-scratch, where filter banks at both scales were directly trained on the breast cancer dataset with random initialization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/8329157/7829370/chang13-2656884-large.gif
2018,8052229,Fig. 1.,Fitting multiple hyperspheres simultaneously with a predefined outlier fraction is the core idea of our proposed method ClusterSVDD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8052229/gorni1-2737941-large.gif
2018,8052229,Fig. 2.,"Results for anomaly detection settings for our ClusterSVDD (right column) and SVDD (center column) for linear settings (top row) and RBF kernel settings (bottom row) by assumption of 10% outlier (
ν=0.1
). Ground truth (left column) shows nominal data in green color and outliers in red. Note that the description learned by our ClusterSVDD is much more concise than for SVDD for the linear and the kernel case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8052229/gorni2-2737941-large.gif
2018,8052229,Fig. 3.,"Clustering results for our ClusterSVDD and
k
-means for linear settings (top row) and RBF kernel settings (bottom row). The ground truth (left column) was generated from four Gaussians with similar variance plus some uniformly generated anomalies (red dots). As can be seen that (kernel)
k
-means assigns one cluster center to fit the anomalies and is therefore not able to reveal the four Gaussian clusters, whereas our ClusterSVDD concisely finds the four Gaussians.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8052229/gorni3-2737941-large.gif
2018,8052229,Fig. 4.,"Anomaly detection accuracy (in AUROC) of standard SVDD against our ClusterSVDD with
k=2,3,4
for varying regularization parameter
ν
and a fraction of 5% of outlier in the data set. Left: linear version. Right: kernelized version (RBF kernel).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8052229/gorni4ab-2737941-large.gif
2018,8052229,Fig. 5.,"Cluster membership identification accuracy (in adjusted Rand index, ARI) of our ClusterSVDD, including standard
k
-means clustering for varying regularization parameter
ν
. The original
k
-means solution is recovered at
ν=1.0
and plotted in red color. Left: linear version. Right: kernelized version (RBF kernel).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8052229/gorni5ab-2737941-large.gif
2018,8052229,Fig. 6.,"Generated structured data consists of multivariate (three dimensions: Feature 0, Feature 1, and Feature 2) Gaussian sequences of length 500 (red) and corresponding two-state label sequences (white and gray areas) of three classes and a fraction of anomalies. Notice that the differences between the classes are subtle, rendering the overall problem nontrivial.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8052229/gorni6-2737941-large.gif
2018,8458105,FIGURE 1.,Framework of smart home system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458105/liu1-2868984-large.gif
2018,8458105,FIGURE 2.,Framework of the proposed ANN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458105/liu2-2868984-large.gif
2018,8458105,FIGURE 3.,The equal probability models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458105/liu3-2868984-large.gif
2018,8458105,FIGURE 4.,The average CP of each device. (a) Devices in 1335. (b) Devices in 1660. (c) Devices in 1669.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458105/liu4abc-2868984-large.gif
2018,8458105,FIGURE 5.,The average SP of each device. (a) Devices in 1335. (b) Devices in 1660. (c) Devices in 1669.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458105/liu5abc-2868984-large.gif
2018,8458105,FIGURE 6.,The average DB of each device. (a) Devices in 1335. (b) Devices in 1660. (c) Devices in 1669. The,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458105/liu6abc-2868984-large.gif
2018,8002611,Fig. 1.,Structure of autoencoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge1-2733448-large.gif
2018,8002611,Fig. 2.,Learning process of hierarchical ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge2-2733448-large.gif
2018,8002611,Fig. 3.,Flowchart of the SS-HELM algorithm for soft sensor modeling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge3-2733448-large.gif
2018,8002611,Fig. 4.,Flowchart of the high-low transform unit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge4-2733448-large.gif
2018,8002611,Fig. 5.,"Prediction and error of ELM
(RMSE=0.0045)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge5-2733448-large.gif
2018,8002611,Fig. 6.,"Prediction and errors of SS-ELM
(RMSE=0.0039)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge6-2733448-large.gif
2018,8002611,Fig. 7.,"Prediction and error of HELM
(RMSE=0.0038)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge7-2733448-large.gif
2018,8002611,Fig. 8.,"Prediction and error of SS-HELM
(RMSE=0.0033)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge8-2733448-large.gif
2018,8002611,Fig. 9.,"Prediction and error of SS-PPCR
(RMSE=0.0057)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge9-2733448-large.gif
2018,8002611,Fig. 10.,"Prediction and error of Co-PLS
(RMSE=0.0051)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge10-2733448-large.gif
2018,8002611,Fig. 11.,"Prediction and error of DBN
(RMSE=0.0036)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge11-2733448-large.gif
2018,8002611,Fig. 12.,"Prediction and error of SDAE-NN
(RMSE=0.0033)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8186327/8002611/ge12-2733448-large.gif
2018,7967622,Fig. 1.,Block diagram illustrating feature engineering (FE) and the training of a machine learning model for CM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss1-2722479-large.gif
2018,7967622,Fig. 2.,"Schematic representation of FE without feature selection, with feature selection, and FL, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss2-2722479-large.gif
2018,7967622,Fig. 3.,"Architecture of the deep convolutional neural network for IRT CM.
C
h×w
k
denotes a convolutional layer with
k
feature maps and receptive field of dimension
h×w
.
P
denotes a pooling layer.
D
n
denotes a dense fully connected layer with
n
neurons.
S
denotes a softmax layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss3-2722479-large.gif
2018,7967622,Fig. 4.,3-D image of the setup. The labels are: (1) Servomotor; (2) coupling; (3) bearing housing; (4) bearing; (5) disk; (6) shaft; (7) thermocouple; and (8) metal plate. The red square indicates what the IRT camera records.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss4-2722479-large.gif
2018,7967622,Fig. 5.,Three shallow grooves in the outer-raceway of a bearing simulating an ORF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss5-2722479-large.gif
2018,7967622,Fig. 6.,"Regions that influence the CNNs output for (a) HB, (b) MILB, (c) EILB, (d) ORF at the 10 o’clock position, (e) ORF at the loaded zone, and (f) hard particles. The closer to 1, the more important a region is for the respective class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss6-2722479-large.gif
2018,7967622,Fig. 7.,"Image of the used setup. (1) Bearing, (2) hydrostatic pad to apply radial load on the bearing, (3) pneumatic muscle for loading the bearing, (4) force cell for friction torque, and (5) temperature measurements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss7-2722479-large.gif
2018,7967622,Fig. 8.,Regions that influence the CNNs output for an REB (a) full of oil and (b) not full of oil.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8293754/7967622/janss8-2722479-large.gif
2018,8307266,Fig. 1.,"(a) Network example. (b) SVM at compromised Node 1. There are seven nodes in this network. Each node contains a labeled training set
D
v
:={(
x
vn
,
y
vn
):n=1,…,
N
v
}
. Node 4 can communicate with its four neighbors: Nodes 2, 3, 5, and 6. An attacker can take over Nodes 1 and 4. The compromised nodes are marked in red. In each node, the learner aims to find the best linear discriminant line, for example, the black dotted line shown in (b). In compromised nodes, an attacker modifies the training data, which leads to a wrong discriminant line of the learner, for example, the black solid line shown in (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307266/zhang1ab-2802721-large.gif
2018,8307266,Fig. 2.,"(a) “Rand” data set. (b) Evolution of the empirical risks of DSVM with an attacker at a fully connected network of three nodes. Training data and testing data are generated from two Gaussian classes. The attacker attacks all three nodes from the beginning of the training process. Dotted lines and solid lines show the results for the case without an attacker and the one with an attacker. Red and green lines show the results of centralized SVMs and DSVM, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307266/zhang2ab-2802721-large.gif
2018,8307266,Fig. 3.,"Global risks of DSVM at a fully connected network with six nodes. Each node contains 40 training samples and 300 testing samples. Left: evolution of the risks on the “Rand” data set when the attacker only attacks one node, but with different starting and stopping times. The attacker has parameters
C
δ
=
10
8
and
C
a
=0.01
. Right: average global equilibrium risks when the attacker attacks different numbers of nodes at the beginning of the training process with the ability
C
a
=1
and
C
δ
=
10
4
, 109, and 104 for “Rand,” “Spam,” and “MNIST” data sets, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307266/zhang3ab-2802721-large.gif
2018,8307266,Fig. 4.,"Average global equilibrium risks of DSVM at a fully connected network with three nodes. Each node contains 80 training samples and 600 testing samples. Left: results with respect to
log
10
(
C
δ
)
when the attacker attacks two nodes with
C
a
=1
. Right: results with respect to
log
10
(
C
a
)
when the attacker attacks one node with
C
δ
=
10
7
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307266/zhang4ab-2802721-large.gif
2018,8290981,Fig. 1.,Network architecture corresponding to (8).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang1-2805692-large.gif
2018,8290981,Fig. 2.,Overall structure of our proposed LEARN network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang2-2805692-large.gif
2018,8290981,Fig. 3.,Examples in the dataset. The display window is [−150 250] HU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang3-2805692-large.gif
2018,8290981,Fig. 4.,"Representative abdominal images reconstructed using various methods. (a) The reference image versus the images reconstructed using (b) FBP, (c) ASD-POCS (
λ=0.07
), (d) Dual-DL (
n=16
,
d
s
=2
,
β=0.03
), (e) PWLS-TGV (
β
1
=1×
10
−2
,
β
2
=1.3×
10
−4
), (f) Gamma-Reg (
λ=3×
10
−3
,
α=1.5
,
β=8
), (g) FBPConvNet, and (h) LEARN respectively. The red arrows point to some key details, which can only be discriminated with the LEARN network. The red box labels a region of interest (ROI), which is magnified in Fig. 5. The display window is [−150 250] HU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang4abcdefgh-2805692-large.gif
2018,8290981,Fig. 5.,"Zoomed region of interest (ROI) marked by the red box in Fig. 4(a). (a) The reference image versus the images reconstructed using (b) FBP, (c) ASD-POCS, (d) Dual-DL, (e) PWLS-TGV, (f) Gamma-Reg, (g) FBPConvNet, and (h) LEARN respectively ((a)-(g) from Fig. 4(a)-(g)). The arrows indicate two locations with significant visual differences. The display window is [−150 250] HU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang5abcdefgh-2805692-large.gif
2018,8290981,Fig. 6.,Means and standard variations for (a) ROI I and (b) ROI II reconstructed using different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang6ab-2805692-large.gif
2018,8290981,Fig. 7.,"Representative thoracic images reconstructed using various methods. (a) The reference image versus the images reconstructed using (b) FBP, (c) ASD-POCS (
λ=0.05
), (d) Dual-DL (
n=25
,
d
s
=2
,
β=0.01
), (e) PWLS-TGV (
β
1
=1×
10
−2
,
β
2
=0.5×
10
−4
), (f) Gamma-Reg (
λ=2×
10
−3
,
α=1.2
,
β=7
), (g) FBPConvNet, and (h) LEARN respectively. The red arrows point to some details, which can be discriminated by the LEARN network. The red box labels an ROI to be magnified in Fig. 8. The profiles along the dotted blue line are in Fig. 9. The display window is [−1000 200] HU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang7abcdefgh-2805692-large.gif
2018,8290981,Fig. 8.,"Zoomed parts over the region of interest (ROI) marked by the red box in Fig. 6(a). (a) The reference image versus the images reconstructed using (b) FBP, (c) ASD-POCS, (d) Dual-DL, (e) PWLS-TGV, (f) Gamma-Reg, (g) FBPConvNet, and (h) LEARN respectively ((a)-(h) from Fig. 6(a)-(f)). The red arrows indicate a selected region for visual difference. The blue dotted circle shows another region, where the results of FBP and FBPConvNet generated similar artifacts. The display window is [−1000 200] HU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang8abcdefgh-2805692-large.gif
2018,8290981,Fig. 9.,"The horizontal profiles along the dotted blue line in Fig. 7(a) of the reference image versus the images reconstructed using (a) FBP, (b) ASD-POCS, (c) Dual-DL, (d) PWLS-TGV, (e) Gamma-Reg, (f) FBPConvNet, and (g) LEARN respectively. Two dotted green boxes label approximately homogenous and edge-rich regions respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang9abcdefg-2805692-large.gif
2018,8290981,Fig. 10.,PSNR and RMSE values for the testing dataset with different numbers of iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang10-2805692-large.gif
2018,8290981,Fig. 11.,The loss curves of LEARN for training and validation datasets during the training stage.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang11-2805692-large.gif
2018,8290981,Fig. 12.,Four slices reconstructed by LEARN from different initial images. The images in the first column were initialized with the FBP results while the images in the second column were initialized with zero images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang12-2805692-large.gif
2018,8290981,Fig. 13.,PSNR values for the reconstructed results using LEARN with different noise levels. The dotted green lines indicate three different noise levels. The corresponding results are in Fig. 13.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang13-2805692-large.gif
2018,8290981,Fig. 14.,"Reconstructed results from 64 views using FBP and LEARN for different noise levels respectively. (a) FBP result with
b
0
=3×
10
7
, (b) LEARN result with
b
0
=3×
10
7
, (c) FBP result with
b
0
=2×
10
7
, (d) LEARN result with
b
0
=2×
10
7
, (e) FBP result with
b
0
=1×
10
7
, and (f) LEARN result with
b
0
=1×
10
7
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8290981/zhang14abcdef-2805692-large.gif
2018,8454272,Fig. 1.,Illustration of the proposed mm-wave-NOMA system model: 1) Black triangles denote the parent points for each cluster; 2) Users are clustered around cluster centers; 3) The partition in the figure describes clustering region for NOMA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui1-2867180-large.gif
2018,8454272,Fig. 2.,"Illustration of K-means based user clusterting where
R=5
m,
K=3
,
M=4
and
U=45
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui2abc-2867180-large.gif
2018,8454272,Fig. 3.,Two different K-means based user clustering models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui3ab-2867180-large.gif
2018,8454272,Fig. 4.,"Comparisons of the sum rates with different clustering algorithms with
U=10
and
K=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui4-2867180-large.gif
2018,8454272,Fig. 5.,"The impact of the number of clusters with
U=8
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui5ab-2867180-large.gif
2018,8454272,Fig. 6.,"The sum rate of K-means based clustering versus matching based user clustering with
U=10
and
K=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui6-2867180-large.gif
2018,8454272,Fig. 7.,"Comparisons of the sum rates under a general mm-wave channel where the number of NLoS paths is
L=3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui7ab-2867180-large.gif
2018,8454272,Fig. 8.,"Comparisons of the sum rates over different numbers of incoming users, where
K=2
and
K
th
=20
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8528717/8454272/cui8-2867180-large.gif
2018,7464278,Fig. 1.,Overview of DiscMLA. The solid lines with arrawheads represent the outer loop and dashed lines represent the inner loops.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8573200/7464278/huang1-2561930-large.gif
2018,7464278,Fig. 2.,The score histogram of 50 seed motifs for all sequences.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8573200/7464278/huang2-2561930-large.gif
2018,7464278,Fig. 3.,The outline of obtaining frequency matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8573200/7464278/huang3-2561930-large.gif
2018,7464278,Fig. 4.,Comparison of various algorithms on training and test data sets for assessing the discriminative power based on the AUC of motifs. The red points represents the AUC of motif found by DiscMLA is better than the methods labeled on horizontal axis. (a) Comparison of motifRG and DiscMLA on training sets. (b) Comparison of motifRG and DiscMLA on test sets. (c) Comparison of DREME and DiscMLA on training sets. (d) Comparison of DREME and DiscMLA on test sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8573200/7464278/huang4-2561930-large.gif
2018,7464278,Fig. 5.,Comparing the accuracy of motifs found by different DML methods. The accuracy is evalurated based on P-values inferred by Tomtom and in -log10 transformation. The red points reprensent the accuracy of DiscMLAUC is better than the method labeled on horizontal axis. (a) The comparision of p-values for motifRG and DiscMLA on 33 common successful data sets. (b) The comparision of p-values for DREME and DiscMLA on 41 common successful data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8573200/7464278/huang5-2561930-large.gif
2018,8478181,Fig. 1.,The illustration of sensor placements: (a) This study: back waist and (b) SisFall open dataset: front waist.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8533451/8478181/chan1ab-2872835-large.gif
2018,8478181,Fig. 2.,The effect of sampling rate on fall detection system results for different machine learning models using the proposed experimental data in this study: (a) accuracy; (b) sensitivity; (c) specificity; and (d) precision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8533451/8478181/chan2abcd-2872835-large.gif
2018,8478181,Fig. 3.,The effect of sampling rate on fall detection system results for different machine learning models using SisFall dataset: (a) accuracy; (b) sensitivity; (c) specificity; and (d) precision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8533451/8478181/chan3abcd-2872835-large.gif
2018,8082535,Fig. 1.,Euclidean distance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou1-2755595-large.gif
2018,8082535,Fig. 2.,Segmentation and support vectors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou2-2755595-large.gif
2018,8082535,Fig. 3.,SRs for majority class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou3-2755595-large.gif
2018,8082535,Fig. 4.,"Three metrics of SVM, SMO, U-SVM, and WU-SVM. (a) AUC. (b) F-measure. (c) G-mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou4abc-2755595-large.gif
2018,8082535,Fig. 5.,"Results of SMOTE, EasyEnsemble, ESOS-ELM, and WU-SVM. (a) AUC. (b) F-measure. (c) G-mean.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou5abc-2755595-large.gif
2018,8082535,Fig. 6.,CPU time comparison. (a) Statistical CPU time for each data set. (b) Average CPU time of all data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou6ab-2755595-large.gif
2018,8082535,Fig. 7.,ROC curves for multiclassification. (a) Murphy. (b) Abalone. (c) CTG. (d) Winequality-red. (e) Winequality-white. (f) Yeast.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8082535/zhou7abcdef-2755595-large.gif
2018,8310951,Fig. 1.,Multisource transfer DQN based on actor learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8310951/wang1-2806087-large.gif
2018,8310951,Fig. 2.,MTDDQN policy learning at different training steps (Breakout game). (a) Step = 1000 000. (b) Step = 4000 000.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8310951/wang2ab-2806087-large.gif
2018,8310951,Fig. 3.,Breakdown policies of two different zones (Breakout game).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8310951/wang3-2806087-large.gif
2018,8310951,Fig. 4.,"Average
Q
value curves on different games. (a) Breakout game. (b) Enduro game. (c) Gopher game. (d) River raid game.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8310951/wang4abcd-2806087-large.gif
2018,8310951,Fig. 5.,Score curves on different games. (a) Breakout game. (b) Enduro game. (c) Gopher game. (d) River raid game.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8310951/wang5abcd-2806087-large.gif
2018,8310951,Fig. 6.,Actor learning effect under different balancing coefficients.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8310951/wang6-2806087-large.gif
2018,8423636,Fig. 1.,Schematic illustration of modulation nonlinear response of a Si-MRM and the sensitive response of modulation nonlinear distortion with respect to wavelength drift.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du1-2861710-large.gif
2018,8423636,Fig. 2.,Simulation setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du2-2861710-large.gif
2018,8423636,Fig. 3.,Transmission spectra of the Si-MRM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du3-2861710-large.gif
2018,8423636,Fig. 4.,The LV curve of the Si-MRM in the simulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du4-2861710-large.gif
2018,8423636,Fig. 5.,Eye diagram of PAM-4 signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du5-2861710-large.gif
2018,8423636,Fig. 6.,The LD curve as a function of the wavelength drift. The inset pictures are the eye-diagrams at different wavelengths.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du6-2861710-large.gif
2018,8423636,Fig. 7.,Eye diagram of PAM-4 signals at different LD and different received optical power (ROP).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du7-2861710-large.gif
2018,8423636,Fig. 8.,BER curves at different LD for hard decision and machine learning of CBT-SVMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du8-2861710-large.gif
2018,8423636,Fig. 9.,BER curves at 36.31% level-deviation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du9-2861710-large.gif
2018,8423636,Fig. 10.,Sensitivity gain (solid curves) and receiver sensitivity power (dashed curves) for the machine learning detection at different LDs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du10-2861710-large.gif
2018,8423636,Fig. 11.,Experimental setup. The Si-MRM picture is the top-view microscope image. The inset shows the side view of the Si-MRM during coupling and modulation. TL: tunable laser; PC: polarization controller; DC: direct current; EA: electrical amplifier; AWG: arbitrary waveform generator; EDFA: Erbium doped fiber amplifier; PD: photodetector; DSO: digital storage oscilloscope.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du11-2861710-large.gif
2018,8423636,Fig. 12.,Transmission spectra of the Si-MRM under different bias voltages.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du12-2861710-large.gif
2018,8423636,Fig. 13.,Frequency response of optical B2B link.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du13-2861710-large.gif
2018,8423636,Fig. 14.,"40-Gbps (a and b) and 50-Gbps (c and d) PAM-4 eyes in the case of optical B2B (a and c), and after 2-km SMF (b and d).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du14-2861710-large.gif
2018,8423636,Fig. 15.,Bathtub curves of a 40-Gbps B2B PAM-4 signal for hard decision and machine learning detection of CBT-SVMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du15-2861710-large.gif
2018,8423636,Fig. 16.,BER curves for 40-Gbps (a) and 50-Gbps PAM-4 (b) signals before and after transmission.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8415803/8423636/du16-2861710-large.gif
2018,7891989,Fig. 1.,Alternating decision tree. (A) Alternating decision tree diagram. An instance is classified by following all paths for which all decision nodes are true and summing any prediction nodes that are traversed. (B) Proposed Model Alternating decision tree diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891989/elhef1-2690848-large.gif
2018,7891989,Fig. 2.,Genetic Algorithm Flowchart. Forming a new generation population of solutions from those selected through a combination of Crossover and Mutation operators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891989/elhef2-2690848-large.gif
2018,7891989,Fig. 3.,ROC curves plots of proposed models in predicting of advanced fibrosis. (A) ROC curve plot of multi-linear regression model with area under the curve = 0.76. (B) ROC curve plot of particle swarm model with area under the curve = 0.73. (C) ROC curve plot of genetic algorithm model with area under the curve = 0.75. (D) ROC curve plot of alternating decision tree model with area under the curve = 0.75.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8371203/7891989/elhef3-2690848-large.gif
2018,8382283,Fig. 1.,"Our taxonomization of PCGML techniques. We have two categorizations: the underlying data structure (graph, grid, or sequence) and the training method (matrix factorization, EM, frequency counting, evolution, and backpropagation). Marks are colored for the specific type of content that was generated: red circles are platformer levels, orange squares are “dungeons,” the dark blue x is real time strategy levels, light blue triangles are collectible game cards, and the purple star is interactive fiction. Citations for each are listed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel1-2846639-large.gif
2018,8382283,Fig. 2.,"Mario levels reconstructed by
n
-grams with
n
set to 1, 2, and 3, respectively. Figures reproduced with permission from [34]. (a) n = 1. (b) n = 2. (c) n = 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel2-2846639-large.gif
2018,8382283,Fig. 3.,"Visualization of the NeuroEvolution approach, showing the input (bottom), an evolved network architecture, and an example output (top). Figures reproduced with permission from [31].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel3-2846639-large.gif
2018,8382283,Fig. 4.,"Example output of the LSTM approach, including generated exemplar player path. Figure reproduced with permission from [21].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel4-2846639-large.gif
2018,8382283,Fig. 5.,Partial card specification and the output. Figure reproduced with permission from [30].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel5-2846639-large.gif
2018,8382283,Fig. 6.,"Sections from a Super Mario Bros. level (top) and Kid Icarus level section both generated using the constrained MdMC approach (bottom-left), and using an MRF approach (bottom right). Figures reproduced with permission from [48], [49].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel6-2846639-large.gif
2018,8382283,Fig. 7.,"Original window is overwritten with a wall making the game unplayable. The autoencoder repairs the window to make it playable, although it chooses a different solution to the problem. Figures reproduced with permission from [22]. (a) Original. (b) Unplayable. (c) Repaired.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel7-2846639-large.gif
2018,8382283,Fig. 8.,Varying resource amounts when generating resource locations for StarCraft II. Figures reproduced with permission from [55].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel8-2846639-large.gif
2018,8382283,Fig. 9.,Example of interpolation between two Zelda rooms (the leftmost and rightmost rooms). Figure reproduced with permission from [32].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel9-2846639-large.gif
2018,8382283,Fig. 10.,Level section from the clustering approach. Figure reproduced with permission from [33].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel10-2846639-large.gif
2018,8382283,Fig. 11.,Example of Scheherazade-IF gameplay. Figure reproduced with permission from [65].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782673/8464709/8382283/togel11-2846639-large.gif
2018,8307246,Fig. 1.,"DKMO—the proposed approach for optimizing kernel machines using DNNs. For a given kernel, we generate multiple dense embeddings using kernel approximation techniques, and fuse them in a fully connected DNN. The architecture utilizes fully connected networks with kernel dropout regularization during the fusion stage. Our approach can handle scenarios when both the feature sources and the kernel matrix are available during training or when only the kernel similarities can be accessed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song1-2804895-large.gif
2018,8307246,Fig. 2.,Effects of kernel dropout on the DKMO training process. We compare the convergence characteristics obtained with the inclusion of the kernel dropout regularization in the fusion layer in comparison to the nonregularized version. Note that we show the results obtained with two different merging strategies—concatenation and summation. We observe that the kernel dropout regularization leads to improved convergence and lower classification error for both the merging styles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song2-2804895-large.gif
2018,8307246,Fig. 3.,M-DKMO—extending the proposed deep kernel optimization approach to the case of multiple kernels. Each of the kernels are first independently trained with the DKMO algorithm in Section III and then combined using a global fusion layer. The parameters of the global fusion layer and the individual DKMO networks are fine tuned in an end-to-end learning fashion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song3-2804895-large.gif
2018,8307246,Fig. 4.,Example samples from the data sets used in our experiments. The feature sources and kernels are designed based on state-of-the-art practices. The varied nature of the data representations are readily handled by the proposed approach and kernel machines are trained for single and multiple kernel cases. (a) Images from different classes in the flowers102 data set. (b) Sequences belonging to three different classes in the nonplant data set for protein subcellular localization. (c) Accelerometer measurements characterizing different activities from the USC-HAD data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song4abc-2804895-large.gif
2018,8307246,Fig. 5.,Single kernel performance on flowers data sets. (a) Flowers17. (b) Flowers102–20. (c) Flowers102–30.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song5abc-2804895-large.gif
2018,8307246,Fig. 6.,Single kernel performance on protein subcellular data sets. (a) Plant. (b) Nonplant. (c) Psort+. (d) Psort−.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song6abcd-2804895-large.gif
2018,8307246,Fig. 7.,"2-D T-SNE visualizations of the representations obtained for the nonplant data set using the base kernel (Kernel 5), uniform multiple kernel fusion, and the learned representations from DKMO and M-DKMO. The samples are colored by their corresponding class associations. (a) Decomp. (b) Proposed DKMO. (c) Uniform. (c) Proposed M-DKMO.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song7abcd-2804895-large.gif
2018,8307246,Fig. 8.,"Visualization of the proposed framework applied on USD-HAD data set. We show the raw three-axis accelerometer signal and extracted three distinct types of features: the time-series statistics, topological structure where we extract TDE descriptors and the correlation kernel. Furthermore, we show the t-SNE visualization of the representations learned by DKMO and M-DKMO, where all points are classes coded according to the colorbar.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song8-2804895-large.gif
2018,8307246,Fig. 9.,Single kernel performance on USC-HAD data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8307246/song9-2804895-large.gif
2018,7927417,Fig. 1.,"Representations learned by RBM and GraphRBM on the MNIST data set, respectively. (a) Two samples from the same digits class ‘0’. (b) Representations learned by RBM. (c) Representations learned by the proposed GraphRBM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/7927417/lv1abc-2692773-large.gif
2018,7927417,Algorithm 1,Online Training of GraphRBM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/7927417/lv4-2692773-large.gif
2018,7927417,Fig. 2.,Example of three-layered DBN with different greedy learning strategies. (a) Standard DBN. (b) Full GraphDBN (fGraphDBN and GraphRBM+GraphRBM). (c) Mixed GraphDBN (mGraphDBN and RBM+GraphDBM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/7927417/lv2abc-2692773-large.gif
2018,7927417,Fig. 3.,"Performance of GraphRBM on the MNIST data set with varying parameters. (a) Sparsity of hidden representations versus the increasing of training epoch, where
p=800
and
λ=0.01
. (b) Clustering performance (NMI value) versus the variation of the neighborhood parameter
p
, where
λ=0.01
. (c) NMI value versus the variation of the sparsity parameter
λ
, where
p=800
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/7927417/lv3abc-2692773-large.gif
2018,8352748,Fig. 1.,"Principle of the thermal video data acquisition presenting (a) thermal images with associated thermo bars and mouth area regions, (b) selected individual digits used for the machine learning to recognize thermal values, and (c) thermal regions in the facial area.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8374099/8352748/proch1abc-2831444-large.gif
2018,8352748,Fig. 2.,"The plot of (a) the mean breathing temperature in the selected mouth area detected by the thermal camera during an activity 40 minutes long including two load periods and two rest periods, 10 minutes each, for a selected experiment; (b) the average breathing temperature and breathing frequency changes evaluated for 56 experiments, and (c) the heart rate detected by the heart rate sensor over a time range of 20 minutes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8374099/8352748/proch2abc-2831444-large.gif
2018,8352748,Fig. 3.,"A two-layer neural network with sigmoidal and softmax transfer functions to recognize individual digits and to find changing temperature ranges of the thermal camera system presenting (a) the system structure and (b) pattern and target values for processing 240 digits separated into matrices of 9 by 6 pixels forming
R=54
features for the definition of each one of them.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8374099/8352748/proch3ab-2831444-large.gif
2018,8352748,Fig. 4.,"The evolution of cross-entropy for the training, validation and test sets of digits related to the epoch number during the learning process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8374099/8352748/proch4-2831444-large.gif
2018,8352748,Fig. 5.,"Analysis of a selected record of physiological data 40 minutes long presenting (a1,a2) the heart rate (HR) data and evaluation of the corresponding delay related to the change of physical activity, (b1,b2) the evolution of the breathing frequency and its delay evaluated from the thermal camera, and (c1,c2) the evolution of the breathing temperature during both load and rest periods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8374099/8352748/proch5-2831444-large.gif
2018,8352748,Fig. 6.,"Results presenting (a) physiological delays for all 56 experiments with histograms of (b) breathing temperature (BT) delays, (c) breathing frequency (BF) delays, and (d) heart rate (HR) delays related to the changes of physical activity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8374099/8352748/proch6abcd-2831444-large.gif
2018,8416799,Fig. 1.,"The target environment #1 (command: “move the bag to the yellow room”) used in our study has a dog, five colored rooms, and three objects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng1-2829980-large.gif
2018,8416799,Fig. 2.,The library of 16 environments is organized by the number of rooms and objects. There is a list of relevant commands for each environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng2-2829980-large.gif
2018,8416799,Fig. 3.,"The target environment #2 (command: “move the bag to the yellow room”) used in our study has a dog, five colored rooms and three objects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng3-2829980-large.gif
2018,8416799,Fig. 4.,"Average number of explicit feedback signals needed to learn (a) target task #1, (b) all tasks (including target task #1), (c) target task #2, or (d) all tasks (including target task #2) on four sets of random curricula (or no curricula) with different simulated trainers. Error bars show standard errors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng4-2829980-large.gif
2018,8416799,Fig. 5.,"Average number of explicit feedback signals needed to learn (a) target task #2 or (b) both the entire curriculum and target task #2, with and without human-generated curricula.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng5-2829980-large.gif
2018,8416799,Fig. 6.,"Average number of explicit feedback signals needed to learn (a) target task #2 or (b) both the entire curriculum and target task #2, with random or human-generated curricula.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng6-2829980-large.gif
2018,8416799,Fig. 7.,Average number of explicit feedback signals needed to learn target task #2 or both the entire curriculum and target task #2 with human-generated curricula that contain more or fewer target concepts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng7-2829980-large.gif
2018,8416799,Fig. 8.,The probability of each environment being included in a human-generated curriculum from both experimental conditions. The purple circle represents the overlap between conditions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng8-2829980-large.gif
2018,8416799,Fig. 9.,Average number of explicit feedback signals needed to learn (a) target task #2 or (b) both the entire curriculum and target task #2 with human-generated curricula using the improved algorithm vs. using the original one.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng9-2829980-large.gif
2018,8416799,Fig. 10.,Average number of explicit feedback signals needed to learn (a) target task #2 or (b) both the entire curriculum and target task #2 with random and human-generated curricula using the improved algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/8416789/8416799/peng10-2829980-large.gif
2018,8247230,Fig. 1.,"Evolution of a wearable network with plug-n-learn feature. Initially, the system consists of two sensors (a). Existing sensors, in the source view, contribute to training the new sensor, in the target view (b). When the training phase is completed, new sensor contributes to the system for Activity Recognition (AR), as shown in (c).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni1-2789890-large.gif
2018,8247230,Fig. 2.,Synchronous multi-view: Observations of both views combine with the semi-label of source.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni2-2789890-large.gif
2018,8247230,Fig. 3.,"Single-view (source) versus multi-view feature space. (a) Instances in one view may not be separable, however by combining them into (b) multi-view feature space, they become separable.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni3-2789890-large.gif
2018,8247230,Fig. 4.,"Synchronous Multi-view Learning starts consists of (a) labeling target instances by semi-labels and continues with (b) clustering target instances. (c) A weighted bipartite graph is built on clusters and activities and after finding the best assignment, and (d) cluster labels are propagated to their instances.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni4-2789890-large.gif
2018,8247230,Fig. 5.,Accuracy of provided label using four approaches under comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni5-2789890-large.gif
2018,8247230,Fig. 6.,Accuracy of five activity recognition classifiers under comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni6-2789890-large.gif
2018,8247230,Fig. 7.,"Comparison of confusion matrices before and after applying SML, when ‘source’ is (a) ‘Right Leg (RL)’ and ‘target’ is (b) ‘Torso (TO)’ and (c) ‘Right Arm (RA)’.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni7-2789890-large.gif
2018,8247230,Fig. 8.,Relative loss of online clustering algorithm comparing to K-means. It shows that with more observations the result of online clustering algorithm is converging to off-line K-means.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7755/8401834/8247230/rokni8-2789890-large.gif
2018,7542173,Fig. 1.,"Schematic examples for accepting N-label corrections as data to use for training and testing. The first two examples (in dashed lines) are admissible, since an N-label is being corrected by a base that is agreed upon by the human and a second read with a quality value
>
60. The last three examples have the circled problems (resp.): (1) the basecaller has left the base ambiguous, (2) the second read only has a quality value of 42, and (3) the human and second read disagree on the identity of the correct base.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma1-2598752-large.gif
2018,7542173,Fig. 2.,"A schematic of three shifted views of a chromatogram window containing an N-label to be processed by the classifier. There are five possible views (three shown), allowing the classifier to accumulate five votes for replacing an N-label. Each circle in the output layer represents the output encodings: (1) whether there is an N-label at a peak (ANN only), and (2) what basecall to label that peak (SVM and ANN).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma2-2598752-large.gif
2018,7542173,Fig. 3.,"Schematics for a standard feed forward neural network with only mean output nodes
μ
y|x
(left), and a network with additional conditional variance nodes
σ
2
y|x
—one for each conditional mean node (right). The schematic shows networks with two hidden layers. Only variables involved with computing output are described in the text,
f(W
a
(h)
)→a
. Other variables describe the input layer
a
(i)
, the lower hidden layer
a
(
h
i
)
, and their connections
W
(i,
h
i
)
,
W
(
h
i
,h)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma3-2598752-large.gif
2018,7542173,Fig. 4.,"Histograms showing the spread of the data along: Sequence position in the basecalled sequence for N-labels ( left), the ratio of the secondary to primary peak amplitudes for N-calls (center), and quality value of N-calls emitted by KB basecaller (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma4-2598752-large.gif
2018,7542173,Fig. 5.,"Nonparametrically fitting the variance output to cumulative error. Cumulative error is shown given basecalls ranked in ascending variance on left-y-axis (solid); the ranking is shown as a quantile in x. The actual variance values are on right-y-axis (dashed). The ranked left-y-axis shows that the vast majority of the results data (
>90
percent) is below 1 percent error. The long diagonal guideline shows the hypothetical plot for error, had it been randomly distributed with respect to variance (dotted).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma5-2598752-large.gif
2018,7542173,Fig. 6.,"Observed cumulative error from the COI test sets as a function of predicted cumulative error. Errors are mapped together using corresponding rank in variance in the two datasets. The far left plot shows the correlation for all COI test sets, and the other three plots show each of the COI test sets, Lepidoptera, Insecta , and Animalia. The far left curve shows high agreement between observed and predicted error—the average case. Breaking down the data, variance output under predicts error in Lepidoptera and Insecta, and over predicts error in Animalia. In all cases, the absolute difference is less than 0.2 percent in error at the 1 percent predicted error mark.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma6-2598752-large.gif
2018,7542173,Fig. 7.,"The amount of N-labels recovered by this method, as observed error increases. The dashed line is the predicted error provided by the validation set, while the solid line is the amount of error observed in the COI test sets. The vertical dashed line corresponds to the 0.8 percent predicted error in the validation set. The horizontal dashed line corresponds to 80 percent of N-labels recovered.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8280529/7542173/ma7-2598752-large.gif
2018,7940055,Fig. 1.,SVs and outliers with 10% flip.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao1-2705429-large.gif
2018,7940055,Fig. 2.,SVs and outliers at each stage with 20% flip.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao2-2705429-large.gif
2018,7940055,Fig. 3.,Number of stages on UCI data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao3-2705429-large.gif
2018,7940055,Fig. 4.,Comparison of three primal CDs and NRBM on the data set Covtype.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao4-2705429-large.gif
2018,7940055,Fig. 5.,Comparison of three primal CDs and NRBM on the data set Ijcnn1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao5-2705429-large.gif
2018,7940055,Fig. 6.,Comparison of three primal CDs and NRBM on the data set A9a.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao6-2705429-large.gif
2018,7940055,Fig. 7.,Comparison of three primal CDs and NRBM on the data set Rcv1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao7-2705429-large.gif
2018,7940055,Fig. 8.,Relationship between the values of objective function and CPU time on A9a.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8392548/7940055/tao8-2705429-large.gif
2018,8081830,Fig. 1.,Structure of the IPMSM sensorless control scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine1-2765398-large.gif
2018,8081830,Fig. 2.,Implementation process of the NN-based ML position estimator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine2-2765398-large.gif
2018,8081830,Fig. 3.,Mesh of the torque–speed operating area.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine3-2765398-large.gif
2018,8081830,Fig. 4.,One electric period acquisition for an operating point.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine4-2765398-large.gif
2018,8081830,Fig. 5.,Ground truth data for regression model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine5-2765398-large.gif
2018,8081830,Fig. 6.,"Performances curve: MSE as function of training, validation, and test samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine6-2765398-large.gif
2018,8081830,Fig. 7.,Error histogram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine7-2765398-large.gif
2018,8081830,Fig. 8.,Fitting results at 750 r/min. (a) Sine signal. (b) Cosine signal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine8-2765398-large.gif
2018,8081830,Fig. 9.,ML-based NN estimator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine9-2765398-large.gif
2018,8081830,Fig. 10.,"(a) Variation of position error with speed. Torque reference = 50 N
⋅
m. (b) Position error at 2000 r/min (RMSE =
3
∘
), Position error at 6000 r/min (RMSE =
11
∘
). Torque reference = 50 N
⋅
m.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine10-2765398-large.gif
2018,8081830,Fig. 11.,Robustness of NN-based algorithm against saturation effects (2000 r/min). (a) Torque reference. (b) Estimated versus real position.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine11-2765398-large.gif
2018,8081830,Fig. 12.,Closed-loop sensorless operation. (a) Mechanical speed (r/min). (b) Output torque versus torque reference.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine12-2765398-large.gif
2018,8081830,Fig. 13.,"Low-speed area: Estimated Versus real position at 1000 r/min. Torque reference = 50 N
⋅
m.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine13-2765398-large.gif
2018,8081830,Fig. 14.,"Estimated versus real position (rad). (a) 2000 r/min (50 N
⋅
m). (b) 4000 r/min (50 N
⋅
m).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine14-2765398-large.gif
2018,8081830,Fig. 15.,"Estimated versus real position (rad). (a) 6000 r/min (50 N
⋅
m). (b) 8000 r/min (50 N
⋅
m).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine15-2765398-large.gif
2018,8081830,Fig. 16.,"Sensitivity to torque level (0 to 200 N
⋅
m) at 1000 r/min.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine16-2765398-large.gif
2018,8081830,Fig. 17.,Speed tracking. Estimated versus measured mechanical speed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine17-2765398-large.gif
2018,8081830,Fig. 18.,Estimated versus reference torque at 2000 r/min.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine18-2765398-large.gif
2018,8081830,Fig. 19.,"Performance comparison between HF signal injection estimator (red), EKF estimator (green), and ML estimator (blue).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8353885/8081830/zine19-2765398-large.gif
2018,7936460,Fig. 1.,"Retinal Image in the optic disc region. (a) Optic disc image with NVD, the red markers; (b) Normal optic disc image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu1-2710201-large.gif
2018,7936460,Fig. 2.,Framework for NVD detection procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu2-2710201-large.gif
2018,7936460,Fig. 3.,Optic disc detection result. (a) Successful detection; (b) inaccurate detection that needs manual refinement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu3-2710201-large.gif
2018,7936460,Fig. 4.,"Illumination correction. (a) Green channel NVD image; (b) intensity background image; (c) illumination corrected image, which is obtained by subtracting image (b) from (a).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu4-2710201-large.gif
2018,7936460,Fig. 5.,Non-local means filtering. (a) Green channel NVD image; (b) non-local means filtered image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu5-2710201-large.gif
2018,7936460,Fig. 6.,Multi-scale Gabor filtering result for NVD Image and Pre-processed Image. (a) Original green channel NVD image; (b) Gabor filtering result for (a); (c) binary vessel image for (a); (d) local means filtered image; (e) Gabor filtering result for (d); (f) binary vessel image for (d).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu6-2710201-large.gif
2018,7936460,Fig. 7.,New vessel candidate image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu7-2710201-large.gif
2018,7936460,Fig. 8.,Visualization of first two principal components for the database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu8-2710201-large.gif
2018,7936460,Fig. 9.,Feature selection result showing the tendency of AUC scoring for the training set with.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu9-2710201-large.gif
2018,7936460,Fig. 10.,Performance of SVM classification on randomly selected training/test database 10 times consecutively. The red bar represents the training set and yellow bar the test set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu10-2710201-large.gif
2018,7936460,Fig. 11.,Probability density estimation of decision function value for training set (blue circular markers) and test set (red square markers).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8350358/7936460/yu11-2710201-large.gif
2018,8010865,Fig. 1.,"Comparison between LLP and MIL problem. The red points denote positive points and blue points denote the negative ones. The numbers along with bags in (a) are the proportions of positive points of each bag. The common ground of LLP and MIL is that we do not know all labels of instances in the two problems. Differences are as follows. There are the concept of positive bags and negative bags in MIL, but none in LLP. In MIL, positive bag means that at least one instance belongs to positive class, and negative bag means that all instances belong to negative class. In LLP, each bag only knows the label proportion information between positive classes and negative classes. (a) LLP problem. (b) MIL problem.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi1ab-2727065-large.gif
2018,8010865,Fig. 2.,"First toy data. “Blue” and “red” denote the first and second bags, respectively. The “o” and “*” denote the positive and negative classes, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi2-2727065-large.gif
2018,8010865,Fig. 3.,Change of object value with the increase of iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi3-2727065-large.gif
2018,8010865,Fig. 4.,"Results under different iterations. With the increase of the number of iterations, the result is getting better and better. After 51st iteration, we get the best boundary. (a) Result at 14th iteration. (b) Result at 27th iteration. (c) Result at 51st iteration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi4abc-2727065-large.gif
2018,8010865,Fig. 5.,"Second toy data. “o” and “*” denote positive class and negative class, and the different colors denote the different classes, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi5-2727065-large.gif
2018,8010865,Fig. 6.,"Error rate changes as the number of weak classifiers grows on the second toy data. When the number of weak classifiers is larger than 10, the error rate is almost no change.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi6-2727065-large.gif
2018,8010865,Fig. 7.,Statistical significance diagram. CD diagram for Nemenyi tests performed on all the 12 data sets. Average ranks of examined methods are presented. Bold lines indicate groups of classifiers which are not significantly different from each other (their average ranks differ by less than CD value).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi7-2727065-large.gif
2018,8010865,Fig. 8.,"Accuracy comparison on different data sets. Each subfigure denotes the result under different bags.
x
-axis—1: vowel. 2: heart. 3: colic. 4: vote. 5: australian. 6: letter. 7: dna. 8: satimage. 9: acoustic. 10: combined. 11: connect-4. 12: covtype. (a) Sizes of bag: 2. (b) Sizes of bag: 4. (c) Sizes of bag: 8. (d) Sizes of bag: 16. (e) Sizes of bag: 32. (f) Sizes of bag: 64.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi8abcdef-2727065-large.gif
2018,8010865,Fig. 9.,"Average accuracy comparison on different data sets. The average accuracy denotes the mean of all bags’ accuracies for some data set.
x
-axis–1: vowel. 2: heart. 3: colic. 4: vote. 5: australian. 6: letter. 7: dna. 8: satimage. 9: acoustic. 10: combined. 11: connect-4. 12: covtype.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi9-2727065-large.gif
2018,8010865,Fig. 10.,"Time’s comparison between Adaboost-LLP and alter-
∝
SVM. (a) heart. (b) colic. (c) vote. (d) australian. (e) dna. (f) satimage. (g) dna. (h) satimage. (i) dna. (j) satimage. (k) dna. (l) satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi10abcdefghijkl-2727065-large.gif
2018,8010865,Fig. 11.,"Accuracy comparison on the different weights of bags. Adaboost-LLP-1 denotes the one with the weight of (7); Adaboost-LLP-2 denotes the one with the weight of (7) and (8); Adaboost-LLP-3 denotes the one with the weight of (12). With the help of weight formula (8), the accuracy of Adaboost-LLP-2 increases about 0.24%. With the help of weight formula (12), the accuracy of Adaboost-LLP-3 again increases about 0.48% on the basis of Adaboost-LLP-2, which indicate that our algorithm benefits from adding these extra weights’ information. (a) Heart. (b) Colic. (c) Vote. (d) Australian. (e) Dna. (f) Satimage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi11abcdef-2727065-large.gif
2018,8010865,Fig. 12.,Class-attribute matrices of the AwAs data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8010865/qi12-2727065-large.gif
2018,8110721,Fig. 1.,the schematic diagram of the proposed algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan1-2774103-large.gif
2018,8110721,Fig. 2.,kernel function comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan2-2774103-large.gif
2018,8110721,Fig. 3.,the block diagram of v-cross validation for parameter optimization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan3-2774103-large.gif
2018,8110721,Fig. 4.,RSSI collection system and target trajectory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan4-2774103-large.gif
2018,8110721,Fig. 5.,Localization performance versus size of training set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan5-2774103-large.gif
2018,8110721,Fig. 6.,CDF comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan6-2774103-large.gif
2018,8110721,Fig. 7.,Localization performance versus weight parameter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan7-2774103-large.gif
2018,8110721,Fig. 8.,Localization performance versus kernel construction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8318405/8110721/yan8-2774103-large.gif
2018,8408773,Fig. 1.,Architecture of GC-based SELM. (a) GC-based ELM. (b) PCA. (c) ELM-AE. (d) SELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8523838/8408773/luo1-2854549-large.gif
2018,8408773,Fig. 2.,10 min-ahead forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8523838/8408773/luo2-2854549-large.gif
2018,8408773,Fig. 3.,30 min-ahead forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8523838/8408773/luo3-2854549-large.gif
2018,8408773,Fig. 4.,60 min-ahead forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8523838/8408773/luo4-2854549-large.gif
2018,8368274,Fig. 1.,Spatial and temporal distribution and correlation of the considered cellular traffic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8432073/8368274/zhang1abcd-2841832-large.gif
2018,8368274,Fig. 2.,Prediction framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8432073/8368274/zhang2-2841832-large.gif
2018,8368274,Fig. 3.,Overall performance on two kinds of wireless traffic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8432073/8368274/zhang3ab-2841832-large.gif
2018,8368274,Fig. 4.,The change of training loss with each epoch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8432073/8368274/zhang4-2841832-large.gif
2018,8368274,Fig. 5.,Prediction results of a random selected cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8432073/8368274/zhang5-2841832-large.gif
2018,8278851,Fig. 1.,"Examples of shape recognition with different complexities. (a) Square, circle, and equilateral triangle which are very simple to recognize. (b) Some rotational shapes that are more difficult to recognize. (c) Most difficult shapes to recognize.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen1abc-2790981-large.gif
2018,8278851,Fig. 2.,Heat maps of the coverage number of the three different algorithms. (a) DQN. (b) PER. (c) DCRL. The depth of color in each image represents the size of coverage number.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen2abc-2790981-large.gif
2018,8278851,Fig. 3.,"Sketch of the designed SP and CP functions. (a) Self-paced prioritized function with
λ=0.6
. (b) Coverage penalty function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen3ab-2790981-large.gif
2018,8278851,Fig. 4.,Procedure of DCRL. 1: agent stores a transition in replay memory. 2. agent scores the complexity of each transition according to curriculum evaluation criteria. 3: agent selects some appropriate transitions to update the network parameters. 4: agent updates the TD error as well as coverage number and increases curriculum factor. 5: agent chooses an action.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen4-2790981-large.gif
2018,8278851,Fig. 5.,"Four different classes of typical games used in the experiments. (a) Three shooter games: Space Invaders, Carnival, and Breakout. (b) Three adversarial games: Boxing, Pong, and Kung-Fu Master. (c) Three racing games: Skiing, River Raid, and Enduro. (d) Three strategy games: Alien, Montezuma’s Revenge, and Ms. PacMan.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen5abcd-2790981-large.gif
2018,8278851,Fig. 6.,"Performance of DCRL with comparison to DRN and PER regarding the average
Q
value. (a) Space invaders. (b) Carnival. (c) Breakout. (d) Boxing. (e) Pong. (f) Kung-Fu master. (g) Skiing. (h) River raid. (i) Enduro. (j) Alien. (k) Montezuma’s revenge. (l) MS. PacMan.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen6abcdefghijkl-2790981-large.gif
2018,8278851,Fig. 7.,"Performance of DCRL with comparison to double DQN and dueling network regarding the average
Q
value. (a) Breakout. (b) Pong. (c) Skiing. (d) Alien.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8360119/8278851/chen7abcd-2790981-large.gif
2018,8364638,Fig. 1.,Informative locations (yellow points) on 3D cartilage layer [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan1-2840082-large.gif
2018,8364638,Fig. 2.,The thickness measurement of six CDI locations on one MR slide of the medial tibiofemoral compartment [12].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan2-2840082-large.gif
2018,8364638,Fig. 3.,ROC curves of ANN classifier with different percentages of PCA components obtained from 18 medial features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan3-2840082-large.gif
2018,8364638,Fig. 4.,ROC curves of SVM classifier with different percentages of PCA components obtained from 18 medial features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan4-2840082-large.gif
2018,8364638,Fig. 5.,ROC curves of random forest classifier with different percentages of PCA components obtained from 18 medial features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan5-2840082-large.gif
2018,8364638,Fig. 6.,ROC curves of naïve Bayes classifier with different percentages of PCA components obtained from 18 medial features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan6-2840082-large.gif
2018,8364638,Fig. 7.,ROC curves of ANN classifier with different percentages of PCA components obtained from 18 lateral features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan7-2840082-large.gif
2018,8364638,Fig. 8.,ROC curves of SVM classifier with different percentages of PCA components obtained from 18 lateral features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan8-2840082-large.gif
2018,8364638,Fig. 9.,ROC curves of random forest classifier with different percentages of PCA components obtained from 18 lateral features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan9-2840082-large.gif
2018,8364638,Fig. 10.,ROC curves of naïve Bayes classifier with different percentages of PCA components obtained from 18 lateral features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan10-2840082-large.gif
2018,8364638,Fig. 11.,ROC curves of ANN classifier with different percentages of PCA components obtained from 36 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan11-2840082-large.gif
2018,8364638,Fig. 12.,ROC curves of SVM classifier with different percentages of PCA components obtained from 36 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan12-2840082-large.gif
2018,8364638,Fig. 13.,ROC curves of random forest classifier with different percentages of PCA components obtained from 36 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan13-2840082-large.gif
2018,8364638,Fig. 14.,ROC curves of naïve Bayes classifier with different percentages of PCA components obtained from 36 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan14-2840082-large.gif
2018,8364638,Fig. 15.,ROC curves of naïve Bayes with different percentages of PCA components obtained from 18 medial features for JSM prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan15-2840082-large.gif
2018,8364638,Fig. 16.,ROC curves of random forest with different percentages of PCA components obtained from 36 features for JSM prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan16-2840082-large.gif
2018,8364638,Fig. 17.,ROC curves of naïve Bayes classifier with different percentages of PCA components obtained from 18 lateral features for JSN prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan17-2840082-large.gif
2018,8364638,Fig. 18.,ROC curves of ANN classifier with different percentages of PCA components obtained from 36 features for JSN prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8364638/shan18-2840082-large.gif
2018,8384233,FIGURE 1.,"The TUMK-ELM framework. TUMK-ELM first projects heterogeneous data into kernel spaces by multiple kernels. It then adopts an iterative two stages approach to integrate heterogeneous information. At the first stage, TUMK-ELM generates a K-Space, in which the data is constructed from multiple kernel spaces and the pseudo-labels are assigned according to the learned optimal kernel. At the second stage, TUMK-ELM learns optimal kernel combination coefficients based on the generated K-Space. After convergence, the optimal kernel contains the integrated information from heterogeneous data that suits for the subsequent analytics tasks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8384233/hao1-2847037-large.gif
2018,8384233,FIGURE 2.,"The precision@
k
-curve of different heterogeneous data learning methods: A better metric yields a higher curve. (a) Curve on iris data set. (b) Curve on glass data set. (c) Curve on seeds data set. (d) Curve on wine data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8384233/hao2abcd-2847037-large.gif
2018,8384233,FIGURE 3.,"The recall@
k
-curve of different heterogeneous data learning methods: A better metric yields a higher curve. (a) Curve on iris data set. (b) Curve on glass data set. (c) Curve on seeds data set. (d) Curve on Wine Data Set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8384233/hao3abcd-2847037-large.gif
2018,8384233,FIGURE 4.,The clustering loss value of TUMK-ELM per iteration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8384233/hao4-2847037-large.gif
2018,8384233,FIGURE 5.,"The stability of TUMK-ELM in terms of parameter
C
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8384233/hao5-2847037-large.gif
2018,8384233,FIGURE 6.,The visualization of data distribution in the optimal kernel space learned by different unsupervised heterogeneous learning methods on iris data set. These figures illustrate the data distribution in the TUMK-ELM learned optimal kernel has clearer boundaries between different clusters. The plotted two-dimensional embedding is converted from the optimal kernel by multidimensional scaling [36]. Different symbols refer to different data clusters per ground truth. (a) Distribution in the RMKKM learned optimal kernel space. (b) Distribution in the LMKKM learned optimal kernel space. (c) Distribution in the MKC-LKAM learned optimal kernel space. (d) Distribution in the TUMK-ELM learned optimal kernel space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8384233/hao6abcd-2847037-large.gif
2018,8027072,Fig. 1.,Hotspot or nonhotspot pattern configuration. Figure from [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park1-2750068-large.gif
2018,8027072,Fig. 2.,"Hit means actual hotspot is inside of the hit region.
l
c
is 1.2
μm
. Figure from [21].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park2-2750068-large.gif
2018,8027072,Fig. 3.,Four types of hotspots. (a) HB. (b) VB. (c) HP. (d) VP. Red boxes are cores. Box at the center of core is hotspot location.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park3abcd-2750068-large.gif
2018,8027072,Fig. 4.,Simulation points. Distance between points is 3 nm for 28-nm design and 5 nm for 32-nm design. (a) Vertical line of points. (b) Horizontal line of points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park4ab-2750068-large.gif
2018,8027072,Fig. 5.,Hotspot candidates. (a) HB candidates. (b) VB candidates. (c) HP candidates. (d) VP candidates.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park5abcd-2750068-large.gif
2018,8027072,Fig. 6.,Our hotspot detection flow. (a) Training flow. (b) Testing flow: note litho-simulation is performed to feed the 12 intensity points to our trained models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park6ab-2750068-large.gif
2018,8027072,Fig. 7.,Metric to select nonhotspots: maximum and minimum intensity difference less than 0.001 is used to select nonhotspots.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8387697/8027072/park7-2750068-large.gif
2018,8260978,Fig. 1.,"Examples of nonstrongly convex functions satisfying the QGC with
μ
1
=2
,
μ
2
=(5/7)
,
μ
3
=4
3
–
√
−6
, and
μ
4
=2
. The quadratic growth lower bound of
f
1
(x)
is itself.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8260978/zhang1-2764960-large.gif
2018,8260978,Fig. 2.,"ϕ
-mixing coefficients of Algorithm 2 with four
Q
listed in Table III, and the mixing length
s
is the same in all the cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8260978/zhang2-2764960-large.gif
2018,8260978,Fig. 3.,Experimental results on i.i.d. data. Different step sizes are compared. The top row displays the results of strongly convex loss functions and nonstrongly convex loss functions satisfying the QGC on synthetic data. The last three rows display the results on expanded real data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8260978/zhang3-2764960-large.gif
2018,8260978,Fig. 4.,"Experimental results on
ϕ
-mixing data. Four different resampling distributions are compared. The top row is the result of strongly convex and nonstrongly convex excess risk on synthetic data. The last three rows are the results on expanded real data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8260978/zhang4-2764960-large.gif
2018,8361791,FIGURE 1.,An example of multi-label learning instance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8361791/yu1-2839340-large.gif
2018,8361791,FIGURE 2.,Description of the procedure of the LW-ELM algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8361791/yu2-2839340-large.gif
2018,8361791,FIGURE 3.,"Running time curves of ELM, WELM and LW-ELM algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8361791/yu3-2839340-large.gif
2018,8038008,Fig. 1.,"Scheme illustrating the layers of control in an FDS. Our focus is mainly on the DDM and the alert–feedback interaction, which regulates the way recent supervised samples are provided.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8038008/dalpo1-2736643-large.gif
2018,8038008,Fig. 2.,"Supervised samples available at the end of day
t
include: 1) feedbacks [
F
(⋅)
] and 2) delayed couples [
D
(⋅)
] occurred before
t−δ
days. In this plot, we have assumed
δ=7
. Patterns indicate different labels, and the size of these regions indicates balanced/unbalanced class proportions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8038008/dalpo2-2736643-large.gif
2018,8038008,Fig. 3.,"Number of fraudulent transactions and cards per day in the data sets described in Table I . It emerges that there are more fraudulent transactions than cards, meaning that some cards have received more than a fraud. (a) Number of fraudulent transactions. (b) Number of fraudulent cards.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8038008/dalpo3ab-2736643-large.gif
2018,8038008,Fig. 4.,"Supervised information used by the classifiers considered in our experiments. In this illustrative example, we set
δ=7
,
M=2
and
Q=7+2=9
. (a) Pooling together all labeled transactions. (b) Separating feedbacks and delayed samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8038008/dalpo4ab-2736643-large.gif
2018,8038008,Fig. 5.,"(a) Values of
CP
k
for
S
,
W
D
, and
A
W
on data set 2014–2015. (b) Number of fraudulent cards on the same period. For the visualization sake, these values have averaged over a sliding window of 15 days. The peak of
CP
k
in (a) corresponds to the peak in number of fraudulent cards in (b). This result confirms that the classifiers become more precise in those days characterized by a large number of fraudulent cards.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8417016/8038008/dalpo5ab-2736643-large.gif
2018,8240912,FIGURE 1.,Overview of the process followed. Summary of the materials and methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8240912/cruzb1-2782678-large.gif
2018,8240912,FIGURE 2.,"Dendrogram representing the clusters found with the predictive model generated using the data from vertical A. Each leaf represents a different cluster obtained (except, in this figure, clusters 8 and 9 that are represented together in the dendrogram due to their closeness in the 9th leaf). The different values that appear near the claves display the Euclidean distance that explains the separation between the different clusters. Finally, the numbers below the leaves (at the bottom of the figure) present the number of users included in the corresponding cluster. Source and full resolution image with all the clusters are available in [26].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8240912/cruzb2-2782678-large.gif
2018,8240912,FIGURE 3.,Dendrogram representing the clusters found with the predictive model generated using the data from vertical B. The meaning of the different visual elements is the same than those presented in Fig 2. Source [26].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8240912/cruzb3-2782678-large.gif
2018,8240912,FIGURE 4.,Dendrogram representing the clusters found with the predictive model generated using the data from vertical C. The meaning of the different visual elements is the same as those presented in the previous dendrogram figures. Source [26].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8240912/cruzb4-2782678-large.gif
2018,8240912,FIGURE 5.,"Descriptive statistics and distribution of values for cluster 8 (vertical C), regarding the use of the Windows operating system. Source [26].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8240912/cruzb5-2782678-large.gif
2018,8463474,FIGURE 1.,Block diagram of the proposed STL-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat1-2869577-large.gif
2018,8463474,FIGURE 2.,Self-taught learning stages.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat2-2869577-large.gif
2018,8463474,FIGURE 3.,"Accuracy, precision, recall, and F-measure values for STL-IDS and single SVM for binary classification based on test data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat3-2869577-large.gif
2018,8463474,FIGURE 4.,"Accuracy, precision, recall, and F-measure values for STL-IDS and single SVM for binary classification based on training data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat4-2869577-large.gif
2018,8463474,FIGURE 5.,"Accuracy, precision, recall, and F-measure values for STL-IDS and single SVM for five-category classification based on test data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat5-2869577-large.gif
2018,8463474,FIGURE 6.,"Accuracy, precision, recall, and F-measure values for STL-IDS and single SVM for five-category classification based on training data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat6-2869577-large.gif
2018,8463474,FIGURE 7.,The accuracy and testing time on KDDTEST+ dataset in the binary classification with different number of hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat7-2869577-large.gif
2018,8463474,FIGURE 8.,The accuracy and testing time on KDDTEST+ dataset in the multiclass classification with different number of hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat8-2869577-large.gif
2018,8463474,FIGURE 9.,The training time on KDDTRAIN+ dataset in the binary and multiclass classification with different number of hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat9-2869577-large.gif
2018,8463474,FIGURE 10.,Accuracy on KDDTEST+ dataset in the binary and multiclass classification with different sparse parameer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat10-2869577-large.gif
2018,8463474,FIGURE 11.,The training time on KDDTRAIN+ dataset in the binary and multiclass classification with different sparse parameter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8463474/alqat11-2869577-large.gif
2018,8016353,Fig. 1.,Schematic view on raw footprint features captured by the Noldus Catwalk system in combination with the nomenclature for paws that we use in this article. In the shown example the gray filled RH and LF indicate that both paws are on ground at the same time and corresponding raw footprint features are captured. Brain lesions were always applied to the right hemisphere of animals in our data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8340950/8016353/frohl1-2701204-large.gif
2018,8016353,Fig. 2.,"Example of open ended, asymmetric DTW based alignment of two step sequences. Top: alignment of first versus second sequence. Bottom: vice versa. The figures depict the step indices which are mapped to each other.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8340950/8016353/frohl2-2701204-large.gif
2018,8016353,Fig. 3.,Learning curve showing prediction performance as a function of the number of animals in the dataset. Prediction performance at each data point was always assessed with a leave-one-animal-out procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8340950/8016353/frohl3-2701204-large.gif
2018,8305616,Fig. 1.,ROC curves for the random forest classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8573200/8305616/monte1-2811471-large.gif
2018,8066455,Fig. 1.,"Two cycles of a biological time series. The information is extracted from the growing sectors using all the three schema: forward, backward, and bilateral growing scheme. The number of the growing sectors is
K=4
and the growing center is selected at the 75% of the cycle duration, for illustrational simplicity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8066455/ghare1-2754294-large.gif
2018,8066455,Fig. 2.,Flowchart (top) of the classification process and the corresponding flow mapping (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8066455/ghare2-2754294-large.gif
2018,8066455,Fig. 3.,Architecture of the DTGNN for the cyclic learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8066455/ghare3-2754294-large.gif
2018,8066455,Fig. 4.,"Variation of the weighted classification rate
Θ
ˆ
with the number of the sectors
K
for all the four case studies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8066455/ghare4-2754294-large.gif
2018,8066455,Fig. 5.,"Effect of the number of neurons in the hidden layer on the weighted classification rate
Θ
ˆ
for the PCG category.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8066455/ghare5-2754294-large.gif
2018,8066455,Fig. 6.,"Results of the A-Test: variation of the classification error
Γ
ζ,z
against
z
value for the four classifiers; the CMLP, the CTDNN, the FTGNN, and the DTGNN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8066455/ghare6-2754294-large.gif
2018,8286918,Fig. 1.,Different IMFs of sag with impulsive transient and multiple spike signal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash1-2803042-large.gif
2018,8286918,Fig. 2.,Detection of multiple PQEs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash2-2803042-large.gif
2018,8286918,Fig. 3.,Structure of ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash3-2803042-large.gif
2018,8286918,Fig. 4.,Proposed HHT-WBELM-based intelligent PQEs recognition system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash4-2803042-large.gif
2018,8286918,Fig. 5.,(a) Schematic diagram of the developed hardware to generate the real-time PQEs and (b) the hardware laboratory setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash5-2803042-large.gif
2018,8286918,Fig. 6.,Localization of real-time PQEs in the presence of system noise: (a) voltage transient signal; (b) voltage notch signal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash6-2803042-large.gif
2018,8286918,Fig. 7.,Different IMFs of (a) voltage sag signal and (b) voltage notch signal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash7-2803042-large.gif
2018,8286918,Fig. 8.,Generation and categorization of real-time (a) voltage flicker and (b) voltage harmonic signal on DSP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash8-2803042-large.gif
2018,8286918,Fig. 9.,Average testing accuracy of different algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8454923/8286918/dash9-2803042-large.gif
2018,8356006,FIGURE 1.,Architecture of IoT-enabled stroke rehabilitation system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng1-2822681-large.gif
2018,8356006,FIGURE 2.,Smart wearable armband (SWA).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng2-2822681-large.gif
2018,8356006,FIGURE 3.,"Target gesture set: a) AG, b) CH, c) OH, d) PT, e) T&M, f) T&L, g) FH, h) EH, i) RE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng3abcdefghi-2822681-large.gif
2018,8356006,FIGURE 4.,Offline classification accuracy using individual feature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng4-2822681-large.gif
2018,8356006,FIGURE 5.,Offline classification accuracy using different combinations of features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng5-2822681-large.gif
2018,8356006,FIGURE 6.,"Data analysis tool using CCEAs [26]. Section 1 illustrates the distance of each class and its closest neighbors in feature space (2D). Section 2 shows a table in the order of the number of times that a movement is the most conflicting neighbor for any of the other movements. Data from a recording session of nine movements was showed. The feature extraction setting is “tmabs”, “tstd”, “twl”, “trms”, and “tvar”. Section 3 shows a table of movement conflicts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng6-2822681-large.gif
2018,8356006,FIGURE 7.,"The distribution of nine classes in feature space (3D) leveraging PCA: (a) the feature extraction setting is “tzc”; (b) the feature extraction setting is “tmabs”, “tstd”, “twl”, “trms”, and “tvar”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng7ab-2822681-large.gif
2018,8356006,FIGURE 8.,Offline classification accuracy and training/validation time using different classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng8-2822681-large.gif
2018,8356006,FIGURE 9.,Offline and real time classification accuracy of nine movements.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng9-2822681-large.gif
2018,8356006,FIGURE 10.,Real time gesture recognition results to control a five-finger dexterous robot.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8253365/8356006/deng10abcdefg-2822681-large.gif
2018,8509634,Fig. 1.,"Setup for user positioning in distributed massive MIMO:
K
single-antenna UEs transmit uplink signals simultaneously to
M
RRHs on the same time-frequency resource. Each RRH records forwards its RSS to the CU via high-speed fronthaul. The CU hosts a machine learning model which takes the RSS vectors as input and provides the users’ location estimates as output.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa1-2876832-large.gif
2018,8509634,Fig. 2.,"Example setup with three RRHs, labelled B1, B2, and B3, and five users, labelled U1, U2, U3, U4, and U5. The radius
d
th
is the distance beyond which
η
changes in value (e.g.,
d
th
=45m
in 3GPP UMi [22] because
η
changes from 2 to 6.7). For any pair of users,
ϕ(.,.)
should be high if the
x−
coordinates are closely-spaced and it should decrease with the separation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa2-2876832-large.gif
2018,8509634,Fig. 3.,"Simulation setup with
M=30
RRHs,
L
˜
=400
training locations, and
L
ˆ
=9
test users.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa3ab-2876832-large.gif
2018,8509634,Fig. 4.,"Average RMSE performance of the CGP, NaGP, TRILAT-LLS, TRILAT-ILLS, and KNN methods under Scenario A and B respectively for different shadowing noise levels, when
M=10,30
. The BCRLBs for the CGP and NaGP methods are also included in the plot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa4abcd-2876832-large.gif
2018,8509634,Fig. 5.,"Plots of the average
2σ
error-bars on the test users’
x
and
y
coordinates and the number of true test user locations within the
2σ
confidence range
([
μ
ˆ
(.)
x
]
l
±2
[
C
ˆ
(.)
x
]
ll
−
−
−
−
−
−
√
,[
μ
ˆ
(.)
y
]
l
±2
[
C
ˆ
(.)
y
]
ll
−
−
−
−
−
−
√
)
of the estimated locations, as provided by the CGP and NaGP methods for different shadowing noise levels, when
M=10
, 30.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa5-2876832-large.gif
2018,8509634,Fig. 6.,"Average LPD performance of the CGP and NaGP methods under Scenario A for different shadowing noise levels, when
M=10
, 30.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa6-2876832-large.gif
2018,8509634,Fig. 7.,"Average RMSE, the associated BCRLBs, and the
2σ
error-bars on the estimated
x
and
y
cordinates, as given by the NaGP and CGP methods for different
M
, when
σ
2
z
=3,5
dB. For comparison, we also plot the RMSE performance of the TRILAT-ILLS and KNN baselines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa7-2876832-large.gif
2018,8509634,Fig. 8.,"Average RMSE performance, and the corresponding BCRLBs, of the CGP and NaGP methods under the presence of correlated shadowing among the test RSS vectors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa8ab-2876832-large.gif
2018,8509634,Fig. 9.,"Localization performance of the CGP and NaGP methods under the WINNER II A1-LOS and A1-NLOS scenarios. We plot the average RMSE, LPD, and the number of true test user locations within the
2σ
confidence range, as given by the GP methods for different
M
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa9-2876832-large.gif
2018,8509634,Fig. 10.,"RMSE performance of the extrapolation GP (ExGP) method for the WINNER-II A1-LOS and A1-NLOS scenarios, when the training inputs are
λ=[0,0.3,0.6,…,3]
, the training outputs are the corresponding location estimates from NaGP, and the test input is
λ=−1
. For reference, we also plotted the RMSE achieved by NaGP for each training
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/8570921/8509634/prasa10-2876832-large.gif
2018,8486713,Fig. 1.,Training of semisupervised graph-based HIC with multiscale kernels and random sampling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang1-2873051-large.gif
2018,8486713,Fig. 2.,Testing of semisupervised graph-based HIC with multiscale kernels and random sampling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang2-2873051-large.gif
2018,8486713,Fig. 3.,"AVIRIS Indian Pines dataset. (a) False color composite image of three bands 150, 27, and 17. (b) Ground-truth map containing 16 mutually exclusive land-cover classes. The numbers in the parentheses represent the labeled samples available for each class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang3-2873051-large.gif
2018,8486713,Fig. 4.,"ROSIS Pavia University dataset. (a) False color composite image of three bands 60, 30, and 2. (b) Ground-truth map containing 9 mutually exclusive land-cover classes. The numbers in the parentheses represent the labeled samples available for each class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang4-2873051-large.gif
2018,8486713,Fig. 5.,"Results for (top row) the average cost of computation time to solve the linear system (8) and (bottom row) standard deviations after ten runs, but varied size
u
of unlabeled samples on the considered datasets. (a) and (c) Indian Pines dataset. (b) and (d) Pavia University dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang5-2873051-large.gif
2018,8486713,Fig. 6.,"Performances of Lap-LSSVM-SS-RS with different sampling configurations on the considered datasets. The top row is for the Indian Pines dataset, whereas the bottom row is for the Pavia University dataset. We are interested in the upper bound of partitions
m
. (a) and (c) OA curves for fixed number of partition but varied unlabeled sample sizes. (b) and (d) OA curves for fixed unlabeled sample size, but varied number of partitions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang6-2873051-large.gif
2018,8486713,Fig. 7.,"Empirical studies on the parametric sensitivity of
γ
A1
,
γ
A2
, and
γ
I
for Lap-LSSVM-SS and Lap-LSSVM-SS-RS on the considered datasets. The first row is for the Indian Pines dataset, whereas the second row is for the Pavia University dataset. (a) and (d) OA curve with respect to different
γ
A1
. (b) and (e) OA curve with respect to different
γ
A2
. (c) and (f) OA curve with respect to different
γ
I
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang7-2873051-large.gif
2018,8486713,Fig. 8.,"OA surface over the Indian Pines dataset for Lap-LSSVM-SS as a function of multiscale kernels
σ
1
and
σ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang8-2873051-large.gif
2018,8486713,Fig. 9.,"OA accuracy and Kappa coefficient versus numbers of labeled samples per class with different classification methods on (top row) Indian Pines dataset, and (bottom row) Pavia University dataset. (a) and (c) OA versus
ℓ
curves. (b) and (d) Kappa versus
ℓ
curves.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang9-2873051-large.gif
2018,8486713,Fig. 10.,"Thematic maps for the Indian Pines dataset (with a size of
145×145
) using 1174 labeled samples and 97 unlabeled samples. (a) Ground-truth. (b) RBF-SVM classified map. (c) BagSVM-sum classified map. (d) BagSVM-product classified map. (e) LapSVM classified map. (f) LapRLS classified map. (g)
S
4
VM
classified map. (h) Lap-LSSVM-SS-RS classified map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang10-2873051-large.gif
2018,8486713,Fig. 11.,"Thematic maps for the Pavia University dataset (with a size of
610×340
) using 810 labeled samples and 255 unlabeled samples. (a) Ground-truth. (b) RBF-SVM classified map. (c) BagSVM-sum classified map. (d) BagSVM-product classified map. (e) LapSVM classified map. (f) LapRLS classified map. (g)
S
4
VM
classified map. (h) Lap-LSSVM-SS-RS classified map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8556520/8486713/huang11-2873051-large.gif
2018,8401331,Fig. 1.,"The single-cell RNAseq analysis workflow. It starts with sequencing of cancer cells and goes to the use of unsupervised learning methods, combined with pathway analysis, to generate a list of a few genes. Those genes become candidates for informing the design of focused laboratory experiments for learning novel biology of drug action.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer1-2851997-large.gif
2018,8401331,Fig. 2.,"The existence of mixtures in the feature space and samples is illustrated by the probability density functions (PDF) of gene expressions in a set of genes across (a) all baseline cells, (b) a set of baseline cells and (c) a set of metformin-treated cells.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer2abc-2851997-large.gif
2018,8401331,Fig. 3.,"The unsupervised learning view of single-cell analysis is explained as follows. Cells exposed to a drug may exhibit differences in their gene expression behavior due to the molecular interactions with the drug, and these differences are not known. The computational problem is to cluster these cells (samples), and extract the genes (features) that are behaving differently compared to other clusters (referred to as differentially expressed genes). These differentially expressed genes are analyzed to study their biological significance in all known disease and molecular pathways.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer3-2851997-large.gif
2018,8401331,Fig. 4.,"In (a) and (b), we project, respectively, the baseline and metformin-treated cell clusters found by MiMoSA onto the first two principal components derived from PCA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer4ab-2851997-large.gif
2018,8401331,Fig. 5.,"The average gene expression of downregulated genes identified by MiMoSA and
k
-means clustering using various values of
k
are illustrated. Specifically, (a) shows the average gene expression levels in clusters found by MiMoSA, while (b) shows the same number of clusters identified by
k
-means clustering in baseline and metformin-treated cells. For a wide-range of
k
, it can be seen in figs (b)–(f) that
k
-means clustering was unable to establish the same cluster of cells that helped identify the significantly downregulated genes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer5abcdef-2851997-large.gif
2018,8401331,Fig. 6.,"These box plots of the distribution of average gene expression of respective clusters illustrate the inability of
k
-means clustering to capture the cells of
M2
in one cluster even if the features comprise only the downregulated genes identified by MiMoSA. In figs (a) – (e), although one could observe downregulation of the genes, they are statistically not significant because
K2
comprised only one cell, and in figs (f)–(h), clusters
K1
and
K2
comprised only one cell.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer6abcdefgh-2851997-large.gif
2018,8401331,Fig. 7.,Illustration of the laboratory experiments performed to establish CDC42’s role in meformin’s anticancer mechanisms in triple-negative breast cancer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8424246/8401331/iyer7-2851997-large.gif
2018,8542764,Fig. 1.,(a) Conventional engineering design flow; and (b) baseline machine learning methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo1ab-2881442-large.gif
2018,8542764,Fig. 2.,Machine learning methodology that integrates domain knowledge during model selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo2-2881442-large.gif
2018,8542764,Fig. 3.,Illustration of (a) supervised learning and (b) unsupervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo3ab-2881442-large.gif
2018,8542764,Fig. 4.,"A generic cellular wireless network architecture that distinguishes between edge segment, with base stations, access points, and associated computing resources, and cloud segment, consisting of core network and associated cloud computing platforms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo4-2881442-large.gif
2018,8542764,Fig. 5.,"Illustration of the supervised learning problem of regression: Given input-output training examples
(
x
n
,
t
n
)
, with
n=1,…,N
, how should we predict the output
t
for an unobserved value of the input
x
?",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo5-2881442-large.gif
2018,8542764,Fig. 6.,"Illustration of the supervised learning problem of classification: Given input-output training examples
(
x
n
,
t
n
)
, with
n=1,…,N
, how should we predict the output
t
for an unobserved value of the input
x
?",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo6-2881442-large.gif
2018,8542764,Fig. 7.,"An illustration of the hypothesis class
p
(
t
|
x
,
w
) assumed by logistic regression using a neural network representation: functions
ϕ
i
, with
i=1,…,
D
′
, are fixed and compute features of the input vector
x=[
x
1
,…,
x
D
]
. The learnable parameter vector
θ
here corresponds to the weights
w
used to linearly combine the features in (7).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo7-2881442-large.gif
2018,8542764,Fig. 8.,"An illustration of the hypothesis class
p
(
t
|
x
,
w
) assumed by multi-layer neural networks. The learnable parameter vector
θ
here corresponds to the weights
w
L
used at the last layer to linearly combine the features
ϕ(x)
and the weight matrices
W
1
,…,
W
L−1
used at the preceding layers in order to compute the feature vector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo8-2881442-large.gif
2018,8542764,Fig. 9.,"Training set in Fig. 5, along with a predictor trained by using the discriminative model (11) and ML for different values of the model order
M
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo9-2881442-large.gif
2018,8542764,Fig. 10.,"Training loss and generalization loss, estimated via validation, as a function of the model order
M
for the example in Fig. 9.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo10-2881442-large.gif
2018,8542764,Fig. 11.,Illustration of typical unsupervised learning models: (a) directed generative models; (b) undirected generative models; (c) discriminative models; and (d) autoencoders.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo11abcd-2881442-large.gif
2018,8542764,Fig. 12.,"The ELBO (15) is a global lower bound on the log-likelihood that is tight at values of the model parameters
θ
0
for which equality (17) holds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo12-2881442-large.gif
2018,8542764,Fig. 13.,"Illustration of the EM algorithm: At each iteration, a tight ELBO is evaluated in the E step by solving the problem of estimating the latent variables (via the posterior distribution
p(z|x,θ)
), and then the ELBO is maximized in the M step by solving a problem akin to supervised learning with the estimated latent variables.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo13-2881442-large.gif
2018,8542764,Fig. 14.,"Illustration of the limitations of ML unsupervised learning, here obtained via the EM algorithm: The ML solution tends to be blurry, missing the modes of the true distribution
p
(
x
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo14-2881442-large.gif
2018,8542764,Fig. 15.,"Discriminator between the hypotheses x
∼ p
(
x
) and x
∼ q
(
x
) based on the statistic
T
(
x
). The performance of the optimal discriminator function
T
(
x
) under different design criteria yields a measure of the difference between the two distributions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/8584171/8542764/simeo15-2881442-large.gif
2018,8252784,Fig. 1.,Motivation of the proposed method. (a) Within-class diversity and (b) between-class similarity are two major challenges that often degenerate the scene classification performance. This motivates us to learn more D-CNN feature representations that have small within-class scatter but big between-class separation. These examples are from the challenging NWPU-RESISC45 data set [1].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han1ab-2783902-large.gif
2018,8252784,Fig. 2.,"Typical architecture of a CNN, which is structured as a series of layers including convolutional layers, pooling layers, and FC layers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han2-2783902-large.gif
2018,8252784,Fig. 3.,"Illustration of the core idea of the proposed D-CNN method. To address the challenges of within-class diversity and between-class similarity, we propose to learn D-CNNs by optimizing a new objective function. Apart from minimizing the cross-entropy loss, we also impose a metric learning regularization term on the CNN features to enforce the D-CNN models to be more discriminative. Thus, in the D-CNN feature spaces, the images from the same scene class are as close as possible and the images of different classes are as far away as possible.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han3-2783902-large.gif
2018,8252784,Fig. 4.,"Illustration of our proposed D-CNN method. By embedding the metric learning constrain to D-CNN model training process, we can obtain a big margin between each similar pair (denoted with the same colors) and dissimilar pair in the learned D-CNN feature space. (a) Original CNN feature space. (b) Our D-CNN feature space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han4ab-2783902-large.gif
2018,8252784,Fig. 5.,"Overall accuracies of our proposed D-CNN methods on the NWPU-RESISC45 data set (10% training ratio) under different parameter settings with three CNN models including (a) AlexNet, (b) VGGNet-16, and (c) GoogLeNet. The metric learning regularization parameter
λ
1
varies from the set of {0.01, 0.05, 0.1, 0.5} and the iteration number changes from 1000 to 15 000 with a stride of 1000.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han5abc-2783902-large.gif
2018,8252784,Fig. 6.,"(Left) Training error and (Right) test error on NWPU-RESISC45 data set (10% training images) with fine-tuned CNN and our proposed D-CNN. Here, VGGNet-16 is the base model and the iteration number changes from 1000 to 15000 with a stride of 1000.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han6ab-2783902-large.gif
2018,8252784,Fig. 7.,Confusion matrices of the UC Merced data set under the training ratio of 80% using the following methods. (a) Fine-tuned VGGNet-16 + SVM. (b) D-CNN with VGGNet-16.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han7ab-2783902-large.gif
2018,8252784,Fig. 8.,Confusion matrices of the AID data set under the training ratio of 50% using the following methods. (a) Fine-tuned VGGNet-16 + SVM. (b) D-CNN with VGGNet-16.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han8ab-2783902-large.gif
2018,8252784,Fig. 9.,Confusion matrices of the NWPU-RESISC45 data set under the training ratio of 20% using the following methods. (a) Fine-tuned VGGNet-16 + SVM. (b) D-CNN with VGGNet-16.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8344524/8252784/han9ab-2783902-large.gif
2018,8294242,Fig. 1.,Examples of attached sensor devices used to detect and measure the swallowing function via (a) a nasal cannula-type flow sensor and (b) a piezoelectric sensor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8496928/8294242/inoue1-2807487-large.gif
2018,8294242,Fig. 2.,"Examples of signal data, including (a) respiratory flow, (b) laryngeal motion, and (c) swallowing sounds. The horizontal axis represents time, with zero indicating the onset of pharyngeal motion; the vertical axis represents signal amplitude, which we normalized to the range from −1 to 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8496928/8294242/inoue2-2807487-large.gif
2018,8294242,Fig. 3.,"Examples of feature extraction from (a) respiratory flow, (b) laryngeal motion, and (c) swallowing sound signals. Green points indicate the spectrum envelope amplitude. We extract
θ
LPC spectrum envelope amplitude values for respiratory flow, laryngeal motion, and swallowing sounds at frequencies intervals of
μ
(r)
,
μ
(l)
, and
μ
(s)
Hz, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8496928/8294242/inoue3-2807487-large.gif
2018,8352798,Fig. 1.,"Behavior of PWLS-ST: Pre-learned sparsifying transform
Ω
with (a)
η=50
and (b)
η=100
. The rows of the
512×512
matrix
Ω
are reshaped into
8×8×8
(3D) patches and the first
8×8
slices of 256 of these 3D patches are displayed for simplicity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng1ab-2832007-large.gif
2018,8352798,Fig. 2.,"RMSE and SSIM for PWLS-ULTRA for various choices of number of clusters
K
(left), and the central slices along three directions for the underlying volume in the cone-beam CT reconstruction experiments (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng2ab-2832007-large.gif
2018,8352798,Fig. 3.,"Pixel-level clustering results (top row) for the central axial slice of the PWLS-ULTRA (
K=5
) reconstruction at
I
0
=1×
10
4
. The pixels in each class are displayed using the intensities in the reconstruction. The corresponding transforms (the first
8×8
slice of
8×8×8
atoms) are in the bottom row.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng3-2832007-large.gif
2018,8352798,Fig. 4.,"Comparison of 2D reconstructions for PWLS-DL (left) and PWLS-ULTRA (
K=15
, right) at
I
0
=1×
10
4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng4ab-2832007-large.gif
2018,8352798,Fig. 5.,"Comparison of the reconstructions and corresponding error images (shown for the central axial, sagittal, and coronal planes) for FDK, PWLS-EP, and PWLS-ULTRA (
K=15
) with patch-based weights at
I
0
=1×
10
4
. The unit of the display window of the error images is HU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng5-2832007-large.gif
2018,8352798,Fig. 6.,"RMSE of each axial slice in the PWLS-EP and PWLS-ULTRA reconstructions for
I
0
=1×
10
4
(left) and
I
0
=5×
10
3
(right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng6ab-2832007-large.gif
2018,8352798,Fig. 7.,"Chest reconstructions (shown for central axial plane) from helical CT data, with the FDK, PWLS-EP, and PWLS-ULTRA (
K=5
) methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng7-2832007-large.gif
2018,8352798,Fig. 8.,"Chest reconstructions (shown for the central axial, sagittal, and coronal planes in the 3D volume) for PWLS-EP with different regularization strengths. (a) 1X. (b) 2X. (c) 0.5X. (d) 0.25X. 1X denotes the chosen regularization parameter in [50] that provides a good trade-off between image resolution and noise reduction. The 2X, 0.5X, and 0.25X denote scaling of the parameter
β
over the 1X case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng8abcd-2832007-large.gif
2018,8352798,Fig. 9.,"Chest reconstructions (shown for the central axial, sagittal, and coronal planes in the 3D volume) for PWLS-ULTRA (
K=5
) with different parameter combinations. (a)
β=2×
10
5
,γ=25
. (b)
β=2×
10
5
,γ=20
. (c)
β=3×
10
5
,γ=20
. (d)
β=3×
10
5
,γ=25
. Larger regularization strength
β
would achieve more noise reduction but simultaneously lower spatial resolution, e.g., compare (a) and (d); larger values of
γ
would achieve lower sparsities and more noise reduction but potentially oversmooth the image, e.g., compare (c) and (d).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng9abcd-2832007-large.gif
2018,8352798,Fig. 11.,Vertical (left) and horizontal (right) profiles of chest reconstructions (plotted from the central axial slice) for the PWLS-EP and PWLS-ULTRA methods. The profile locations are shown in green lines in Fig. 7.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng11ab-2832007-large.gif
2018,8352798,Fig. 10.,"Abdomen reconstructions (shown for the central axial, sagittal, and coronal planes, and air cropped) from low-dose (120kVp, 150mA and 35mA with rotation time 0.8 seconds) helical CT data (the same patient) for PWLS-EP and PWLS-ULTRA with patch-based weights (
K=5
). (a) 150 mA, PWLS-EP. (b) 150 mA, PWLS-ULTRA-
{
τ
j
}
. (c) 35 mA, PWLS-EP. (d) 35 mA, PWLS-ULTRA-
{
τ
j
}
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng10abcd-2832007-large.gif
2018,8352798,Fig. 12.,"Reconstruction with PWLS-ULTRA (
K=15
) without weights
τ
j
(left) at
I
0
=1×
10
4
compared to the reconstruction with the oracle scheme without weights
τ
j
(right), where the cluster memberships were pre-determined from the ground truth. RMSE and SSIM values of 30.7 and 0.978 (left), and 29.0 and 0.982 (right) respectively, for the volumes, indicates that more precise clustering can provide better reconstructions and sharper edges (see zoom-ins).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8370207/8352798/zheng12ab-2832007-large.gif
2018,8439940,Fig. 1.,This illustration shows the elevated system externals of eMLEE. LT interacts with eFES and eABT on the deeper level. It coordinates and regulates the metrics of the learning process in parallel mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin1-2866046-large.gif
2018,8439940,Fig. 2.,"Illustration of Loss and Noise interoperation based on x, y, and z dimensions, for algorithms blend.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin2-2866046-large.gif
2018,8439940,Fig. 3.,"– Illustration of optimum fitness logical (x, y, z) triangle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin3-2866046-large.gif
2018,8439940,Fig. 4.,"Illustration of
B
A
n
(Blue)
,
T
A
n
(Red)
as it theoretically spread in optimum space of x, y, z dimensions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin4-2866046-large.gif
2018,8439940,Fig. 5.,Illustration of eABT Logical Table Internals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin5-2866046-large.gif
2018,8439940,Fig. 6.,This illustration shows the concept of LT modular elements in 3-D space as discussed earlier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin6-2866046-large.gif
2018,8439940,Fig. 7.,It shows the LT optimum fitness ability in each dimension. We noticed error at the negative value.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin7-2866046-large.gif
2018,8439940,Fig. 8.,"This shows the ideal behavior of the LT optimum fitness function. As we see, the blue section is virtually absent. And it further elaborates that z-dimension has the maximum convergence of the function, as ideally desired.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin8-2866046-large.gif
2018,8439940,Fig. 9.,This is the real (experimental) behavior of Figure 8.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin9-2866046-large.gif
2018,8439940,Fig. 10.,This test shows the variance of the LT module for the cost function for all three co-ordinates and then z (optimum-fitness). This is the ideal behavior.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin10-2866046-large.gif
2018,8439940,Fig. 11.,This test shows the real (experimental) behavior.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin11-2866046-large.gif
2018,8439940,Fig. 12.,Demonstration to illustrate the entropy-based feature distribution in space based on binary system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin12-2866046-large.gif
2018,8439940,Fig. 13.,Illustrates of the N-experimental iteration of the conceptual flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin13-2866046-large.gif
2018,8439940,Fig. 14.,This illustrates the ideal outcome of eFES LT fitness function in 3D space. Notice the z-axis has the least blue color.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin14-2866046-large.gif
2018,8439940,Fig. 15.,"This illustrates the real(experimental) analysis of the test, we performed on validation eFES module.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin15-2866046-large.gif
2018,8439940,Fig. 16.,"This test shows the three metrics (Underfitting (UF), Overfitting (UF), and optimum-fitting (OpF) for 500 experimental-run. LT module successfully identified and process the features that contribute to each metric accordingly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin16-2866046-large.gif
2018,8439940,Fig. 17.,"This test shows the matching function built in eFES algorithm for random vs correlated data points. As we observed, it shows the promising behavior (expected) when it is highly correlated than just the random test.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin17-2866046-large.gif
2018,8439940,Fig. 18.,LT framework at low-level.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin18-2866046-large.gif
2018,8439940,Fig. 19.,"Experiments (A) to (C) shows the poor optimization for z with CF without using LT objects. However, we observe in (D) to (F) that model is learning to optimize itself for optimum CF for z-dimension using LT objects. The spike noticed in (F) is suspected to be error and will need future investigation. (A) through (C) were conducted using standard procedure where LT objects were not used.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin19abcdef-2866046-large.gif
2018,8439940,Fig. 20.,"The outcome shown here exhibits the model behavior for ratio
R
in terms of each dimension of x,y and z. It is observed that regression is relatively higher for each dimension and is considered pre-mature learning of the model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin20-2866046-large.gif
2018,8439940,Fig. 21.,"The observation here shows improvement and considered mature classifier learning of the LT process. As we noticed that z-dimension (as hoped in the design of the model) is depreciating with respect of the error ratio
R
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin21-2866046-large.gif
2018,8439940,Fig. 22.,"Adder real and ideal function is shown here. We observe that when experiment size is at lower end, it shows higher % and as the experiment size increases, the function outcomes drop and this behavior is in line with model internals as expected. The triangular spike is a training error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin22-2866046-large.gif
2018,8439940,Fig. 23.,Remover real and ideal function is shown here. It shows that model exhibited theoretical stability of its internals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin23-2866046-large.gif
2018,8439940,Fig. 24.,"This test shows the matching function built in eFES algorithm for random vs correlated data points. As we observed, it shows the promising behavior (expected) when it is highly correlated than just the random test without using LT objects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin24-2866046-large.gif
2018,8439940,Fig. 25.,"This test shows the candle stick analysis commonly used for stocks predictions. True Positive (TP), False Positive (FP), Fitness Factor (FF), Correlating Factor (FF), Bias Factor (BF) shows the move between 0 to 100 % for 20 experimental-run tests. This analysis helps particularly in understanding the direction of the move of the metric when classifier learns. For example, if we can consider the speed of the process, we can see the move in green candle from almost 0 to 98 % throughout the process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8439940/uddin25-2866046-large.gif
2018,8255636,Fig. 1.,"Three-layer network of an autoencoder. The first layer (input layer
X
) and the second layer (hidden layer
H
) with the weight matrix
W
forms an encoder, and a decoder is formed by the second layer and the third layer (output layer
X
^
) with transpose matrix of
W
. The weight training is done in the way of minimizing the discrepancy between the input
X
and its reconstructed output
X
^
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim1-2762002-large.gif
2018,8255636,Fig. 2.,Perceptron as a unit building block of neural network. This single-layer network forms a set of MAC unit determining a neuronal output.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim2-2762002-large.gif
2018,8255636,Fig. 3.,"Proposed current-mode MAC core which is driven by each input pixel. Multiplication is performed in current-mode by switching a bias current
I
B
through a PMOS switch. The OSC frequency is translated to be accumulation result.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim3-2762002-large.gif
2018,8255636,Fig. 4.,"Timing diagram of PWM with an example of 4b input indicates that the total ‘high’ duration of the output represents the amount of input
x
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim4-2762002-large.gif
2018,8255636,Fig. 5.,"Circuit diagrams of (a) fully-connected crossbar array, (b) bitwise neural computation with 6b synapse, and (c) serialized neuronal processor (SNP).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim5-2762002-large.gif
2018,8255636,Fig. 6.,Piecewise linearization of sigmoid (s: scale factor).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim6-2762002-large.gif
2018,8255636,Fig. 7.,"(a) Three-layer network merged in a transposable memory forms the encoding and the decoding paths. (b) Six of 1b synapse circuit for transposable read are combined for 6b synaptic weight. Horizontal signal
RWL
enc
from
X
k
accesses the weight w, generating vertical signal
MEM
enc
to
H
k
, likewise, vertical signal
RWL
dec
from
H
k
accesses the weight
w
, generating horizontal signal
MEM
dec
to
X
^
k
. (c) Timing diagram of 1-cycle of encoding and decoding for image recovery.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim7-2762002-large.gif
2018,8255636,Fig. 8.,"(a) Procedure of RBM learning for AE;
X
k
,
H
k
,
X
^
k
, and
H
^
k
are generated in series. (b) Virtual table-based learning uses only one 64-by-2b-table to extract
Δ
w
ij
′
s
according to each address (three MSBs of
h
j
and
h
^
j
) to update all the weights in a row. (c) Timing diagram of learning performing a row-by-row update; the learning of two input sets requires eight cycles of global clock
CL
K
G
, or 4 ms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim8-2762002-large.gif
2018,8255636,Fig. 9.,Chip photograph and layout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim9-2762002-large.gif
2018,8255636,Fig. 10.,"(a) 50 images prepared for training; each image is represented in the form of 2D
16x16
pixels. (b) Measured results of recovered images from untrained initial random weights. (c) Measured results of recovered images from trained weights.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim10-2762002-large.gif
2018,8255636,Fig. 11.,Measured accuracy of recovered images as the number of trained images varies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim11-2762002-large.gif
2018,8255636,Fig. 12.,"(a) 10 face data [29]; each image is represented in the form of 2D
16x16
pixels used in training. (b) Measured results of recovered images from trained weights.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim12-2762002-large.gif
2018,8255636,Fig. 13.,Power consumption breakdown.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim13-2762002-large.gif
2018,8255636,Fig. 14.,"Simulated learning performance with (a) the original algorithm [21] when the numbers of hidden neurons are 250, 500 and 1000, (b) the proposed table-based learning for training images when the number of hidden neurons is 1000 and the synapse resolutions are set to no limit, 4b, 6b, 8b and 10b, and (c) the proposed table-based learning for testing images when the number of hidden neurons is 1000 and the synapse resolutions are set to no limit, 4b, 6b, 8b and 10b.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/8269836/8255636/sim14-2762002-large.gif
2018,8305661,Fig. 1.,"Trace
x
om
related to input spike at
t
m
jumps to a maximum value after the delay
ε
om
. Then it decays exponentially through time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher1-2797801-large.gif
2018,8305661,Fig. 2.,"Synaptic weight between
ith
input neuron and the
hth
excitatory hidden neuron
w
hi
is potentiated in proportion to the value of STDP time window [
Ψ(t−
t
i
)
] at
t=
t
^
f
o
to generate hidden spike at the desired time
t=
t
^
f
o
. The generated excitatory input will be fed to the
oth
output neuron, and it increases the total PSP of the neuron at the desired time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher2-2797801-large.gif
2018,8305661,Fig. 3.,"w
hi
, the synaptic weight between
ith
input neuron and the
hth
excitatory hidden neuron, is reduced in proportion to
Ψ(t−
t
i
)
, at
t=
t
f
o
(the time of the
fth
actual output spike of the
oth
output neuron). The reduction might lead to the cancelation of the hidden spike at
t
h
and consequently the reduction of the total PSP of the
oth
output neuron generated at
t=
t
f
o
and remove the actual output at
t=
t
f
o
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher3-2797801-large.gif
2018,8305661,Fig. 4.,Comparison of the learning method accuracy on the IRIS data training set when one and three readout neurons are used.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher4-2797801-large.gif
2018,8305661,Fig. 5.,Recognition accuracy for different numbers of desired spikes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher5-2797801-large.gif
2018,8305661,Fig. 6.,Runing time of a learning epoch is increased linearly as a function of (a) number of training patterns and (b) number of input synapses.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher6ab-2797801-large.gif
2018,8305661,Fig. 7.,Evolution of the accuracy of the proposed method over different learning epochs on BUPA liver disorders data. It needs 24 learning epochs to pass the accuracy level of 60%. SRESN [46] needs 715 epochs to reach about to the same level of accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8305661/taher7-2797801-large.gif
2018,8693729,Fig. 1.,3R3W bitcell schematic and layout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye1-2911885-large.gif
2018,8693729,Fig. 2.,Test-chip die micrograph and characteristics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye2-2911885-large.gif
2018,8693729,Fig. 3.,Cross-sectional circuit of the DP read.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye3-2911885-large.gif
2018,8693729,Fig. 4.,Decode global control circuit for the DP read and write.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye4-2911885-large.gif
2018,8693729,Fig. 5.,(a) RRC0. (b) RRC1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye5ab-2911885-large.gif
2018,8693729,Fig. 6.,Timing diagrams for the DP read and write.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye6-2911885-large.gif
2018,8693729,Fig. 7.,Cross-sectional circuit of the DP write.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye7-2911885-large.gif
2018,8693729,Fig. 8.,(a) WRC0. (b) WRC1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye8ab-2911885-large.gif
2018,8693729,Fig. 9.,"Measured maximum clock frequency (
F
MAX
) versus supply voltage (
V
DD
) shmoos for SA and DP RFs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye9-2911885-large.gif
2018,8693729,Fig. 10.,"Measured memory BW versus
V
DD
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye10-2911885-large.gif
2018,8693729,Fig. 11.,Simulated energy versus measured BW.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye11-2911885-large.gif
2018,8693729,Fig. 12.,Simulated BW versus number of physical bitcell read and write ports.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye12-2911885-large.gif
2018,8693729,Fig. 13.,"Simulated
F
MAX
versus number of RF read and write ports.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye13-2911885-large.gif
2018,8693729,Fig. 14.,Area for SA and DP RFs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye14-2911885-large.gif
2018,8693729,Fig. 15.,Comparison with prior work.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8714091/8693729/nguye15-2911885-large.gif
2018,7968265,Fig. 1.,Block diagram of the proposed ensemble classification system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8401715/7968265/korfi1-2723463-large.gif
2018,7968265,Fig. 2.,Performance of the compared classification systems on each label-set using (a) G-Mean and (b) AUC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8401715/7968265/korfi2-2723463-large.gif
2018,7968265,Fig. 3.,"Characteristics of the best overall classifier (RU+SMOTE)-MLP for 60% of broken pixels. (a) ROC, (b) Confusion Matrix and (c) Evaluation metrics’ values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8401715/7968265/korfi3-2723463-large.gif
2018,7968265,Fig. 4.,Frequency of selection (horizontal axis) for the available input features (vertical axis).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8401715/7968265/korfi4-2723463-large.gif
2018,8064210,Fig. 1.,Comparison of algorithms to handle imbalanced data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang1-2751612-large.gif
2018,8064210,Fig. 2.,"∘
: negative, ■: positive, and
∗
: synthetic. Comparison of minority instances generated in input and feature space. (a) Input space
x=[
x
1
, 
x
2
]
T
. (b) Feature space
x=[
x
2
1
, 
x
2
2
, 
2
–
√
x
1
x
2
]
T
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang2ab-2751612-large.gif
2018,8064210,Fig. 3.,"Hierarchical classifier framework for a
t
-class monotonically increasing data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang3-2751612-large.gif
2018,8064210,Fig. 4.,"∘
: negative,
□
: positive, and
∗
: synthetic. Effect of oversampling on Spiral data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang4-2751612-large.gif
2018,8064210,Fig. 5.,"∘
: negative,
□
: positive, and
∗
: synthetic. Effect of oversampling on CGrid data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang5-2751612-large.gif
2018,8064210,Fig. 6.,Critical difference obtained with the Bonferroni-Dunn test for baseline algorithms on benchmark data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang6-2751612-large.gif
2018,8064210,Fig. 7.,PD measurement of high-voltage equipment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang7-2751612-large.gif
2018,8064210,Fig. 8.,Comparison of performance of multiclass frameworks for WK-SMOTE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8440865/8064210/pang8-2751612-large.gif
2018,8194845,FIGURE 1.,Classification accuracy on four UCI data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8194845/liu1-2782884-large.gif
2018,8231213,Fig. 1.,"Illustration of the framework introduced in this study to perform prediction of HT in acute ischemic stroke. During training, the machine learning is presented with a set of voxel samples (X) made of DWI values, PWI time-series at that location, and arterial input function (AIF). Each sample is labeled (Y) with the presence of HT in follow-up GRE. After training, the model can be used to predict HT on previously unseen patients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8440576/8231213/scalz1-2783241-large.gif
2018,8231213,Fig. 2.,Illustration of the accuracy in terms of ROC and PR curves for various predictive models of HT based on source PWI. (a) ROC curves. (b) PR curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8440576/8231213/scalz2-2783241-large.gif
2018,8231213,Fig. 3.,"Prediction of spatial occurrence from baseline source PWI and DWI for various machine learning models (Linear, Decision trees, SVM, SR-KDA, Neural Networks). Red indicates higher likelihood of upcoming HT. The groundtruth is depicted on the rightmost column and corresponds to the GRE at 24-h follow-up where the extent of the hemorrhage is indicated by a red region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8440576/8231213/scalz3-2783241-large.gif
2018,8357388,Fig. 1.,A generic view of supervised machine learning in compilers. (a) Feature engineering. (b) Leaning a model. (c) Deployment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl1abc-2817118-large.gif
2018,8357388,Fig. 2.,"An OpenCL thread coarsening example reproduced from [17]. The original OpenCL code is shown in (a) where each thread takes the square of one element of the input array. When coarsened by a factor of two (b), each thread now processes two elements of the input array.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl2ab-2817118-large.gif
2018,8357388,Fig. 3.,"There are, in general, two approaches to determine the optimal compiler decision using machine learning. The first one is to learn a cost or priority function to be used as a proxy to select the best performing option (a). The second one is to learn a predictive model to directly predict the best option (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl3ab-2817118-large.gif
2018,8357388,Fig. 4.,A simple view of the GP approach presented in [34] for tuning compiler cost functions. Each candidate cost function is represented as an expression tree (a). The workflow of the GP algorithm is presented in (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl4ab-2817118-large.gif
2018,8357388,Fig. 5.,"A simple regression-based curve-fitting example. There are five training examples in this case. A function
f
is trained with the training data, which maps the input
x
to the output
y
. The trained function can predict the output of an unseen
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl5-2817118-large.gif
2018,8357388,Fig. 6.,A decision tree for determining which device (CPU or GPU) to use to run an OpenCL program. This diagram is reproduced from [68].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl6-2817118-large.gif
2018,8357388,Fig. 7.,"Random forests are an ensemble learning algorithm. It aggregates the outputs of multiple decision trees to form a final prediction. The idea is to combine the predictions from multiple individual models together to make a more robust, accurate prediction than any individual model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl7-2817118-large.gif
2018,8357388,Fig. 8.,"A simplified view of the internal state for the DeepTune DNN framework [78] when it predicts the optimal OpenCL thread coarsening factor. Here, a DNN is learned for each of the four target GPU architectures. The activations in each layer of the four models increasingly diverge (or specialize) toward the lower layers of the model. It is to note that some of the DeepTune layers are omitted to aid presentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl8abcd-2817118-large.gif
2018,8357388,Fig. 9.,"Using k-means to group data points into three clusters. In this example, we group the data points into three clusters on a 2-D feature space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl9-2817118-large.gif
2018,8357388,Fig. 10.,Using an EA to perform iterative compilation. The algorithm starts from several initial populations of randomly chosen compiler flag sequences. It evaluates the performance of individual sequences to remove poorly performing sequences in each population. It then applies crossover and mutation to create a new generation of populations. The algorithm returns the best performing program binary when it terminates.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl10-2817118-large.gif
2018,8357388,Fig. 11.,The working mechanism of reinforcement learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl11-2817118-large.gif
2018,8357388,Fig. 12.,Dynamic features can be extracted from multiple layers of the computing environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl12-2817118-large.gif
2018,8357388,Fig. 13.,"Standard feature-based modeling (a) versus reaction-based modeling (b). Both models try to predict the speedup for a given compiler transformation sequence. The program feature-based predictor takes in static program features extracted from the transformed program, while the reaction-based model takes in the target transformation sequence and the measured speedups of the target program, obtained by applying a number of carefully selected transformation sequences. Diagrams are reproduced from [131].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl13ab-2817118-large.gif
2018,8357388,Fig. 14.,"Using PCA to reduce dimensionality of a 3-D feature space. The principal components are first computed (a). Then, the first two principal components (
P
C
1
and
P
C
2
) are selected to represent the original 3-D feature space on a new 2-D space (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/8510998/8357388/oboyl14ab-2817118-large.gif
2018,8458406,FIGURE 1.,The pipeline of trust prediction models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458406/yan1-2869699-large.gif
2018,8458406,FIGURE 2.,The ROC curve of classifier1 and classifier 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458406/yan2-2869699-large.gif
2018,8458406,FIGURE 3.,The PR curve of classifier 3 and classifier 4.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8274985/8458406/yan3-2869699-large.gif
2018,8310962,Fig. 1.,"Overview of h-DDPG architecture. The trapeziums represent fully connected layers. These fully connected layers are layers from meta critic, actors, and basic critic, respectively, from top to bottom. The square-dotted lines are connections with back propagation while the dashed lines are not.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang1-2805379-large.gif
2018,8310962,Fig. 2.,Comparison of parameters needed with or without mlpconv layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang2-2805379-large.gif
2018,8310962,Fig. 3.,"Introduction of the three simulation scenarios. They are, from top to bottom, the approaching object scenario (Scenario 1), the approaching specific target scenario (Scenario 2), and the doorway scenario (Scenario 3). Pictures from left to right in each row are top-down views of scenarios, image captures of the camera and the samples of solutions made by the agent. In the samples of solution, the yellow triangles indicate initial orientation, and stars indicate target objects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang3-2805379-large.gif
2018,8310962,Fig. 4.,Performance of each actor in the three scenarios. The pictures at the bottom right of each curve are samples of moving trajectories of the actor. The yellow triangles indicate the initial orientation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang4-2805379-large.gif
2018,8310962,Fig. 5.,Performance of actors in scenario 1 when training with an incompetent actor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang5-2805379-large.gif
2018,8310962,Fig. 6.,Performance comparison at 5000 episode between training with different meta critic reward values in scenario 1,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang6-2805379-large.gif
2018,8310962,Fig. 7.,"Performance of the meta critic. In each curve, the bars are success rate of finishing the task. The curves are average rewards in each test episode, and the shadows indicate the standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8495104/8310962/yang7-2805379-large.gif
2018,8017580,Fig. 1.,"A projection of topological similarities between 8,263 graphs measured by our RW-Log-Laplacian kernel. Based on the topological similarity, our approach shows what a graph would look like in different layouts and estimates their corresponding aesthetic metrics. We clustered the graphs based on their topological similarities for the purpose of the user study. The two graphs in each pair are the most topologically similar, but not isomorphic, to each other. The projection is computed with t-SNE [83], and the highlighted graphs are visualized with FM3 layout [37]. An interactive plot is available in the supplementary materials [1].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8165924/8017580/24tvcg01-kwon-2743858-fig-1-source-large.gif
2018,8017580,Fig. 2.,"All connected graphlets of 3, 4, or 5 vertices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8165924/8017580/24tvcg01-kwon-2743858-fig-2-source-large.gif
2018,8017580,Fig. 3.,"Examples of graphlet frequencies. The x-axis represents connected graphlets of size
k∈{3,4,5}
and the y-axis represents the weighted frequency of each graphlet. Four graphs are drawn with sfdp layouts [45]. If two graphs have similar graphlet frequencies, i.e., high topological similarity, they tend to have similar layout results (a and c). If not, the layout results look different (a and b). However, in rare instances, two graphs can have similar graphlet frequencies (b and d), but vary in graph size, which might lead to different looking layouts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8165924/8017580/24tvcg01-kwon-2743858-fig-3-source-large.gif
2018,8017580,Fig. 4.,"Computation time results in log scale. The plots show estimation times for our RW-Log-Laplican kernel, which has the highest accuracy. The plot on the left shows layout computation time while the plot on the right shows aesthetic metric computation time. As the number of vertices increases, the gap between our estimation and layout methods enlarges in both layout time and aesthetic metric computation time. Some layout methods could not be run on all graphs due to computation time and memory limitation of the implementation. Treemap and Gosper overlap in the layout plot because the majority of the computation time is spent on hierarchical clustering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8165924/8017580/24tvcg01-kwon-2743858-fig-4-source-large.gif
2018,8017580,Fig. 5.,"A task from the user study. For each task, the participants were given one target graph and nine choice graphs. They were then asked to rank the three most similar graphs in order of decreasing similarity. The three images on the right show the central nodes of (a), (b), and (c).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8165924/8017580/24tvcg01-kwon-2743858-fig-5-source-large.gif
2018,8017580,Fig. 6.,"Summary of the user study results. (a) Response rate of perceptual similarity rank
(rp)
for each topological similarity rank
(
r
T
)
. The plot in the middle (b) Shows the response rate on topological similarity rank of 1 per layout method while the plot on the right (c) Shows per target graph.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/8165924/8017580/24tvcg01-kwon-2743858-fig-6-source-large.gif
2018,8060572,Fig. 1.,"Considered DC configuration: location of servers, cooling system, and multilayer network topology.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle1-2760517-large.gif
2018,8060572,Fig. 2.,Overall diagram of the proposed scenario.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle2-2760517-large.gif
2018,8060572,Fig. 3.,Time slot and sample description.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle3-2760517-large.gif
2018,8060572,Fig. 4.,Overall process of our ML approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle4-2760517-large.gif
2018,8060572,Fig. 5.,Energy consumed by the DC for one day.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle5-2760517-large.gif
2018,8060572,Fig. 6.,"Average, worst-case percentage amount, and total number of violations for one day.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle6-2760517-large.gif
2018,8060572,Fig. 7.,Total amount of data exchanged among the servers for one day.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle7-2760517-large.gif
2018,8060572,Fig. 8.,Energy consumed by DC for one week.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle8-2760517-large.gif
2018,8060572,Fig. 9.,"Average, worst-case percentage amount, and total number of violations for one week.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle9-2760517-large.gif
2018,8060572,Fig. 10.,"Network traffic of (a) ToR, (b) aggregation-layer switches, and (c) core router for one week.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/8411796/8060572/pahle10abc-2760517-large.gif
2018,8262668,Fig. 1.,An illustration of Riemannian manifold and tangent space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu1-2794415-large.gif
2018,8262668,Fig. 2.,"The graphical illustration of the ELM-TS-RE algorithm. The overall ensemble includes a graph-based approach for dimensionality reduction, tangent space (TS) mapping for feature extraction, and extreme learning machine (ELM) for classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu2-2794415-large.gif
2018,8262668,Fig. 3.,Timing scheme of the paradigm for motor imagery task. a) dataset II of competition IV; b) in-house datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu3ab-2794415-large.gif
2018,8262668,Fig. 4.,"The distribution of two most discriminative features learned by Isomap, LLE, CSP, BLP and BRLP for the left/right hand motor imagery data of A01.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu4-2794415-large.gif
2018,8262668,Fig. 5.,"The
r
2
-values of the two most discriminative features learned by Isomap, LLE, CSP, BLP and BRLP, corresponding to Fig. 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu5-2794415-large.gif
2018,8262668,Fig. 6.,The distribution of two most discriminative features learned with and without test data by BRLP algorithm. a) BRLP with test data (full training data); b) BRLP without test data (full training data); c) BRLP with test data (half training data); d) BRLP without test data (half training data).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu6abcd-2794415-large.gif
2018,8262668,Fig. 7.,"Topographic maps of the spatial filters learned by BRLP, BLP and CSP methods for the left/right hand motor imagery data from the 9 subjects of dataset IIa of BCI competition IV.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu7-2794415-large.gif
2018,8262668,Fig. 8.,Comparison of classification performance of all studied algorithms on the motor imagery dataset via 10-fold cross-validation. a) dataset IIa of BCI competition IV; b) in-house datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu8ab-2794415-large.gif
2018,8262668,Fig. 9.,The graphical illustration of filter bank ELM-TS-RE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu9-2794415-large.gif
2018,8262668,Fig. 10.,Convergence curves of mapping matrix obtained by executing the BRLP on dataset IIa of BCI competition IV within 40 iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu10-2794415-large.gif
2018,8262668,Fig. 11.,"The
r
2
-values of the most discriminative features learned by BRLP under the different
λ
and
N
s
for subject S03.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu11-2794415-large.gif
2018,8262668,Fig. 12.,"The effect of
N
s
on the channel weight learned from the left/right hand motor imagery data of subject S03. a) the wave of channel weight under different
N
s
; b) visualization of channel weight.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu12ab-2794415-large.gif
2018,8262668,Fig. 13.,"Classification accuracy of ELM-TS-RE versus number of neuron (
N
h
) under the different
θ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8307208/8262668/yu13-2794415-large.gif
2018,8301555,Fig. 1.,Results on STL-10. (a) Approximation error; (b) Classification accuracy drop; (c) Parameters reduction; (d) Speedup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8402070/8301555/yang1-2808910-large.gif
2018,8301555,Fig. 2.,Prediction results of the virtual machine with the highest workload.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8402070/8301555/yang2-2808910-large.gif
2018,8118310,Fig. 1.,"Sample images of (a) breast cancer, (b) muscle, and (c) pancreatic neuroendocrine tumor (NET). Haemotoxylin and eosin (H&E) staining is used for the first two, while immunohistochemical (IHC) staining is used for the last. These images exhibit significant challenges for automated nucleus/cell detection and segmentation, including but not limited to background clutter, touching nuclei, and weak nucleus/cell boundaries.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing1abc-2766168-large.gif
2018,8118310,Fig. 2.,"Applications of deep learning in microscopy image analysis in terms of (a) network architectures, (b) tasks, (c) microscope types, and (d) image data (organs/specimens). For image data, the number of papers is listed in the Supplementary Material if it is not greater than 2. CNN, FCN, RNN and SAE represent convolutional neural network, fully convolutional network, recurrent neural network, and stacked autoencoder, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing2abcd-2766168-large.gif
2018,8118310,Fig. 3.,"Architecture of the deep voting model [43]. The C, M, and F represent the convolutional layer, the max-pooling layer, and the fully connected layer, respectively. In the last layer, the voting offset and confidence units are marked with different colors. In this case, the number of voting positions for each local patch
k
is 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing3-2766168-large.gif
2018,8118310,Fig. 4.,Nucleus detection using deep voting [43] on several example NET images. Yellow dots: detected cell centers. Small green circles: gold standards.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing4-2766168-large.gif
2018,8118310,Fig. 5.,Overview of the nuclei segmentation based on a denoising autoencoder in [74]. (a) Denoising autoencoder trained with structural labels. (b) Gradient map. (c) Reconstructed cell boundary. (d) Segmentation result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing5abcd-2766168-large.gif
2018,8118310,Fig. 6.,SDAE in [74].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing6-2766168-large.gif
2018,8118310,Fig. 7.,Results on sample images [74]. (a) Original image patches. (b) Gradient maps. (c) Outputs of the SDAE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8468244/8118310/xing7abc-2766168-large.gif
2018,8240634,Fig. 1.,Simulation area layout with transmitters’ coverage and room numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8239616/8240634/ghora1-2787651-large.gif
2018,8240634,Fig. 2.,"Localization performance with the same 434 primary training data and adding extra training data for ADELM, Horus, k-NN with k = 7, and ELM and DELM based localization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8239616/8240634/ghora2-2787651-large.gif
2019,8304750,Fig. 1.,"Accounting for uncertainty in a classification label using a clinical expert's confidence in the diagnosis of ARDS. Critical care trained clinicians were asked to independently review patients’ EHR data and determine if any individuals in the cohort had ARDS, while also rating their confidence of the diagnosis using the following scale: equivocal, slight, moderate, or high.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama1-2810820-large.gif
2019,8304750,Fig. 2.,"Effects of different sampling thresholds on prediction generalizability with SVM. With our sampling strategy, SVM performs very well on the training data at any threshold. We indicate the loss in training accuracy when the same model makes a prediction on a hold-out testing set to properly assess the effects of changing the sampling threshold and empirically determine the value for optimal results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama2-2810820-large.gif
2019,8304750,Fig. 3.,Effects of different sampling thresholds on prediction generalizability with SVM and label uncertainty. We confirm that the sampling strategy and threshold effects observed in Fig. 2 is maintained when the SVM model is formulated to account for label uncertainty.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama3-2810820-large.gif
2019,8304750,Fig. 4.,Flowchart of this study's protocol with 5-fold cross-validation and hyper-parameter optimization using grid search. All samples from the same patient are kept exclusively in either the training or testing set. Hyper-parameter optimization was implemented for separately each model (with and without label uncertainty weight) to give an accurate assessment of performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama4-2810820-large.gif
2019,8304750,Fig. 5.,Average decay of correlation from all patients. Error bars represent standard error of the mean and each point represents correlation in relation to time (hours) from the initial observation sampled on each patient.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama5-2810820-large.gif
2019,8304750,Fig. 6.,Average decay of correlation from all patients during (a) negative diagnosis of ARDS and (b) positive diagnosis of ARDS. Error bars represent standard error of the mean and each point represents correlation in relation to time (hours) from the initial observation sampled on each patient.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama6-2810820-large.gif
2019,8304750,Fig. 7.,ROC curve comparing SVM with and without label uncertainty. Performance metrics are reported in Table I.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8304750/reama7-2810820-large.gif
2019,8624570,Fig. 1.,"A block diagram to represent different schemes, methods and experimental case studies presented in this paper. We develop both supervised and unsupervised learning algorithms to characterize tumors. For the supervised learning scheme, we propose a new 3D CNN architecture based on a Graph Regularized Sparse Multi-task learning and perform evaluations for lung nodule characterization from CT scans. For unsupervised learning scheme, we propose a new clustering algorithm,
∝
SVM, and test it for the categorization of lung nodules from CT scans and pancreatic cysts (IPMN) from MRI scans.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8782670/8624570/husse1-2894349-large.gif
2019,8624570,Fig. 2.,"(A) A visualization of lung nodules having different levels of attributes. On moving from the top (attribute absent) to the bottom (attribute prominently visible), the prominence level of the attribute increase. Different attributes including calcification, sphericity, margin, lobulation, spiculation and texture can be seen in (a-f). The graph in (g) depicts the number of nodules with different malignancy levels in our experiments using the publicly available dataset [32]. An overview of the proposed 3D CNN based graph regularized sparse MTL approach is presented in (B).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8782670/8624570/husse2abcdefg-2894349-large.gif
2019,8624570,Fig. 3.,"An outline of the proposed unsupervised approach. Given the input images, we compute GIST features and perform
k
-means clustering to get the initial set of labels which can be noisy. Using the set of labels, we compute label proportions corresponding to each cluster/group (Eq. (9)). We finally employ
∝
SVM to learn a discriminative model using the features and label proportions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8782670/8624570/husse3-2894349-large.gif
2019,8624570,Fig. 4.,"Axial T2 MRI scans illustrating pancreas. The top row shows different ROIs of pancreas, along with a magnified view of a normal pancreas (outlined in blue). The bottom row shows ROIs from subjects with IPMN in the pancreas, which is outlined in red.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8782670/8624570/husse4-2894349-large.gif
2019,8624570,Fig. 5.,"Influence of deep learning features obtained from different layers of a VGG network with and without ReLU non-linearities. The graph on the left shows accuracy, sensitivity and specificity for unsupervised lung nodule classification (clustering), whereas the right one shows the corresponding results for IPMN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/8782670/8624570/husse5-2894349-large.gif
2019,7551240,Fig. 1.,"A preprocessing step of energy minimization improves QA performance of the MESHI score method. 10x10 cross-validation experiments (CASP8, CASP9, and CASP10).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8863041/7551240/crive1-2602269-large.gif
2019,7551240,Fig. 2.,"Average of MRMSE for each combination of gamma and cost, and for the training and validation sets. In the legend, meval and metrain represent the average median error of validation and training sets, respectively, with an associated cost.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8863041/7551240/crive2-2602269-large.gif
2019,7551240,Fig. 3.,"Histogram of MRMSE when 19 random folds are used to predict CASP10 with SVM-e using 59 features and parameters
γ=0.01,c=3
, and
ϵ=0.1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8863041/7551240/crive3-2602269-large.gif
2019,7551240,Fig. 4.,"Ensemble Learning improves QA performance. 10x10 cross-validation experiments (all data set) indicate that per-target error (A) and correlation (B) medians improve with larger ensembles of predictors. It is apparent that most in the performance improvement is achieved even with an ensemble of as few as ten predictors. Yet increasing the ensemble size has a negligible effect on the overall runtime, which is dominated by the energy minimization process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8863041/7551240/crive4-2602269-large.gif
2019,7551240,Fig. 5.,"The sensitivity region of a representative predictor. True quality is the GDT_TS score of the minimized decoys. Learned quality (S in eq. 3) is a monotonic non-linear transformation of the true quality. The learning process optimizes the predictor to predict the ""learned"" qualities. Then, when the predictor is applied to a decoy, the ""learned"" quality is back-transformed to the normal GDT_TS range. The weight of each prediction is the curve slope at that point. Thus, in this example, predictions outside the 0.6 to 0.8 range are weighted very low in both learning and prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/8863041/7551240/crive5-2602269-large.gif
2019,8502863,Fig. 1.,"An example of averaged MCG signal with selected TT-interval for a healthy subject. The black dash line indicates mean global field power (MGFP) across 36 channels. Three fiducial markers: T-wave onset, T-wave end, and T-wave peak are defined on MGFP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang1-2877649-large.gif
2019,8502863,Fig. 2.,"An overview of the IHD detection system, featuring a stratified 10-fold cross validation and ensemble learning of three machine learning classifiers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang2-2877649-large.gif
2019,8502863,Fig. 3.,"Shannon entropy of 347 control subjects (blue dotted lines) and 227 IHD subjects (red dotted lines) across 36 channels. The mean Shannon entropy of IHD group (yellow square marker) is higher than that for control group (green circular marker), indicating that the signal system of IHD group is more complicated and unpredictable.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang3-2877649-large.gif
2019,8502863,Fig. 4.,Comparison of a health subject (left) and an ischemic subject (right) with LAD blockage. (a) and (b) T-wave morphology. (c) and (d) Gini index during TT-interval. (e) and (f) Singular values during TT-interval.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang4-2877649-large.gif
2019,8502863,Fig. 5.,Receiver operating characteristics curve of mixed machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang5-2877649-large.gif
2019,8502863,Fig. 6.,Precision recall curve of mixed machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang6-2877649-large.gif
2019,8502863,Fig. 7.,Top ten features of IHD detection. Red shaded blocks represent positive magnetic pole related features; blue shaded blocks represent negative magnetic pole related features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang7-2877649-large.gif
2019,8502863,Fig. 8.,"Magnetic field contour maps of TT-interval for (a) one post-surgery and (b) one ischemic subject. For each subject, the whole TT-interval was segmented in to 24 shorter time windows and 18 time domain features were extracted for each sub-window. Time domain feature characteristics were less stable for IHD subject, compared to that for control subject. Center red arrow: maximum cardiac current. Black straight line: distance between positive (yellow-green region) and negative magnetic pole (red-blue region). Blue circle: pole area, defined as the area with magnetic field strength no smaller than 4/5 times of the corresponding pole strength. Small black arrows: cardiac current density maps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang8-2877649-large.gif
2019,8502863,Fig. 9.,Top 5 representative features for the prediction of stenosis on three arteries. Red shaded blocks represent positive magnetic pole related features; blue shaded blocks represent negative magnetic pole related features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang9-2877649-large.gif
2019,8502863,Fig. 10.,"Illustration three coronary arteries (LCX, LAD, RCA) and their blood supply regions. The whole system is projected onto a magnetic field map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8718429/8502863/zhang10-2877649-large.gif
2019,8809747,Fig. 1.,(a) Inkjet-printed chipless RFID tags. (b) Details showing the two T-shaped resonators encoding 2 bits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong1-2937055-large.gif
2019,8809747,Fig. 2.,"Wire measurement
S
21
results for the tags “00,”“01,” “10,” and “11.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong2-2937055-large.gif
2019,8809747,Fig. 3.,Illustration of the measurement setup of the chipless RFID system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong3-2937055-large.gif
2019,8809747,Fig. 4.,(a) Measurement setup. (b) Measurement setup from the reader side.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong4-2937055-large.gif
2019,8809747,Fig. 5.,Basic concept of the SVM classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong5-2937055-large.gif
2019,8809747,Fig. 6.,Proposed SVM tag ID detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong6-2937055-large.gif
2019,8809747,Fig. 7.,Confusion matrix for the SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong7-2937055-large.gif
2019,8809747,Fig. 8.,Wire measured and time-gated signal for tag “10” at a distance of (a) 10 cm and (b) 30 cm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8809747/jeong8-2937055-large.gif
2019,8514005,Fig. 1.,"Block diagram of the proposed method for fQRS detection and quantification. After preprocessing and QRS segmentation using VMD, features were extracted using PRSA and VMD after which a classifier was trained and evaluated. The final fQRS score represents the certainty of QRS fragmentation in an ECG lead.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova1-2878492-large.gif
2019,8514005,Fig. 2.,"Example of ECG signal with the corresponding modes of the output of Variational Mode Decomposition with k = 5 and
α
= 100.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova2-2878492-large.gif
2019,8514005,Fig. 3.,"Illustration of the three main steps for QRS segmentation, applied to the signal shown in Fig. 2. First the R peaks are detected in
u
2
3,norm
. Then the QRS complex is segmented in
u
3,norm
. Finally the QRS locations are optimized in the original ECG signal. The R peaks are depicted in blue, the beginning and end of the QRS complex in respectively red and green.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova3-2878492-large.gif
2019,8514005,Fig. 4.,"Illustration of the three steps to create the PRSA curve for a normal heartbeat (top) and a heartbeat with fragmentation (bottom). In the middle panel, two increasing and two decreasing anchor points are highlighted in respectively red, cyan and green, blue. Their corresponding windows are also indicated in the right panel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova4-2878492-large.gif
2019,8514005,Fig. 5.,Box plots showing all feature values grouped by the total score given by 5 experts. The box plots represent the median value in a group together with the interquartile range. Outliers are shown in red.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova5-2878492-large.gif
2019,8514005,Fig. 6.,ROC curves for fQRS detection with all classifiers together with the corresponding AUC values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova6-2878492-large.gif
2019,8514005,Fig. 7.,"Illustrating the fQRS quantification score for the second group (Section III-D) using SVM (Linear, Polynomial and RBF kernels), KNN, NB, and TB classifiers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8824156/8514005/goova7-2878492-large.gif
2019,8673876,Fig. 1.,"An illustration of an 8 electrode grid before (transparent) and after (opaque) an electrode shift. The arrows indicate the direction of electrode shift and the virtual correction, respectively. The cross section of the arm is adapted from the 1921 German edition of “Anatomie des Menschen”, which is in the public domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm1-2907200-large.gif
2019,8673876,Fig. 2.,"Overview over the considered transfer learning setting, from left to right. We first train a pattern recognition model on data recorded under lab conditions, i.e. from the source domain (original). The colors of the points indicate their according class. Then, the incoming data is disturbed via an electrode shift such that the model may not be appropriate (disturbed). To estimate the disturbance, we record a few new data instances from a few of the classes in the disturbed condition, i.e. in the target domain (record new data). Grey points indicate possible positions of future data. Finally, the transfer learning step utilizes the gathered data to learn a transformation of disturbed data, such that the original classification model is applicable again (transfer learning).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm2ab-2907200-large.gif
2019,8673876,Fig. 3.,"An illustration of our proposed scheme to estimate the undisturbed features of the jth electrode
x
t, j
from the disturbed features
x
^
t,j−1
and
x
^
t, j
after an electrode shift by c
∈
[0, 1] electrodes. The x-axis displays the angle around the forearm, the y-axis the feature amplitude. Note that we assume that the feature can be linearly interpolated between the electrodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm3-2907200-large.gif
2019,8673876,Fig. 4.,"An example for the selection procedure for the best value of c. The X-axis denotes different values of c where a positive sign indicates a lateral, and a negative sign indicates a medial shift. The y-Axis displays the value of the cost function (3) for both degrees of freedom (wrist rotation in blue and hand opening/closing in orange), as well as the mean of both (gray). The gray, dashed, vertical line indicates the c with minimum error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm4-2907200-large.gif
2019,8673876,Fig. 5.,"The different grasp-forms the virtual grasper can perform: open, close, rotate left, rotate right and the simultaneous combinations of those.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm5-2907200-large.gif
2019,8673876,Fig. 6.,Layout of the Box and Beans test: The beans are to be transferred from the start compartment to the finish compartment. They have to be passed over the wall between both compartments and fall to or be placed on the ground to successfully score a point. The score and remaining time can be seen at the top left of the screen.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm6-2907200-large.gif
2019,8673876,Fig. 7.,Study set-up: The patient is wearing the Myo armband around his forearm with the elbow resting on the table and facing the computer screen.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm7-2907200-large.gif
2019,8673876,Fig. 8.,"The average response for the questionnaire on a 7-point Likert scale after using the original pattern recognition system (condition A, grey bar), after using the system under electrode shift (condition B, blue dotted bar), after using the system after transfer learning (condition C, orange striped bar). Error bars indicate standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm8-2907200-large.gif
2019,8673876,Fig. 9.,"The median number of transferred beans in 60s across nine trials. The result for each patient and participant are shown via three bars, one for the performance using the original pattern recognition system (condition A, gray), one for the performance after electrode shift (condition B, blue, dotted), and one for the performance after transfer learning (condition C, orange, striped). Error bars indicate the lower and upper quartile. Significant differences between conditions B and C are indicated with brackets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8709919/8673876/prahm9-2907200-large.gif
2019,8706527,Fig. 1.,Using hybrid RF features in UMAP algorithm allowed different environments to be classified into distinctive clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8706527/shuba1-2915047-large.gif
2019,8706527,Fig. 2.,"Confusion matrix using
k
-NN algorithm (
k
= 1) based on hybrid RF features CTF + FCF.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8706527/shuba2-2915047-large.gif
2019,8706527,Fig. 3.,"Comparison of RMSE using
k
-NN algorithm (
k
= 1) based on various primary and hybrid RF features for different environments. (a) Lab (highly cluttered). (b) Narrow corridor (medium cluttered). (c) Lobby (low cluttered). (d) Sports hall (open space).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8706527/shuba3-2915047-large.gif
2019,8601343,Fig. 1.,"An example of trunk sway recorded during an exercise trial (feet apart stance on foam surface with eyes closed). In this particular example, the participant demonstrates more sway (i.e., variation) in the pitch direction relative to the roll direction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8637978/8601343/wiens1-2891000-large.gif
2019,8601343,Fig. 2.,Distribution of rating levels provided by the physical therapist and participants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8637978/8601343/wiens2-2891000-large.gif
2019,8601343,Fig. 3.,Sixty-one features were calculated to summarize performance across each set of six trials based on ten commonly used kinematic metrics and six statistical descriptors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8637978/8601343/wiens3-2891000-large.gif
2019,8601343,Fig. 4.,"Validation and evaluation of the classifier using leave-one-participant-out cross validation. For visualization purposes, only the first cross-validation is shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8637978/8601343/wiens4-2891000-large.gif
2019,8453000,Fig. 1.,M2M communication networks with H2H UEs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li1-2867937-large.gif
2019,8453000,Fig. 2.,"Block diagram of the process of RA in time slot
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li2-2867937-large.gif
2019,8453000,Fig. 3.,"Number of newly activated MTC devices and the number of H2H UEs accessing to the BS during the activation time for
N=10000,R=64
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li3-2867937-large.gif
2019,8453000,Fig. 4.,"Comparisons of the actual values and results of the LA-based estimation method in different scenarios (
R=32,48,64
) with
N=2000
,
L=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li4-2867937-large.gif
2019,8453000,Fig. 5.,"ACB factors obtained with full information of
N
i
, obtained with full information of
Λ
i
, and obtained by the LA-ACB scheme versus time slot in different scenarios (
R=32,48,64
) with
N=2000
,
L=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li5-2867937-large.gif
2019,8453000,Fig. 6.,"Access success probabilities of ACB
N
i
with full information of
N
i
, ACB
Λ
i
with full information of
Λ
i
and LA-ACB versus
N
in different scenarios (
R=32,48,64
) with
L=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li6-2867937-large.gif
2019,8453000,Fig. 7.,"Service time required for serving all MTC devices of ACB
N
i
, ACB
Λ
i
, D-ACB, and LA-ACB versus
N
in different scenarios (
R=32,48,64
) with
L=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8453000/li7-2867937-large.gif
2019,8443399,Fig. 1.,Flowchart of the pool-based active learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu1-2855446-large.gif
2019,8443399,Fig. 2.,"First row: the original distribution of the first four data sets. Second row: classification hyperplane produced by ELM (
L=100
,
C=
2
10
) on 10% randomly extracted initial labeled instances. Third row: classification hyperplane produced by WELM (
L=100
,
C=
2
10
) on 10% randomly extracted initial labeled instances. Each column denotes one data set (
D1
–
D4
). □ and ■ represent the unlabeled and labeled instances of the majority class, while ○ and ● denote the unlabeled and labeled instances belonging to the minority class, respectively (from left to right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu2-2855446-large.gif
2019,8443399,Fig. 3.,"Original distribution, classification hyperplane produced by ELM (
L=100,C=
2
10
) and WELM (
L=100,C=
2
10
) on 10% randomly extracted initial labeled instances of
D5
data set, respectively. □ and ■ represent the unlabeled and labeled instances of the majority class, while ○ and ● denote the unlabeled and labeled instances belonging to the minority class, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu3-2855446-large.gif
2019,8443399,Fig. 4.,"Distribution of the unlabeled set, the classification hyperplane produced by ELM (
L=100,C=
2
10
) on 10% randomly extracted labeled instances, and the classification hyperplane produced by ELM (
L=100,C=
2
10
) on 10% representative initially labeled instances on
D6
data set, respectively. □ and ■ represent the unlabeled and labeled instances of the majority class, while ○ and ● denote the unlabeled and labeled instances belonging to the minority class, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu4-2855446-large.gif
2019,8443399,Fig. 5.,"Classification hyperplane (black line), contour lines (gray lines) directed by four different thresholds (0.1, 0.3, 0.5, and 1.0), and their margin ranges produced by WELM (
L=100,C=
2
10
) on 10% randomly extracted labeled instances on D5 data set. □ and ■ represent the unlabeled and labeled instances of the majority class, while ○ and ● denote the unlabeled and labeled instances belonging to the minority class, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu5-2855446-large.gif
2019,8443399,Fig. 6.,"Learning curves of seven compared active learning algorithms on three representative data sets: yeast-ME3, abalone19, and segmentation-grass, respectively (from left to right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu6-2855446-large.gif
2019,8443399,Fig. 7.,"Comparison of seven compared algorithms on the number of labeled instances belonging to the minority class in synchronization with the increase of labeling rounds on three representative data sets: cardiotocographyc1, seed2, and abalone19, respectively (from left to right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu7-2855446-large.gif
2019,8443399,Fig. 8.,"Difference between the performance of each stopping point (denoting by a specific threshold
μ
) and the best performance on three representative data sets: seed2, cardiotocographyC1, and Box H/ACA snoRNA, respectively (from left to right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8668600/8443399/yu8-2855446-large.gif
2019,8618409,Fig. 1.,Approximated probability (21) in solid line versus the actual probability in dashed line.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8844334/8618409/sum1-2889072-large.gif
2019,8618409,Fig. 2.,"State distributions. Left bars: probabilities obtained by (22). Middle bars: results obtained by sampling the BM with weight/bias noise added. Right bars: results obtained by sampling the BM without weight/bias noise but the temperature is changed to
ξT
. Here,
T=1
. (a)
S
W
=
S
B
=0
. (b)
S
W
=
S
B
=0.01
. (c)
S
W
=
S
B
=0.25
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8844334/8618409/sum2abc-2889072-large.gif
2019,8572804,Fig. 1.,Significant types of Dementia [4]–[6].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang1-2886237-large.gif
2019,8572804,Fig. 2.,Biomedical imaging modalities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang2-2886237-large.gif
2019,8572804,Fig. 3.,"MRI scans of healthy control (a-b) and AD (c-d), [22].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang3-2886237-large.gif
2019,8572804,Fig. 4.,"(a) Various PET scans of a patient with Alzheimer's disease at the age of 77: A) right hippocampus, B) FDG-PET, and C) PiB accumulation [31]. (b) Single-subject FDG-PET maps for the two bvFTD variants.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang4-2886237-large.gif
2019,8572804,Fig. 5.,Group analysis of gray matter accumulation between the DLB and normal controls using SPECT scans [50].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang5-2886237-large.gif
2019,8572804,Fig. 6.,The fundamental step by step computer-aided diagnosis of dementia using machine-learning approaches [52]–[55].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang6-2886237-large.gif
2019,8572804,Fig. 7.,"Deep learning CNN based hierarchical feature learning and classification to prognosis dementia using Softmax classifier [74], [78].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8572804/zhang7-2886237-large.gif
2019,8908728,Fig. 1.,Different methods for low-power front-end design: (a) conventional CS with reconstruction for inference. (b) on-sensor-analysis only transmits abnormal data after simple analytics. (c) Proposed on-CS-sensor-analysis by robust and lightweight inference in compressed domain without reconstruction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu1abc-2940642-large.gif
2019,8908728,Fig. 2.,"Two parts are in the proposed robust and lightweight CL engine design, including (a) sub-eigenspace transformation from
x
^
to
s
and (b) hardware-friendly design for ensemble of ELM models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu2ab-2940642-large.gif
2019,8908728,Fig. 3.,"Proposed E-ELM framework, including (a) PCA-based dictionary learning and construction of the eigenspace transformation matrix in the off-line training stage and (b) utilizing the transformation matrix in the on-line inference stage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu3ab-2940642-large.gif
2019,8908728,Fig. 4.,"Proposed SE-ELM framework, including (a) subspace-based dictionaries division and construction of the sub-eigenspace transformation matrix in the off-line training stage and (b) utilizing the transformation matrix in the on-line inference stage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu4ab-2940642-large.gif
2019,8908728,Fig. 5.,The illustration of signal representation using the proposed subspace-based dictionaries.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu5-2940642-large.gif
2019,8908728,Fig. 6.,"Comparison of the inference accuracy over the increasing number of ELM classifiers between SVM, E-ELM, and ELM models with different numbers of hidden neurons (
L
) in the ideal case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu6-2940642-large.gif
2019,8908728,Fig. 7.,"Comparison of the inference accuracy over the increasing number of ELM classifiers between SVM, E-ELM, PR-ELM, and ELM models with 400 hidden neurons in the white noise case when (a) SNR is 5, (b) SNR is 10, (c) SNR is 15, and (d) SNR is 20.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu7abcd-2940642-large.gif
2019,8908728,Fig. 8.,"Comparison of the inference accuracy over the increasing number of ELM classifiers between DDHR-SVM, SVM, SE-ELM, E-ELM, PR-ELM, and ELM models with 400 hidden neurons in the worst case when (a) SNR is 5, (b) SNR is 10, (c) SNR is 15, and (d) SNR is 20.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu8abcd-2940642-large.gif
2019,8908728,Fig. 9.,Hardware-friendly design for ensemble of ELM by updating the instance weights of each training data with AdaBoost algorithm and generating different input weights for each ELM model with dedicated design of LFSR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu9-2940642-large.gif
2019,8908728,Fig. 10.,"Proposed hardware architecture design for the ensemble of SE-ELM, including (a) hardware sharing for eigenspace transformation and ELM output weighting multiplication, (b) ELM input weighting multiplication (EIW), (c) ensemble output unit, and (d) data control unit (DCU).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu10abcd-2940642-large.gif
2019,8908728,Fig. 11.,Execution flow of the proposed ensemble of SE-ELM engine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu11-2940642-large.gif
2019,8908728,Fig. 12.,Inference accuracy of the proposed ensemble of SE-ELM in the worst case with different fixed-point precisions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu12-2940642-large.gif
2019,8908728,Fig. 13.,Layout of the proposed ensemble of SE-ELM engine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8922968/8908728/wu13-2940642-large.gif
2019,8412086,Fig. 1.,Training a denoising autoencoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8412086/zhou1-2856820-large.gif
2019,8412086,Fig. 2.,Training process in SDAE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8412086/zhou2-2856820-large.gif
2019,8412086,Fig. 3.,DFL feature learning using stacked denoising autoencoders.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8412086/zhou3-2856820-large.gif
2019,8412086,Fig. 4.,Overview of the DFL framework including the various data processing blocks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8412086/zhou4-2856820-large.gif
2019,8412086,Fig. 5.,"Workflow of the various feature selection experiments. MFS/RFS stands for manual/raw feature selection. PCA stands for principal component analysis. USDAE/OSDAE stands for uniform/optimized stacked denoising autoencoder. SVM and ANN are the support vector machine and artificial neural network classifiers, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8412086/zhou5-2856820-large.gif
2019,8735863,FIGURE 1.,Distribution of the two lines and two moons datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8735863/ma1ab-2922385-large.gif
2019,8735863,FIGURE 2.,"Learning results of our method on two lines and two moon datasets with different values of parameters
λ
1
and
λ
2
, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8735863/ma2ab-2922385-large.gif
2019,8735863,FIGURE 3.,"Performance comparison of ACC of ELM, SS-ELM, Safe-SSELM and our method on UCI datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8735863/ma3-2922385-large.gif
2019,8735863,FIGURE 4.,"(a) is the classification performance of ELM, SS-ELM, Safe-SSELM and our method with different number of labeled samples on the Yale, ORL and COIL20(B) datasets; (b) is the classification performance of ELM, SS-ELM, Safe-SSELM and our method with different number of unlabeled samples on the Yale, ORL and COIL20(B) datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8735863/ma4ab-2922385-large.gif
2019,8423081,Fig. 1.,"Pipeline of the proposed geomagnetic data reconstruction framework, which includes four main modules, including data set building, classic machine learning regression, RNN regression, and the selected model for regression and reconstruction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu1-2852632-large.gif
2019,8423081,Fig. 2.,Flow structure of RNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu2-2852632-large.gif
2019,8423081,Fig. 3.,Update function cell of LSTM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu3-2852632-large.gif
2019,8423081,Fig. 4.,LSTM RNN architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu4-2852632-large.gif
2019,8423081,Fig. 5.,Procedure of preprocess geomagnetic time-series data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu5-2852632-large.gif
2019,8423081,Fig. 6.,Flowchart of training a regression model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu6-2852632-large.gif
2019,8423081,Fig. 7.,2-D field geomagnetic data for test. (a) Original 2-D field geomagnetic data. (b) Decimated data with 30% regular missing traces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu7ab-2852632-large.gif
2019,8423081,Fig. 8.,Six 2-D field geomagnetic data used to feed into the training model. (a) 1st dataset. (b) 2nd dataset. (c) 3rd dataset. (d) 4th dataset. (e) 5th dataset. (f) 6th dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu8abcdef-2852632-large.gif
2019,8423081,Fig. 9.,Reconstruction results of different models. (a) Linear. (b) SVM. (c) Random forests. (d) Gradient boosting. (e) LSTM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu9abcde-2852632-large.gif
2019,8423081,Fig. 10.,3-D field geomagnetic data for test. (a) Original 3-D field geomagnetic data. (b) Decimated data with 50% regular missing traces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu10ab-2852632-large.gif
2019,8423081,Fig. 11.,Six 3-D field geomagnetic data used to feed into the training model. (a) 1st dataset. (b) 2nd dataset. (c) 3rd dataset. (d) 4th dataset. (e) 5th dataset. (f) 6th dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu11abcdef-2852632-large.gif
2019,8423081,Fig. 12.,Reconstruction results of different models. (a) Linear. (b) SVM. (c) Random forests. (d) Gradient boosting. (e) LSTM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/8587258/8423081/liu12abcde-2852632-large.gif
2019,8636540,Fig. 1.,"Motivation of our proposed LLSVM. The red positive sign, green negative sign, and blue circle represent the positive example, negative example, and unlabeled example, respectively. (a) Ideal decision boundary can be found based on the large-margin theory if we know the ground truth labels of training data. (b) Due to the unknown negative data in PU learning, the direct deployment of large-margin theory will lead to the biased decision boundary that classifies all examples into positive. (c) Label calibration is conducted to push the biased decision boundary in (b) to the correct one.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8636540/gong1abc-2892403-large.gif
2019,8636540,Fig. 2.,Regularizers (red curves) adopted by our LLSVM and their approximations (blue dashed curves).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8636540/gong2-2892403-large.gif
2019,8636540,Algorithm 1:,Summarization of the Proposed LLSVM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8636540/gong5-2892403-large.gif
2019,8636540,Fig. 3.,Experiments on TwoGaussian data set. The first row compares the decision boundaries of LLSVM (magenta line) for PU learning with 30 positive examples and the fully supervised binary SVM (cyan dashed line) with all known labels. The second row conducts the same comparison when the proposed LLSVM is trained on 50 positive examples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8636540/gong3-2892403-large.gif
2019,8636540,Fig. 4.,"Parametric sensitivity of (a)
α
, (b)
β
, and (c)
γ
of our LLSVM on CIFAR and GermanCredit data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8636540/gong4abc-2892403-large.gif
2019,8716527,Fig. 1.,Mobile crowd-machine learning system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8866693/8716527/nguye1-2917133-large.gif
2019,8716527,Fig. 2.,Energy consumption comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8866693/8716527/nguye2-2917133-large.gif
2019,8716527,Fig. 3.,Training latency comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8866693/8716527/nguye3-2917133-large.gif
2019,8716527,Fig. 4.,"The number of data units taken by the server as
η
1
varies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8866693/8716527/nguye4-2917133-large.gif
2019,8792202,Fig. 1.,"Timeline of regular HARQ compared to early HARQ. (HARQ RTT: HARQ round trip time; TTI: transmission time interval;
T
RX
: processing time at the receiver; A/N: ACK/NACK feedback transmission; Re-TX: retransmission;
T
1
: time from initial reception to feedback transmission
T
2
: time from transmission of feedback to the end of the processing of the retransmission at the receiver).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg1-2934001-large.gif
2019,8792202,Fig. 2.,"Probabilistic model for single-retransmission E-HARQ (terminal nodes in bold face lead to an effective block error). We use binary random variables
e/
e
′
to reflect the state of the transmission (
e/
e
′
=0
: no block error,
e/
e
′
=1
: block error) and a binary random variable
f
to quantify the feedback sent (
f=0
: ACK,
f=1
: NACK).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg2-2934001-large.gif
2019,8792202,Fig. 3.,Selected examples for classification performance based on VNR-features in the pedestrian channel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg3ab-2934001-large.gif
2019,8792202,Fig. 4.,Selected examples for system performance in the pedestrian channel for two-retransmission E-HARQ with unlimited system resources.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg4ab-2934001-large.gif
2019,8792202,Fig. 5.,"Exemplary system performance comparison for rate 1/2 and 5/6 prediction schemes in high load and medium load scenarios (green dashed line indicates
FNR
eval
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg5abcd-2934001-large.gif
2019,8792202,Fig. 6.,Effects of the scheduling gain in the high load regime for the strict and relaxed latency constraint.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg6ab-2934001-large.gif
2019,8792202,Fig. 7.,"Architecture for supervised anomaly detection using a jointly trained supervised autoencoder (
x
: input,
x
rec
: reconstructed input,
x
bot
: bottleneck features,
y
: predicted label).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg7-2934001-large.gif
2019,8792202,Fig. 8.,Non-converging and converging resource distribution functions over time of an overloaded system (left) and a balanced system (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8871138/8792202/hellg8ab-2934001-large.gif
2019,8886351,Figure 1.,"An example of a bot-net used for a distributed DoS attack. The attacking machine, at the top, has gained controls of a large number of “bots” as shown in the middle. The target is then overwhelmed by traffic generated by the bots. Figure curtousey of Coelho Everaldo https://commons.wikimedia.org/w/index.php?curid=3980651.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/8956012/8886351/paffe1-2950183-large.gif
2019,8886351,Figure 2.,"An example of an attack detected using an RDA. On the left we plot a time series showing a measure, called the “S Value,” of how anomalous the network traffic is over time. We overlay that plot with vertical lines that indicate when an attack begins (solid red line) and ends (dashed red line). Such methods also call for the DDoS attack to be attributed to particular network hosts. On the right we have a number of network hosts all shown at red dots, and many of these hosts are all accessing the same target host.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/8956012/8886351/paffe2-2950183-large.gif
2019,8663421,Fig. 1.,Proposed EM-ML system using ANN as a core machine learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8705607/8663421/alzah1-2903787-large.gif
2019,8663421,Fig. 2.,Schematic plot of ULA of half-wavelength dipole antennas located on the x-axis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8705607/8663421/alzah2-2903787-large.gif
2019,8663421,Fig. 3.,"MUSIC spectra for the estimation of (left) a single source at
θ=
50
∘
and (right) two sources at
θ1=
30
∘
and
θ2=
60
∘
for an array with spacing of (a) and (b)
0.2λ
and (c) and (d)
0.3λ
, where the signal-to-noise ratio is 20 dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8705607/8663421/alzah3-2903787-large.gif
2019,8765415,Fig. 1.,Diffraction measurement setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8765415/brown1-2929289-large.gif
2019,8765415,Fig. 2.,Body diffraction loss measurement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8765415/brown2-2929289-large.gif
2019,8765415,Fig. 3.,"GP diffraction loss estimation for
θ={0, 1, 2,…,360}
and
f
= 12 GHz. The estimated diffraction loss is represented for 1, 4, and 9 training measured data compared with the measured test data for two different scenarios: (a) test data is #10 and the rest are training data, (b) test data is #5 and the rest are training data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8765415/brown3-2929289-large.gif
2019,8765415,Fig. 4.,(a) Correlation and (b) NSME between the test measured data and GP estimated values for the test data considering up to nine measurements for training for three different scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8765415/brown4-2929289-large.gif
2019,8382154,Fig. 1.,Flowchart of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei1-2845866-large.gif
2019,8382154,Fig. 2.,Flowchart of DPN encoding. (a) Basic DPN of 4 degrees. (b) Basic DPN for multiple features. (c) Multimodal DPN (MDPN) for multiple features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei2-2845866-large.gif
2019,8382154,Fig. 3.,Comparison of different values of Hiddensize on H.pylori dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei3-2845866-large.gif
2019,8382154,Fig. 4.,Results using different encoding methods on three datasets with RELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei4-2845866-large.gif
2019,8382154,Fig. 5.,Results using MDPN encoding on three datasets with different classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei5-2845866-large.gif
2019,8382154,Fig. 6.,ROC curves for three different datasets using MDPN encoding with different classifiers. (a) ROC curves of MP-MDPN-ELM on H. pylori dataset. (b) ROC curves of MP-MDPN-SVM on H. pylori dataset. (c) ROC curves of MP-MDPN-RELM on H. pylori dataset. (d) ROC curves of MP-MDPN-ELM on Human dataset. (e) ROC curves of MP-MDPN-SVM on Human dataset. (f) ROC curves of MP-MDPN-ELM on Yeast dataset. (g) ROC curves of MP-MDPN-SVM on Yeast dataset. (h) ROC curves of MP-MDPN-RELM on Yeast dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei6-2845866-large.gif
2019,8382154,Fig. 7.,Predicted results of one-core network for CD9.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei7-2845866-large.gif
2019,8382154,Fig. 8.,Predicted results of multicore network for Ras-Raf-Mek-Erk-Elk-Srf pathway.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei8-2845866-large.gif
2019,8382154,Fig. 9.,Predicted results of crossover network for Wnt-related pathway.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8705605/8382154/lei9-2845866-large.gif
2019,8428413,Fig. 1.,Learning process of personalized medicine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu1-2864254-large.gif
2019,8428413,Fig. 2.,Review methodology process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu2-2864254-large.gif
2019,8428413,Fig. 3.,CNN structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu3-2864254-large.gif
2019,8428413,Fig. 4.,RNN structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu4-2864254-large.gif
2019,8428413,Fig. 5.,RBM structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu5-2864254-large.gif
2019,8428413,Fig. 6.,AE neural network structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu6-2864254-large.gif
2019,8428413,Fig. 7.,"Framework of the review on different aspects of personalized medicine separated into three domains of drug development, disease characteristics, and therapeutic effects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu7-2864254-large.gif
2019,8428413,Fig. 8.,Distribution of research attention by the publication year.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu8-2864254-large.gif
2019,8428413,Fig. 9.,Distortion of research topics in personalized medicine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8643115/8428413/qu9-2864254-large.gif
2019,8763934,FIGURE 1.,Balanced accuracy heatmap of feature selection methods (in columns) and feature classification methods (in rows) in ten-fold cross validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8763934/shi1-2928975-large.gif
2019,8763934,FIGURE 2.,AUC heatmap of feature selection methods (in columns) and feature classification methods (in rows) in ten-fold cross validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8763934/shi2-2928975-large.gif
2019,8763934,FIGURE 3.,Balanced accuracy heatmap of feature selection methods (in columns) and feature classification methods (in rows) in percentage split.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8763934/shi3-2928975-large.gif
2019,8763934,FIGURE 4.,AUC heatmap of feature selection methods (in columns) and feature classification methods (in rows) in percentage split.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8763934/shi4-2928975-large.gif
2019,8763934,FIGURE 5.,The balanced accuracy (in columns) for different feature selection methods with different selected feature numbers (in rows).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8763934/shi5-2928975-large.gif
2019,8763934,FIGURE 6.,The NFTI coefficients of selected feature in each feature type for the fifteen feature selection methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8763934/shi6-2928975-large.gif
2019,8765385,Fig. 1.,Corresponding Markov model and the Q-value arithmetic function logic block diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h1-2929086-large.gif
2019,8765385,Fig. 2.,Corresponding interval type-2 membership function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h2-2929086-large.gif
2019,8765385,Fig. 3.,Learning model framework of the corresponding learning automata.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h3-2929086-large.gif
2019,8765385,Fig. 4.,Corresponding pattern recognition system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h4-2929086-large.gif
2019,8765385,Fig. 5.,Corresponding overall structure frame.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h5-2929086-large.gif
2019,8765385,Fig. 6.,Corresponding model of the two-layer adaptive fuzzy superposition system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h6-2929086-large.gif
2019,8765385,Fig. 7.,Learning automaton reinforcement learning system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h7-2929086-large.gif
2019,8765385,Fig. 8.,"Corresponding convergence charts of the initial and intermediate stages when the noise is 0 and 0.4, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h8-2929086-large.gif
2019,8765385,Fig. 9.,Corresponding convergence step diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8892749/8765385/h9-2929086-large.gif
2019,8567933,Fig. 1.,"Conventional PML method. (a) 2-D FDTD with PML. (b)
TE
z
PML grid.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao1-2885570-large.gif
2019,8567933,Fig. 2.,Architecture of a one-hidden-layer HTBF ANN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao2-2885570-large.gif
2019,8567933,Fig. 3.,"Configuration of HTBF-based PML in a 2-D
TE
z
FDTD lattice.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao3-2885570-large.gif
2019,8567933,Fig. 4.,Application scenario of an HTBF-based PML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao4-2885570-large.gif
2019,8567933,Fig. 5.,FDTD grid geometry on the 15 mm × 15 mm area with one excited source and with two probes at points A and B. (a) HTBF-based PML. (b) Conventional PML with the size of 5-cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao5-2885570-large.gif
2019,8567933,Fig. 6.,"Comparison of relative error between HTBF-based PML, 1-cell conventional PML, and 5-cell conventional PML. (a) Relative error at Point A and Point B. (b) Relative error of the entire 15 × 15 square.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao6-2885570-large.gif
2019,8567933,Fig. 7.,FDTD grid geometry on the 15 mm × 15 mm area with two excited sources and with two probes at Points A and B. (a) HTBF-based PML. (b) Conventional PML with the size of 5-cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao7-2885570-large.gif
2019,8567933,Fig. 8.,"Comparison of relative error between HTBF-based PML, 1-cell conventional PML, and 5-cell conventional PML. (a) Relative error at Point A and Point B. (b) Relative error of the entire 15 × 15 square.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8602385/8567933/yao8-2885570-large.gif
2019,8839068,FIGURE 1.,Working principle of an artificial neuron.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar1-2938951-large.gif
2019,8839068,FIGURE 2.,Schematic structure of feedforward propagation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar2-2938951-large.gif
2019,8839068,FIGURE 3.,Working principle of experimental setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar3-2938951-large.gif
2019,8839068,FIGURE 4.,Operational environment for tests.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar4-2938951-large.gif
2019,8839068,FIGURE 5.,Hct and sO2 combinations of tested samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar5-2938951-large.gif
2019,8839068,FIGURE 6.,Linear regression model fit on test set for hematocit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar6ab-2938951-large.gif
2019,8839068,FIGURE 7.,Linear regression model fit on test set for oxygen saturation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/8579633/8839068/decar7ab-2938951-large.gif
2019,8854064,FIGURE 1.,AF-DELM network structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le1-2944877-large.gif
2019,8854064,FIGURE 2.,DG-AF-DELM networks structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le2-2944877-large.gif
2019,8854064,FIGURE 3.,"Comparison of c stability between DG-AF-DELM and PSO-AF-DELM. (a) Bodyfat, (b) Pyrim, and (c) Triazines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le3abc-2944877-large.gif
2019,8854064,FIGURE 4.,"The relationship between weight, age and best deadlift.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le4-2944877-large.gif
2019,8854064,FIGURE 5.,"The effect of the number of hidden layer nodes on the performance of each model: (a) RMSEP, (b) R2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le5ab-2944877-large.gif
2019,8854064,FIGURE 6.,The training time of different models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le6-2944877-large.gif
2019,8854064,FIGURE 7.,The prediction results of the DG-AF-DELM model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8854064/le7-2944877-large.gif
2019,8540058,Fig. 1.,"Traditional method for enabling feature-based decision making capability on an existing device. The final classification algorithm is a function of
N
features, represented by
N
nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj1-2881889-large.gif
2019,8540058,Fig. 2.,Proposed approach uses the system knowledge of codesigned hardware to pull multiple features out of a single time series of data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj2-2881889-large.gif
2019,8540058,Fig. 3.,"Cross section of the printer media path is depicted. The highlighted region contains an optical sensor consisting of an LED (1) and phototransistor (2) that measures the amount of infrared light transmitted by a sheet of media (4) as it is processed by the printer. Media fed by upstream feed rollers (3) passes through the sensor, beyond a media guide (5), and into a set of downstream feed rollers (6). Hardware (physical design of the media path) and firmware (system timings and relative velocities of the feed rollers) were tuned during development to enhance data orthogonality by controlling the position and shape of the media relative to the sensor in the spatial/temporal domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj3-2881889-large.gif
2019,8540058,Fig. 4.,"Electrical design schematic for the optical sensor system is shown. The connection labeled “Analog Output” is the voltage signal measured by the analog-to-digital converter and used in the classification system. Nominal circuit values were selected to optimize the sensor gain, response, and sensitivity for a broad range of media types.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj4-2881889-large.gif
2019,8540058,Fig. 5.,"Normalized analog sensor output for 20 separate sheets of a standard office paper are plotted. Data were collected for 100 mm of media travel. The population mean and 99.7% confidence bands for this given media are plotted for reference. Larger values on the
y
-axis correspond to an instant in time, or media leading edge position relative to the sensor, where less light was detected by the phototransistor. It is clear that there is a significant amount of noise in the measurement. This may be attributed to both intrinsic media properties,
φ
k
, extrinsic system properties,
Z
k
, and uncontrollable external factors,
Y
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj5-2881889-large.gif
2019,8540058,Fig. 6.,Normalized analog sensor output for the mean and 99.7% confidence band of each class are plotted. The population for each class consists of 360 training samples from each media listed in Table II. A standard classification problem utilizing a traditional feature set would be intractable due to the continuous overlapping nature of the data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj6-2881889-large.gif
2019,8540058,Fig. 7.,Simplified model of the response of the sensing system to the movement of media through the printer was developed. Observed measurement differences due to the dynamic interaction of the media with the sensor were leveraged to decouple confounded variables.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj7-2881889-large.gif
2019,8540058,Fig. 8.,"Representative input features after scaling for an assortment of media types. While the features contain information that can do corner case separation, the features individually suffer from boundary confusion (significantly overlapping error bars between categories).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/3516/8642453/8540058/bajaj8-2881889-large.gif
2019,8668456,Fig. 1.,"Constellation diagrams when 16-QAM and 8-PSK modulations are used. From left to right columns, i)
y
r,k
[t]
, ii)
d
k
[t]
, iii)
d
¯
k
[t]
, and iv) the clusters and corresponding bit sequences are shown. From top to bottom rows, environments are as follows: i)
1/
σ
2
v
=30 dB
and
N=1
for 16-QAM, ii)
1/
σ
2
v
=10dB
and
N=1
for 16-QAM, iii)
1/
σ
2
v
=10dB
and
N=8
for 16-QAM, iv)
1/
σ
2
v
=30dB
and
N=1
for 8-PSK, and v)
1/
σ
2
v
=10dB
and
N=1
for 8-PSK. The
x
- and
y
-axes represent the in-phase and quadrature components, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8723653/8668456/jung1-2905622-large.gif
2019,8668456,Fig. 2.,"BER performance (QPSK and 16-QAM) over
1/
σ
2
v
when
N=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8723653/8668456/jung2-2905622-large.gif
2019,8668456,Fig. 3.,"BER performance (16-QAM) over
N
when
1/
σ
2
v
={5,10}dB
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8723653/8668456/jung3-2905622-large.gif
2019,8845682,Fig. 1.,Machine learning-based model calibration procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8939298/8845682/adeog1-2942819-large.gif
2019,8845682,Fig. 2.,Estimates of PG model parameters using a three-inputs network with 20 hidden nodes. (a) I. (b) II. (c) III. (d) IV.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8939298/8845682/adeog2-2942819-large.gif
2019,8845682,Fig. 3.,Estimates of Saleh–Valenzuela model parameters using a nine-inputs network with 20 hidden nodes. (a) I. (b) II.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8939298/8845682/adeog3-2942819-large.gif
2019,8845682,Fig. 4.,"Averaged (left) and instantaneous (right) PDP computed from measurements and estimated PDP from the models. Estimated model parameters are: PG (
g=0.65
,
N
s
=31
,
P
vis
=0.93
) and SV (
Γ=15.6
,
γ=14
,
Λ=0.55
,
λ=2.9
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8939298/8845682/adeog4-2942819-large.gif
2019,8764436,Fig. 1.,Machine learning network with three-layer BPNN and softmax regression classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8863555/8764436/fan1-2929148-large.gif
2019,8764436,Fig. 2.,"Hierarchical division of positioning area, where
B
c
is the number of blocks in each column.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8863555/8764436/fan2-2929148-large.gif
2019,8764436,Fig. 3.,CDF of different methods of data-preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8863555/8764436/fan3-2929148-large.gif
2019,8784297,Fig. 1.,Experimental setup: Transmitter and receiver.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8784297/cabra1-2932076-large.gif
2019,8784297,Fig. 2.,Measurement location: The floor plan and a real photo.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8784297/cabra2-2932076-large.gif
2019,8784297,Fig. 3.,First 50 s of the received power for the scenario.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8784297/cabra3-2932076-large.gif
2019,8784297,Fig. 4.,Mean cross-validation accuracy as a function of the number of features chosen.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8784297/cabra4-2932076-large.gif
2019,8784297,Fig. 5.,Received power behavior in part of the feature space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8784297/cabra5-2932076-large.gif
2019,8784297,Fig. 6.,Confusion matrix of the cross-validation predictions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8784297/cabra6-2932076-large.gif
2019,8603823,Fig. 1.,"Schematic representation of the location of a TSS and the regions of promoter, exons, introns, and
3
′
UTR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang1-2891239-large.gif
2019,8603823,Fig. 2.,Framework of DCDE-MSVMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang2-2891239-large.gif
2019,8603823,Fig. 3.,The heat maps demonstrate the statistical divergence (KLD) between kmer (k=3) distributions of promoters and non-promoters (exons) in the sub-classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang3-2891239-large.gif
2019,8603823,Fig. 4.,AUC Comparison of different numbers of training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang4-2891239-large.gif
2019,8603823,Fig. 5.,"(a-c) Performance comparisons of three sub-classifiers (Promoter & Exon, Promoter & Intron and Promoter &
3
′
UTR) with DCDE feature or single kmer feature. (d) Performance comparisons of three sub-classifiers integrated by BD with DCDE feature or single kmer feature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang5-2891239-large.gif
2019,8603823,Fig. 6.,"Visualization of informative kmers positional encoding matrices of promoters by all kmers (k=3) settled by KLD, JD, and JSD in three sub-classifies (Promoter & Exon: b-d, Promoter & Intron: e-g and Promoter &
3
′
UTR: h-j). Visualization of original kmers encoding matrices without informative kmers settlement (a).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang6abcdefghij-2891239-large.gif
2019,8603823,Fig. 7.,"Performance comparisons of three sub-classifiers with or without SVM classification. (a) Promoter & Exon. (b) Promoter & Intron. (c) Promoter &
3
′
UTR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang7abc-2891239-large.gif
2019,8603823,Fig. 8.,ACP comparisons of three sub-classifiers with or without SVM classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang8-2891239-large.gif
2019,8603823,Fig. 9.,"Performance comparison of DCDE-MSVMs (KLD, JD, and JSD), PromoterExplorer, REPI, LS-GKM, and SeqGL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang9-2891239-large.gif
2019,8603823,Fig. 10.,"Comparison of ROC curves for LS-GKM, SeqGL, PromoterExplorer, REPI and DCDE-MSVMs (KLD, JD, and JSD).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang10-2891239-large.gif
2019,8603823,Fig. 11.,"AUC Comparison of DCDE-MSVMs, SeqGL, LS-GKM, PromoterExplorer and REPI.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7728/8698256/8603823/huang11-2891239-large.gif
2019,8500348,Fig. 1.,Cloud DevOps architecture for machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad1-2876844-large.gif
2019,8500348,Fig. 2.,Cloud DevOps approach for model selection and auto-tuning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad2-2876844-large.gif
2019,8500348,Fig. 3.,Supervised learning accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad3-2876844-large.gif
2019,8500348,Fig. 4.,Machine learning parameters versus accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad4-2876844-large.gif
2019,8500348,Fig. 5.,ML’s accuracy with label categorization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad5-2876844-large.gif
2019,8500348,Fig. 6.,Accuracy with clustering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad6-2876844-large.gif
2019,8500348,Fig. 7.,Reflexive DevMLOps architecture for autoselection and autotuning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad7-2876844-large.gif
2019,8500348,Fig. 8.,ML ranking change dynamically.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad8-2876844-large.gif
2019,8500348,Fig. 9.,Dynamic model selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad9-2876844-large.gif
2019,8500348,Fig. 10.,Autotuning example for gradient boosting classifier on unsw dataset label: ‘NO-ATTACK’.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad10-2876844-large.gif
2019,8500348,Fig. 11.,Container scaling for label: “NO-ATTACK”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad11-2876844-large.gif
2019,8500348,Fig. 12.,Model accuracy versus number of features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad12-2876844-large.gif
2019,8500348,Fig. 13.,Recursive feature elimination for random forest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad13-2876844-large.gif
2019,8500348,Fig. 14.,Statically-tuned ensemble learning accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad14-2876844-large.gif
2019,8500348,Fig. 15.,Deep learning accuracies for each label.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad15-2876844-large.gif
2019,8500348,Fig. 16.,Deep learning accuracies in multilabel classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad16-2876844-large.gif
2019,8500348,Fig. 17.,Critical path in model selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/8685251/8500348/elfad17-2876844-large.gif
2019,8753479,FIGURE 1.,"The general overview of the proposed methodology. Physiological data and vehicle information are selected as input features. The performance of the driver is determined by lane deviation. After preprocessing of input features, they are mixed with the response variable. Three classifiers and their ensembles before and after applying EOAs are applied into the model. In the final step, performance metric will be used to evaluate the performance of each classifier and their ensembles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar1-2926444-large.gif
2019,8753479,Algorithm 1,Particle Swarm Optimization (PSO),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar13-2926444-large.gif
2019,8753479,Algorithm 2,Ant Lion Optimizer (ALO),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar14-2926444-large.gif
2019,8753479,Algorithm 3,Grey Wolf Optimizer (GWO),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar15-2926444-large.gif
2019,8753479,Algorithm 4,Whale Optimization Algorithm (WOA),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar16-2926444-large.gif
2019,8753479,FIGURE 2.,"The internal architecture of the proposed methodology. Dataset is divided into two parts (0.9 data is used for training and 0.1 for testing). Three classifiers such as SVM, KNN, NB are used to evaluate the performance of the driver’s data. Their outputs are forwarded to the for EOAs to obtain weights for different classifiers. These weights are fed in to the Bagging, Boosting, and Voting ensemble learning techniques. The final result is used for evaluation of driver’s performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar2-2926444-large.gif
2019,8753479,Algorithm 5,Proposed Algorithm (Integration of Ensemble and Evolutionary Machine Learning Algorithms),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar17-2926444-large.gif
2019,8753479,FIGURE 3.,"Confusion matrices for classical algorithms. A: Confusion matrices for classical algorithms, B: Confusion matrices for combination of ensemble learning techniques and classical algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar3-2926444-large.gif
2019,8753479,FIGURE 4.,Improvement rates after applying ensemble learning techniques compared to simple machine learning algorithms (%).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar4-2926444-large.gif
2019,8753479,FIGURE 5.,Median AUC of the classical algorithms before and after applying ensemble learning techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar5-2926444-large.gif
2019,8753479,FIGURE 6.,"Confusion matrices for hybrid of A) Bagging, B) Boosting and C) Voting and other 4 EOAs: ALO, WOA, PSO and GWO.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar6-2926444-large.gif
2019,8753479,FIGURE 7.,"Evaluation of performance metrics after applying ALO into the Bagging, Boosting and Voting ensembles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar7-2926444-large.gif
2019,8753479,FIGURE 8.,"Evaluation of performance metrics after applying WOA into the Bagging, Boosting and Voting ensembles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar8-2926444-large.gif
2019,8753479,FIGURE 9.,"Evaluation of performance metrics after applying GWO into the Bagging, Boosting and Voting ensembles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar9-2926444-large.gif
2019,8753479,FIGURE 10.,"Evaluation of performance metrics after applying PSO into the Bagging, Boosting and Voting Ensembles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar10-2926444-large.gif
2019,8753479,FIGURE 11.,"The best scores obtained by ALO, WOA, PSO and GWO for \$(a)\$ Bagging \$(b)\$ Boosting \$(c)\$ Voting Ensembles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar11-2926444-large.gif
2019,8753479,FIGURE 12.,Improvement rates after applying EA methods with ensemble learning techniques single ensemble learning techniques (%).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8753479/abdar12-2926444-large.gif
2019,8674746,FIGURE 1.,"Training stages of incremental learning. Training samples belonging to new classes will be gradually added to the training dataset step by step. During each step, we attempt to adjust the model trained on the last dataset to learn the new added classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8674746/guo1-2907645-large.gif
2019,8674746,FIGURE 2.,"Training procedure of our proposed dynamic-connected deep neural network. First, we adopt the training with pruning method on the original dataset. The solid lines denote the learned core-part of the model and the dashed lines denote the learned edge-part of the model. Then, on the extended dataset with new classes, we first expand the model’s edge-part and then train the two part with different learning rate.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8674746/guo2-2907645-large.gif
2019,8674746,FIGURE 3.,Some random selected images in MNIST (left) and CIFAR-100 (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8674746/guo3ab-2907645-large.gif
2019,8660506,Fig. 1.,PW principle. (a) Tx training symbols. (b) Rx detection based on training symbols. (c) Rx decision regions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari1abc-2902973-large.gif
2019,8660506,Fig. 2.,"Transmission diagram: IQ: In-phase and quadrature modulator, ICR: Integrated coherent receiver. Dashed lines represent the partly-considered components.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari2-2902973-large.gif
2019,8660506,Fig. 3.,"Q
factor vs. window size
R
for DM and DUM systems.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari3-2902973-large.gif
2019,8660506,Fig. 4.,"Q
factor vs. input power for DM 16-QAM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari4-2902973-large.gif
2019,8660506,Fig. 5.,"Q
factor vs. input power for DM 64-QAM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari5-2902973-large.gif
2019,8660506,Fig. 6.,"Q
factor vs. input power for DUM 16-QAM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari6-2902973-large.gif
2019,8660506,Fig. 7.,"Q
factor vs. input power for DUM 64-QAM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari7-2902973-large.gif
2019,8660506,Fig. 8.,"Q
factor vs. input power for DUM with DBP.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/8675695/8660506/amari8-2902973-large.gif
2019,8676028,Fig. 1.,Architecture of ML-LFIL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon1-2908019-large.gif
2019,8676028,Fig. 2.,Functional block diagram of ML-LFIL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon2-2908019-large.gif
2019,8676028,Fig. 3.,Illustrative example. (a) Normal scenario. (b) (1–2) disconnects. (c) (1–2) is replaced by (1–9).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon3abc-2908019-large.gif
2019,8676028,Fig. 4.,"Precision/recall/
F
1
-score of ML-LFIL-S1 in 30-node network with only link disconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon4-2908019-large.gif
2019,8676028,Fig. 5.,"Precision/recall/
F
1
-score of ML-LFIL-S1 in 30-node network with both link disconnections and link reconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon5-2908019-large.gif
2019,8676028,Fig. 6.,Delay regressor accuracy of MLP in ML-LFIL-S2 in 30-node network with only link disconnections for different hidden layers and hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon6-2908019-large.gif
2019,8676028,Fig. 7.,Link disconnection/reconnection accuracy of ML-LFIL-S2 in 30-node network for different threshold values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon7-2908019-large.gif
2019,8676028,Fig. 8.,"Precision/recall/
F
1
-score of ML-LFIL in 30-node network with both link disconnections and reconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon8-2908019-large.gif
2019,8676028,Fig. 9.,"Precision/recall/
F
1
-score of ML-LFIL-S1 in 60-node network with only link disconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon9-2908019-large.gif
2019,8676028,Fig. 10.,"Precision/recall/
F
1
-score of ML-LFIL-S1 in 60-node network with both link disconnections and link reconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon10-2908019-large.gif
2019,8676028,Fig. 11.,"Precision/recall/
F
1
-score of ML-LFIL in 60-node network with both link disconnections and reconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon11-2908019-large.gif
2019,8676028,Fig. 12.,"Precision/recall/
F
1
-score of ML-LFIL-S1 in the Interoute network with only link disconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon12-2908019-large.gif
2019,8676028,Fig. 13.,"Precision/recall/
F
1
-score of ML-LFIL-S1 in the Interoute network with link disconnections and reconnections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon13-2908019-large.gif
2019,8676028,Fig. 14.,Precision/recall/ \$F_{1}\$ -score of ML-LFIL in the Interoute network with link disconnections and reconnections.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon14-2908019-large.gif
2019,8676028,Fig. 15.,Fault detection accuracy of ML-LFIL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/8784345/8676028/truon15-2908019-large.gif
2019,8735773,Fig. 1.,"Grand averages as recorded from the Cz electrode and relevant ERP topographies across the two groups for the Frequency Deviant (FDev), Duration Deviant (DDev), and Intensity Deviant (IDev). Dotted waveforms represent group responses to standard tones (Std). Adapted with permission from [5].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8755894/8735773/boshr1-2922553-large.gif
2019,8735773,Fig. 2.,A flowchart outlining the overall machine learning procedure used in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8755894/8735773/boshr2-2922553-large.gif
2019,8735773,Fig. 3.,Average classification accuracy vs. the number of selected features. Shaded region indicates the standard error of the mean across the cross-validation steps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8755894/8735773/boshr3-2922553-large.gif
2019,8735773,Fig. 4.,"The SHAP values of all subjects for the 25 most-used features. Features are ranked top to bottom (top being the highest ranked). A single point represents a subject’s SHAP value for a corresponding feature (ordinate). A positive (negative) SHAP value indicates the feature’s impact towards classifying a subject as concussed (control). Color indicates the true value of each feature, as opposed to the derived SHAP value, from blue (low) to red (high; see color bar on the right). Combined with the distribution on the abscissa, the feature values (color) for all subjects indicates the directionality effect of a particular feature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8755894/8735773/boshr4-2922553-large.gif
2019,8735773,Fig. 5.,Subject averages of responses to all experimental conditions of the five commonly misclassified subjects. The averaged response for the two groups is presented in the first row. Waveforms represent data as recorded from the Pz electrode. Figure legend presented in the bottom left corner.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8755894/8735773/boshr5-2922553-large.gif
2019,8825807,Fig. 1.,"The framework of classifying different acupuncture manipulations. First, multichannel EEG signals induced by TR and LT manipulations of healthy subjects were recorded. Second, connectivity matrices were calculated by PLV method. Third, we reconstructed functional brain networks and extracted network features. Finally, taken the network features as inputs, machine learning classifiers were established to classify different acupuncture manipulations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang1-2939655-large.gif
2019,8825807,Fig. 2.,"The mean PLV across all channels between three acupuncture states. Mean PLV was calculated by averaging PLV value over all channel pairs of each subject. The larger mean PLV, the stronger functional connection of whole brain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang2-2939655-large.gif
2019,8825807,Fig. 3.,"The mean PLV of individual channels between three acupuncture states. Mean PLV of one channel was calculated by averaging PLV value over all its connected channels. The larger mean PLV of individual channels, the stronger functional connection between one channel with other channels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang3-2939655-large.gif
2019,8825807,Fig. 4.,Functional connectivity matrices (upper) and brain networks (lower) of three acupuncture states. (a) Before-acupuncture. (b) TR manipulation. (c) LT manipulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang4abc-2939655-large.gif
2019,8825807,Fig. 5.,"The graph theory features of three acupuncture states with a, b, c, d, e and f representing clustering coefficient, global efficiency, local efficiency, node betweenness, edge betweenness and small world network index, respectively. The cross pattern represented outliers that lie outside the upper and lower edges of box-plots. The outliers greatly differed from normal values and were not included in the statistical analysis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang5abcdef-2939655-large.gif
2019,8825807,Fig. 6.,The ROC and AUC value of different machine learning classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang6-2939655-large.gif
2019,8825807,Fig. 7.,The joint distribution of two features from acupunctured by TR manipulation and acupunctured by LT manipulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8868226/8825807/wang7abcd-2939655-large.gif
2019,8402232,Fig. 1.,NR-ELM method with a two-layer structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li1-2845857-large.gif
2019,8402232,Fig. 2.,"Illustration of local connections for the
j
th (
j=1,2,…,
K
1
) feature map of the first layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li2-2845857-large.gif
2019,8402232,Fig. 3.,"Illustration of the
j
th (
j=1,2,…,
K
2
) second-layer feature map.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li3-2845857-large.gif
2019,8402232,Fig. 4.,Example images in the MNIST database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li4-2845857-large.gif
2019,8402232,Fig. 5.,"Accuracy of NR-ELM in terms of the number of the first-layer feature maps
K
1
(with
K
2
=2,445
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li5-2845857-large.gif
2019,8402232,Fig. 6.,"Accuracy of NR-ELM in terms of the number of the second-layer feature maps
K
2
(with
K
1
=680
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li6-2845857-large.gif
2019,8402232,Fig. 7.,Example images in the Caltech face database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li7-2845857-large.gif
2019,8402232,Fig. 8.,Examples of testing images with random rotation angles less than 45°.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li8-2845857-large.gif
2019,8402232,Fig. 9.,"Classification accuracy of testing images with different rotation angles on the Caltech face database. The testing images are rotated with random angles within three maximal angles, while the training images are not rotated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li9-2845857-large.gif
2019,8402232,Fig. 10.,"Accuracy with different values of
α
on the Caltech face database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li10-2845857-large.gif
2019,8402232,Fig. 11.,"Accuracy with different values of
λ
on the Caltech face database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li11-2845857-large.gif
2019,8402232,Fig. 12.,Example images in the CIFAR-10 database. Each row indicates 10 images from each class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li12-2845857-large.gif
2019,8402232,Fig. 13.,Accuracy with/without dense SIFT preprocessing on the CIFAR-10 database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li13-2845857-large.gif
2019,8402232,Fig. 14.,"Accuracy with different values of
α
on the CIFAR-10 database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li14-2845857-large.gif
2019,8402232,Fig. 15.,"Accuracy with different values of
λ
on the CIFAR-10 database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8621620/8402232/li15-2845857-large.gif
2019,8826322,Fig. 1.,Workflow of the CNN model training for surface current estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8826322/karao1-2939762-large.gif
2019,8826322,Fig. 2.,"Block diagram of the CNN structure. Details of each layer are given as initials, i.e., k represents the size of convolution kernels and f represents the number of convolution filters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8826322/karao2-2939762-large.gif
2019,8826322,Fig. 3.,"Surface current estimation for 64
λ
NASA Almond geometry.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8826322/karao3-2939762-large.gif
2019,8826322,Fig. 4.,"Surface current estimation for 64
λ
Flamme geometry.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8826322/karao4-2939762-large.gif
2019,8666972,FIGURE 1.,"Histograms of NLOS samples of
τ
MED
,
τ
RMS Delay
and
N
P
. (a) Distribution of
τ
MED
. (b) Distribution of
τ
RMSD
. (c) Distribution of
N
P
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8666972/fan1abc-2903236-large.gif
2019,8666972,FIGURE 2.,"Histograms of LOS samples of
τ
MED
,
τ
RMS Delay
and
N
P
. (a) Distribution of
τ
MED
. (b) Distribution of
τ
RMSD
. (c) Distribution of
N
P
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8666972/fan2abc-2903236-large.gif
2019,8666972,FIGURE 3.,LOS and NLOS mixture of probability distributions with actual label of 1000 data points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8666972/fan3-2903236-large.gif
2019,8666972,FIGURE 4.,"Unlabeled data points of LOS and NLOS distributions with unknown parameters mean
(
μ
1
,
μ
2
)
and covariance
(
Θ
1
,
Θ
2
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8666972/fan4-2903236-large.gif
2019,8666972,FIGURE 5.,"Channel classification by EM-GMM algorithm along with mean of LOS
(
μ
1
)
and NLOS
(
μ
2
)
distributions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8666972/fan5-2903236-large.gif
2019,8666972,FIGURE 6.,Performance graph using different set of features combination.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8666972/fan6-2903236-large.gif
2019,8713992,FIGURE 1.,Outline of the paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama1-2916648-large.gif
2019,8713992,FIGURE 2.,Taxonomy of unsupervised learning techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama2-2916648-large.gif
2019,8713992,FIGURE 3.,Illustration of an ANN (left); Different types of ANN topologies (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama3ab-2916648-large.gif
2019,8713992,FIGURE 4.,Taxonomy of unsupervised neural networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama4-2916648-large.gif
2019,8713992,FIGURE 5.,Clustering process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama5-2916648-large.gif
2019,8713992,FIGURE 6.,Clustering taxonomy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama6-2916648-large.gif
2019,8713992,FIGURE 7.,"Blind signal separation (BSS): A mixed signal composed of various input signals mixed by some mixing process is blindly processed (i.e., with no or minimal information about the mixing process) to show the original signals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama7-2916648-large.gif
2019,8713992,FIGURE 8.,"Intuitively, we expect the ML model’s performance to improve with more data but to deteriorate in performance if the model becomes overly complex for the data. Figure adapted from [317].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8713992/usama8-2916648-large.gif
2019,8819975,FIGURE 1.,Network diagrams of three Facebook datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8819975/shao1-2938202-large.gif
2019,8819975,FIGURE 2.,Diagram of link prediction results of each algorithm on 4 data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8819975/shao2-2938202-large.gif
2019,8653348,Fig. 1.,ARs% comparison of RBC-IDS and ASCH-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8721285/8653348/otoum1-2901792-large.gif
2019,8653348,Fig. 2.,DRs% comparison of RBC-IDS and ASCH-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8721285/8653348/otoum2-2901792-large.gif
2019,8653348,Fig. 3.,FN% comparison of RBC-IDS and ASCH-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8721285/8653348/otoum3-2901792-large.gif
2019,8653348,Fig. 4.,ROC comparison of RBC-IDS and ASCH-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8721285/8653348/otoum4-2901792-large.gif
2019,8653348,Fig. 5.,"F
1
Score comparison of RBC-IDS and ASCH-IDS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8721285/8653348/otoum5-2901792-large.gif
2019,8357457,Fig. 1.,Mechanical structural topology of the PMSLM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen1-2835413-large.gif
2019,8357457,Fig. 2.,Layer analysis structural diagram of the PMSLM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen2-2835413-large.gif
2019,8357457,Fig. 3.,Variations of optimization objectives with the magnet width and height.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen3-2835413-large.gif
2019,8357457,Fig. 4.,Diagram of the ELM modeling method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen4-2835413-large.gif
2019,8357457,Fig. 5.,Mesh grid of FEA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen5-2835413-large.gif
2019,8357457,Fig. 6.,Magnetic field intensity distribution of FEA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen6-2835413-large.gif
2019,8357457,Fig. 7.,Schematic diagram of the ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen7-2835413-large.gif
2019,8357457,Fig. 8.,"Accuracy test results. (a) Accuracy test of
F
at different speeds. (b) Accuracy test of
γ
at different speeds. (c) Accuracy test of the THD at different speeds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen8-2835413-large.gif
2019,8357457,Fig. 9.,"R
2
results of different modeling methods. (a)
R
2
of
F
at different speeds. (b)
R
2
of
γ
at different speeds. (c)
R
2
of
THD
at different speeds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen9-2835413-large.gif
2019,8357457,Fig. 10.,Social hierarchy of gray wolf.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen10-2835413-large.gif
2019,8357457,Fig. 11.,Evolutionary process diagrams.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen11-2835413-large.gif
2019,8357457,Fig. 12.,Test platform for the PMSLM. (a) Prototype PMSLM. (b) Test platform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen12-2835413-large.gif
2019,8357457,Fig. 13.,Thrust and thrust ripple experiments results. (a) Thrust curve at a moving speed of 1 m/s. (b) Thrust curve at a moving speed of 0.5 m/s. (c) Thrust curve at a moving speed of 0.1 m/s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen13-2835413-large.gif
2019,8357457,Fig. 14.,Results of THD experiments. (a) THD experiment at 1 m/s. (b) THD experiment at 0.5 m/s. (c) THD experiment at 0.1 m/s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8478009/8357457/jiwen14-2835413-large.gif
2019,8717641,FIGURE 1.,"The basic framework of this paper. The main body of this paper consists of three parts: general machine learning models in section II, diversity in machine learning in section III–V, and extensive applications in section VI.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong1-2917620-large.gif
2019,8717641,FIGURE 2.,"Flowchart for training process of general machine learning (including the active learning, supervised learning and unsupervised learning). We can find that when the training data is labeled, the training process is supervised. In contrast, the training process is unsupervised. Besides, it should be noted that when both the labeled and unlabelled data are used for training, the training process is semi-supervised.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong2-2917620-large.gif
2019,8717641,FIGURE 3.,"Flowchart of a special form of supervised machine learning with single model. Since diversity mainly occurs in the training batch in the data-preprocessing, this work will mainly discuss the diversity of samples in the training batch for data diversification. Generally, the more diversification and balance each training batch is, the more effectiveness the training process is. In addition, it should be noted that the factors in the same layer of the model can be diversified to improve the representational ability of the model (which is called D-model in this paper). Moreover, when we obtain multiple choices from the model, the obtained choices are desired to provide more complement information. Therefore, some works focus on the diversification of multiple choices (which we call inference diversification).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong3-2917620-large.gif
2019,8717641,FIGURE 4.,"Flowchart of supervised machine learning with multiple parallel models. We can find that a best strategy to diversify the training set for different models can improve the performance of multiple models. Furthermore, we can diversify different models directly to enforce different model to provide more complement information for further analysis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong4-2917620-large.gif
2019,8717641,FIGURE 5.,"Effects of D-model on improving the performance of the machine learning model. Under the model diversification, each parameter factor of the machine learning model tends to model unique information and the whole machine learning model can model more useful information from the objects. Thus, the representational ability can be improved. The figure shows the results from the image segmentation task in [31]. As showed in the figure, the extracted features from the model can better discriminate different objects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong5-2917620-large.gif
2019,8717641,FIGURE 6.,"Effects of D-models for improving the performance of the machine learning model. The figure shows the image segmentation task from the prior work [27]. The single model often produce solutions with low expected loss and step into the sub-optimal results. Besides, general ensemble learning usually provide multiple choices with great similarity. Therefore, this work summarizes the methods which can diversify the ensemble learning (D-models). As the figure shows, under the model diversification, each model of the ensemble can produce different outputs reflecting multi-modal belief [27].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong6-2917620-large.gif
2019,8717641,FIGURE 7.,"Effects of inference diversification for improving the performance of the machine learning model. The results come from prior work [31]. Through inference diversification, multiple diversified choices can be obtained. Then, under the help of other methods, such as re-ranking in [31], the final solution can be obtained.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8717641/gong7-2917620-large.gif
2019,8294247,Fig. 1.,Vibration signal and its spectrum of the machine tool under time-varying working condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo1-2807414-large.gif
2019,8294247,Fig. 2.,Framework of the early fault detection method based on deep learning and dynamics identification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo2-2807414-large.gif
2019,8294247,Fig. 3.,Data acquisition system on CNC machining center in a real manufacturing factory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo3-2807414-large.gif
2019,8294247,Fig. 4.,Stability diagrams of the identification results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo4-2807414-large.gif
2019,8294247,Fig. 5.,Health index based on the similarities of the feature vectors over time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo5-2807414-large.gif
2019,8294247,Fig. 6.,"Health indicator based on different feature vectors. (a) Natural frequency features, (b) DAE features, (c) FD features, and (d) TD features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo6-2807414-large.gif
2019,8294247,Fig. 7.,"Performance of the machine tool in the overall degradation process. (a) The surface quality in different health stages, and (b) the final failure of the feed drive.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/8453958/8294247/luo7-2807414-large.gif
2019,8911318,FIGURE 1.,Framework of Distbelief.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan1-2955547-large.gif
2019,8911318,FIGURE 2.,Training process in single-machine and distributed-machine environments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan2-2955547-large.gif
2019,8911318,FIGURE 3.,Parameter average method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan3-2955547-large.gif
2019,8911318,FIGURE 4.,Loss function weight reorder stochastic gradient descent method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan4-2955547-large.gif
2019,8911318,FIGURE 5.,Situation of falling into the local optimum.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan5-2955547-large.gif
2019,8911318,FIGURE 6.,LR-SGD experimental result with the BSP model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan6-2955547-large.gif
2019,8911318,FIGURE 7.,LR-SGD experimental result with the SSP model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan7-2955547-large.gif
2019,8911318,FIGURE 8.,LR-SGD training time compared with that of the BSP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan8-2955547-large.gif
2019,8911318,FIGURE 9.,LR-SGD training time compared with that of the SSP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911318/fan9-2955547-large.gif
2019,8361070,Fig. 1.,Returned echo signal of sidescan sonar imagery.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he1-2819278-large.gif
2019,8361070,Fig. 2.,(a) Raw sonar image. (b) Result of MRF segmentation on (a).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he2-2819278-large.gif
2019,8361070,Fig. 3.,Single-hidden-layer feedforward neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he3-2819278-large.gif
2019,8361070,Fig. 4.,Flowchart of the proposed SE-ELM-MRF method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he4-2819278-large.gif
2019,8361070,Fig. 5.,Flowchart of an SE-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he5-2819278-large.gif
2019,8361070,Fig. 6.,Architecture of the CNN used in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he6-2819278-large.gif
2019,8361070,Fig. 7.,"(a) Raw sidescan sonar images. (b) Segmentation results of
k
-means clustering. (c) Segmentation results of MRF. (d) Corresponding ground truth images obtained using manual segmentation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he7-2819278-large.gif
2019,8361070,Fig. 8.,Flowchart shows the way to get the training data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he8-2819278-large.gif
2019,8361070,Fig. 9.,"Comparison of the segmentation results of
k
-means clustering and MRF on the six testing data sets. (a) Raw sidescan sonar images. (b) Ground truth. (c) Segmentation results of
k
-means clustering. (d) Segmentation results of MRF.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he9-2819278-large.gif
2019,8361070,Fig. 10.,"Performance comparison of ELM and SE-ELM when they are operated on the validation data set 50 times, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he10-2819278-large.gif
2019,8361070,Fig. 11.,"Comparison of the segmentation results of ELM, KELM, SVM, CNN, and SE-ELM. (a) Raw sidescan sonar images. (b) Ground truth. (c) Segmentation results of ELM. (d) Segmentation results of KELM. (e) Segmentation results of SVM. (f) Segmentation results of CNN. (g) Segmentation results of SE-ELM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he11-2819278-large.gif
2019,8361070,Fig. 12.,Comparison of the methods using MRF postprocessing. (a) Raw sidescan sonar images. (b) Ground truth. (c) Segmentation results of ELM and MRF. (d) Segmentation results of KELM and MRF. (e) Segmentation results of SVM and MRF. (f) Segmentation results of CNN and MRF. (g) Segmentation results of SE-ELM and MRF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he12-2819278-large.gif
2019,8361070,Fig. 13.,Comparison of the methods used in the experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he13-2819278-large.gif
2019,8361070,Fig. 14.,Accuracy improvement after and before MRF postprocessing. “ELM-MRF over ELM” stands for the performance improvement of ELM-MRF when it is compared with ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/48/8691630/8361070/he14-2819278-large.gif
2019,8751970,FIGURE 1.,"The time frame of two examples data of the built SCD group. (A). the first half hour in advance of a VF onset, (B). 1 minute before VF onset, and (C). 1 minute from normal ECG signal.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai1abc-2925847-large.gif
2019,8751970,FIGURE 2.,Schematic diagram of the proposed methodology on measurable arrhythmias risk marker for SCD prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai2-2925847-large.gif
2019,8751970,FIGURE 3.,"Schematic diagram of ECG signal analysis and measurement. (A). Five intervals calculation of ECG signal, (B, C) QRS complex wave detection, (D, E) T wave delineation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai3abcde-2925847-large.gif
2019,8751970,FIGURE 4.,"Examples of ECG processing of one normal segment and one SCD segment: (A). the original ECG siganl from SCD group and Normal group respectively, (B). the wave detection result of ECG siganl from SCD group and Normal group respectively, and (C). five intervals of one heart beat in an ECG signal.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai4abc-2925847-large.gif
2019,8751970,FIGURE 5.,Rules of screening of a heartbeat used in this study (A) and two examples (B) a too long length of RR interval (top) and a too short length of RR interval (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai5ab-2925847-large.gif
2019,8751970,FIGURE 6.,"Accuracy of the proposed method by using the random forest classifier during the thirty-minute prior to the SCD onset. The first five minutes are the average of the accuracy per minute, and the last twenty-five minutes is the average of the accuracy every five minutes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai6-2925847-large.gif
2019,8751970,FIGURE 7.,Results of the proposed method with the integrated SCDI during the thirty-minute prior to the SCD onset from SCD group and Normal group. SCD zone 1 and SCD zone 2 means the distribution of SCDI in SCD group and normal group means the distribution of SCDI in normal group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai7-2925847-large.gif
2019,8751970,FIGURE 8.,Variation of ECG index with the integrated SCDI for normal and SCD classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8751970/lai8-2925847-large.gif
2019,8820028,Fig. 1.,Magnetic noise in low-power parallel fluxgate magnetometer with a dc jump (samples 230–240) and two other clutter-based anomalies that bear high resemblance to dc jump.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165412/8566206/8820028/alimi1-2938463-large.gif
2019,8820028,Fig. 2.,"ROC of kernel method in shielded and cluttered environment (left-hand side), and the three detection methods in cluttered environment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165412/8566206/8820028/alimi2-2938463-large.gif
2019,8820028,Fig. 3.,"Accuracies (TP+TN) of the detection methods versus different values of SNR (top), fractions of full-sized training set used for training (middle), and subsets of features used for training (bottom).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165412/8566206/8820028/alimi3-2938463-large.gif
2019,8606256,Fig. 1.,Time-frequency OFDM frame structure exploited by the sparse machine learning based algorithms for IN recovery and cancelation in the wireless vehicular communication system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu1-2891617-large.gif
2019,8606256,Fig. 2.,Computing flowchart of the iterative sparse machine learning based algorithm of SCEM for IN recovery.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu2-2891617-large.gif
2019,8606256,Fig. 3.,Graphical visualization of one realization of the IN recovery using the proposed SCEM algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu3-2891617-large.gif
2019,8606256,Fig. 4.,MSE performance of the IN reconstruction using the proposed algorithms of SCEM and RSCEM compared with state-of-art CS-based methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu4-2891617-large.gif
2019,8606256,Fig. 5.,"MSE performance of the IN recovery using the proposed and CS-based algorithms against the measurement vector length
R
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu5-2891617-large.gif
2019,8606256,Fig. 6.,BER performance of different schemes for IN mitigation under the wireless vehicular transmission channel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu6-2891617-large.gif
2019,8606256,Fig. 7.,"Comparison of the computational complexity of the proposed and CS-based algorithms with respect to IN dimension
N
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8667487/8606256/liu7-2891617-large.gif
2019,8768319,Fig. 1.,System model of the adaptive SM-MIMO system based on machine learning techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang1-2929404-large.gif
2019,8768319,Fig. 2.,Proposed feed-forward DNN based multi-class classifier for adaptive SM-MIMOs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang2-2929404-large.gif
2019,8768319,Fig. 3.,"BER comparison of the KNN- and SVM-based TAS-SM schemes for 3 bits/symbol with
N
t
=4
,
N
r
=2
,
L=2
and QPSK, where the conventional FVG and the joint FVG are considered.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang3-2929404-large.gif
2019,8768319,Fig. 4.,"BER comparison of the proposed SVM-based TAS-SM schemes by employing different feature vectors. To be specific, the conventional FVG of (16), the separate FVG of (17) and the joint FVG of (18) are considered. Here, the parameter setup is
N
t
=8
,
N
r
=2
,
L=2
and QPSK.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang4-2929404-large.gif
2019,8768319,Fig. 5.,"BER comparison of the proposed DNN-based TAS-SM schemes when different feature vectors are employed. Here, the parameter setup is
N
t
=8
,
N
r
=2
,
L=2
and QPSK.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang5-2929404-large.gif
2019,8768319,Fig. 6.,"The effects of the feature vectors on the BER performance for the SVM-based TAS-SM, where we have
N
t
=4
,
N
r
=2
,
L=2
and QPSK. Here, beside the features given in (16)–(18), other features of the channel matrix, such as the phases of its elements as well as the correlation and the modulus of its column vectors are also adopted as the inputs of the SVM classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang6-2929404-large.gif
2019,8768319,Fig. 7.,"BER comparison of the proposed SVM and DNN-based TAS-SM schemes under different setups. Here, three setups, namely the setup A of
N
t
=8
,
N
r
=2
,
L=2
and 16-QAM, the setup B of
N
t
=16
,
N
r
=2
,
L=2
and 16-QAM, and the setup C of
N
t
=8
,
N
r
=2
,
L=2
and QPSK, are utilized.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang7-2929404-large.gif
2019,8768319,Fig. 8.,"BER comparison of the proposed SVM and DNN-based PA-SM schemes with
N
t
=2
,
N
r
=2
, and QPSK. Moreover, similar to the TAS, we also investigate the effects of the feature vectors on the BER performance in the PA-aided adaptive SM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang8-2929404-large.gif
2019,8768319,Fig. 9.,"BER comparison of the proposed machine learning based TAS-SM and the conventional optimization-driven TAS-SM schemes, such as the max-norm based TAS-SM and the optimal max-
d
min
based TAS-SM. The parameter setup is
N
t
=4
,
N
r
=2
,
L=2
and QPSK.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang9-2929404-large.gif
2019,8768319,Fig. 10.,"BER comparison of the proposed machine learning based PA-SM and the conventional optimization-driven PA-SM schemes, such as the norm-based TAS-SM and the optimal max-
d
min
based TAS-SM. The parameter setup is
N
t
=2
,
N
r
=2
and BPSK. For comparison, we also utilize the identical-throughput VBLAST and PA-aided VBLAST as benchmarkers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang10-2929404-large.gif
2019,8768319,Fig. 11.,"BER comparison of the proposed DNN-based OFDM-IM with AM, the conventional OFDM-IM and the exhaustive-search max-
d
min
based OFDM-IM with AM. The parameter setup is
(
L
IM
,
K
IM
)=(4,3)
and QPSK.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8805183/8768319/yang11-2929404-large.gif
2019,8859275,Fig. 1.,A typical scenario of reckless lane changing maneuver and its corresponding cellular automata model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8935346/8859275/fang1-2945398-large.gif
2019,8859275,Fig. 2.,The three-tier system architecture and the corresponding functions of each tier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8935346/8859275/fang2-2945398-large.gif
2019,8859275,Fig. 3.,The signaling process of CDPR service for a vehicle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8935346/8859275/fang3-2945398-large.gif
2019,8859275,Fig. 4.,The receiver operating characteristic (ROC) curves of different models for driving behavior rating. (a) SVM models under different kernel functions. (b) Decision-tree models under different predictor models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8935346/8859275/fang4-2945398-large.gif
2019,8859275,Fig. 5.,The detection accuracy as a function of the number of monitored reckless driving events.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/8935346/8859275/fang5-2945398-large.gif
2019,8633395,Fig. 1.,Flowchart of the dengue SVM-ANN genomic classifier. 1-Data acquisition was performed by Illumina genotyping of all dengue patients and then stored into a database. 2- Data preprocessing was performed to encode and normalize data into a suitable format for the Machine Learning step. 3- Feature selection was performed in order to find the best SNP feature set. 4- An ANN classifier was trained for severe dengue prognosis based on the features previously selected.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8844138/8633395/aciol1-2897285-large.gif
2019,8633395,Fig. 2.,"ANN architecture. An input layer with each one of the selected SNPs is followed by 3 hidden layers (with 5, 3, and 1 neuron, respectively), and, finally, an output layer with just one neuron representing the SD cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8844138/8633395/aciol2-2897285-large.gif
2019,8633395,Fig. 3.,"Evaluation performance using both the test-set and bolstered error estimation methods for the SVM-ANN Dengue Genome Classifier: A - Confusion Matrix; B - Accuracy, Sensitivity, Specificity, Precision, F1 Score, Area Under ROC Curve; C - Receiver Operating Characteristic (ROC) Curve; and D - Precision-Recall (PR) Curve.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8844138/8633395/aciol3-2897285-large.gif
2019,8633395,Fig. 4.,"Selected SNPs by the proposed dengue classification approach in the context of the IFN cellular pathway and Interferon stimulated genes (ISG) (in red). (A) When a virus enters the cell, the ssRNA is recognized by the OAS and RNaseL proteins, which cleaves RNA and activates RIG-1 and MDA5 receptors through the OASL protein. Another recognition mechanism is through the TLR receptors 7, 8, and 13. After this, the MYD88 adapter activates a protein cascade with by IRAK4 protein activation. Both pathways, activated by OAS and TLR8, culminate in the linking of transcription factors that will activate the interferons production pathway. (B) Then, the IFN connects to its receptor in the cell plasma membrane, activating a protein cascade that leads to the expression of antiviral effectors, such as MX1, IRF1, OAS, and RNaseL, as well as positive regulators (IRF1, OAS, RNaseL, and CD274). The latter activates the expression of other ISGs, while antiviral molecules will act at different stages of the viral cycle. (C) MX1 inhibits virus entry and replication, OAS/RNaseL and IFIT inhibit the translation of viral RNA, while IFIT also inhibits viral replication.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/8844138/8633395/aciol4-2897285-large.gif
2019,8632974,Fig. 1.,System model for the proposed Q-learning framework in cellular IoT networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8685247/8632974/sharm1-2896929-large.gif
2019,8632974,Fig. 2.,"Throughput versus
N
for the framed S-ALOHA RA scheme with and without Q-learning (
α=0.1
,
K=400
,
L=100
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8685247/8632974/sharm2-2896929-large.gif
2019,8632974,Fig. 3.,"Probability of collision versus
N
for the framed S-ALOHA with and without Q-learning (
α=0.1
,
K=400
,
L=100
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8685247/8632974/sharm3-2896929-large.gif
2019,8632974,Fig. 4.,"Convergence behavior of Q-learning schemes applied to the framed S-ALOHA for two different values of
α
(
N=400
,
K=400
,
L=100
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8685247/8632974/sharm4-2896929-large.gif
2019,8710231,FIGURE 1.,"Testing error for nonlinear dynamic system regression with hyper-parameters (C, L). (a) Gaussian function as activation function; (b) sigmoid function as activation function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li1ab-2915970-large.gif
2019,8710231,FIGURE 2.,"Number of output weights for each range varies with parameters (C,
L
). (a) Gaussian function as activation function; (b) Sigmoid function as activation function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li2ab-2915970-large.gif
2019,8710231,FIGURE 3.,Regression prediction results of nonlinear dynamic system using OAL1-ELM: (a) result of single-step prediction; and (b) result of multi-step prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li3ab-2915970-large.gif
2019,8710231,FIGURE 4.,Regression prediction testing errors of nonlinear dynamic system using OAL1-ELM: (a) result of single-step prediction; and (b) result of multi-step prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li4ab-2915970-large.gif
2019,8710231,FIGURE 5.,Test accuracy varies with the regularization parameter C and the number of hidden layer nodes L: (a) Gaussian for Landsat satellite; (b) sigmoid for Landsat satellite; (c) Gaussian for Vehicle Silhouettes; (d) sigmoid for Vehicle Silhouettes; (e) Gaussian for MAGIC Gamma Telescope; and (f) sigmoid for MAGIC Gamma Telescope.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li5abcdef-2915970-large.gif
2019,8710231,FIGURE 6.,"Number of each output weight range with optimal parameters (
C
,
L
) and Gaussian function: (a) Landsat satellite; (b) Vehicle Silhouettes; and (c) MAGIC Gamma Telescope. Each subplot in (a), (b) and (c) presents the output weights connected to the output for one class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li6abc-2915970-large.gif
2019,8710231,FIGURE 7.,Process flowchart for the fractionation of aviation kerosene.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8710231/li7-2915970-large.gif
2019,8846725,Fig. 1.,Rectifier design with the improved matching network. (a) Layout. (b) Fabricated prototype.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8846725/costa1-2943250-large.gif
2019,8846725,Fig. 2.,Efficiency and output voltage results for the rectifier with the proposed matching technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8846725/costa2-2943250-large.gif
2019,8846725,Fig. 3.,(a) Schematic of the 2.45 GHz meander monopole. (b) Photograph of the fabricated meandered-line antenna array.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8846725/costa3-2943250-large.gif
2019,8846725,Fig. 4.,Data collected and power patterns for three consecutive Tuesdays in location A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8846725/costa4-2943250-large.gif
2019,8846725,Fig. 5.,Comparison of actual power with predicted power using ANN and SVM in location A for a week period.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8846725/costa5-2943250-large.gif
2019,8846725,Fig. 6.,Locations (A–E) and predicted path using SVM for one-day period. (a) Available power. (b) Rectifier's efficiency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/8891824/8846725/costa6-2943250-large.gif
2019,8688400,FIGURE 1.,Compressive spectrum sensing in block diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza1-2909976-large.gif
2019,8688400,FIGURE 2.,"The averages of
∥r
∥
2
(up) and
|G|
(down) versus sparse coding iteration for received signals under
H
0
(left) and
H
1
(right). In (a), (b), (c), and (d), the SNR values are −5, 0, 5, and 10 dB, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza2abcd-2909976-large.gif
2019,8688400,FIGURE 3.,Classifier model training in block diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza3-2909976-large.gif
2019,8688400,FIGURE 4.,"For Algorithm 1 with a learned dictionary,
P
D
and
P
F
versus SNR for several
M
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza4-2909976-large.gif
2019,8688400,FIGURE 5.,"For Algorithm 1 with a sampled dictionary,
P
D
and
P
F
versus SNR for several
M
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza5-2909976-large.gif
2019,8688400,FIGURE 6.,The SVM learning curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza6-2909976-large.gif
2019,8688400,FIGURE 7.,"For algorithm 2 using SVM with a learned dictionary,
P
D
and
P
F
versus SNR for several
M
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza7-2909976-large.gif
2019,8688400,FIGURE 8.,"For algorithm 2 using DNN with a learned dictionary,
P
D
and
P
F
versus SNR for several
M
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza8-2909976-large.gif
2019,8688400,FIGURE 9.,"For algorithm 2 using SVM with a sampled dictionary,
P
D
and
P
F
versus SNR for several
M
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza9-2909976-large.gif
2019,8688400,FIGURE 10.,"For algorithm 2 using DNN with a sampled dictionary,
P
D
and
P
F
versus SNR for several
M
values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza10-2909976-large.gif
2019,8688400,FIGURE 11.,"Measurement setup and environment. (a) Measurement setup: transmitter, receiver, laptop computer, router, and their connections. (b) General overview of the measurement environment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8688400/nazza11ab-2909976-large.gif
2019,8631171,FIGURE 1.,Threat models: Multiple conditions for a DGA to function in a network environment where filtering results in a firewall that protects the communication and an empty cell in an Internet domain that results in an NXDOMAIN error. Note that the domains listed in the figure belong to existing live threats [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong1-2891588-large.gif
2019,8631171,FIGURE 2.,"An example sample dataset from Bambenek Consulting gives domain names, malware origins, DGA schema, and date collected.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong2-2891588-large.gif
2019,8631171,FIGURE 3.,The proposed machine learning framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong3-2891588-large.gif
2019,8631171,FIGURE 4.,Two-level model of classification and clustering [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong4-2891588-large.gif
2019,8631171,FIGURE 5.,(A) A representative example of a clustering (B) HMM training procedure (C) Workflow of the HMM model prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong5abc-2891588-large.gif
2019,8631171,FIGURE 6.,An example of the HMM model prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong6-2891588-large.gif
2019,8631171,FIGURE 7.,The proposed deep learning model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong7-2891588-large.gif
2019,8631171,FIGURE 8.,(A) Accuracy of different machine learning algorithms (B) Classification time of different machine learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong8ab-2891588-large.gif
2019,8631171,FIGURE 9.,(A) J48 classification accuracy for each DGA domain with different data sizes (B) J48 classification false positive rate [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong9ab-2891588-large.gif
2019,8631171,FIGURE 10.,(A) Clustering accuracy for each DGA (B) Clustering accuracy for each two DGA groups [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong10ab-2891588-large.gif
2019,8631171,FIGURE 11.,(A) The match accuracy for HMM models with different sequence lengths. (B) The false positive rate for HMM models with different sequence lengths.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong11ab-2891588-large.gif
2019,8631171,FIGURE 12.,"(A) J48, LSTM and DNN accuracy comparison with the number of domains ranging from 1000 to 50000. (B) J48, LSTM and DNN accuracy comparison with a large number of domains ranging from 50k to 1M.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong12ab-2891588-large.gif
2019,8631171,FIGURE 13.,"(A) Comparison of false positives among J48, LSTM and DNN. (B) ROC curves among the different number of domains, 50K, 100K, 500K, and 1M.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong13ab-2891588-large.gif
2019,8631171,FIGURE 14.,Training and validation log loss over periods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong14-2891588-large.gif
2019,8631171,FIGURE 15.,"(A) Comparison of accuracies with different learning rates, 0.00001, 0.0001, 0.001,0.01,0.05, 0.1 and 0.5. (B) Comparison of log losses with different learning rate, 0.00001, 0.0001, 0.001,0.01,0.05, 0.1 and 0.5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong15ab-2891588-large.gif
2019,8631171,FIGURE 16.,"(A) Accuracy comparison among different optimization algorithms, Gradient Descent, Adam, and Adagrad. (B) Time comparison among these different optimization algorithms, where Adagrad has the least time used.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8631171/xiong16ab-2891588-large.gif
2019,8002637,Fig. 1.,Comparison between a method without and with stability consideration. (a) Learning method that cannot ensure convergence to the attractor. (b) Learning method that can guarantee the stability of the dynamical system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou1ab-2705279-large.gif
2019,8002637,Fig. 2.,Flowchart of the dynamical system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou2-2705279-large.gif
2019,8002637,Fig. 3.,Structure of the ELM used in FSM-DS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou3-2705279-large.gif
2019,8002637,Fig. 4.,Increasing activation functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou4-2705279-large.gif
2019,8002637,Fig. 5.,Reasonability of the linearization of the activation function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou5-2705279-large.gif
2019,8002637,Fig. 7.,Qualitative performance evaluation of FSM-DS on the library of 20 human handwriting motions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou7-2705279-large.gif
2019,8002637,Fig. 6.,"Learning of demonstrations with quadratic Lyapunov functions respectively in the form of. (Left)
(1/2)
x
T
x
. (Right)
(1/2)
x
T
P
0
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou6ab-2705279-large.gif
2019,8002637,Fig. 8.,Aldebaran NAO robot.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou8-2705279-large.gif
2019,8002637,Fig. 9.,Teaching the robot by back driving it.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou9-2705279-large.gif
2019,8002637,Fig. 10.,"Trajectory of reproductions in Cartesian coordinate system. The red dotted lines represent demonstrations and the solid lines represent reproductions.
(
x
1
,
x
2
,
x
3
)
is a point in the Cartesian coordinate system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou10-2705279-large.gif
2019,8002637,Fig. 11.,NAO performs the task of dropping a small ball in the basket after avoiding an obstacle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou11-2705279-large.gif
2019,8002637,Fig. 12.,Trajectory and velocity profile of reproductions. The red dotted lines are from demonstrations and the solid lines represent reproductions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou12-2705279-large.gif
2019,8002637,Fig. 13.,"Robot follows the trajectories from the DS. Given the target point, the model can produce similar trajectories starting from arbitrary points in space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou13-2705279-large.gif
2019,8002637,Fig. 14.,Ability of the model to adapt its trajectory on-the-fly in response to a change in the target’s position.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou14-2705279-large.gif
2019,8002637,Fig. 15.,Reproduction trajectory for the moving target. The reproduction could adapt to the change of environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8715690/8002637/ou15-2705279-large.gif
2019,8608007,Fig. 1.,Concept of low-power machine learner (ML) as wake-up detector for IoT device: (a) conventional IoT device with full-functioned processing or with all data sending to cloud. (b) IoT smart device with a low-power machine learning engine as a wake-up detector to trigger data transfer/processing when non-trivial data is detected; A random projection based machine learning is used with random projection core shared with a PUF engine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen1ab-2889779-large.gif
2019,8608007,Fig. 2.,"Proposed algorithm for machine learning engine: (a) the original Extreme Learning Machine in [15], (b) the proposed modified ELM with hidden layer dimension expansion and its hardware mapping in the proposed machine learning engine.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen2ab-2889779-large.gif
2019,8608007,Fig. 3.,"Chip architecture for the proposed machine learning and PUF engine with 3 major functional blocks: a digital input layer, a current mirror cross-bar array and a digital output layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen3-2889779-large.gif
2019,8608007,Fig. 4.,"Current mirror cross-bar (CMCB) array: circuit diagrams of 3 sub-blocks including input DAC, CMCB cell and quantizer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen4-2889779-large.gif
2019,8608007,Fig. 5.,Expansion of hidden layer neurons beyond the number of physical neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen5-2889779-large.gif
2019,8608007,Fig. 6.,"Functionality of scanner unit for DE can be expressed as a matrix operation. Essentially, it creates all possible unique differences from a set of N hidden layer outputs thus effectively creating
C
N
2
expanded output nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen6-2889779-large.gif
2019,8608007,Fig. 7.,Diagram of the output layer processing element.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen7-2889779-large.gif
2019,8608007,Fig. 8.,PE-class assignment by scrambler: The assign form illustrates the index of output neuron to which each PE is assigned with different number of output neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen8-2889779-large.gif
2019,8608007,Fig. 9.,PUF response generation by the engine with binary input mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen9-2889779-large.gif
2019,8608007,Fig. 10.,Die photo and testing set-up photo of the proposed ML and PUF engine in 65nm CMOS: (a) die photo; (b) testing board stacks with FPGA board in charge of data transferring to and from PC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen10ab-2889779-large.gif
2019,8608007,Fig. 11.,Transfer curves of quantizers with respective to input value of each row in CMCB array: (a) Quantizers in unsigned mode. (b) Quantizers in signed mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen11ab-2889779-large.gif
2019,8608007,Fig. 12.,Input weight matrix characterization: (a) Normalized random input weights as a 2-D matrix. (b) Log-normal distribution of the random input weights.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen12ab-2889779-large.gif
2019,8608007,Fig. 13.,"Variation of the CMCB outputs under VDD and temperature fluctuation w.r.t nominal condition at30°C, 1.2 V: Variations are derived by getting average values of 128 CMCB outputs under different conditions and then calculating difference of the average values and the average value at nominal condition in terms of percentage, as displayed in (a) Variation under VDD fluctuation. (b) Variation under temperature fluctuation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen13ab-2889779-large.gif
2019,8608007,Fig. 14.,Error rate of on-chip inference under both unsigned and signed mode in quantizer: Error rate decreases with larger expansion factor. Note that the effective number of hidden neurons is equal to expansion factor multiplied by 128.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen14-2889779-large.gif
2019,8608007,Fig. 15.,Classification performance on MNIST data set under different VDD and temperature: error rate are derived under signed operation with ReLU and with expansion factor of 8. (a) Error rate v.s. VDD. (b) Error rate v.s. temperature variation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen15ab-2889779-large.gif
2019,8608007,Fig. 16.,Power measurement results: (a) power break-down of the system (b) energy efficiency of the CMCB array in terms of energy per MAC in different working modes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen16ab-2889779-large.gif
2019,8608007,Fig. 17.,"Measured Auto-Correlation Factor (ACF) for 500K response bits: the ACF lies mostly between the 95% confidence bounds of a white noise signal with
μ=0
and
σ=0.0014
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen17-2889779-large.gif
2019,8608007,Fig. 18.,"Reliability testing under different conditions: (a) BER under various TMV modes, INT-TMV-12/25/50/100 are INT-TMV with 12%/25%/50%/100% of quantization window length, Ext. TMV-3/5/7/ 11/13/15/17 are external TMV modes with 3/5/7/11/13/15/17 votes; (b) BER in temperature range from
−
30
∘
C
to
90
∘
C
with different
P
th
; (c) BER in supply voltage range from
0.85 V
to
1.4 V
with different
P
th
; (d) Trade-off between BER and valid CPRs with respect to
P
th
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen18a-2889779-large.gif
2019,8608007,Fig. 19.,Uniqueness testing: (a) Measured uniqueness between the response bits of test chip 1 and chip 10 in histogram; (b) Pair-wise uniqueness value for 10 chips.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen19ab-2889779-large.gif
2019,8608007,Fig. 20.,Hamming distance of 256 response bits: The ratio of inter and intra die HDs is around 55 for TMV-15 mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/8715688/8608007/chen20-2889779-large.gif
2019,8730377,Fig. 1.,"SVM algorithm equation (linear classifier) trained to classify the heartbeat condition. The horizontal axis represents the human heartbeat rate, while the vertical axis represents the human movement speed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind1-2920747-large.gif
2019,8730377,Fig. 2.,Overview of the hardware-implemented SVM architecture design.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind2-2920747-large.gif
2019,8730377,Fig. 3.,Workflow of the FPGA-based fault emulation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind3-2920747-large.gif
2019,8730377,Fig. 4.,Structure of the fault-injection node.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind4-2920747-large.gif
2019,8730377,Fig. 5.,Framework of the method used to perform the fault emulation campaign.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind5-2920747-large.gif
2019,8730377,Fig. 6.,Histogram of the critical failure rate of the injection nodes on the SVM architecture as given by 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind6-2920747-large.gif
2019,8730377,Fig. 7.,Histogram representing the correlation among the most critical failure rate [see (2)] nodes and their position relative to the SVM’s circuitry implemented in an FPGA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind7-2920747-large.gif
2019,8730377,Fig. 8.,Zynq-7000 setup under the radiation test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind8-2920747-large.gif
2019,8730377,Fig. 9.,FPGA board installed at the GENEPi2 accelerator neutron facility.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind9-2920747-large.gif
2019,8730377,Fig. 10.,Method used on the radiation test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind10-2920747-large.gif
2019,8730377,Fig. 11.,Percentage of failures that have been provoked by 11 neutron radiation-induced errors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind11-2920747-large.gif
2019,8730377,Fig. 12.,"Map of the failures provoked by the neutron radiation-induced errors in the function of the input vectors
x
⃗ 
that have been tested. The row numbers (1–11) represent the labels attributed to the radiation-induced errors and the column numbers (1–150) represent the labels attributed to the input vectors. Each color point means if the radiation-induced error provoked a critical failure (red point), a tolerable failure (blue point), or no failure (green point).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind12-2920747-large.gif
2019,8730377,Fig. 13.,Percentage of neutron radiation-induced errors that provoked 1650 tolerable and critical failures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/23/8764684/8730377/trind13-2920747-large.gif
2019,8787738,FIGURE 1.,Flowchart of reputation evaluation of crowdsourcing participants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8787738/huang1-2933147-large.gif
2019,8787738,FIGURE 2.,Confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8787738/huang2-2933147-large.gif
2019,8787738,FIGURE 3.,Accuracy of 10 fold cross validation of crowdsourcing participants’ reputation evaluation models based on ReliefF and LDA data dimension reduction methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8787738/huang3-2933147-large.gif
2019,8787738,FIGURE 4.,Accuracy of 10 fold cross validation of crowdsourcing participants’ reputation evaluation models based on PCA and MIV data dimension reduction methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8787738/huang4-2933147-large.gif
2019,8787738,FIGURE 5.,Kruskal-wallis test results of accuracy for reputation evaluation models of crowdsourcing participants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8787738/huang5-2933147-large.gif
2019,8398529,Fig. 1.,Pictorial representation of ionospheric delay and scintillation phenomena. The red dashed lines are the line-of-sight signal paths from the GNSS satellites to the receiver on earth; the green continuous signal accounts for propagation distortions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty1-2850385-large.gif
2019,8398529,Fig. 2.,"Flow diagram of the machine learning process. In the first stage, the learning process, depicted in the red box, a model is defined, using training data, selected features and a specific learning algorithm, such as a decision tree. The model is then applied on the test dataset in the classification task, in the blue box, to generate the output.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty2-2850385-large.gif
2019,8398529,Fig. 3.,"Comparison of traditional scintillation detection methods for two different scintillation events. Top panels report the trend of the
S
4
, the manual annotation, the detection results of the hard and semihard rules and the value of the
S
4
threshold
T
S
4
=0.4
. Bottom panels report the elevation and
C/
N
0
trends, and their respective thresholds used in traditional rules,
T
θ
el
=
30
∘
and
T
C/
N
0
=37dBHz
. (a) April 2, PRN 23. (b) March 26, PRN 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty3-2850385-large.gif
2019,8398529,Fig. 4.,"Correlation matrix, considering the observables of the signal.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty4-2850385-large.gif
2019,8398529,Fig. 5.,"Decision tree detection results for two different scintillation events and for set
L
1
. Top panels report the manual annotation (ground truth), the detection results of the hard and semihard rules, and the machine learning detection results. Bottom panels report the trend of the
S
4
and the value of the
S
4
threshold
T
S
4
=0.4
. (a) April 2, PRN 23. (b) March 26, PRN 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty5-2850385-large.gif
2019,8398529,Fig. 6.,Illustration of the samples averaging and window overlapping procedure used in the work.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty6-2850385-large.gif
2019,8398529,Fig. 7.,"Correlation matrix considering the features defined in set
L
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty7-2850385-large.gif
2019,8398529,Fig. 8.,Summary of the accuracy and F-score obtained for the different scintillation detection techniques presented.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty8-2850385-large.gif
2019,8398529,Fig. 9.,"Representation of the capabilities of the features defined in
L
1
to detect scintillation events.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty9-2850385-large.gif
2019,8398529,Fig. 10.,Accuracy and F-score of the decision tree algorithm versus the number of points used in the training set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty10-2850385-large.gif
2019,8398529,Fig. 11.,"Decision tree detection results for different test cases. Top panels report the manual annotation, the detection results of the hard and semihard rules, and the machine learning detection results, computed over set
L
2
. Middle panels report the trend of the
S
4
and the value of the
S
4
threshold
T
S
4
=0.4
. Bottom panels report the elevation and
C/
N
0
trends, and their respective thresholds used in traditional rules,
T
θ
el
=
30
∘
and
T
C/
N
0
=37dBHz
. (a) April 2, PRN 23. (b) March 26, PRN 10. (c) April 2, PRN 1. (d) March 26, PRN 2. (e) April 2, PRN 3. (f) March 26, PRN 6.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty11-2850385-large.gif
2019,8398529,Fig. 12.,"Example of early scintillation detection, April 2, PRN 17.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty12-2850385-large.gif
2019,8398529,Fig. 13.,"Example detection for novel and untrained data, March 25, PRN 9.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/8637087/8398529/linty13-2850385-large.gif
2019,8353460,Fig. 1.,Example of excerpts of different set of ECG recordings used in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8353460/kamal1-2832610-large.gif
2019,8353460,Fig. 2.,Block diagram of the PSPR based feature extraction and classification process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8353460/kamal2-2832610-large.gif
2019,8353460,Fig. 3.,Framework of the proposed online machine learning pipeline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8353460/kamal3-2832610-large.gif
2019,8353460,Fig. 4.,Block diagram of the scalable PSPR PTP calculation for real-time computing on Spark.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8353460/kamal4-2832610-large.gif
2019,8353460,Fig. 5.,Execution time taken by two platforms for processing different batch sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8602373/8353460/kamal5-2832610-large.gif
2019,8756062,Fig. 1.,"An illustration of the present study. Single ISFET is used as a sensor to detect both ion and light, where the signals are coupled together. Then a sequential data collecting approach at various biases is used to create a virtual sensor array, i.e. SReC, for modeling. Machine learning techniques are utilized to separate and quantify the individual sensing signals, and thus transform the ISFET to a multi-modal sensor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8859534/8756062/lin1-2927038-large.gif
2019,8756062,Fig. 2.,"An illustration of the proposed data analytical approach. First, the collected data undergoes data normalization. Then classification models for semi-quantitation and regression models for quantitation for measured data are built and validated. In the end, the proposed algorithm-assisted system is optimized by feature reduction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8859534/8756062/lin2-2927038-large.gif
2019,8756062,Fig. 3.,"(a) Transfer characteristics of the DG-ISFET under various illumination intensity from
0 μW
/cm2 to
760 μW
/cm2and pH values from 5 to 9. In this chart, Vbg is set at 0.5V and Vsg is swept from 0.2 to 1.1 V. (b) A photocurrent heat-map obtained from 760
μW
/cm2 to 0
μW
/cm2 illumination in pH5 solution, with respect to different Vbg and Vsg. (c) A pH sensitivity heat-map with respect to different Vbg and Vsg under
0 μW
/cm2 Illumination. The sensitivity is defined by current differences at fixed bias, i.e.
μA
/pH. (d) A pH-light bi-functional sensitivity heat-map matrix. Every node in the matrix represents an individual sensing behavior, i.e. virtual sensor, modulated by tuning gate voltages. The upper triangle in every node represents light sensitivity and the lower triangle presents pH sensitivity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8859534/8756062/lin3abcd-2927038-large.gif
2019,8756062,Fig. 4.,"Semi-quantification results of the dual-modal sensor by machine learning classifiers, where 20 pH-light conditions are assigned as 20 classes. (a) Classification results of support vector classifier at 2000 training epoch. (b) Classification results of back-propagation neural network classifier at 2000 training epoch. (c) Comparing the two algorithms by validating the relationship between classifying accuracy and training epoch.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8859534/8756062/lin4abc-2927038-large.gif
2019,8756062,Fig. 5.,"Comparison of self-validating quantification results: regression performance v.s. training speed. Pearson’s correlation coefficient (PCC) is adopted to evaluate the performance of regression. (a) Calculation of pH values, where SVM has better PCC and converges faster than BPNN. Both algorithm exhibit the ability to reach PCC = 0.99 before 5000 cycles. (b) Calculation of light intensity, where SVM has better PCC and converges faster than BPNN. Only BPNN can achieve PCC = 0.99 before 5000 cycles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8859534/8756062/lin5ab-2927038-large.gif
2019,8756062,Fig. 6.,"(a) An illustration of the present strategy for feature reduction. 32 features are separated into four 8-feature subgroups by Vbg value. The final subgroup of four features reduced from the 8-feature subgroup representing the least-required training feature set. The color of upper triangles represents the light sensitivity and lower triangle represents the pH sensitivity. (b) Comparison of all reduced feature set. Light calculation is used as an example. PCC is adopted as the index for evaluation. Groups of feature number equal or larger than four achieve good PCC, i.e. PCC > 0.99, which indicates four virtual sensors would be the minimal feature set to build a highly accurate model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/8859534/8756062/lin6ab-2927038-large.gif
2019,8700294,Fig. 1.,ELM network for DA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8887580/8700294/ma1-2909543-large.gif
2019,8700294,Fig. 2.,(a) and (b) Image and ground truth of PU data. (c) and (d) Image and ground truth of DC data. (e) and (f) Image and ground truth of HU data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8887580/8700294/ma2abcdef-2909543-large.gif
2019,8700294,Fig. 3.,"Sensitivity analysis of parameters in EHDA. (a) Sensitivity analysis of
γ
. (b) Sensitivity analysis of
λ
. (c) Sensitivity analysis of
α
1
. (d) Sensitivity analysis of
α
2
. (e) Sensitivity analysis of
p
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/8887580/8700294/ma3abcde-2909543-large.gif
2019,8331839,Fig. 1.,"Overnight SpO2 examples of no-SAHS (AHI < 5 e/h), mild SAHS (5 \leq AHI < 15 e/h), moderate SAHS (15 \leq AHI < 30 e/h), and severe SAHS (
AHI≥30e/h
) subjects.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8660593/8331839/gutie1-2823384-large.gif
2019,8331839,Fig. 2.,"Violin plots of each extracted feature divided by SAHS-severity degree (only training set). Numbers in x-axis represent the severity of SAHS: 1 stands for no-SAHS, 2 for mild, 3 for moderate, and 4 for severe. All p-values from Kruskal-Wallis test were corrected using the Bonferroni criterion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8660593/8331839/gutie2-2823384-large.gif
2019,8331839,Fig. 3.,"Histogram of features selected over
B=1000
bootstrap sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8660593/8331839/gutie3-2823384-large.gif
2019,8331839,Fig. 4.,"Scatter plot facing
OD
I
3
and SpecEn for the four SAHS severity degrees. Dashed lines show examples of regions in which each of the classes prevails.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8660593/8331839/gutie4-2823384-large.gif
2019,8331839,Fig. 5.,"Averaged Cohen's κ based on (a) Number of hidden neurons, N, (BY-MLP) and (b) Number of base classifiers, Q, and learning rate, υ, (AB-LDA).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8660593/8331839/gutie5-2823384-large.gif
2019,8798633,FIGURE 1.,"Example of bit vector representation of document sentences, taken from the medical dataset used in one of the experiments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8798633/berge1-2935416-large.gif
2019,8798633,FIGURE 2.,"The Tsetlin Machine architecture, introducing clause polarity, a summation operator collecting “votes,” and a threshold function arbitrating the final output.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8798633/berge2-2935416-large.gif
2019,8798633,FIGURE 3.,Six tsetlin automata with 100 states per action. Each automaton learns to either exclude or include a candidate literal (a term or its negation) in a clause.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8798633/berge3-2935416-large.gif
2019,8798633,FIGURE 4.,"Step-by-step example of Tsetlin Machine-based learning, demonstrating how clauses are composed and refined.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8798633/berge4-2935416-large.gif
2019,8798633,FIGURE 5.,The learning behavior (y-axis is score) of the Tsetlin Machine on the 20 newsgroups dataset across 200 epochs (x-axis is epochs).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8798633/berge5-2935416-large.gif
2019,8798633,FIGURE 6.,The execution time in seconds (y-axis) for the fast GPU version of the Tsetlin Machine on the 20 newsgroups dataset across 200 epochs (x-axis) on server A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8798633/berge6-2935416-large.gif
2019,8698760,FIGURE 1.,The forget gate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8698760/abdel1-2913349-large.gif
2019,8793230,Fig. 1.,"Convergence of the BiS algorithm with different settings of
γ
k
[heuristic 1:
γ
k
in (30) and heuristic 2:
γ
k
in (31)]. (a) Objective value. (b)
γ
k
. (c) Residual value
||
X
k
−
Y
k
|
|
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8793230/hu1abc-2927819-large.gif
2019,8793230,Fig. 2.,Convergence of the objective with respect to number of iterations (top) and time (bottom) on the a9a data set. (a) Objective value versus iterations. (b) Objective value versus CPU time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8793230/hu2ab-2927819-large.gif
2019,8793230,Fig. 3.,Objective versus CPU time on the MovieLens data sets. (a) MovieLens-100K. (b) MovieLens-1M. (c) MovieLens-10M.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8793230/hu3abc-2927819-large.gif
2019,8793230,Fig. 4.,Objective versus CPU time on the ORL data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8886738/8793230/hu4-2927819-large.gif
2019,8794825,Fig. 1.,"(a) A typical machine learning hardware accelerator based on analog computing unit, suffering from data conversion overhead (energy and power consumption in analog-to-digital conversion as well as digital-to-analog conversion). (b) Proposed processor architecture, which integrates an approximate analog in-memory computing coprocessor, while significantly reducing the data conversion overhead.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung1ab-2934929-large.gif
2019,8794825,Fig. 2.,"Proposed processor architecture with approximate analog in-memory computing coprocessor. Inset schematic shows a destructive ARFC read cycle, for which a subsequent refresh cycle will follow. (SCS: switched capacitor sampler, ZCB copier: zero-crossing based voltage buffering copier, SC DAC: switched capacitor digital-to-analog converter, ADC: successive approximation analog-to-digital converter,
E
i,n
: analog bus line enable.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung2-2934929-large.gif
2019,8794825,Fig. 3.,"(a) 5-stage pipeline operation of a baseline MIPS-32 variant processor when executing a floating-point multiplication, which takes 3 clock cycles. (b) Pipeline operation with analog in-memory computing coprocessor, which executes a approximate multiplication in 1 clock cycle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung3ab-2934929-large.gif
2019,8794825,Fig. 4.,"Operation of the proposed coprocessor during the execution of a MACA8 instruction (
k
= 8). (a) Handling cache miss during operand fetch cycle. (b) analog register file cache (ARFC) refresh cycle, which simultaneously occurs with the first half of the mixed-signal multiplier execution cycles. (c) Completion of the mixed-signal multiplier execution cycle. (d) Merge and write-back cycles, which eliminate the ADC operation when data dependency does not exist. (red colored signal path represents active blocks during analog pipeline operation.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung4abcd-2934929-large.gif
2019,8794825,Fig. 5.,"Analog pipeline of the proposed coprocessor, whose operation aligns with the digital pipeline of the processor core.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung5-2934929-large.gif
2019,8794825,Fig. 6.,"Analog register file based on switched-capacitor analog memory cell, which compensates analog signal line capacitance imbalance between read and write cycles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung6-2934929-large.gif
2019,8794825,Fig. 7.,"8-bit switched-capacitor DAC, which consists of a 4-bit MSB unit with a 4-bit LSB unit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung7-2934929-large.gif
2019,8794825,Fig. 8.,Mixed-signal approximate analog computing circuits (8-bit computation is performed by combining a 4-bit MSB unit with a 4-bit LSB unit). (a) 4-bit switched-capacitor multiplier. (b) 4-bit exponent function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung8ab-2934929-large.gif
2019,8794825,Fig. 9.,Zero-crossing based (ZCB) low-power voltage buffering copier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung9-2934929-large.gif
2019,8794825,Fig. 10.,"Analog signal quality degradation of an ARFC entry due to leakage current. (a) Loss of the analog signal after one processor clock cycle (Mean
=17.487μV
, Std.
=1.328μV
, Min.
=14.349μV
, Max.
=22.218μV
). (b) Number of clock cycles to keep the signal loss below 0.5 LSB (Mean = 44.93 cycles, Std. = 3.31 cycles, Min. = 35.16 cycles, Max. = 54.45). (c) Error voltage accumulation with 1000 Monte Carlo transient simulations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung10abc-2934929-large.gif
2019,8794825,Fig. 11.,"Analog signal quality degradation of an ARFC entry due to the input offset voltage of ZCB voltage copier. (a) Error accumulation on the analog signal stored in an ARFC entry after one refresh or write cycle (Mean
=−8.96μV
, Std.
=838.41μV
, Min.
=−2.853mV
, Max.
=3.215mV
). (b) Reduced error accumulation by comparator input offset calibration over +/−2.5 mV range with 6-bit resolution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung11ab-2934929-large.gif
2019,8794825,Fig. 12.,"Cross compiler customized for the vectorization of C codes for approximate analog computing instructions, which reduce the necessary memory footprint by accelerating applications without using machine learning software frameworks such as TensorFlow Lite.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung12-2934929-large.gif
2019,8794825,Fig. 13.,Data structure in the machine-dependent code generation stage of a customized cross-compiler for delayed write-back considering leakage current and process mismatch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung13-2934929-large.gif
2019,8794825,Fig. 14.,Execution time speed-up of benchmark computations with the baseline processor model employing 3/5/7-cycle 32-bit digital floating-point multipliers and the analog in-memory computing coprocessor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung14-2934929-large.gif
2019,8794825,Fig. 15.,Instruction execution profiling on benchmark computations with the baseline processor model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung15-2934929-large.gif
2019,8794825,Fig. 16.,Processor resource access profiling on benchmark computations with the baseline processor model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung16-2934929-large.gif
2019,8794825,Fig. 17.,Execution time speed-up of benchmark computations with analog vector width scaling on the proposed coprocessor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung17-2934929-large.gif
2019,8794825,Fig. 18.,Instruction execution profiling on benchmark computations with analog vector processing with the proposed coprocessor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung18-2934929-large.gif
2019,8794825,Fig. 19.,"Physical layout floorplan of the Coara coprocessor in 45nm CMOS technology, which occupies \$685 \,\mu m \times 200 \,\mu m = 0.274 \, mm^{2}\$ area.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung19-2934929-large.gif
2019,8794825,Fig. 20.,"Energy consumption comparison of Coara processor models. The baseline processor model, which uses a 32-bit floating-point multiplier for LeNet5 algorithm, is compared to the Coara processor model with various configurations on the analog in-memory computing coprocessor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung20-2934929-large.gif
2019,8794825,Fig. 21.,"Energy consumption comparison of Coara processor models. The baseline processor model, which uses a 32-bit floating-point multiplier for MobileNet algorithm, is compared to the Coara processor model with various configurations on the analog in-memory computing coprocessor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung21-2934929-large.gif
2019,8794825,Fig. 22.,Output feature map generation from input feature map and weight kernel using convolution sum computation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503868/8842576/8794825/chung22-2934929-large.gif
2019,8736251,Fig. 1.,"An edge computing system, in which training of a model parametrized by vector
w
takes place at an edge processor based on data received from a device using a protocol with timeline illustrated in Fig. 2 (OH = overhead).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8829150/8736251/skatc1-2922658-large.gif
2019,8736251,Fig. 2.,"Transmission and training protocol: when (a)
T≤
B
d
(
n
c
+
n
o
)
; and (b)
T>
B
d
(
n
c
+
n
o
)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8829150/8736251/skatc2ab-2922658-large.gif
2019,8736251,Fig. 3.,"Upper bound (14)–(15) versus block size
n
c
for various values of the overhead
n
o
. The full dots represent values of
n
c
at which we have
T=
B
d
(
n
c
+
n
o
)
(see Fig. 2), crosses represent the optimized value
n
~
c
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8829150/8736251/skatc3-2922658-large.gif
2019,8736251,Fig. 4.,"Training loss versus training time for different values of the block size
n
c
. Solid line: experimental and theoretical optima.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8829150/8736251/skatc4-2922658-large.gif
2019,8793120,Fig. 1.,The structure of kernel reinforcement learning using only subspace in RKHS spanned by the activated cluster (blue). The action is chosen probabilistically by a softmax policy. The weight is updated only from the nearest cluster to the chosen action.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8826565/8793120/wang1-2934176-large.gif
2019,8793120,Fig. 2.,"Three cases for clustering learning. The top block shows when the spike input (red dot) is far from all clusters, it will form a new cluster (red shadow) of its own. The middle block shows when the spike input (red dot) is near the clusters, it will join the nearest one (blue shadow). The centroid of that cluster will be updated (red cross). The bottom block shows when the spike input (red dot) is within the quantization threshold to the existing points (blue dot), it will be quantized, and the cluster remains unchanged.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8826565/8793120/wang2-2934176-large.gif
2019,8793120,Fig. 3.,"(a) The statistical performance of NICEJac and QAGKRL across different simulation scenarios in terms of data overlapping level. (* means the results pass the Student’s t-test with
p
< 0.05) (b) The PCA of the synthetic spike data for manual control mode (green circle) and brain control mode (orange circle). (c) The projection of the synthetic spike data in BC to the PCA space of MC. Centroids 1~4 are inherited from MC (centroid 1 and 3 not activated), and new centroids are generated and activated (centroid 5~8) in BC. (d) The learning curve from MC to BC. (e) The average convergence performance in MC using different cluster threshold. (f) The average convergence performance in BC using different cluster threshold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8826565/8793120/wang3abcdef-2934176-large.gif
2019,8793120,Fig. 4.,Average success rates with different quantization and cluster thresholds for NICEJac.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8826565/8793120/wang4-2934176-large.gif
2019,8793120,Fig. 5.,"(a) The learning curve of two algorithms on three different stages: manual control, brain control with joystick, and brain control without joystick. (b) The PCA of the spike data and the centroids generated in MC. (c) The projection of the spike data in BC with joystick to the PCA space of MC. The solid points represent the neural patterns in BC with joystick. The transparent points represent the neural patterns in MC. The centroids initially inherited from MC (centroid 1–3) are still active and updated, and new centroids are generated (centroid 4–7) in BC with joystick. After convergence at this stage, centroids 5–7 are inactive (hollow) and centroids 1–4 are activated (solid). (d) The projection of the spike data in BC to the PCA space of MC. The solid points represent the neural patterns in BC without joystick. The transparent points represent the neural patterns in the previous stages. Centroids 1–3 are initially inherited from MC. Centroids 4–7 are initially inherited from BC with joystick. The active ones are solid and the inactive ones are hollow.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8826565/8793120/wang5abcd-2934176-large.gif
2019,8627962,Fig. 1.,Flowchart of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang1-2895797-large.gif
2019,8627962,Fig. 2.,Experimental plan of the applied faults. (a) Motor 1. (b) Motor 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang2-2895797-large.gif
2019,8627962,Fig. 3.,Experimental test bench used in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang3-2895797-large.gif
2019,8627962,Fig. 4.,Experimental schematic diagram for the system setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang4-2895797-large.gif
2019,8627962,Fig. 5.,Implementation of different faults in the experimental test bench. (a) One BRB. (b) Two BRB. (c) Three BRB. (d) Bearing fault—general roughness type. (e) UNB condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang5-2895797-large.gif
2019,8627962,Fig. 6.,"Stator current I2 for Motor 2 using MP (one BRB fault, 100% loading). (a) Indices of selected coefficients. (b) Original signal and signal components. (c) Signal and its approximation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang6-2895797-large.gif
2019,8627962,Fig. 7.,"z-axis vibration signal for Motor 2 using MP (one BRB fault, 100% loading). (a) Indices of selected coefficients. (b) Original signal and signal components. (c) Signal and its approximation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang7-2895797-large.gif
2019,8627962,Fig. 8.,"One feature, Mean, versus motor loadings and different types of faults processed by OMP using the stator current I2. (a) Motor 1. (b) Motor 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang8-2895797-large.gif
2019,8627962,Fig. 9.,Processed one phase stator current signal I2 using DWT for Motor 2 under a one BRB fault and 100% loading condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang9-2895797-large.gif
2019,8627962,Fig. 10.,Processed z-axis vibration signal using DWT for Motor 2 under a one BRB fault and 100% loading condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang10-2895797-large.gif
2019,8627962,Fig. 11.,Classification accuracy for all faults implemented on Motor 1 at 100% loading using the selected classifiers. (a) Stator current I2. (b) z-axis vibration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang11-2895797-large.gif
2019,8627962,Fig. 12.,Classification accuracy for all faults implemented on Motor 2 at 100% loading using the selected classifiers. (a) Stator current I2. (b) z-axis vibration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang12-2895797-large.gif
2019,8627962,Fig. 13.,100% classification accuracy obtained by fine KNN for Motor 2 at 100% loading using the current I2. (a) Confusion matrix. (b) ROC curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang13-2895797-large.gif
2019,8627962,Fig. 14.,Curve fitting results for features of Motor 2 with a one BRB fault using the stator current I2. (a) Mean. (b) Median. (c) Standard deviation. (d) Median absolute value. (e) Mean absolute value. (f) L1 norm. (g) L2 norm. (h) Maximum norm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang14-2895797-large.gif
2019,8627962,Fig. 15.,"Curve fitting results for features of Motor 2, one BRB fault using the z-axis vibration signal. (a) Mean. (b) Median. (c) Standard deviation. (d) Median absolute value. (e) Mean absolute value. (f) L1 norm. (g) L2 norm. (h) Maximum norm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang15-2895797-large.gif
2019,8627962,Fig. 16.,"Classification accuracy for all faults using features calculated by curve fitting equations for three loadings (90%, 60%, and 20%) that has never been tested by experiments (Motor 2, the stator current I2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/28/8694107/8627962/liang16-2895797-large.gif
2019,8798758,Fig. 1.,Part of different traffic sign images in the BelgiumTSC dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8855041/8798758/zeng1-2933410-large.gif
2019,8786127,Fig. 1.,The structure of machine learning auto-MDM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8863555/8786127/ko1ab-2932417-large.gif
2019,8786127,Fig. 2.,"An exemplary clustering set for the auto-MDM when
(N,k)=(2,1)
,
M
t
∈{0,2,4}
, and
K
c
=4
. For illustration, only two dimensional sub-carrier vector is used: each ‘star’ marker represents the centroid of each cluster.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8863555/8786127/ko2-2932417-large.gif
2019,8786127,Fig. 3.,"Average BEP of the auto-MDM when
N=4,k=(1,2),
M
t
=(0,2,4),Q=500
, and
K
c
=100
. For comparison, classical IM-OFDM [3], Coordinated Interleaving IM-OFDM [5] and spread IM-OFDM [8] are depicted.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/8863555/8786127/ko3-2932417-large.gif
2019,8911382,FIGURE 1.,"(a, c) Encryption and decryption schemes of low probability of intercept communication system based on machine learning methods; (b) Training process of the decryptor based on machine learning methods; (d) Interception process based on unsupervised clustering methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911382/zheng1abcd-2955509-large.gif
2019,8911382,FIGURE 2.,"(a, b, c) Euclidean distance matrix of 93, 32 and 16 groups during mode selection process; (d, e) Field patterns and phase distributions for 16 groups used in experiment, which are {1, 2, 3, 4, 5, 6, 7, 8}, {2, 3, 4, 5, 6, 7, 8}, {1, 2, 4, 5, 6, 7, 8}, {1, 2, 3, 5, 6, 7, 8}, {1, 2, 3, 4, 6, 7, 8}, {1, 2, 3, 4, 5, 7, 8}, {1, 2, 3, 4, 5, 6, 8}, {2, 3, 4, 5, 6, 7}, {1, 2, 3, 6, 7, 8}, {2, 4, 5, 6, 7, 8}, {1, 3, 5, 6, 7, 8}, {1, 2, 4, 6, 7, 8}, {1, 2, 3, 4, 6, 8}, {1, 2, 3, 4, 5, 7}, {2, 4, 6, 7, 8} and {1, 2, 3, 5, 7}, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911382/zheng2abcde-2955509-large.gif
2019,8911382,FIGURE 3.,"(a) Accuracy of communication and interception under different sampling numbers (
K=4,8,16,32
); (b) Intercept accuracy in different directions (
δθ=
0
∘
,
10
∘
,
20
∘
,
30
∘
,
40
∘
,
50
∘
); (c) Accuracy of communication and interception under different data dimension D (
D=MK
); (d) Intercept accuracy under different data amounts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911382/zheng3abcd-2955509-large.gif
2019,8911382,FIGURE 4.,(a) Accuracy of communication and interception based on structured radio beams; (b) Accuracy of communication and interception based on beams with single PS-OAM mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8911382/zheng4ab-2955509-large.gif
2019,7931567,Fig. 1.,Structure of an RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang1-2701419-large.gif
2019,7931567,Algorithm 1:,Parameter Updating Algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang8-2701419-large.gif
2019,7931567,Fig. 2.,Single hidden layer RWNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang2-2701419-large.gif
2019,7931567,Fig. 3.,Multiple hidden layer RWNNs with different training mechanisms. (a) RWA1. (b) RBM-GI.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang3ab-2701419-large.gif
2019,7931567,Algorithm 3:,RWA1: MLFNs Training Algorithm for Classification Problems [Fig. 3(a)],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang9-2701419-large.gif
2019,7931567,Fig. 4.,Training process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang4-2701419-large.gif
2019,7931567,Algorithm 4:,RBM-GI [Fig. 3(b)],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang10-2701419-large.gif
2019,7931567,Fig. 5.,Testing accuracy on 5-hidden layer structures. (a) Pen-Based Recognition of Handwritten Digits. (b) Landsat Satellite.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang5ab-2701419-large.gif
2019,7931567,Fig. 6.,Sample images in MNIST dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang6-2701419-large.gif
2019,7931567,Fig. 7.,Sample images in ORL dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/8736908/7931567/wang7-2701419-large.gif
2019,8843936,Fig. 1.,Model of the gas sensing chamber (internal view of the chamber is shown in the inset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8859607/8843936/bhask1-2942145-large.gif
2019,8843936,Fig. 2.,Schematic diagram of the detection circuit with sensor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8859607/8843936/bhask2-2942145-large.gif
2019,8843936,Fig. 3.,Photograph of the experimental setup (sensing unit is shown in the inset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8859607/8843936/bhask3-2942145-large.gif
2019,8843936,Fig. 4.,Sensor response graph showing the voltage output signal for a test sample.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8859607/8843936/bhask4-2942145-large.gif
2019,8843936,Fig. 5.,Scatter plot and regression equation showing the relationship between the proposed sensor output voltage and the urea concentration in the saliva sample.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8859607/8843936/bhask5-2942145-large.gif
2019,8843936,Fig. 6.,ROC plot of the proposed model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8859607/8843936/bhask6-2942145-large.gif
2019,8444669,Fig. 1.,Road map of ML-based SDN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu1-2866942-large.gif
2019,8444669,Fig. 2.,The general SDN architecture. The data plane consists of physical and virtual forwarding devices. The southbound interface connects the data plane and the control plane. The control plane is the “brain” of SDN architecture. The eastbound/westbound interface enables communication among multiple controllers. The northbound interface connects the control plane and the application plane. The application plane is composed of SDN-based business applications.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu2-2866942-large.gif
2019,8444669,Fig. 3.,"OpenFlow-based SDN network. The OpenFlow controller can manage the traffic forwarding by modifying flow entries in switches’ flow tables. For example, by adding two flow entries (i.e., Entry2 and Entry3) at SW1 and SW2, the communications between 192.168.100.1 and 192.168.100.2 are allowed. However, packets from 192.168.100.3 to 192.168.100.2 are denied at SW2 due to security policies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu3-2866942-large.gif
2019,8444669,Fig. 4.,The general processing procedure of a machine learning approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu4-2866942-large.gif
2019,8444669,Fig. 5.,Common machine learning algorithms applied to SDN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu5-2866942-large.gif
2019,8444669,Fig. 6.,"Example of k-NN algorithm, for
k=
5. Among the five closest neighbors, one neighbor belongs to class A and four neighbors belong to class B. In this case, the unlabeled example will be classified into class B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu6-2866942-large.gif
2019,8444669,Fig. 7.,"A basic neural network with three layers: an input layer, a hidden layer and an output layer. An input has
m
features (i.e.,
X
1
,
X
2
,…,
X
m
) and the input can be assigned to
n
possible classes (i.e.,
Y
1
,
Y
2
,…,
Y
n
). Also,
W
l
ij
denotes the variable link weight between the
i
th neuron of layer
l
and the
j
th neuron of layer
l+1
, and
a
l
k
denotes the activation function of the
k
th neuron in layer
l
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu7-2866942-large.gif
2019,8444669,Fig. 8.,"A typical recurrent NN and its unrolled form.
X
t
is the input at time step
t
.
h
t
is the hidden state at time step
t
.
Y
t
is the output at time step
t
.
U
,
V
and
W
are parameters in the recurrent NN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu8-2866942-large.gif
2019,8444669,Fig. 9.,"An example of SVM classifier with an optimal linear hyperplane. There are two classes in the figure, and each class has one support vector. As it can be seen, there are many possible separating hyperplanes between two classes, such as
H
1
,
H
2
and
H
3
, but only one optimal separating hyperplane (i.e.,
H
2
) can maximize the margin.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu9-2866942-large.gif
2019,8444669,Fig. 10.,"Example of k-means algorithm, for
k=2
. (a) Randomly choosing two data points as two centroids; (b) label each node with the closest centroid, resulting that node A and B are a class, node C, D and E are another class; (c) assign new centroids; (d) label each node with the closest centroid again, resulting that node A, B and C are a class, node D and E are another class; (e) the algorithm is converged.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu10abcde-2866942-large.gif
2019,8444669,Fig. 11.,"A basic diagram of a RL system. The agent takes an action according to the current state and then receives a reward.
r(
s
t
,
a
t
)
denotes the immediate reward that the agent receives after performing an action
a
t
at the state
s
t
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu11-2866942-large.gif
2019,8444669,Fig. 12.,High-level SDN-enabled TrafficVision architecture and the workflow of TV Engine [148]. TV Engine has three major tasks: (1) Collecting flow statistics and ground-truth training data from end devices and access devices. (2) A decision tree classifier is applied to identify application name. (3) A k-NN classifier is applied to identify flow types.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu12-2866942-large.gif
2019,8444669,Fig. 13.,"A dynamic routing architecture [150]. The architecture consists of a ML-based meta-layer and a heuristic algorithm layer. The input of heuristic algorithm and its corresponding output are the training dataset of the ML-based meta-layer. After the training phase, the ML-based meta-layer can make the optimal routing decisions directly and independently.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu13-2866942-large.gif
2019,8444669,Fig. 14.,"The SDN-enabled overlay network with CRE [153]. CRE consists of three main modules: CRAM, NMM and PTM [154]. CRAM stands for Cognitive Routing Algorithm Module, NMM for Network Monitoring Module, PTM for Pathto-OF Translator Module. CRAM uses random NN and RL to find network paths which maximize the objective function set by the tenant’s traffic engineering policy. NMM is in charge of collecting required network state information to update the random NN in the CRAM. PTM is responsible for converting the optimal paths found by the CRAM into the appropriate OpenFlow messages, which guide the SDN controller to reroute the network traffic.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu14-2866942-large.gif
2019,8444669,Fig. 15.,"RSU cloud architecture [172]. RSU cloud is composed of traditional RSUs and RSU microdatacenters. Traditional RSUs as fixed roadside infrastructures can perform Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. RSU microdatacenters are the fundamental components of the RSU clouds, which host services to satisfy the demands of mobile vehicles. The difference between traditional RSUs and RSU microdatacenters is that additional hardware and software components are deployed in RSU microdatacenters to provide virtualization and communication capabilities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu15-2866942-large.gif
2019,8444669,Fig. 16.,"A ML-based AC method in the SDN controller [178], [179]. Each expert is an online AC algorithm, such as Greedy, Agrawal [180] and AAP-pd [181], [182]. AC Decision Maker selects the best decision from all experts according to varying traffic conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu16-2866942-large.gif
2019,8444669,Fig. 17.,"The intelligent control loop in SELFNET framework [191]. The sensors in the network generate sensor data. The monitor analyzes and aggregates the sensor data to detect network problems. The diagnoser is capable of diagnosing the root cause of network problems reported by the monitor. Based on reasons of these problems, a set of corrective and preventive methods are decided by the Decision-Maker (DM). Action Enforcer (AE) is in charge of providing implementable actions to be enforced in the network infrastructure. The orchestrator and actuator are responsible for executing the implementable actions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu17-2866942-large.gif
2019,8444669,Fig. 18.,"A typical smart home network architecture with IoT-IDM [203]. The IoT-IDM is composed of five key modules: Device Manager, Sensor Element, Feature Extractor, Detection, and Mitigation. Device Manager is a database, known as IoT Profile, to store security related information of IoT smart devices. Sensor Element is responsible for logging network activities on a target smart device. Feature Extractor can extract features from the captured network traffic. Detection is in charge of identifying suspicious activities. The Mitigation module is able to take appropriate actions before the identified attacks occur.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu18-2866942-large.gif
2019,8444669,Fig. 19.,"An example of SDN-based HTS system [215]. NCC stands for Network Control Centre, GW for gateway. The NCC/GW Manager collects the SINR information from NCCs and Queue status from GWs. These information is used by machine learning techniques to estimate the outage and congestion. Rerouting Engine decides policies based on the estimation results to modify network state through the SDN controller.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/8649699/8444669/yu19-2866942-large.gif
2019,8863922,Fig. 1.,"An example of a (a) Slot waveguide showing
E
x
field profile, (b) Strip waveguide showing
H
y
field profile, and Directional coupler showing
H
y
field profile for (c) even supermode and (d) odd supermode.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh1-2946572-large.gif
2019,8863922,Fig. 2.,"General artificial neural network (ANN) representation, i.e. one input layer, two hidden layers, and one output layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh2-2946572-large.gif
2019,8863922,Fig. 3.,The flow chart of ANN implementation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh3-2946572-large.gif
2019,8863922,Fig. 4.,"Histogram of different datasets for slot waveguide with varying (a) width of waveguides, (b)
n
eff
, and (c)
P
conf
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh4-2946572-large.gif
2019,8863922,Fig. 5.,"Mean squared error (
mse
) using training dataset-3 for (a) different number of nodes with 2 hidden layers and (b) different number of hidden layers with 50 nodes in each hidden layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh5-2946572-large.gif
2019,8863922,Fig. 6.,"Variation of (a)
n
eff
and (b)
P
conf
with waveguide width for different activation functions at waveguide height = 225 nm using training dataset-3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh6-2946572-large.gif
2019,8863922,Fig. 7.,"Mean squared error (
mse
) using (a) training dataset-3 for MLPRegressor and PyTorch (b) training, validation, and test dataset-3 for PyTorch, having 2 hidden layers with 50 nodes in each layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh7-2946572-large.gif
2019,8863922,Fig. 8.,"Slot waveguide design predicting
n
eff
at waveguide height = 225 nm with (a) PyTorch using dataset-1, (b) MLPRegressor using dataset-1, (c) PyTorch using dataset-2, and (d) MLPRegressor using dataset-2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh8-2946572-large.gif
2019,8863922,Fig. 9.,"Slot waveguide design showing contour of absolute percentage error for predicting
n
eff
at waveguide height = 225 nm with (a) PyTorch using dataset-1, (b) MLPRegressor using dataset-1, (c) PyTorch using dataset-2, and (d) MLPRegressor using dataset-2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh9-2946572-large.gif
2019,8863922,Fig. 10.,"Slot waveguide design predicting
P
conf
at waveguide height = 225 nm with (a) PyTorch using dataset-1, (b) MLPRegressor using dataset-1, (c) PyTorch using dataset-2, (d) MLPRegressor using dataset-2, (e) PyTorch using dataset-3, and (f) MLPRegressor using dataset-3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh10-2946572-large.gif
2019,8863922,Fig. 11.,"Slot waveguide design showing contour of absolute percentage error for predicting
P
conf
at waveguide height = 225 nm with (a) PyTorch using dataset-1, (b) MLPRegressor using dataset-1, (c) PyTorch using dataset-2, (d) MLPRegressor using dataset-2, (e) PyTorch using dataset-3, and (f) MLPRegressor using dataset-3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh11-2946572-large.gif
2019,8863922,Fig. 12.,"Variation of
P
conf
with width at waveguide height = 225 nm for different data sizes of training dataset-3 using MLPRegressor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh12-2946572-large.gif
2019,8863922,Fig. 13.,"Strip waveguide design (a) predicting
n
eff
at waveguide height = 230 nm, (b) showing contour of absolute percentage error for predicting
n
eff
at waveguide height = 230 nm, (c) predicting
n
eff
at waveguide width = 510 nm, (d) showing contour of absolute percentage error for predicting
n
eff
at waveguide width = 510 nm, (e) predicting
n
eff
with change in wavelength at waveguide width = 510 nm, and (f) showing contour of absolute percentage error for predicting
n
eff
at waveguide width = 510 nm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh13-2946572-large.gif
2019,8863922,Fig. 14.,"Directional coupler design (a) predicting
L
c
at waveguide height = 230 nm and (b) showing contour of absolute percentage error for predicting
L
c
at waveguide height = 230 nm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/8930658/8863922/chugh14-2946572-large.gif
2019,8651542,Fig. 1.,Schematic diagram of the relationship between the array correlation matrixes and DOA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/8648307/8651542/wu1-2901641-large.gif
2019,8651542,Fig. 2.,Schematic diagram of CWSVR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/8648307/8651542/wu2-2901641-large.gif
2019,8651542,Fig. 3.,The DOA estimation result for wideband signal by CWSVR and RSS-MUSIC. (without ambiguity).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/8648307/8651542/wu3-2901641-large.gif
2019,8651542,Fig. 4.,The DOA estimation result for wideband signal by CWSVR and RSS-MUSIC. (with ambiguity).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/8648307/8651542/wu4-2901641-large.gif
2019,8651542,Fig. 5.,Performance comparison of wideband DOA estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/8648307/8651542/wu5-2901641-large.gif
2019,8550648,Fig. 1.,Reconstructed waveform after applying ASD on guilty and innocent subjects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8425588/8550648/jamsh1abc-2883859-large.gif
2019,8550648,Fig. 2.,Sensitivity of P300 ERP component for the each subject after selection of ASD algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8425588/8550648/jamsh2-2883859-large.gif
2019,8550648,Fig. 3.,Specificity of non-P300 ERP component for the each subject after selection of ASD algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8253410/8425588/8550648/jamsh3-2883859-large.gif
2019,8890821,FIGURE 1.,Correlation between farm-specific data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8890821/lee1-2951522-large.gif
2019,8890821,FIGURE 2.,"Averaged difference between effective temperature and comfort temperature over growing period of pig for different values of DG, FI, GP, and MSY.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8890821/lee2-2951522-large.gif
2019,8890821,FIGURE 3.,"Decision tree to predict DG for full feature set. The rectangular shaped box indicates the conditional, the blue and red box denote the case that the DG of pig is larger than or equal to 0.8 kg and less than 0.8 kg, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8890821/lee3-2951522-large.gif
2019,8890821,FIGURE 4.,"Decision tree to predict DG for partial feature set. The rectangular shaped box indicates the conditional, the blue and red box denote the case that the DG of pig is larger than or equal to 0.8 kg and less than 0.8 kg, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8600701/8890821/lee4-2951522-large.gif
2019,8664630,Fig. 1.,System architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang1-2904348-large.gif
2019,8664630,Fig. 2.,"Illustration of the values of
w
i
(t)
and
w
˜
i
(t)
at node
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang2-2904348-large.gif
2019,8664630,Algorithm 1,Distributed Gradient Descent (Logical View),https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang12-2904348-large.gif
2019,8664630,Fig. 3.,Illustration of definitions in different intervals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang3-2904348-large.gif
2019,8664630,Algorithm 2,Procedure at the Aggregator,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang13-2904348-large.gif
2019,8664630,Algorithm 3,"Procedure at Each Edge Node
i",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang14-2904348-large.gif
2019,8664630,Fig. 4.,"Loss function values and classification accuracy with different
τ
. Only SVM and CNN classifiers have accuracy values. The curves show the results from the baseline with different fixed values of
τ
. Our proposed solution (represented by a single marker for each case) gives an average
τ
and loss/accuracy that is close to the optimum in all cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang4-2904348-large.gif
2019,8664630,Fig. 5.,SVM (SGD) with different numbers of nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang5-2904348-large.gif
2019,8664630,Fig. 6.,SVM (SGD) with different global aggregation times.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang6-2904348-large.gif
2019,8664630,Fig. 7.,SVM (SGD) with different total time budgets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang7-2904348-large.gif
2019,8664630,Fig. 8.,Instantaneous results of SVM (DGD) with the proposed algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang8-2904348-large.gif
2019,8664630,Fig. 9.,"Impact of
φ
on the average value of
τ
∗
in the proposed algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang9-2904348-large.gif
2019,8664630,Fig. 10.,Synchronous vs. asynchronous distributed DGD with SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang10-2904348-large.gif
2019,8664630,Fig. 11.,Synchronous vs. asynchronous distributed SGD with SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/8715825/8664630/wang11-2904348-large.gif
2019,8656568,Fig. 1.,"Geolocation of the study area in Beijing, China, with elevation and false color images generated from Landsat 8 data (R: band 5; G: band 4; B: band 3). (a) Study area A. (b) Study area B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li1-2896923-large.gif
2019,8656568,Fig. 2.,Schematic of the LST downscaling procedure by using the machine learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li2-2896923-large.gif
2019,8656568,Fig. 3.,"Comparison among the error distribution of TsHARP LST, ANN LST, SVM LST, and RF LST in (a) study area A and (b) study area B by box plots.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li3-2896923-large.gif
2019,8656568,Fig. 4.,"Spatial distribution of LSTs in study area A. (a) 990-m MODIS LST, (b) 90-m ASTER LST, (c) 90-m downscaled ANN LST, (d) 90-m downscaled RF LST, (e) 90-m downscaled SVM LST, and (f) 90-m downscaled TsHARP LST.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li4-2896923-large.gif
2019,8656568,Fig. 5.,Distribution of LST errors between the estimated and reference LSTs. (a) 90-m downscaled TsHARP LST for study area A. (b) 90-m downscaled ANN LST for study area A. (c) 90-m downscaled SVM LST for study area A. (d) 90-m downscaled RF LST for study area A. (e) 90-m downscaled TsHARP LST for study area B. (f) 90-m downscaled ANN LST for study area B. (g) 90-m downscaled SVM LST for study area B. (h) 90-m downscaled RF LST for study area B.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li5-2896923-large.gif
2019,8656568,Fig. 6.,"Spatial distribution of LSTs in study area B. (a) 990-m MODIS LST, (b) 90-m ASTER LST, (c) 90-m downscaled ANN LST, (d) 90-m downscaled RF LST, (e) 90-m downscaled SVM LST, and (f) 90-m downscaled TsHARP LST.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li6-2896923-large.gif
2019,8656568,Fig. 7.,RF variable importance scores averaged across two study areas.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8789745/8656568/li7-2896923-large.gif
2019,8458414,Fig. 1.,"The multi-metric deformable registration algorithm uses a different aggregation of metrics depending on the context, which is determined by the dominant class in the corresponding source image support area. In the example, we can observe that the liver (in yellow) and the kidney (in violet) are the dominant classes for the two highlighted control points. Note that, during training, both source (a) and target (b) semantic labels are required to compute the loss function
Δ
. However, at test time, we only require semantic labels for the source image (a) to choose the dominant class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8458414/ferra1-2869700-large.gif
2019,8458414,Fig. 2.,"Sample slices from three different volumes of the RT Parotids, RT Abdominal, and IBSR datasets.The top row represents the sample slices from three different volumes of the RT Parotids dataset. The middle row represents the sample slices of the RT Abdominal dataset, and the last row represents the sample slices from the IBSR dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8458414/ferra2-2869700-large.gif
2019,8458414,Fig. 3.,"Overlapping of the segmentation masks in different views for one registration case from RT Abdominal (first and second rows) and RT Parotids (third and fourth rows) datasets. The first column corresponds to the overlapping before registration between the source (in blue) and target (in red) segmentation masks of the different anatomical structures of both datasets. From second to sixth column, we observe the overlapping between the warped source (in green) and the target (in red) segmentation masks, for the multi-weight algorithm (MW) and for the single metric algorithm using sum of absolute differences (SAD), mutual information (MI), normalized cross correlation (NCC), and discrete wavelet transform (DWT) as similarity measure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8458414/ferra3-2869700-large.gif
2019,8458414,Fig. 4.,"Qualitative results for one slice of one registration case from IBSR dataset. In this case, since showing overlapped structures in the same image is too ambiguous given that the segmentation masks almost cover the complete image, we are showing the intensity difference between the two volumes. This is possible since images are coming from the same modality and they are normalized. The first column shows the difference of the original volumes before registration. From second to sixth column, we observe the difference between the warped source and the target images, for the multi-weight algorithm (MW) and the single metric algorithm using sum of absolute differences (SAD), mutual information (MI), normalized cross crorrelation (NCC), and discrete wavelet transform (DWT) as similarity measure.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8458414/ferra4-2869700-large.gif
2019,8458414,Fig. 5.,"Results for the RT Parotids (a) RT Abdominal (b) and IBSR (c) datasets for the single-metric registration (sad, mi, ncc, dwt) and the multi-metric registration (mw). The weights for the multi-metric registration are learned using the framework proposed in this work. The red square is the mean and the red bar is the median. It is clear from the results that using the learned linear combination of the metrics outperforms the single-metric based registration. Two baselines are included to confirm that the proposed method achieves state of the art accuracy: the orange line corresponds to Elastix and the green line to ANTs SyN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8458414/ferra5-2869700-large.gif
2019,8458414,Fig. 6.,"Example of learned weights for RT Parotids (a), RT Abdominal (b) and IBSR (c) datasets. Since the structures of interest in every dataset present different intensity distributions, different metric aggregations are learned.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8458414/ferra6-2869700-large.gif
2019,8506346,Fig. 1.,Average of de-artifacted epochs corresponding to standard (red) and deviant (blue) stimuli of a typical healthy subject at channel Fz. The obligatory N1 component is elicited for both standard and deviant stimuli while the MMN occurs only for the deviant.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman1-2877738-large.gif
2019,8506346,Fig. 2.,Subject-average signals corresponding to standard and deviant tones for patients 1 and 2 for the sites positioned at electrodes Fz and Cz.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman2-2877738-large.gif
2019,8506346,Fig. 3.,"Similarity for active P1 intervals of patients 1 and 2 vs. the P1 interval index where
β
is set to 0.5. The red and blue graphs respectively correspond to
S
Y
1
(
x
q
std
)
and
S
Y
2
(
x
q
dev
)
. The active P1 intervals are used as a predictor of eventual emergence.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman3-2877738-large.gif
2019,8506346,Fig. 4.,"Sub-session average signals corresponding to standard and deviant stimuli, at channel Fz, of patient 1 at P1 intervals (a) 13 of Session 4, and (b) 10 of Session 5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman4-2877738-large.gif
2019,8506346,Fig. 5.,"Similarities corresponding to the P1 intervals of Sessions 4 and 5 of patient 1. (a) and (c) correspond to standard stimuli, (b) and (d) correpond to deviant stimuli. P1 intervals of interest are indicated by arrows.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman5-2877738-large.gif
2019,8506346,Fig. 6.,"Sub-session average signals corresponding to standard and deviant stimuli, at channel Fz, of patient 2 at P1 interval (a) 5 of Session 2 and (b) 9 of Session 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman6-2877738-large.gif
2019,8506346,Fig. 7.,"Similarities corresponding to the P1 intervals of Sessions 2 and 3 of patient 2. (a) and (c) correpond to standard stimuli, (b) and (d) correspond to deviant stimuli. P1 intervals of interest are indicated by arrows.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8752487/8506346/arman7-2877738-large.gif
2019,8424196,Fig. 1.,Two segments for NGSIM data collection. (a) I-80 highway. (b) US-101 highway.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8725286/8424196/yang1ab-2854827-large.gif
2019,8424196,Fig. 2.,Segment for S1 data collection. (a) Video shot by UAV. (b) Chengpeng freeway.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8725286/8424196/yang2ab-2854827-large.gif
2019,8424196,Fig. 3.,Optimal combination results. (a) Gipps-BPNN model. (b) Gipps-RF model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8725286/8424196/yang3ab-2854827-large.gif
2019,8424196,Fig. 4.,Example of the ring-road tests.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6979/8725286/8424196/yang4-2854827-large.gif
2019,8758206,Fig. 1.,(a) Sketch of an alteration system. Different alteration zones surround the ore body located in the center of the system. Drill holes are represented by the red lines. (b) Example of a drill-core tray and a drill-core sample. The length of the drill-cores in the tray tends to be 1 m.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr1-2924292-large.gif
2019,8758206,Fig. 2.,Major spectral absorption bands in the VNIR-SWIR. Sample spectra from the USGS [14].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr2-2924292-large.gif
2019,8758206,Fig. 3.,SisuRock drill-core scanner equipped with an AisaFenix VNIR-SWIR HS sensor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr3-2924292-large.gif
2019,8758206,Fig. 4.,SEM-MLA instrument.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr4-2924292-large.gif
2019,8758206,Fig. 5.,Summary of the process for the acquisition of SEM data with the MLA software.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr5-2924292-large.gif
2019,8758206,Fig. 6.,Flowchart of the proposed machine learning technique to fuse HS and high-resolution mineralogical data for mapping minerals in drill-core samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr6-2924292-large.gif
2019,8758206,Fig. 7.,"Summary of the proposed resampling process: (a) Sketch of an MLA image with a grid indicating the HS pixels. For each pixel, the most dominant mineral in the MLA image is used to define the MLA resample map shown in (b). (c) Abundances associated with each of the resultant classes obtained by the proposed soft labeling strategy. (d) Spectrum of class 2, corresponding to the pixel in the upper-right corner, with the specific absorption features of each mineral in the mixture. Mineral abbreviations after [48].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr7-2924292-large.gif
2019,8758206,Fig. 8.,"RGB image of the drill-core samples used to test the proposed system. The area surrounded by red is where the thin sections for the SEM-MLA analysis were performed. The original MLA images, obtained from the SEM-MLA analysis, are shown below the drill-cores and are stretched for visualization. The pixel size in the original MLA images is about 3 
μ
m. Mineral abbreviations after [48].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr8-2924292-large.gif
2019,8758206,Fig. 9.,Classification results obtained by selecting training samples from the mosaic of the MLA resampled images and using supervised classification algorithms: Random forest (RF) and support vector Machine (SVM). Mineralogical abundances in the resultant classes are given in the upper-right bar graph. Mineral abbreviations after [48].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr9-2924292-large.gif
2019,8758206,Fig. 10.,"Mineral maps obtained by the SAM on specific wavelength ranges that encompass the unique features of the endmembers. The extreme spectral responses, considered as the endmembers, were derived by using the chain of PPI and n-D Visualizer for v1 and by using VCA, for v2. The nondiagnostic endmember represents K-feldspars, silicates, or sulfates, amongst other VNIR-SWIR nonactive minerals. Mineral abbreviations after [48].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr10-2924292-large.gif
2019,8758206,Fig. 11.,Training and test center spectra from mineral maps obtained by RF and SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/8979194/8758206/contr11-2924292-large.gif
2020,9050784,Fig. 1.,"The ROC of the proposed scheme for
P=30
dBm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9137748/9050784/sarik1-2984430-large.gif
2020,9050784,Fig. 2.,Number of active users vs. number of cooperating SUs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9137748/9050784/sarik2-2984430-large.gif
2020,9050784,Fig. 3.,Sensing error vs. Tx power and number of samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9137748/9050784/sarik3-2984430-large.gif
2020,9084095,FIGURE 1.,Flow chart of proposed framework. The red circle shows neurons which are incrementally added to the network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir1-2991810-large.gif
2020,9084095,FIGURE 2.,Different modes of online data augmentation used in this study on randomly selected image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir2-2991810-large.gif
2020,9084095,FIGURE 3.,Class incremental extreme learning machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir3-2991810-large.gif
2020,9084095,FIGURE 4.,Row 1 displays inter-class similarity in UECFOOD100. Row 2 shows intra-class variations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir4-2991810-large.gif
2020,9084095,FIGURE 5.,"Row 1 displays inter-class similarity in Food101. (a) apple pie, (b) bread pudding, (c) baklava, (d) carrot cake. Row 2 shows intra-class dissimilarity of clan-chowder.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir5abcd-2991810-large.gif
2020,9084095,FIGURE 6.,Row 1 displays inter-class similarity in UECFOOD256 and row 2 shows intra-class dissimilarity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir6-2991810-large.gif
2020,9084095,FIGURE 7.,Row 1 displays inter-class similarity in PFID and row 2 shows intra-class dissimilarity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir7-2991810-large.gif
2020,9084095,FIGURE 8.,Row 1 displays inter-class similarity in Pakistani Food and row 2 shows intra-class dissimilarity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir8-2991810-large.gif
2020,9084095,FIGURE 9.,Comparison of classification performance on model fine-tuned with augmented dataset and non-augmented dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir9-2991810-large.gif
2020,9084095,FIGURE 10.,Weights of features using Relief-F method exhibits that many features do not contribute significantly towards classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir10-2991810-large.gif
2020,9084095,FIGURE 11.,Comparison of training time for different configurations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir11-2991810-large.gif
2020,9084095,FIGURE 12.,Comparison of catastrophic forgetting measures of each session of UECFOOD 256 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir12-2991810-large.gif
2020,9084095,FIGURE 13.,Comparison of catastrophic forgetting measures of each session of Pakistani Food dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir13-2991810-large.gif
2020,9084095,FIGURE 14.,Stability Analysis of Adaptive CIELM. The grey line in the graph exhibits the incrementally added neurons when full access to all classes are given.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir14-2991810-large.gif
2020,9084095,FIGURE 15.,Stability Analysis of ARCIEKLM. The grey line in the graph exhibits the incrementally added hidden neurons when full access to all classes are given.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir15-2991810-large.gif
2020,9084095,FIGURE 16.,Stability Analysis by using configurations of previous study [53]. The grey line in the graph exhibits the incrementally added hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir16-2991810-large.gif
2020,9084095,FIGURE 17.,Comparison with other networks for PFID dataset. The red bar shows the average accuracy for incrementally trained network while blue bar shows final accuracy. The network with no red bar does not have ability to learn incrementally.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir17-2991810-large.gif
2020,9084095,FIGURE 18.,Comparison with other networks for UECFOOD100 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir18-2991810-large.gif
2020,9084095,FIGURE 19.,Comparison with other networks for Food101 dataset comparison with other networks for UECFOOD100 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9084095/tahir19-2991810-large.gif
2020,9062601,Fig. 1.,Diagram of the paper’s structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9109394/9062601/lam1-2986884-large.gif
2020,9062601,Fig. 2.,The structure of FNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9109394/9062601/lam2-2986884-large.gif
2020,9062601,Fig. 3.,Diagram of the overall structure of the experiment. The solid lines indicate the process of the implementation and the dashed lines represent the auxiliary information provided for training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9109394/9062601/lam3-2986884-large.gif
2020,9062601,Fig. 4.,"Ten classes of finger movements. The sub-figures from left to right correspond to the single finger movements (Thumb (T), Index (I), Middle (M), Ring (R) and Little (L) in this first line) and combined finger movements (ThumbIndex (TI), ThumbMiddle (TM), ThumbRing (TR), ThumbLittle (TL) and hand close (HC) in this second line), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9109394/9062601/lam4-2986884-large.gif
2020,9062601,Fig. 5.,"Windowing scheme with window size
w
and increment
ϵ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9109394/9062601/lam5-2986884-large.gif
2020,9062601,Fig. 6.,The structure of the CNN used in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9109394/9062601/lam6-2986884-large.gif
2020,8944156,Fig. 1.,"Micro-motion of an electrode array leads to appearance/disappearance of a neuron’s signals at a particular electrode as seen in recording conditions 1,2. Furthermore, in a more dire scenario as seen in recording condition 3, certain electrodes break down and stop yielding any meaningful recorded signal. Figure is not drawn to scale and is adapted from [17] (CC-BY license).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik1-2962708-large.gif
2020,8944156,Fig. 2.,"(a) Shows an NHP trained to control a joystick. Joystick movement was restricted to right, left and forward directions only. In the top portion, a multi-electrode array is shown to be implanted in the hand and arm regions of primary motor cortex [7], (b) Experiment 1 setup: NHP A was seated in a robotic wheelchair. The wheelchair translated in the forward, left or right directions in discrete time steps of 100 ms depending on the position of the joystick [7], (c) Experiment 2 setup: NHP B is trained to drive a virtual wheelchair using a joystick in a manner similar to experiment 1. Figures (a), (b) are adapted from [7] (CC-BY license) and Figure (c) is adapted from [19] (CC-BY license).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik2abc-2962708-large.gif
2020,8944156,Fig. 3.,"The principal angle is a parsimonious scalar metric to measure the similarity of neural activity across days [17], [27]. (a) and (b) present principal angles between the subspaces of neural activity recorded each day for NHPs A and B respectively. Smaller principal angles denote greater similarity and one can see recorded days being more similar to each other in NHP B than NHP A. (c) presents measured signal to noise ratio (SNR) values following the methodology given in [28] with mean (solid line) and standard deviation (shaded region) values across electrodes plotted for recorded days in NHPs A and B. As far as signal stability is concerned, we can see the average SNR values across days to be stable across days for both NHPs. NHP A shows greater variance in the SNR values among its electrodes compared to NHP B, showing that some electrodes capture better spikes than the others.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik3abc-2962708-large.gif
2020,8944156,Fig. 4.,"(a) Schematic representation of sparse ensemble machine learning is presented here. Input to each of the
M
Base Learners (BLs) -
X
j
→
is obtained by sparsely sampling entire input space -
X
⃗ 
with replacement. Outputs
o
j
→
from all
M
constituent BLs are combined to obtain the final decoded vector
o
⃗ 
, (b) Partitioning of data for training and testing to benchmark fixed models against daily retrained models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik4ab-2962708-large.gif
2020,8944156,Fig. 5.,"k-fold cross-validation to obtain optimal value of number of inputs (
U
) to each base learner as per Equation (6) for NHP A - (a), (b), (c), (d) and NHP B - (e), (f), (g), (h). The results are obtained for different number of base learners (
M
) swept from 10 to 25 across different classifiers. The optimal value of
U
comes out to be 14 (
q=2
) and 35 (
q=5
) across classifiers for NHP A and NHP B respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik5abcdefgh-2962708-large.gif
2020,8944156,Fig. 6.,"k-fold cross-validation to obtain optimal value of number of base learners (
M
) for NHP A and NHP B across different classifiers for the value of
U
obtained in Fig. 5. The optimal value of
M
comes out to be around 13 for both NHPs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik6-2962708-large.gif
2020,8944156,Fig. 7.,"Decoder results across days for NHP A - (a), (b), (c), (d) and NHP B - (e), (f), (g), (h) have been presented. Classification models and their S parse E nsemble (SE) versions have been compared alongside RF, LSTM and daily retrained-LDA models in the above plots. Outputs from constituent base learners are simply summed to arrive at the final output in SE models in the above reported case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik7abcdefgh-2962708-large.gif
2020,8944156,Fig. 8.,"Decoder results across days for NHP A - (a), (b), (c), (d) and NHP B - (e), (f), (g), (h) while dropping the most informative electrode on the test day are presented. Fixed classification models and their S parse E nsemble (SE) versions have been compared alongside RF, LSTM and daily retrained-LDA models in the above plots. Outputs from constituent base learners are simply summed to arrive at the final output in SE models in the above reported case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik8abcdefgh-2962708-large.gif
2020,8944156,Fig. 9.,"Comparing Sparse Ensemble ELM (
SE_ELM
), Sparse Ensemble MLP (
SE_MLP
) against Ensemble ELM (
Ens_ELM
), Ensemble MLP (
Ens_MLP
) with summation as a method of combining outputs in unperturbed NHP A dataset. This figure highlights that introducing sparsity in the inputs to base learners lends robustness to ensemble based learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik9-2962708-large.gif
2020,8944156,Fig. 10.,Average decoded results across test days for NHPs A and B in (a) and (b) respectively comparing summation and voting methods to arrive at the final ensemble output across different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8944156/shaik10ab-2962708-large.gif
2020,8906014,Fig. 1.,Residual block for DDAE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa1-2953620-large.gif
2020,8906014,Fig. 2.,"Overall speech dereverberation architecture using (a) conventional HELM, (b) HELM(Hwy), and (c) HELM(Res).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa2abc-2953620-large.gif
2020,8906014,Fig. 3.,Offline and online stages of the ensemble HELM (eHELM) dereverberation framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa3-2953620-large.gif
2020,8906014,Fig. 4.,"Average STOI, SRMR, FwSSNR, Cep, and LLR scores of Reverb, Wu–Wang, CDR, IDEA
D
(Res), and eHELM
D
(Res) in the matched testing conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa4-2953620-large.gif
2020,8906014,Fig. 5.,"Average STOI, SRMR, FwSSNR, Cep, and LLR scores of Reverb, Wu–Wang, CDR, IDEA
D
(Res), and eHELM
D
(Res) in the mismatched testing conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa5-2953620-large.gif
2020,8906014,Fig. 6.,"Average STOI, SRMR, FwSSNR, Cep, and LLR scores of Reverb, Wu–Wang, CDR, IDEA
D
(Res), and eHELM
D
(Res) under the matched testing conditions for the MHINT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa6-2953620-large.gif
2020,8906014,Fig. 7.,"Average STOI, SRMR, FwSSNR, Cep, and LLR scores of Reverb, Wu–Wang, CDR, IDEA
D
(Res), and eHELM
D
(Res) under the mismatched testing conditions for the MHINT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa7-2953620-large.gif
2020,8906014,Fig. 8.,"Average subjective listening scores of Wu–Wang, CDR, IDEA
D
(Res), and eHELM
D
(Res) for RT60 = 0.6, 0.7, and 1.0 s of MHINT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa8-2953620-large.gif
2020,8906014,Fig. 9.,"Average subjective listening scores of Wu–Wang, CDR, IDEA
D
(Res), and eHELM
D
(Res) for large room (RT60 = 0.7 s) of SimData with distance
∈
{Near, Far}, and for the four rooms of RealData (= Lecture, Meeting, Office, and Stairways) of REVERB challenge corpus.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9286937/8906014/hussa9-2953620-large.gif
2020,8930581,Fig. 1.,"SportSole consists of two insole modules and a data logger. The insole module includes a logic unit (A) and a multi-cell piezo-resistive sensor with embedded IMU (B). The data logger consists of a single-board computer and a small Wi-Fi router (C). The instrumented insoles are fitted inside regular sneakers, and the logic unit is housed inside a customized 3D-printed enclosure and attached to the wearer’s shoes with a clip (D).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot1abcd-2958679-large.gif
2020,8930581,Fig. 2.,Positions of the reflective markers on a subject’s sneakers [54].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot2-2958679-large.gif
2020,8930581,Fig. 3.,Experimental protocol: each 6-minute task included a 3-minute slow walking/running bout at 85% of the preferred speed and a 3-minute fast walking/running bout at 115% of the preferred speed. The sequence of the slow and fast bouts was randomized.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot3-2958679-large.gif
2020,8930581,Fig. 4.,"Experimental setup: a subject is running on a treadmill instrumented with force plates while wearing the SportSole. An eight-camera motion capture system, which is synchronized to the instrumented footwear using a custom sync board, tracks the motion of the subject’s feet.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot4-2958679-large.gif
2020,8930581,Fig. 5.,"Average left (blue) and right (red) foot trajectories projected onto the plane of progression, for a representative subject during the walking task. The annotations indicate how SL and FC were computed from the foot trajectory.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot5-2958679-large.gif
2020,8930581,Fig. 6.,"Flow chart of the proposed calibration approaches. The main difference between subject-specific and generic models is that the latter models do not require a subject’s own reference data to train the models. In subject-specific SVR models, genetic algorithm (GA) was implemented for optimal feature selection and tuning of the hyperparameters. For generic SVR models, a nested leave-one-out cross-validation was implemented in GA to find the best subset of features and the optimal hyperparameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot6-2958679-large.gif
2020,8930581,Fig. 7.,"MAE of the estimates of SL, SV, and FC for subject-specific (SS) and generic (GN) training methods. The top row shows the MAE resulting from applying models trained and tested with Session 1 data. The bottom row shows the MAE obtained by applying models trained with Session 1 data to raw data from Session 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot7-2958679-large.gif
2020,8930581,Fig. 8.,"Bland-Altman plots of SL, SV, and FC for LASSO and SVR models during walking and running tasks using SS training method. Limits of agreement are specified as average error (black and red solid lines) ±1.96SD (black and red dashed lines).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot8-2958679-large.gif
2020,8930581,Fig. 9.,"Bland-Altman plots of SL, SV, and FC for LASSO and SVR models during walking and running tasks using GN training method. Limits of agreement are specified as average error (black and red solid lines) ±1.96SD (black and red dashed lines).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot9-2958679-large.gif
2020,8930581,Fig. 10.,"Bland-Altman plots of SL, SV, and FC derived by using SS models trained with Session 1 data, applied to Session 2 data. Limits of agreement are specified as average error (black and red solid lines) ±1.96SD (black and red dashed lines).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8952818/8930581/zanot10-2958679-large.gif
2020,9246521,FIGURE 1.,Process flow chart of the machine learning implementation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid1-3035318-large.gif
2020,9246521,FIGURE 2.,"Vehicle tags corresponding to two, three and four vehicles in a platoon.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid2-3035318-large.gif
2020,9246521,FIGURE 3.,Description of Dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid3-3035318-large.gif
2020,9246521,FIGURE 4.,Mesh detail of 4-vehicle platoon for CFD study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid4-3035318-large.gif
2020,9246521,FIGURE 5.,Comparison of Loss functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid5-3035318-large.gif
2020,9246521,FIGURE 6.,New features set for polynomial regression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid6-3035318-large.gif
2020,9246521,FIGURE 7.,"Parameters of SVR, (taken from [56]),
ε=
margin,
ξ
is the distance from nearest points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid7-3035318-large.gif
2020,9246521,FIGURE 8.,Neural Network Model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid8-3035318-large.gif
2020,9246521,FIGURE 9.,Activation Functions Comparisons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid9-3035318-large.gif
2020,9246521,FIGURE 10.,ANN-I model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid10-3035318-large.gif
2020,9246521,FIGURE 11.,ANN-II model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid11-3035318-large.gif
2020,9246521,FIGURE 12.,CFD visualization of the aerodynamic characteristics of 4-vehicle platoon using pressure contours and air-velocity streamlines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid12-3035318-large.gif
2020,9246521,FIGURE 13.,Polynomial Regression training and validation loss.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid13-3035318-large.gif
2020,9246521,FIGURE 14.,Comparison of models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246521/sajid14-3035318-large.gif
2020,8731748,Fig. 1.,"Some concepts of the Geometric CG algorithm can be visualized in this figure. At first, it depicts two nearby points
X
and
Y
on a manifold
M
together with their tangent spaces
T
X
M
and
T
Y
M
(green areas). Second, it shows a CG update from the point
X
to the point
Y
in a search direction
h
along the curve
Γ
X,h
(t)
. The geodesic
Γ
X,h
(t)
in the direction of
h
connects the two points
X
and
Y
. Third, it introduces the gradient at
Y
, i.e., the euclidean gradient
∇
J
(Y)
and its projection onto the tangent space
grad
J
(Y)∈
T
Y
M
, namely, Riemannian gradient. Finally, as the new CG search direction at
Y
,
H
is computed via vector transport
T
X,th
(h)
and
grad
J
(Y)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen1-2921031-large.gif
2020,8731748,Fig. 2.,"Impact of the regularizers to the recognition rate on the USPS digits (SparLow/R refers to PCA-SparLow methods without
g
d
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen2-2921031-large.gif
2020,8731748,Fig. 3.,Trace of performance over optimization process initialized with different sparse coding methods on 15-Scenes dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen3-2921031-large.gif
2020,8731748,Fig. 4.,Comparison on recognition results with different dictionary sizes for PIE faces. The classifier is 1NN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen4-2921031-large.gif
2020,8731748,Fig. 5.,Impact of targeted low dimensionality and number of labelled samples to the recognition rate of 1NN classification on the USPS digits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen5-2921031-large.gif
2020,8731748,Fig. 6.,Trace of performance over optimization process of supervised SparLow with or without regularizers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen6-2921031-large.gif
2020,8731748,Fig. 7.,Face recognition on 68 class PIE faces. The classifier is 1NN. Randomly choose 8160 training samples and 3394 testing samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen7-2921031-large.gif
2020,8731748,Fig. 8.,Visualisation of facial features on PIE faces [33]. The presented features are generated via Eq. (63). From top to bottom: (1) PCA eigenfaces; (2) Laplacianfaces; (3) LLEfaces.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen8-2921031-large.gif
2020,8731748,Fig. 9.,"3D visualisation of PIE faces (class 5, 35, 65). From top to bottom: Applying OLPP/PCA/ONPP in original space, in sparse space with respect to initial dictionary
D
ˆ
, and in sparse space with respect to learned dictionary via SparLow, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen9-2921031-large.gif
2020,8731748,Fig. 10.,2D visualization of PIE faces (class 5).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen10-2921031-large.gif
2020,8731748,Fig. 11.,"Recognition results using proposed MFA-SparLow in PCA projected subspace on Caltech-101 dataset.
n
train
=30
,
r=3060
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9247540/8731748/shen11-2921031-large.gif
2020,9165545,FIGURE 1.,"Convergence study in linear and nonlinear cases under four different data ratios. The error rate (
y
- axis) decreases with the increase of training data (
x
- axis), and remains at a stable low level after training about 20 instances. The results show that the error rate of RBF kernel in nonlinear case is lower than that in linear case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165545/ma1-3015954-large.gif
2020,9165545,FIGURE 2.,"Classification performance under different loss parameters
a
. The tendency of error rate on all the data is displayed in red and the tendency of error rate on positive data is displayed in blue. Vertical bar represents the variance of the error rate, which is recorded after every 10 samples trained. The figures demonstrates that the proposed algorithm is insensitive with the parameter
a
on datasets Covtype and Shuttle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165545/ma2-3015954-large.gif
2020,9165545,FIGURE 3.,"Study of relationship between parameter
a
and different data ratios. The value in every block is G-mean under corresponding parameter
a
and data ratio. From the comprehensive perspective, it can be find that G-mean on the diagonal are always great, which means that a larger degree of imbalance requires a larger
a
. And the classification performance is insensitive to the parameter
a
under different class imbalance degree.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165545/ma3-3015954-large.gif
2020,9222163,FIGURE 1.,Spam detection block diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac1-3030751-large.gif
2020,9222163,Algorithm 1:,Multinomial Naïve Bayes,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac10-3030751-large.gif
2020,9222163,Algorithm 2:,Stochastic Gradient Descent,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac11-3030751-large.gif
2020,9222163,Algorithm 3:,Decision Tree - CART Algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac12-3030751-large.gif
2020,9222163,Algorithm 4:,Random Forest,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac13-3030751-large.gif
2020,9222163,Algorithm 5:,Multi-Layer Perceptron,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac14-3030751-large.gif
2020,9222163,Algorithm 6:,Base Model ImplementationWith SKFCV,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac15-3030751-large.gif
2020,9222163,FIGURE 2.,Stratified K-fold cross validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac2-3030751-large.gif
2020,9222163,Algorithm 7:,PSO Implementation,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac16-3030751-large.gif
2020,9222163,FIGURE 3.,Particle swarm optimization – accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac3-3030751-large.gif
2020,9222163,Algorithm 8:,GA Implementation,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac17-3030751-large.gif
2020,9222163,FIGURE 4.,Genetic algorithm – accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac4-3030751-large.gif
2020,9222163,FIGURE 5.,Stochastic gradient descent alpha/num comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac5-3030751-large.gif
2020,9222163,FIGURE 6.,Multinomial Naïve bayes alpha/num comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac6-3030751-large.gif
2020,9222163,FIGURE 7.,Random forest alpha/num comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac7-3030751-large.gif
2020,9222163,FIGURE 8.,Decision tree alpha/num comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac8-3030751-large.gif
2020,9222163,FIGURE 9.,Multi-layer perceptron alpha/num comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9222163/issac9-3030751-large.gif
2020,9098940,Fig. 1.,System models of edge machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9190044/9098940/liu1-2996605-large.gif
2020,9098940,Fig. 2.,"Accuracy of centralized machine learning versus number of resource blocks
N
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9190044/9098940/liu2-2996605-large.gif
2020,9098940,Fig. 3.,Accuracy of centralized machine learning versus transmitted SNR of WDs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9190044/9098940/liu3-2996605-large.gif
2020,9098940,Fig. 4.,"Accuracy of centralized machine learning versus SNR threshold
γ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9190044/9098940/liu4-2996605-large.gif
2020,9098940,Fig. 5.,"Accuracy of distributed machine learning versus ratio of dataset sizes
D
1
/
D
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9190044/9098940/liu5-2996605-large.gif
2020,9098940,Fig. 6.,"Accuracy of distributed machine learning versus number of WDs
K
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9190044/9098940/liu6-2996605-large.gif
2020,8665908,Fig. 1.,"Process diagram of steps taken during literature survey stage. After an initial preliminary survey, the review literature was categorized into three data centric categories. Each category was further surveyed in detail with additional keywords used before final screening, evaluation, and review.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8963780/8665908/zhou1-2904488-large.gif
2020,8665908,Fig. 2.,"Diagram of SVM separating hyperplane across observations of a binary class
{−1,+1}
. The optimal separating hyperplane produces the maximum margin distance between class boundaries defined as parallel hyperplanes which lie on superficially located observations called support vectors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8963780/8665908/zhou2-2904488-large.gif
2020,8665908,Fig. 3.,PCA of a gaussian distribution showing the orthogonal eigenvectors. The arrow vectors shown indicate the first and second principal component eigenvectors. The first eigenvector pointing top-right lies on the direction of greatest variance as seen within the distribution whilst the second eigenvector lies orthogonal to the first indicating the direction of second greatest variance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8963780/8665908/zhou3-2904488-large.gif
2020,8665908,Fig. 4.,"Diagram of a NN and multilayer perceptron with input, hidden and output layer. Input is passed through the network as a forward pass for error calculation before being backpropagated through the network to update weights and bias.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8963780/8665908/zhou4-2904488-large.gif
2020,8665908,Fig. 5.,"Examples of DL architectures. Circles are MLP nodes, squares are convolutional filters and diamonds are pooling layers. Note how RNNs have a recurrent connection within the hidden layers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/8963780/8665908/zhou5-2904488-large.gif
2020,8804381,Fig. 1.,"Framework of the proposed algorithm. Constraint
L
S
(
N
S
)
imposed on the output part ensures the student network to have higher confidence in the prediction than that of the teacher network. Constraint
L
G
(
N
S
)
imposed on the gradients encourages the student network to preserve its confidence in the prediction if there is perturbation on the data.
f(x)
represents the network’s prediction for the ground-truth label
y
of input
x
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8804381/chang1-2929114-large.gif
2020,8804381,Fig. 2.,Accuracies obtained by different networks trained on three data sets and under various values of SNR. (a) MNIST. (b) CIFAR-10. (c) CIFAR-100.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8804381/chang2abc-2929114-large.gif
2020,8804381,Fig. 3.,Example images (left column) and their corresponding prediction scores by different networks (right columns). (a) and (e) Pure images. (c) and (g) Their corresponding prediction scores. (b) and (f) Disturbed images. (g) and (h) Their corresponding prediction scores.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8804381/chang3abcdefgh-2929114-large.gif
2020,8804381,Fig. 4.,Accuracy of robust student network obtained by different hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8804381/chang4-2929114-large.gif
2020,8943404,FIGURE 1.,Basic-DML vs semi-DML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen1-2962525-large.gif
2020,8943404,FIGURE 2.,The relationship of three kinds of datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen2-2962525-large.gif
2020,8943404,FIGURE 3.,Cross-learning mechanism.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen3-2962525-large.gif
2020,8943404,FIGURE 4.,An example of a virtual topology consists of three training loops.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen4-2962525-large.gif
2020,8943404,FIGURE 5.,Data poison detection scheme in the basic-DML scenario.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen5-2962525-large.gif
2020,8943404,FIGURE 6.,Detection scheme in the semi-DML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen6-2962525-large.gif
2020,8943404,FIGURE 7.,Comparison of the simulation results and the model results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen7-2962525-large.gif
2020,8943404,FIGURE 8.,The probability of finding threats in the DML scenario with different training loops.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen8-2962525-large.gif
2020,8943404,FIGURE 9.,Classification accuracy of the basic-DML scenario with SVM in three cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen9-2962525-large.gif
2020,8943404,FIGURE 10.,Classification accuracy of Logistic Regression in three cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen10-2962525-large.gif
2020,8943404,FIGURE 11.,Wasted resources of different schemes in semi-DML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen11-2962525-large.gif
2020,8943404,FIGURE 12.,Waste rate of different schemes in semi-DML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen12-2962525-large.gif
2020,8943404,FIGURE 13.,Correct rate of different schemes in semi-DML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen13-2962525-large.gif
2020,8943404,FIGURE 14.,Select y of x nodes in a loop.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8943404/chen14-2962525-large.gif
2020,9277658,Fig. 1.,Scatter plots for the LS-SVM (top panel) and sparse PCE (bottom panel) surrogate models trained with increasing number of samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8566057/9328370/9277658/memon1-3042122-large.gif
2020,9277658,Fig. 2.,Comparison between the PDFs of the network nodal voltages obtained from the MC simulations as well as with the LS-SVM and sparse PCE models trained with 500 samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8566057/9328370/9277658/memon2-3042122-large.gif
2020,9195478,Fig. 1.,(a) Overall architecture of the PIM. (b) Configuration register.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8952828/9195478/mukhe1ab-3023703-large.gif
2020,9195478,Fig. 2.,(a) VMM operation with 2-bit numbers. (b) Scalable operation. (c) Flexible precision operation with 2-bit and 8-bit numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8952828/9195478/mukhe2abc-3023703-large.gif
2020,9195478,Fig. 3.,(a) Function of the floating-point module in controller. (b) Data mapping in SRAMs for floating point (7 bit). (c) Numerical example of floating-point computation with 13-bit numbers and 16-bit output.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8952828/9195478/mukhe3abc-3023703-large.gif
2020,9195478,Fig. 4.,(a) Data mapping of SRAMs for complex computation. (b) Summary of the fabricated chip with area and power distribution. (c) BER and efficiency versus frequency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8952828/9195478/mukhe4abc-3023703-large.gif
2020,9195478,Fig. 5.,(a) Inference process in RF ML application. (b) Modulation detection accuracy for RF ML application. (c) Performance in FIR filter application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8011414/8952828/9195478/mukhe5abc-3023703-large.gif
2020,9269471,Fig. 1.,"(a) Behavior training timeline. The subjects were trained to perform the one-lever-press tasks in the first 10 days. Then two microelectrodes were implanted in the M1 and mPFC separately. After 20 days’ recovery from the surgery, the rats were re-trained to pick up the one-lever-press task for 10 days and then move on to learn the new two-lever-press discrimination task for the next 30 days. (b) Two-lever-press discrimination task diagram. (The dotted box shows the one-lever-press task diagram). The trials were initialized by a high-pitch (10 kHz) or low-pitch (4 kHz) audio cue. The subjects needed to press the corresponding lever according to the given cue within 5 s and then hold the lever for 500 ms for water rewards.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang1ab-3039970-large.gif
2020,9269471,Fig. 2.,"Behavior performance on the two-lever-press discrimination task across the whole learning period. The rats were well trained with the one-lever-press task before Day 40 with successful performances reach over 80%. Then they moved on to learn the two-lever-press discrimination task in the following days, and their performance across subjects is plotted in the upper plot. The red ticks in the bottom plot show the days chosen for analysis, which represent the beginning, the middle, and the end phases of the whole learning period. Stars indicate the days that the performance has a significant increase for each subject.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang2-3039970-large.gif
2020,9269471,Fig. 3.,"The design of the internally rewarded reinforcement learning framework. For each coming trial, mPFC signals are decoded after cue onset to first classify the cue information. The M1 activity post cue onset is decoded to generate an action. Then, the post-action mPFC activity is used to extract the internal reward information given the classified cue. The classified reward or non-reward results are used to update the decoder parameters before the next trial starts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang3-3039970-large.gif
2020,9269471,Fig. 4.,The raster plots (upper) and the average firing rate (bottom) across trials ofan M1neuron and two mPFC neurons. (a) Firing activities of an M1 neuron before and after press onset forhigh (blue) and low lever press (red) respectively. (b) Firing activities of an mPFC neuron before and after cue onset for high (blue) and low cue (red) respectively. (c) Firing activities of an mPFC neuron before and after reward onset for reward (blue) and non-reward cases (red) respectively under specific cue.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang4abc-3039970-large.gif
2020,9269471,Fig. 5.,The probability distribution of the spiking occurrence of the two most important neurons considering both cue (rows) and reward information (columns). The unit of the x and y-axis is the number of spikes per 100ms and the color represents the probability of occurrence. (a) reward cases under high cue (b) non-reward cases under high cue (c) reward cases under low cue (d) non-reward cases under low cue.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang5abcd-3039970-large.gif
2020,9269471,Fig. 6.,"(a), (b) The confidence level of SVM decision values under cue(left) and reward (middle). The unit of x-axis is the decision value and y-axis is the number of occurrences. The blue and red bars show the decision values which have high confidence, while the brown bars indicate that the values with low confidence. (c) Relationship between convergence time and threshold when classifying reward information. The time to converge will increase as we increase the threshold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang6abc-3039970-large.gif
2020,9269471,Fig. 7.,"Statistical decoding results of two subjects (subject-01: top row; subject-02: bottom row) using different neural inputs and reward information. (a) and (d). Learning curve of decoding the movement intentions with AGREL under four different conditions for two subjects. The unit of the x-axis is the epoch (each contains 200 data points), and the y-axis is the decoding accuracy. (b) and (e). The decoder accuracy on the testing data under four different conditions. Internally rewarded decoder using only M1 activity as the input (green box) achieves a comparable result as externally rewarded decoder with the same input (blue box). Using mPFC and M1 activity as the input and the mPFC activity as the internal critic (red box) is significantly better than the M1 activity as input with the external reward (blue box) on subject-01 (
p=0.013
) and subject-02 (
p=0.019
) and close to using mPFC and M1 activity as the input with the external critic (orange dash line) (c) and (f). Learning curve of subject behavior and decoder from two subjects on five days throughout the learning. The x-axis is the training days, and y-axis is the success ratio (# of decoded actions that get reward out of all trials). The blue bars show the adaptive decoder performance trained with the external reward, the same as in (b) and (e). The red bars is the internally rewarded model the same as in (b) and (e). A star exists if there is a significant difference between blue and red bars (
p<0.05
). The pink bars are generated when we fix the weight of decoder as the first day (Day 41). The success ratio varies over time, and does not increase stably as the adaptive model as well as the subject behavioral learning curve.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9269471/wang7abcdef-3039970-large.gif
2020,9134722,Fig. 1.,"Left: Causal graphical model of the law school success test case [8]; race and sex are protected variables,
ϵ
∗
are latent variables, GPA, LSAT, and FYA are measured variables [9]. Right: Even when the output variable is directly modeled by latent variables, it might still have non-linear dependance on protected attributes especially by proxy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8012254/9115096/9134722/chakr1-3007845-large.gif
2020,9226466,FIGURE 1.,Contrastive learning in the Generative-Discriminative and Supervised-Unsupervised spectrum. Contrastive methods belong to the group of discriminative models that predict a pseudo-label of similarity or dissimilarity given a pair of inputs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha1-3031549-large.gif
2020,9226466,FIGURE 2.,"Contrastive learning in the Instance Discrimination pretext task for self-supervised visual representation learning. A positive pair is created from two randomly augmented views of the same image, while negative pairs are created from views of two different images. All views are encoded by the a shared encoder and projection heads before the representations are evaluated by the contrastive loss function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha2-3031549-large.gif
2020,9226466,FIGURE 3.,"Overview of the Contrastive Representation Learning framework. Its components are: a similarity and dissimilarity distribution to sample positive and negative keys for a query, one or more encoders and transform heads for each data modality and a contrastive loss function evaluate a batch of positive and negative pairs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha3-3031549-large.gif
2020,9226466,FIGURE 4.,"An intuitive diagram represents the learning signal captured by the contrastive loss through the query, positive and negative keys. Contrastive methods allow the desired invariances to be specified through the similarity and dissimilarity distributions. Each circle represents the information signal contained in each view. The signal that is not mutual between query and positive keys are invariant features, since their representations are made as similar as possible. The signal that is not mutual between the negative key and the query or positive keys are covariant features, since these representations must be able to distinguish between those to minimise similarity to the negative key.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha4-3031549-large.gif
2020,9226466,FIGURE 5.,Illustration of learning similarity between multiple modalities. Each modality has an encoder and the representations extracted by different encoders are contrasted with each other to learn a joint embedding space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha5-3031549-large.gif
2020,9226466,FIGURE 6.,Illustration of some common image augmentation methods. Different views from a random set of augmentations of the same images are usually considered positive pairs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha6-3031549-large.gif
2020,9226466,FIGURE 7.,"Illustration of extracting query and keys using the context-instance relationship. In a), the context is a global summary vector of the entire image, while the instances are the local features in the set of intermediate feature maps. In b), the past context is aggregated with a RNN contextualisation head and the instance are representations of future time steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha7-3031549-large.gif
2020,9226466,FIGURE 8.,Illustration of sampling query and keys using the sequential coherence property of video data. The positive keys are defined as frames inside a small window surrounding the query frame. The negative keys are frames from the same video but are far away in time to the query.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha8-3031549-large.gif
2020,9226466,FIGURE 9.,"Illustration of contrastive methods on clusters. In addition to individual sample’s vector, there can also have cluster prototypes with different levels of granularity. Contrastive loss can operate on both the sample and cluster level.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9226466/lekha9-3031549-large.gif
2020,9093068,Fig. 1.,"Overview of the different tasks considered in this work. Given a LUS image sequence, we propose approaches for: (orange) prediction of the disease severity score for each input frame and weakly supervised localization of pathological patterns; (pink) aggregation of frame-level scores for producing predictions on videos; (green) estimation of segmentation masks indicating pathological artifacts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9153182/9093068/demi1-2994459-large.gif
2020,9093068,Fig. 2.,The distribution of the probes and the scores of frames grouped by hospital and overall statistics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9153182/9093068/demi2-2994459-large.gif
2020,9093068,Fig. 3.,"Illustration of the architecture for frame-based score prediction. An STN modeled by
Φ
stn
predicts two transformations
θ
1
and
θ
2
which are applied to the input image producing two transformed versions
x
1
and
x
2
that localize pathological artifacts. The feature extractor
Φ
cnn
is applied to
x
1
to generate the final prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9153182/9093068/demi3-2994459-large.gif
2020,9093068,Fig. 4.,"Examples of the image crops produced by the Reg-STN network. The first column shows input images acquired with linear and convex sensors, respectively. In the second column we report the heatmaps produced by GradCam [44] and the bounding boxes obtained by thresholding. In the remaining columns, original image overlayed with bounding boxes and the two respective crops (in red and green) produced when the Reg-STN models: a) only translation and a fixed scaling; b) all possible transformations viz. translation, scaling and rotation, are shown. In each case the Reg-STN focuses on the most salient parts which contains the pathological artifacts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9153182/9093068/demi4ab-2994459-large.gif
2020,9093068,Fig. 5.,"Four examples of B-mode input image frames (first column), their annotations (second column) including COVID-19 biomarkers (moderate/score 2: orange, severe/score 3: red), and signs of healthy lung (blue). The corresponding semantic segmentations and contours of COVID-19 markers by deep learning are given in the third and fourth colomn, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9153182/9093068/demi5abcd-2994459-large.gif
2020,9093068,Fig. 6.,"Two examples (A, B) of class uncertainty in the segmentations, showing B-mode input image frames (first column), annotations (second column), including COVID-19 biomarkers (moderate/score 2: orange, severe/score 3: red), the corresponding semantic segmentations by deep learning (third column), and pixel-level COVID-19 class uncertainty by MC-dropout (fourth column).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9153182/9093068/demi6ab-2994459-large.gif
2020,9064510,FIGURE 1.,"Overview of machine learning systems, which illustrates the two phases, the learning algorithm, and different entities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue1-2987435-large.gif
2020,9064510,FIGURE 2.,Attacks on machine learning systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue2-2987435-large.gif
2020,9064510,FIGURE 3.,Overview of poisoning attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue3-2987435-large.gif
2020,9064510,FIGURE 4.,Overview of backdoor attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue4-2987435-large.gif
2020,9064510,FIGURE 5.,Pipeline of adversarial example attacks in the context of deep learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue5-2987435-large.gif
2020,9064510,FIGURE 6.,"Overview of three privacy-related attacks on machine learning models: model extraction attack, membership inference attack, and model inversion attack.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue6-2987435-large.gif
2020,9064510,FIGURE 7.,Summary of defense techniques for machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9064510/xue7-2987435-large.gif
2020,9165760,FIGURE 1.,The number of increasing and decreasing cases (trading days) in each year for the diversified financials group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav1-3015966-large.gif
2020,9165760,FIGURE 2.,The number of increasing and decreasing cases (trading days) in each year for the petroleum group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav2-3015966-large.gif
2020,9165760,FIGURE 3.,The number of increasing and decreasing cases (trading days) in each year for the diversified financials group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav3-3015966-large.gif
2020,9165760,FIGURE 4.,The number of increasing and decreasing cases (trading days) in each year for the basic metals group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav4-3015966-large.gif
2020,9165760,FIGURE 5.,Predicting stock movement with continuous data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav5-3015966-large.gif
2020,9165760,FIGURE 6.,Predicting stock movement with binary data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav6-3015966-large.gif
2020,9165760,FIGURE 7.,Schematic illustration of Decision tree [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav7-3015966-large.gif
2020,9165760,FIGURE 8.,Schematic illustration of Random forest [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav8-3015966-large.gif
2020,9165760,FIGURE 9.,Schematic illustration of SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav9-3015966-large.gif
2020,9165760,FIGURE 10.,Schematic illustration of KNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav10-3015966-large.gif
2020,9165760,FIGURE 11.,Schematic illustration of ANN [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav11-3015966-large.gif
2020,9165760,FIGURE 12.,An illustration of relationship between inputs and output for ANN [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav12-3015966-large.gif
2020,9165760,FIGURE 13.,An illustration of recurrent network [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav13-3015966-large.gif
2020,9165760,FIGURE 14.,Average of F1-Score based on average logarithmic running per sample for continuous data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav14-3015966-large.gif
2020,9165760,FIGURE 15.,Average of F1-Score based on average logarithmic running per sample for binary data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav15-3015966-large.gif
2020,9165760,FIGURE 16.,The average of F1-Score with continuous and binary data for all models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165760/mosav16-3015966-large.gif
2020,8611195,Fig. 1.,"Some visual results of our approach on the Human3.6M benchmark [5]. (a) illustrates the intermediate 3D poses estimated by the 2D-to-3D pose transformer module, (b) denotes the final 3D poses refined by the 3D-to-2D pose projector module, and (c) denotes the ground-truth. The estimated 3D joints are reprojected into the images and shown by themselves from the side view (next to the images). As shown, the predicted 3D poses in (b) have been significantly corrected, compared with (a). Best viewed in color. Note that, red and green indicate left and right, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9055268/8611195/lin1-2892452-large.gif
2020,8611195,Fig. 2.,"An overview of the proposed 3D human pose machine framework. Our model predicts the 3D human poses for the given monocular image frames, and it progressively refines its predictions with the proposed self-supervised correction. Specifically, the estimated 2D pose
p
2d
t
with the corresponding pose representation
f
2d
t
for each frame of the input sequence is first obtained and further passed into two neural network modules: i) a 2D-to-3D pose transformer module for transforming the pose representations from the 2D domain to the 3D domain to intermediately predict the human joints
p
3d
t
in the 3D coordinates, and ii) a 3D-to-2D pose projector module to obtain the projected 2D pose
p
^
2d
t
after regressing
p
3d
t
into
p
^
3d
t
. Through minimizing the difference between
p
2d
t
and
p
^
2d
t
, our model is capable of bidirectionally refining the regressed 3D poses
p
^
3d
t
via the proposed self-supervised correction mechanism. Note that the parameters of the 2D-to-3D pose transformer module for all frames are shared to preserve the temporal motion coherence.
3K
and
2K
denotes the dimension of the vector for representing the 3D and 2D human pose formed by
K
skeleton joints, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9055268/8611195/lin2-2892452-large.gif
2020,8611195,Fig. 3.,"Detailed sub-network architecture of our proposed 3D-to-2D pose projector module in the (a) training phase and (b) testing phase. The Fully Connected (FC) layers for the regression function are in blue, while those for the projection function are in yellow. The black arrows represent the forward data flow, while the dashed arrows denote the backward propagation used to update the network parameters and perform gradual pose refinement in (a) and (b), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9055268/8611195/lin3-2892452-large.gif
2020,8611195,Fig. 4.,"Qualitative comparisons on the Human3.6M dataset. The 3D poses are visualized from the side view, and the cameras are depicted. The results from Zhou et al. [14], Pavlakos et al. [19], Lin et al. [21], Zhou et al. [23], Tome et al. [20], our model and the ground truth are illustrated from left to right. Our model achieves considerably more accurate estimations than all the compared methods. Best viewed in color. Red and green indicate left and right, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9055268/8611195/lin4-2892452-large.gif
2020,8611195,Fig. 5.,"Qualitative comparisons of ours and ours w/o self-correction on the Human3.6M dataset. The input image, estimated 2D pose, ours w/o self-correction, ours and ground truth are listed from left to right, respectively. With the ground truth as reference, one can easily observe that the inaccurately predicted human 3D joints in ours w/o self-correction are effectively corrected in ours. Best viewed in color. Red and green indicate left and right, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9055268/8611195/lin5-2892452-large.gif
2020,8611195,Fig. 6.,"Some qualitative comparisons of our model and Zhou et al. [23] on two representative datasets in the wild, i.e., KTH Football II [60] (first row) and MPII datasets [48] (the remaining rows). For each image, the original viewpoint and a better viewpoint are illustrated. Best viewed in color. Red and green indicate left and right, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9055268/8611195/lin6-2892452-large.gif
2020,8998227,Fig. 1.,"Scattered field (z-directed) solution comparison between weak solution obtained using predicted macro basis functions, actual solution, and second-order weak solution. (a) Real component. (b) Imaginary component. Predicted solution using the proposed macro basis function approach agrees almost perfectly with the actual solution, despite using only 14% as many basis functions. The second-order solution shown uses the same number of basis functions as the predicted solution but does not agree with the actual solution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9070031/8998227/notar1-2973937-large.gif
2020,8998227,Fig. 2.,"Scattered field (z-directed) solution comparison between weak solution obtained using predicted macro basis functions, actual solution, and naïve predicted solution. (a) Real component. (b) Imaginary component. Naïve predicted solution is obtained by plotting the solution directly predicted by the network without the macro basis function approach.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9070031/8998227/notar2-2973937-large.gif
2020,8998227,Fig. 3.,"(a) Real and (b) imaginary RMS error histograms for all 1000 validation problems. Predicted case is for the proposed macro basis function approach. Naïve case gives the error of the solutions directly predicted with the network using no macro basis function approach (the typical, existing approach). The second-order case serves as a comparison to the proposed approach. The second-order case and predicted case use the same number of basis functions, but the proposed method yields error an order of magnitude lower.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9070031/8998227/notar3-2973937-large.gif
2020,8827632,Fig. 1.,Papers about integrated application using deep learning and RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu1-2940415-large.gif
2020,8827632,Fig. 2.,Papers about image feature extraction using deep learning and RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu2-2940415-large.gif
2020,8827632,Fig. 3.,Basic network structure of RBM including m visible units and n hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu3-2940415-large.gif
2020,8827632,Fig. 4.,"Symmetric triangular membership function
w
~
ij
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu4-2940415-large.gif
2020,8827632,Fig. 5.,FRBM framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu5-2940415-large.gif
2020,8827632,Fig. 6.,"Relation of RBM, FRBM, RRBM, and F3RBM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu6-2940415-large.gif
2020,8827632,Fig. 7.,Algorithm flowchart for F3RBM to realize data classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu7-2940415-large.gif
2020,8827632,Fig. 8.,"Comparing the average reconstruction error of RBM, FRBM, and F3RBM in the training process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu8-2940415-large.gif
2020,8827632,Fig. 9.,"MNIST original sample (left) and the reconstruction result (right) using regular RBM (ALT = 210.3 S, RMSE = 70.0).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu9-2940415-large.gif
2020,8827632,Fig. 10.,"MNIST original sample (left) and the reconstruction result (right) using FRBM (ALT = 356.8 S, RMSE = 57.2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu10-2940415-large.gif
2020,8827632,Fig. 11.,"MNIST training sample (left) and the reconstruction result (right) using F3RBM (ALT = 283.6 S, RMSE = 57.7).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu11-2940415-large.gif
2020,8827632,Fig. 12.,Convergence performance of different number of hidden units during training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu12-2940415-large.gif
2020,8827632,Fig. 13.,Class names and example images in Fashion-MNIST dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu13-2940415-large.gif
2020,8827632,Fig. 14.,Reconstruction errors of mentioned models under different hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu14-2940415-large.gif
2020,8827632,Fig. 15.,(a) Original images. (b) Reconstructed images by RBM. (c) Reconstructed images by FRBM. (d) Reconstructed images by F3RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu15-2940415-large.gif
2020,8827632,Fig. 16.,Partial image from the Olivetti Faces dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu16-2940415-large.gif
2020,8827632,Fig. 17.,Reconstruction errors of mentioned models under 200 and 1000 hidden units.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu17-2940415-large.gif
2020,8827632,Fig. 18.,"Four randomly selected images and their reconstruction images. (a) Original images. (b) Reconstructed images by RBM (ALT = 122.62 S, RMSE = 13.83). (c) Reconstructed images by FRBM (ALT = 297.34 S, RMSE = 9.35). (d) Reconstructed images by F3RBM (ALT = 248.17 S, RMSE = 9.39).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu18-2940415-large.gif
2020,8827632,Fig. 19.,Some face images with different signal-noise ratios. (a) Original images (b) 10% random noises. (c) 30% random noises. (d) 50% random noises.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu19-2940415-large.gif
2020,8827632,Fig. 20.,Faces pictures under different types of noise. (a) Gaussian noise (the mean is 0 and the variance is 0.02). (b) Salt and pepper noise (the density is 0.01). (c) Poisson noise. (d) Speckle noise (the density is 0.05).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/91/9214550/8827632/lu20-2940415-large.gif
2020,9256259,FIGURE 1.,"PPO rewards and conditioning
ψ
in PyBullet environments with different hyperparameters. Each curve is obtained by averaging the results of 30 agents (10 for each seed).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9256259/asadu1-3037276-large.gif
2020,9256259,FIGURE 2.,"Comparison of PPO and PPO with conditioning
ψ
in PyBullet environments. Each curve is obtained by averaging results of 30 agents (10 for each seed).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9256259/asadu2-3037276-large.gif
2020,9256259,FIGURE 3.,"Comparison of TRPO and TRPO with conditioning
ψ
in PyBullet environments. Each curve is obtained by averaging results of 30 agents (10 for each seed).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9256259/asadu3-3037276-large.gif
2020,9256259,FIGURE 4.,PPO and PPO with conditioning regularization. Success rate on CoinRun environment on train and test levels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9256259/asadu4-3037276-large.gif
2020,9256259,FIGURE 5.,"Results on PyBullet Environments for PPO and PPO reg and JPO, on optimal hyperparameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9256259/asadu5-3037276-large.gif
2020,9072352,Fig. 1.,"Architecture of LeNet-5, a CNN, for digit recognition [35].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung1-2988253-large.gif
2020,9072352,Fig. 2.,Examples of transforming one-dimensional vectors into images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung2-2988253-large.gif
2020,9072352,Fig. 3.,Architecture information of DNN and CNN models in the first experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung3-2988253-large.gif
2020,9072352,Fig. 4.,Flowchart of learning image recognition experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung4-2988253-large.gif
2020,9072352,Fig. 5.,Order of variable arrangements in learning image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung5-2988253-large.gif
2020,9072352,Fig. 6.,Three types of visualized images of two students in one Medical Terminology course.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung6-2988253-large.gif
2020,9072352,Fig. 7.,Three types of visualized images of two students in one American History course.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9199389/9072352/hung7-2988253-large.gif
2020,8939363,Fig. 1.,"(A) Illustrations of different hand gestures [15], and (B) locations of the twelve Delsys EMG sensors on the upper limb [26].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid1ab-2961706-large.gif
2020,8939363,Fig. 2.,"Illustrations of the sliding window with overlapping and majority voting strategy. Here, the MV uses five outcomes from a classifier.
T
a
, window size;
T
d
, the processing time for feature extraction and machine learning algorithm;
T
inc
, increment size; MV, majority voting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid2-2961706-large.gif
2020,8939363,Fig. 3.,Flowchart showing offline classification using both single-window and multi-window majority-voting scheme with the cross-validation technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid3-2961706-large.gif
2020,8939363,Fig. 4.,"(A) Majority voting scheme using the proposed multi-window strategy with 80% overlapping, (B) influence of different window and overlapping sizes on the machine learning performance (p<0.05). The legend shows the window size in ms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid4ab-2961706-large.gif
2020,8939363,Fig. 5.,Spearman’s rank-order correlation coefficients between accuracies from each window size and different overlapping sizes ranging from 0% to 80%. Whiskers indicate one standard error of the mean.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid5-2961706-large.gif
2020,8939363,Fig. 6.,"Comparison between classification accuracies without any post-processing, with single-window majority voting, and with multi-window majority voting is plotted on the left y-axis, while total processing time for feature extraction, machine learning algorithm, and different majority voting schemes is presented on the right y-axis for machine learning algorithms including (A) k-Nearest Neighbor (kNN), (B) Linear Discriminant Analysis (LDA), (C) Logistic Regression (LR), (D) Naïve Bayes (NB), (E) Support Vector Machine (SVM), and (F) Random Forest (RF). Whiskers indicate one standard error of the mean. MV, majority voting; SWMV, single-window majority voting; MWMV, multi-window majority voting; and pt, processing time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid6abcdef-2961706-large.gif
2020,8939363,Fig. 7.,"Histograms of mean accuracy differences between MWMV and SWMV for 30ms and 50ms sliding window sizes with 80% overlapping using machine learning algorithms including (A) kNN, (B) LDA, (C) LR, (D) NB, (E) SVM, and (F) RF. See Fig. 6 caption for the abbreviations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid7abcdef-2961706-large.gif
2020,8939363,Fig. 8.,"EMG data segment for the majority voting scheme in [5]. Here, the majority vote based decision is taken at
d
point using the window size
T
a
=256ms
,
T
inc
=τ=16ms
, total votes
(2m+1)=17
, and
T
d
=128
ms. Therefore, the total data segment length becomes 512ms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/8995668/8939363/wahid8-2961706-large.gif
2020,8993757,FIGURE 1.,System model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he1-2973253-large.gif
2020,8993757,FIGURE 2.,The structure of the deep neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he2-2973253-large.gif
2020,8993757,FIGURE 3.,The structure of the deep neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he3-2973253-large.gif
2020,8993757,FIGURE 4.,Data generation and training process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he4-2973253-large.gif
2020,8993757,FIGURE 5.,Batch size selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he5-2973253-large.gif
2020,8993757,FIGURE 6.,Learning rate selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he6-2973253-large.gif
2020,8993757,FIGURE 7.,SE versus maximum transmit power in scenario I when the objective problem is maximizing SE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he7-2973253-large.gif
2020,8993757,FIGURE 8.,EE versus maximum transmit power in scenario I when the objective problem is maximizing SE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he8-2973253-large.gif
2020,8993757,FIGURE 9.,SE versus maximum transmit power in scenario II when the objective problem is maximizing SE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he9-2973253-large.gif
2020,8993757,FIGURE 10.,EE versus maximum transmit power in scenario I when the objective problem is maximizing SE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he10-2973253-large.gif
2020,8993757,FIGURE 11.,SE versus maximum transmit power in scenario I when the objective problem is maximizing EE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he11-2973253-large.gif
2020,8993757,FIGURE 12.,EE versus maximum transmit power in scenario I when the objective problem is maximizing EE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he12-2973253-large.gif
2020,8993757,FIGURE 13.,SE versus maximum transmit power in scenario II when the objective problem is maximizing EE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he13-2973253-large.gif
2020,8993757,FIGURE 14.,EE versus maximum transmit power in scenario II when the objective problem is maximizing EE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8993757/he14-2973253-large.gif
2020,9165721,FIGURE 1.,Normal cornea versus a cornea affected by keratoconus [6].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri1-3016060-large.gif
2020,9165721,FIGURE 2.,Machine learning algorithms that can be used in keratoconus detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri2-3016060-large.gif
2020,9165721,FIGURE 3.,Logical diagram of the machine learning setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri3-3016060-large.gif
2020,9165721,FIGURE 4.,Classes cluster for the machine learning algorithm with 2 classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri4-3016060-large.gif
2020,9165721,FIGURE 5.,Classes cluster for the machine learning algorithm with 3 classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri5-3016060-large.gif
2020,9165721,FIGURE 6.,Receiver operating characteristic ROC curve of the 2-class problem (normal group versus combined forme fruste or keratoconus).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri6-3016060-large.gif
2020,9165721,FIGURE 7.,Receiver operating characteristic ROC curve of the 2-class problem (normal versus forme fruste keratoconus).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri7-3016060-large.gif
2020,9165721,FIGURE 8.,Confusion matrix of the selected machine learning for the 2-class problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri8-3016060-large.gif
2020,9165721,FIGURE 9.,Confusion matrix of the selected machine learning for the 3-class problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri9-3016060-large.gif
2020,9165721,FIGURE 10.,Machine-learning parallel coordinate plot of the developed machine learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9165721/lavri10-3016060-large.gif
2020,9127444,Fig. 1.,Types of RFID-based sensor: (a) single-tag and (b) multi-tag systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim1ab-3004035-large.gif
2020,9127444,Fig. 2.,Operation principle of the RFID-based sensor tag: (a) equivalent circuit model of the sensor tag and (b) frequency response of a sensor tag.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim2ab-3004035-large.gif
2020,9127444,Fig. 3.,Read-range analysis of RFID-based sensor tags.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim3-3004035-large.gif
2020,9127444,Fig. 4.,(a) SVM classification and (b) its application to chipless RFID reader.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim4ab-3004035-large.gif
2020,9127444,Fig. 5.,"(a) Printed chipless RFID tags, (b) wired measurement of tag ID for SVM machine learning data set collection and (c) measurement setup for tag reading.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim5abc-3004035-large.gif
2020,9127444,Fig. 6.,Simplified building block of artificial neural networks (ANN).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim6-3004035-large.gif
2020,9127444,Fig. 7.,"Simplified
N
-stage active matching circuit topology for the proposed smart adaptive matching network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim7-3004035-large.gif
2020,9127444,Fig. 8.,(a) Fabricated Rx coil and (b) stacked concentric three Tx coils.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim8ab-3004035-large.gif
2020,9127444,Fig. 9.,"Calculated
|
S
11
|
according to distance between Tx and Rx coils.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim9-3004035-large.gif
2020,9127444,Fig. 10.,Power transfer efficiency with and without the selective Tx.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim10-3004035-large.gif
2020,9127444,Fig. 11.,(a) Schematic of the proposed WPT system with a drone and (b) its position measurement setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim11ab-3004035-large.gif
2020,9127444,Fig. 12.,"(a) A unit Tx charging coil and (b) a fabricated 2
×
2 Tx coil array prototype.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim12ab-3004035-large.gif
2020,9127444,Fig. 13.,Drone position prediction at (a) 1-inch height and (b) 1.25-inch height.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433271/9177370/9127444/kim13ab-3004035-large.gif
2020,9167200,FIGURE 1.,Demonstration of major types of Breast Cancer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9167200/liu1-3016715-large.gif
2020,9167200,FIGURE 2.,Classification of Algorithms in Machine Learning [62].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9167200/liu2-3016715-large.gif
2020,9167200,FIGURE 3.,Paper selection process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9167200/liu3-3016715-large.gif
2020,9167200,FIGURE 4.,Number of papers per year.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9167200/liu4-3016715-large.gif
2020,8956077,Fig. 1.,Flowchart of the developed cyberattack detection method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/9003434/8956077/cui1-2965797-large.gif
2020,8956077,Fig. 2.,Robustness analysis of spatiotemporal patterns using GGL against ambient noises.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/9003434/8956077/cui2abcd-2965797-large.gif
2020,8956077,Fig. 3.,Performance diagram of contingency table for different methods on the IEEE 13-node test feeder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/9003434/8956077/cui3abcd-2965797-large.gif
2020,8956077,Fig. 4.,Performance diagram of contingency table for different methods on the IEEE 123-node test feeder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/9003434/8956077/cui4abcd-2965797-large.gif
2020,8956077,Fig. 5.,"Comparison of different scaling attack parameters (
λ
S
=0.2
, 0.5, and 1.0) on the IEEE 13-node test feeder.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5165411/9003434/8956077/cui5-2965797-large.gif
2020,8758199,Fig. 1.,Proposed transfer learning method for diagnostics. The fault diagnosis knowledge learned from sufficient high-quality data of multiple rotating machines is transferred to diagnose the target machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li1-2927590-large.gif
2020,8758199,Fig. 2.,"Proposed deep neural network architecture. Rectified linear units (ReLU) and leaky rectified linear units (LReLU) denote the normal and leaky rectified linear units activation functions, respectively [30]. GRL denotes the gradient reversal layer, which reverses the sign of the gradient in back-propagation (BP). N
∗
represents the number of the machine health conditions in each domain.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li2-2927590-large.gif
2020,8758199,Fig. 3.,Test rigs and machinery faults of the four datasets in this article.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li3-2927590-large.gif
2020,8758199,Fig. 4.,Diagnostic performances of different methods when the CWRU data are tested.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li4-2927590-large.gif
2020,8758199,Fig. 5.,Diagnostic performances of different methods when the IMS data are tested.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li5-2927590-large.gif
2020,8758199,Fig. 6.,Diagnostic performances of different methods when the Bogie data are tested.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li6-2927590-large.gif
2020,8758199,Fig. 7.,Diagnostic performances of different methods when the Crack data are tested.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li7-2927590-large.gif
2020,8758199,Fig. 8.,"Visualizations of the learned data representations. Different colors denote different domains, and the texts represent the corresponding machine conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/8964502/8758199/li8-2927590-large.gif
2020,8712527,Fig. 1.,Structure of the Paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang1-2916177-large.gif
2020,8712527,Fig. 2.,Challenges of existing cellular networks to support emerging massive machine-type communications.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang2-2916177-large.gif
2020,8712527,Fig. 3.,Potential enabling techniques for mMTC in cellular IoT networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang3-2916177-large.gif
2020,8712527,Fig. 4.,Access intensity for two 3GPP based MTC traffic models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang4-2916177-large.gif
2020,8712527,Fig. 5.,"Illustration of four-stage message handshake-based RA procedure in LTE-based cellular systems (PRACH: Physical Random Access Channel, PDCCH: Physical Downlink Control Channel, PUSCH: Physical Uplink Shared Channel, PDSCH: Physical Downlink Shared Channel).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang5-2916177-large.gif
2020,8712527,Fig. 6.,Classification of existing machine/deep learning techniques (Deep learning techniques are indicated in the italic format).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang6-2916177-large.gif
2020,8712527,Fig. 7.,Illustrations of the principles of supervised and unsupervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang7-2916177-large.gif
2020,8712527,Fig. 8.,Illustration of the principles of a reinforcement learning technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9031610/8712527/wang8-2916177-large.gif
2020,8950393,FIGURE 1.,EMG examination device MEB-9200K outline image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li1-2964390-large.gif
2020,8950393,FIGURE 2.,Schematic of the position of the electrode slices in the facial motor nerve conduction examination.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li2-2964390-large.gif
2020,8950393,FIGURE 3.,Facial motor nerve conduction examination electromyography.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li3-2964390-large.gif
2020,8950393,FIGURE 4.,Schematic of the position of the electrode slices in the ABR examination.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li4-2964390-large.gif
2020,8950393,FIGURE 5.,ABR examination electromyography.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li5-2964390-large.gif
2020,8950393,FIGURE 6.,Schematic of random forest for CART algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li6-2964390-large.gif
2020,8950393,FIGURE 7.,Flow chart of the data processing and verification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li7-2964390-large.gif
2020,8950393,FIGURE 8.,Comparison of the characteristic values of F-MNCS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li8-2964390-large.gif
2020,8950393,FIGURE 9.,Confusion matrixes of four algorithms for F-MNCS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li9-2964390-large.gif
2020,8950393,FIGURE 10.,Feature extraction of ABR by random forest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li10-2964390-large.gif
2020,8950393,FIGURE 11.,Confusion matrixes of three algorithms for ABR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8950393/li11-2964390-large.gif
2020,8667698,Fig. 1.,Schematic of a conventional ClassRBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim1-2899386-large.gif
2020,8667698,Fig. 2.,Learning and inference procedures of the conventional CD-1 (blue arrow) and ACD (red dotted arrow) algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim2-2899386-large.gif
2020,8667698,Fig. 3.,Several probabilistic neural networks. (a) Fully connected Bayesian feedforward. (b) ClassRBM with the ACD algorithm. (c) ClassRBM with the conventional CD algorithm. (d) Comparison table of these networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim3abcd-2899386-large.gif
2020,8667698,Fig. 4.,Overall hardware architecture of an RPC and RPU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim4-2899386-large.gif
2020,8667698,Fig. 5.,"Overall hardware architecture of the SE_matrix and the FAV module (size:
1760×576
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim5-2899386-large.gif
2020,8667698,Fig. 6.,Overall hardware architecture of the update module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim6-2899386-large.gif
2020,8667698,Fig. 7.,"Effect of stochastic update. Red curve:
y=−5.964
x
−0.3158
+94.58
. Blue curve:
y=−4.356
x
−1.582
+92.06
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim7-2899386-large.gif
2020,8667698,Fig. 8.,"Effect of ensemble inference (500 epoch training,
N
e
=15
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim8-2899386-large.gif
2020,8667698,Fig. 9.,"(a)
N
npc
versus maximum classification accuracy with CD and ACD during 100 epochs (Stochastic_Update ON, N_hot ON, learning rate
(η)=1
). (b) Accuracy difference between ACD and CD.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim9ab-2899386-large.gif
2020,8667698,Fig. 10.,"(a) Ensemble number (
N
e
) versus maximum classification accuracy during 500 training epochs (Stochastic_Update ON, N_hot ON, learning rate
(η)=1
, ACD,
N
npc
=16
). (b) Maximum classification accuracy versus
N
npc
comparison between the baseline (
N
e
=1
) and ensemble inference case (
N
e
=15
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim10ab-2899386-large.gif
2020,8667698,Fig. 11.,"Comparison of the maximum classification accuracy during 1000 training epochs, the baseline case (CD,
N
npc
=1and
N
e
=1
) versus the case with (CD,
N
npc
=1
and
N
e
=15
) versus the case with (ACD,
N
npc
=15
and
N
e
=1
) versus the case with (ACD,
N
npc
=15and
N
e
=15
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim11-2899386-large.gif
2020,8667698,Fig. 12.,Semisupervised on-chip training and inference results. (a) Labeled ratio versus maximum classification accuracy during 500 training epochs with CD and ACD. (b) Labeled ratio versus difference in maximum classification accuracy between ACD and CD cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim12ab-2899386-large.gif
2020,8667698,Fig. 13.,"(a) Inference accuracy versus
N
npc
for various conditions for the Stochastic_Update and N_hot mode values. Stoc_O(X): Stochastic_Update is ON(OFF). N_hot_O(X):N_hot mode is ON(OFF).
N
npc
versus accuracy difference between Stoc_O and Stoc_X for (b) N_hot_O and (c) N_hot_X.
N
npc
versus accuracy difference between N_hot_O and N_hot_X for (d) Stoc_O and (e) Stoc_X.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim13abcde-2899386-large.gif
2020,8667698,Fig. 14.,"(a) Inference accuracy versus learning rate (
η
) for various conditions for the Stochastic_Update and N_hot mode values.
η
versus accuracy difference between Stoc_O and Stoc_X for (b) N_hot_O and (c) N_hot_X.
η
versus accuracy difference between N_hot_O and N_hot_X for (d) Stoc_O and (e) Stoc_X.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim14abcde-2899386-large.gif
2020,8667698,Fig. 15.,Relationship between hardware modes and variables.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/8949827/8667698/kim15-2899386-large.gif
2020,9027907,FIGURE 1.,Diagram of the CIR denoising.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu1-2979220-large.gif
2020,9027907,FIGURE 2.,Kernel PCA results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu2-2979220-large.gif
2020,9027907,FIGURE 3.,"The elbow method of
k
-means.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu3-2979220-large.gif
2020,9027907,FIGURE 4.,Confusion matrix of 4 classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu4-2979220-large.gif
2020,9027907,FIGURE 5.,Two typical scenarios of HSR on ZX line.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu5-2979220-large.gif
2020,9027907,FIGURE 6.,The distribution of datapoints.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu6-2979220-large.gif
2020,9027907,FIGURE 7.,Confusion matrix of the measured data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9027907/liu7-2979220-large.gif
2020,9203807,FIGURE 1.,System setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9203807/azimi1-3025808-large.gif
2020,9203807,FIGURE 2.,TCN residual block.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9203807/azimi2-3025808-large.gif
2020,9203807,FIGURE 3.,"Flowchart of detection system. For the event detector, the green and magenta lines are the start and end of the CSA events, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9203807/azimi3-3025808-large.gif
2020,9203807,FIGURE 4.,F-score of power detector method on training data as a function of FC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9203807/azimi4-3025808-large.gif
2020,9203807,FIGURE 5.,"Accuracy of the SVM classifier on validation data as a function of the ratio of oversampled “A” class instances to under-sampled “N” class instances, with 0.1 increases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9203807/azimi5-3025808-large.gif
2020,9000801,Fig. 1.,"(a) Overall architecture of our RGDN. Given a blurry image
y
and the corresponding blur kernel
k
, the optimizer (i.e., GDU)
U(⋅)
produces a new estimate
x
t+1
from the estimate from the previous step
x
t
. Note that a universal optimizer is used for all steps with shared parameters. (b) Structure of the optimizer
U(⋅)
. Each colored block of the optimizer (left) corresponds to an operation in the classical gradient descent method (right). (c)
R(⋅)
,
H(⋅)
, and
D(⋅)
share a common architecture of a CNN block with different parameters to be learned. Both the input and the output (for the optimizer and all subnetworks) are
H×W×C
tensors, where
C
is the number of channels of the input image
y
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong1abc-2968289-large.gif
2020,9000801,Fig. 2.,"Visualization of each component of the learned gradient descent optimizer. The input of
H(⋅)
in (b) is the gradient from the loss function, i.e.,
A
T
Ax−
A
T
y
. All images are scaled for visualization in pseudocolor. The images are best viewed by zooming in. (a) Blurry image y. (b) Input of H(
⋅
). (c) Output of D(
⋅
). (d) Output of H(
⋅
). (e) Output of R(
⋅
). (f) R(
⋅
)+ H(
⋅
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong2abcdef-2968289-large.gif
2020,9000801,Fig. 3.,"Visualization of the generated updating gradient on a toy image with elementary contents. (a) Input blurry image. (b) Ground-truth image. (c) Intermediate estimate
x
t
with
t=3
. (d) Gradient of the data fitting term,
−∇f(
x
3
)=−(
A
T
A
x
3
−Ay)
. (e) Generated gradient of the learned optimizer. (f) Residual between the current estimate
x
3
and the final target image, i.e., the ground-truth image
x
GT
, which can be seen as an “ideal” updating gradient to obtain the ground-truth image with one step updating. The generated gradient in (e) is much more similar to the “ideal” gradient than the original gradient from data fitting term. Note that (d)–(f) are visualized with scaling and pseudocolor. The images are best viewed by zooming in.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong3abcdef-2968289-large.gif
2020,9000801,Fig. 4.,"Intermediate results of RGDN. (a) and (g) Input blurry images
y
and the corresponding blur kernels
k
. (a) Image with noise level 0.15%. (g) Image from the training data. Both the images and the kernels in (a) are not in the training set. (b)–(e) Intermediate results of the RGDN at steps #3, #20, #30, and #40. (f) and (l) Ground-truth images. (h)–(k) Results on steps #1–#3 and #5 since we perform five steps during training.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong4abcdefghijkl-2968289-large.gif
2020,9000801,Fig. 5.,"Visual comparison on the images with different noise levels. The first two rows show the results from the data set [44] with
σ
= 2%. The bottom two rows show the results on an image from the generated BSD-Blur data set with
σ
= 3%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong5-2968289-large.gif
2020,9000801,Fig. 6.,"PSNR/fitting error versus iteration: empirical convergence analysis of the learned optimizer on image deconvolution. (a) and (b) Results on the images from the Levin et al.’s data set [48] and the BSD-Blur, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong6ab-2968289-large.gif
2020,9000801,Fig. 7.,Deconvolution results on a real-world image. (a) Input. (b) Levin et al. [4]. (c) CSF [8]. (d) MLP [9]. (e) EPLL [7]. (f) IRCNN [13]. (g) FDN [15]. (h) RGDN (ours).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong7abcdefgh-2968289-large.gif
2020,9000801,Fig. 8.,Deconvolution results on a text image from [1]. (a) Input. (b) Levin [4]. (c) EPLL [7]. (d) MLP [9]. (e) CSF [8]. (f) IRCNN [13]. (g) FDN [15]. (h) RGDN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong8abcdefgh-2968289-large.gif
2020,9000801,Fig. 9.,Deconvolution results on a real-world image with high level of noise. The images are best viewed by zooming in. (a) Input. (b) Levin et al. [4]. (c) IRCNN [13]. (d) FDN [15]. (e) RGDN (ours).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong9abcde-2968289-large.gif
2020,9000801,Fig. 10.,"Deconvolution results on a real-world image with unknown noise and saturation. The images are best viewed by zooming in. Note that the proposed RGDN is free of hyperparameter and the other methods require the noise level
σ
as input. (f) and (g) Results of FDN given different hyperparameters. (a) Input. (b) FD [5]. (c) MLP [9]. (d) EPLL [7]. (e) IRCNN [13]. (f) FDN [15],
σ=0
:59%. (g) FDN [15],
σ=1
%. (h) RGDN (ours).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9273274/9000801/gong10abcdefgh-2968289-large.gif
2020,9007737,FIGURE 1.,"Major ML-based chains from which scientific outcomes can be derived: The commonly used, basic ML chain (light gray box) learns a black box model from given input data and provides an output. Given the black box model and input-output relations, a scientific outcome can be derived by explaining the output results utilizing domain knowledge. Alternatively, a transparent and interpretable model can be explained using domain knowledge leading to scientific outcomes. Additionally, the incorporation of domain knowledge can promote scientifically consistent solutions (green arrows).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9007737/garck1-2976199-large.gif
2020,8974443,Fig. 1.,Examples of machine learning on graphs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao1-2970047-large.gif
2020,8974443,Fig. 2.,Graph placement and execution roles in TuX2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao2-2970047-large.gif
2020,8974443,Fig. 3.,Separate vertex arrays for bipartite graph in example of MF. Edge arrays are omitted for conciseness.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao3-2970047-large.gif
2020,8974443,Fig. 4.,"SSP with bounded staleness. A block with
(i,j)
stands for a task with Id
j
in clock
i
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao4-2970047-large.gif
2020,8974443,Fig. 5.,The vertex degree distribution of a typical skewed mini-batch for Microsoft AdsData dataset in one server.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao5-2970047-large.gif
2020,8974443,Fig. 6.,Programming MF with MEGA model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao6-2970047-large.gif
2020,8974443,Fig. 7.,Programming LDA with MEGA model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao7-2970047-large.gif
2020,8974443,Fig. 8.,Programming BlockPG with MEGA model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao8-2970047-large.gif
2020,8974443,Fig. 9.,TuX2's checkpointing workflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao9-2970047-large.gif
2020,8974443,Fig. 10.,Effects of data layout (32 servers).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao10-2970047-large.gif
2020,8974443,Fig. 11.,"Effect of Heterogeneity (BlockPG, 32 servers).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao11-2970047-large.gif
2020,8974443,Fig. 12.,"Performance comparison of worker vertex-cut, thread vertex-cut, and hybrid vertex-cut.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao12-2970047-large.gif
2020,8974443,Fig. 13.,"Run time and breakdown to converge to the same point under different slacks (MF, 32 servers).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao13-2970047-large.gif
2020,8974443,Fig. 14.,"Run time and breakdown to converge to the same point under different slacks (BlockPG, 32 servers).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao14-2970047-large.gif
2020,8974443,Fig. 15.,Convergence w/ varying mini-batch sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao15-2970047-large.gif
2020,8974443,Fig. 16.,Checkpoint overhead for slack 4 on BlockPG. BC): Bulk-consistent checkpoint; SC): Stale-consistent checkpoint; BE): Best-effort checkpoint; No chkpt): No checkpoint.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao16-2970047-large.gif
2020,8974443,Fig. 17.,Checkpoint detail breakdown.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao17-2970047-large.gif
2020,8974443,Fig. 18.,"TuX2 vs. PowerGraph (MF, Netflix, log scale).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao18-2970047-large.gif
2020,8974443,Fig. 19.,TuX2 vs. Petuum (log scale).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao19-2970047-large.gif
2020,8974443,Fig. 20.,Mini-batch time across workers (BlockPG).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9003718/8974443/xiao20-2970047-large.gif
2020,8851168,Fig. 1.,"(a) The proposed machine learning based AS scheme for LS-MIMO GSM. (b) The real time 16
×
4 test-bed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8851168/gecge1ab-2944179-large.gif
2020,8851168,Fig. 2.,"(a) Simulation results for the (i.i.d.) channel under the channel estimation errors. (b) Simulation results for the time-correlated channel (
α≈0.66
) and the channel estimation errors. (c) Real-time system results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8851168/gecge2abc-2944179-large.gif
2020,9138418,FIGURE 1.,The normal architecture of GANs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9138418/zhang1-3008433-large.gif
2020,9138418,FIGURE 2.,The normal architecture of adversarial attacks based on GANs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9138418/zhang2-3008433-large.gif
2020,9138418,FIGURE 3.,The process of generating AEs using targeted BFAM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9138418/zhang3-3008433-large.gif
2020,9138418,FIGURE 4.,"Impact of
α
on the attack performance of BFAM against LR-based NIDS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9138418/zhang4-3008433-large.gif
2020,9138418,FIGURE 5.,Impact of the number of modifiable features on the attack performance against machine learning based AMDSs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9138418/zhang5-3008433-large.gif
2020,9138418,FIGURE 6.,Impact of the number of modifiable features on the attack performance against machine learning based NIDSs. (a) displays the impact on the attack performance of AEs of DoS; (b) displays the impact on the attack performance of AEs of Probe; (c) displays the impact on the attack performance of AEs of R2L&U2R.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9138418/zhang6abc-3008433-large.gif
2020,8945312,FIGURE 1.,The results of important variables extraction by using optimal subset regression at K = 9.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8945312/chen1-2963053-large.gif
2020,8945312,FIGURE 2.,The results of important variables extraction by RF at K = 9.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8945312/chen2-2963053-large.gif
2020,8945312,FIGURE 3.,"The probability distribution of the samples in the complete CKD data set (at K = 11), the horizontal axis and the vertical axis represent the probabilities that the samples were judged as notckd by the LOG and the RF, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8945312/chen3-2963053-large.gif
2020,8945312,FIGURE 4.,The structure of the perceptron used in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8945312/chen4-2963053-large.gif
2020,8945312,FIGURE 5.,The decision line (blue line) obtained by the perceptron in the training data set. The horizontal axis and the vertical axis represent the probabilities that samples were judged as notckd by models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8945312/chen5ab-2963053-large.gif
2020,9340435,Fig. 1.,Visualization of the (a) left and (b) right ROIs using BrainNet viewer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9340435/ravan1ab-3019685-large.gif
2020,9340435,Fig. 2.,"Comparison of ROC curve between SVM, RF, and LDA approaches.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9340435/ravan2-3019685-large.gif
2020,9340435,Fig. 3.,Scatter plot of the feature space showing MR (red circles) vs. LR (black stars) projected onto the first two major nonlinear principal component directions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9340019/9340435/ravan3-3019685-large.gif
2020,8907371,Fig. 1.,Prototype of the PPG acquisition device acquiring the signal from the finger of a subject. Several such devices were used for conducting the validation study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar1-2954553-large.gif
2020,8907371,Fig. 2.,A schematic illustration of feature extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar2-2954553-large.gif
2020,8907371,Fig. 3.,Flowchart depicting the Stacking Regressor model used for Hb prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar3-2954553-large.gif
2020,8907371,Fig. 4.,A screenshot of the PPG signal as seen on the display panel of the in-house PPG acquisition device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar4-2954553-large.gif
2020,8907371,Fig. 5.,"Raw PPG signal for Hb values (a) 1.6 g/dL, (b) 9 g/dL and (c) 14.8 g/dL (d) Hb distribution depicted for all data samples (left) pregnant subset of data samples (middle), and non-pregnant subset of data samples (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar5-2954553-large.gif
2020,8907371,Fig. 6.,(a) Raw PPG signal. (b) Filtered PPG signal. (c) Peak detected PPG signal. (d) An ideal plethysmograph. (e) Signal with dicrotic notches. (f) Signal with a segment of movement artefact.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar6-2954553-large.gif
2020,8907371,Fig. 7.,"Performance metrics: (a) RMSE and (b) Pearson's correlation coefficient (PCC), of machine learning algorithms: Lasso, Ridge, Elastic Net, Ada Boost, SVR and Stacked Regressor models trained using different feature combinations of Attn-Attenuation; c-Sum of Hb moieties; S-Pairwise ratio parameters, Age and PS-Pregnancy Status. Effect of the feature space on CEGA Performance for (c) Region-A (d) Region-B and (e) Region-C.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar7-2954553-large.gif
2020,8907371,Fig. 8.,RMSE and PCC values averaged across all combinations of feature sets for individual regressor models and the combined stacked regressor model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar8-2954553-large.gif
2020,8907371,Fig. 9.,(a) Goodness of fit plot between estimated and actual Hb values. (b) Bland-Altman plot depicting the distribution of bias values within 95% limits of agreement (±1.96 standard deviation from mean bias). Data from 1466 subjects were used to generate these plots.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar9-2954553-large.gif
2020,8907371,Fig. 10.,Error grid analysis for individual regressor models and the combined stacked regressor model averaged across all combinations of feature sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar10-2954553-large.gif
2020,8907371,Fig. 11.,"Error Grid Analysis: Region A (Green) consists of subjects with an error within ±1 g/dL or classified under correct anemia condition, thereby ensuring the right clinical treatment. Subjects not falling under this region, with an error within ±2 g/dL or classified under a condition adjacent (one-class shift) to the true anemia condition belong to Region B (Yellow). Subjects falling under this region would not receive inappropriate/dangerous treatment and would be either slightly undertreated (failing to increase the dose of oral medication) or overtreated (e.g., increased dose of oral iron folate tablets). Subjects not falling under Region A or Region B, with an error beyond ±2 g/dL or classified under an anemia condition beyond a one-class shift belonged to Region C (Red). Subjects falling under this region face the risk of inappropriate treatment (missing out on being referred for recommended blood transfusion or parenteral iron therapy. Note that an actual blood transfusion, in any public health context, will never be done without an invasive testing of blood parameters, including hemoglobin).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar11-2954553-large.gif
2020,8907371,Fig. 12.,Goodness of fit plots and Bland-Altman plots of the individual models within the Stacked regressor model. A bimodality in distribution of estimated Hb values was introduced due to the Ada Boost model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8907371/achar12-2954553-large.gif
2020,8946758,Fig. 1.,"Schematic presentation of a PIR sensor with dual sensing elements aligned in a motion plane (top) and its output signal captured when walking (bottom) with respect to the walking directions (a) left to right, (b) right to left.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun1ab-2963326-large.gif
2020,8946758,Fig. 2.,"Array of four PIR sensors, each of which is oriented with the moving direction at 45° increments (i.e., 0°, 45°, 90°, 135°).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun2-2963326-large.gif
2020,8946758,Fig. 3.,Array of four PIR sensors each of which is equipped with a single-zone Fresnel lens partially shielded by metallic foils.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun3-2963326-large.gif
2020,8946758,Fig. 4.,"Our DAQ system consisting of an Adafruit Feather M0, Adalogger FeatherWing, SD card, and rechargeable battery.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun4-2963326-large.gif
2020,8946758,Fig. 5.,Implemented PIR sensor sensing system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun5-2963326-large.gif
2020,8946758,Fig. 6.,"Proposed architecture for a PIR sensing DAQ and the retrieval system based on the open-source hardware platform (i.e., Adafruit Feather M0) and oneM2M-based IoT software platforms (i.e., Mobius and nCube).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun6-2963326-large.gif
2020,8946758,Fig. 7.,(a) Schematic diagram of the monitored area where the PIR array is mounted at the center on the ceiling considering the eight directions we want to detect. (b) Captured photograph during a data collection experiment of a subject.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun7ab-2963326-large.gif
2020,8946758,Fig. 8.,PIR output signals captured from our developed DAQ system when a subject is walking in eight directions. (a)D1 (b)D2 (c)D3 (d)D4 (e)D5 (f)D6 (g)D7 (h)D8.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun8abcdefgh-2963326-large.gif
2020,8946758,Fig. 9.,Time series of PIR_1 from 30 subjects’ movement toward a direction. The PIR data sets from all 30 subjects are displayed where each color represents each subject.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun9-2963326-large.gif
2020,8946758,Fig. 10.,"Deep learning components deployed in the model including the convolution layer for feature construction, the pooling layer for feature reduction, the dropout layer for model reduction, and the fully connected layer for classification.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun10-2963326-large.gif
2020,8946758,Fig. 11.,"Variation of filter size. In clockwise direction, filter sizes are
4×4
,
4×3
,
4×1
, and
4×2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun11-2963326-large.gif
2020,8946758,Fig. 12.,Loss and accuracy according to epoch from CNN with optimal structure and four sensors. The upper figure shows the loss and the lower figure shows the accuracy for training and validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun12-2963326-large.gif
2020,8946758,Fig. 13.,Scalability of algorithms is checked by tracking the accuracy according to the number of subjects used in the experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun13-2963326-large.gif
2020,8946758,Fig. 14.,Generality of algorithm is tracked according to ratio of training and testing subjects. 27:3 indicates the number of samples in training is 27 while the number of samples in testing is 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun14-2963326-large.gif
2020,8946758,Fig. 15.,Recognition accuracy is tracked according to the time duration of data used in the training phase. The time duration increases from 0.5 to 3 s by 0.5 s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun15-2963326-large.gif
2020,8946758,Fig. 16.,"Comparison of single-person walking (top) and multiple-person walking (bottom) toward \${E}\$ ( \${D}1\$ , left) and NE ( \${D}8\$ , right) directions under the PIR_1 senor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun16-2963326-large.gif
2020,8946758,Fig. 17.,Application example of our proposed PIR-based movement direction detecting system for analyzing in-store customer behaviors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun17-2963326-large.gif
2020,8946758,Fig. 18.,"Architecture diagram for building an IoT system consisting of a oneM2M server platform (e.g., Mobius) and oneM2M device platform (e.g., nCube) connected with the proposed PIR-based sensing devices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9066849/8946758/yun18-2963326-large.gif
2020,8963960,FIGURE 1.,Framework for hate speech detection in South African tweets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8963960/oriol1-2968173-large.gif
2020,8963960,FIGURE 2.,Meta-learning model with multiple features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8963960/oriol2-2968173-large.gif
2020,8963960,FIGURE 3.,Multi-tier meta-learning model with multiple meta-features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8963960/oriol3-2968173-large.gif
2020,8963960,FIGURE 4.,TPR for the best techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8963960/oriol4-2968173-large.gif
2020,8963960,FIGURE 5.,"Precision, Recall and F-measure for the best techniques.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8963960/oriol5-2968173-large.gif
2020,8830434,Fig. 1.,Scenario A: SSS in the PPP modeled primary users networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8830434/liu1-2940442-large.gif
2020,8830434,Fig. 2.,Scenario B: SSS in the PCP modeled primary users networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8830434/liu2-2940442-large.gif
2020,8830434,Fig. 3.,"CDF of
I
0
in Scenario B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8830434/liu3-2940442-large.gif
2020,8830434,Fig. 4.,"PDF of
I
1
in Scenario B where
R
s
=60m
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8830434/liu4-2940442-large.gif
2020,8830434,Fig. 5.,"CDF of
I
1
in Scenario B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/8954851/8830434/liu5-2940442-large.gif
2020,9109724,Fig. 1.,"Application scenario (ML
i
: machine learning model
i
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu1-3000373-large.gif
2020,9109724,Fig. 2.,Comparison of coreset construction algorithms (coreset size: 8).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu2ab-3000373-large.gif
2020,9109724,Algorithm 1,"Robust Coreset Construction (
P,ϵ,ρ)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu19-3000373-large.gif
2020,9109724,Algorithm 2,"Distributed Robust Coreset Construction
((
P
j
)
n
j=1
,N,K)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu20-3000373-large.gif
2020,9109724,Fig. 3.,Evaluation on Fisher’s iris dataset with varying coreset size (label: ‘species’).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu3abcd-3000373-large.gif
2020,9109724,Fig. 4.,Evaluation on Facebook metrics dataset with varying coreset size (label: ‘type’).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu4abcd-3000373-large.gif
2020,9109724,Fig. 5.,Evaluation on Pendigits with varying coreset size (label: ‘digit’).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu5abcd-3000373-large.gif
2020,9109724,Fig. 6.,Evaluation on MNIST with varying coreset size (label: ‘labels’).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu6abcd-3000373-large.gif
2020,9109724,Fig. 7.,Evaluation on HAR with varying coreset size (label: ‘labels’).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu7abcd-3000373-large.gif
2020,9109724,Fig. 8.,"Detailed evaluation on Fisher’s iris dataset (label: ‘species’, coreset size: 20).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu8abcd-3000373-large.gif
2020,9109724,Fig. 9.,"Detailed evaluation on Facebook metrics dataset (label: ‘type’, coreset size: 40).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu9abcd-3000373-large.gif
2020,9109724,Fig. 10.,"Detailed evaluation on Pendigits dataset (label: ‘digit’, coreset size: 40).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu10abcd-3000373-large.gif
2020,9109724,Fig. 11.,"Detailed evaluation on MNIST dataset (label: ‘labels’, coreset size: 50).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu11abcd-3000373-large.gif
2020,9109724,Fig. 12.,"Detailed evaluation on HAR dataset (label: ‘labels’, coreset size: 50).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu12abcd-3000373-large.gif
2020,9109724,Fig. 13.,"Evaluation on MNIST in distributed setting (label: ‘labels’, coreset size: 400, \$K=10\$ ).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu13abcd-3000373-large.gif
2020,9109724,Fig. 14.,"Evaluation on HAR in distributed setting (label: ‘activity’, coreset size: 200, \$K=10\$ ).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu14abcd-3000373-large.gif
2020,9109724,Fig. 15.,Evaluation on MNIST in distributed setting with varying coreset size ( \$K=10\$ ).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu15abcd-3000373-large.gif
2020,9109724,Fig. 16.,Evaluation on HAR in distributed setting with varying coreset size ( \$K=10\$ ).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu16abcd-3000373-large.gif
2020,9109724,Fig. 17.,Evaluation on MNIST in distributed setting with varying \$K\$ ( \$N=400\$ ).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu17abcd-3000373-large.gif
2020,9109724,Fig. 18.,Evaluation on HAR in distributed setting with varying \$K\$ ( \$N=200\$ ).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9198251/9109724/lu18abcd-3000373-large.gif
2020,8999545,Fig. 1.,Illustration of the NN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9112801/8999545/qi1-2973972-large.gif
2020,8999545,Fig. 2.,Convergence of the training of the NN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9112801/8999545/qi2-2973972-large.gif
2020,8999545,Fig. 3.,Comparisons of spectral efficiency for different SNR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9112801/8999545/qi3-2973972-large.gif
2020,9061028,Fig. 1.,Intersection area estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9137748/9061028/wang1-2986676-large.gif
2020,9061028,Fig. 2.,"Localization performance of the KELM-HQ algorithm with 300 unknown nodes and different number of anchors in a
100m×100m
square field and the transmission range of each node
r
is 10m: (a) 5 anchors; (b) 20 anchors; (c) 35 anchors; (d) 50 anchors. The blue circles denote the randomly deployed unknown nodes whose locations are to be determined. The red crosses denote the localization result of the KELM-HQ algorithm and the green lines denote the localization errors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9137748/9061028/wang2abcd-2986676-large.gif
2020,9061028,Fig. 3.,"Localization error of the fast-SVM, the GADV-Hop, the DV-Hop-ELM and the KELM-HQ algorithm for 5 to 35 anchors and 300 unknown nodes randomly deployed in a
100m×100m
square field and the transmission range of each node
r
is 10m.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9137748/9061028/wang3-2986676-large.gif
2020,9013031,Fig. 1.,Schematic of BiCS 3-D NAND architecture and the tapered vertical NAND flash memory string and cross section of the control gate part.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko1-2971784-large.gif
2020,9013031,Fig. 2.,Calibration of the TCAD simulation set up by reproducing the experimental string-current multi-WL characteristics of ten WL string.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko2-2971784-large.gif
2020,9013031,Fig. 3.,"Quantile-quantile plot (Q-Q plot) for the key electrical parameters (
V
th
, SS,
g
m
, and
I
ON
)
shift due to individual and combined effects of five variability sources.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko3-2971784-large.gif
2020,9013031,Fig. 4.,MIMO ANN structure. All input–output values include the source of the process variation (PV) and the dimensions of the device and represent corresponding key electrical parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko4-2971784-large.gif
2020,9013031,Fig. 5.,"ML algorithm training procedure that benchmarks the 3-D TCAD simulation is adjusted to the error of the result. In all, 600 simulations are used for the configured device dimensions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko5-2971784-large.gif
2020,9013031,Fig. 6.,Key electrical parameters distribution for 600 samples of 3-D NAND flash memory devices. All distributions are formed in normal distribution and show the correspondence between ML algorithm and TCAD simulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko6-2971784-large.gif
2020,9013031,Fig. 7.,Results of TCAD simulation and ML algorithm for the standard deviation of key electrical parameters. The average error rate is less than 1%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko7-2971784-large.gif
2020,9013031,Fig. 8.,"Cross section of a 3-D NAND flash memory device containing a taper angle.
I
d
–
V
g
plot of 600 3-D NAND flash memory devices with and without taper angle.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko8-2971784-large.gif
2020,9013031,Fig. 9.,Shift of the key electrical parameters distribution of 3-D NAND flash memory devices with an increase of taper angle. The illustrations in each figure show an increase in the standard deviation of the key electrical parameter distribution with increasing taper angle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko9-2971784-large.gif
2020,9013031,Fig. 10.,Comparison of scatter plots and correlations of the key electrical parameters between the TCAD data and the ML approach results from 3-D NAND flash memory devices.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko10-2971784-large.gif
2020,9013031,Fig. 11.,Variation of the mean and standard deviation of the distribution of the key electrical parameters as the number of stacked layers of 3-D NAND flash memory devices increases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9046113/9013031/ko11-2971784-large.gif
2020,8941255,Fig. 1.,Experimental setup for multiple persons equidistant from impulse radar under laboratory conditions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8948272/8941255/sarka1-2961962-large.gif
2020,8941255,Fig. 2.,Radar echo received from two equidistant subjects. (a) Raw. (b) Preprocessed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8948272/8941255/sarka2-2961962-large.gif
2020,8941255,Fig. 3.,Change in the magnitude of variance at each range bin with a number of subjects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8948272/8941255/sarka3-2961962-large.gif
2020,8941255,Fig. 4.,Change in the magnitude of dominant singular values with a number of subjects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8948272/8941255/sarka4-2961962-large.gif
2020,8941255,Fig. 5.,Confusion matrix for EBT classifier when applied on both features variance and singular value.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/8948272/8941255/sarka5-2961962-large.gif
2020,8974224,FIGURE 1.,Bed rate for patients by city.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb1-2970178-large.gif
2020,8974224,FIGURE 2.,Main features used on the sample dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb2-2970178-large.gif
2020,8974224,FIGURE 3.,SEMLHI framework users.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb3-2970178-large.gif
2020,8974224,FIGURE 4.,SEMLHI conceptual framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb4-2970178-large.gif
2020,8974224,FIGURE 5.,SEMLHI framework components.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb5-2970178-large.gif
2020,8974224,FIGURE 6.,Machine algorithm model components.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb6-2970178-large.gif
2020,8974224,Algorithm 1,"k-Nearest Neighbor Algorithm for Multilevel Learning by Using Correlation, Diagnosis Code, and Label Weight With Frequency",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb12-2970178-large.gif
2020,8974224,FIGURE 7.,"Age and category feature summary with other variables (test results, gender, and ICD-10).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb7-2970178-large.gif
2020,8974224,FIGURE 8.,Machine learning algorithms used for health classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb8-2970178-large.gif
2020,8974224,FIGURE 9.,High accuracy of logistic regression compared with that of other algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb9-2970178-large.gif
2020,8974224,FIGURE 10.,Mechanism of the machine algorithm model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb10-2970178-large.gif
2020,8974224,FIGURE 11.,"The software module includes subclasses including reuse, performance, testing, privacy, and security.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8974224/moreb11-2970178-large.gif
2020,9151176,Fig. 1.,"(a) is an example fundus image from IDRiD dataset with retinal lesions and optic disk/cup highlighted, (b) left is a healthy OCT B-scan cross-section that shows retinal layers, and right is an OCT B-scan of patient with AMD signs [20].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9151176/sarha1-3012134-large.gif
2020,8684326,Fig. 1.,The structure of capsule network based model for discovery of breast cancer-related genes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9216708/8684326/peng1-2909905-large.gif
2020,8684326,Fig. 2.,Computation process from PrimaryCaps to BCGCaps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9216708/8684326/peng2-2909905-large.gif
2020,8684326,Fig. 3.,Boxplot of the FDR of CapsNetMMD. Threshold τ varies from 0.5 to 0.95. The red line with green markers represents variations of the mean values of FDR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9216708/8684326/peng3-2909905-large.gif
2020,8684326,Fig. 4.,"Performance comparison of CapsNetMMD with LightGBM, XGBoost, NN, SVM, Adaboost, and KNN. A. ROC curves. B. AUC values and Sn values at three stringent levels of Sp. C. Rank Cutoff curves. D. Precision Recall curves.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9216708/8684326/peng4-2909905-large.gif
2020,8684326,Fig. 5.,Venn diagram of the distribution of potential prognostic candidate genes in three categories.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9216708/8684326/peng5-2909905-large.gif
2020,8684326,Fig. 6.,"The KM plots of RPL14, CHD4, LAPTM4A, and DYNC1H1 using CNA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9216708/8684326/peng6-2909905-large.gif
2020,9262846,FIGURE 1.,EEG measurement and diagnosis blockchain database construction process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung1-3038948-large.gif
2020,9262846,FIGURE 2.,The EMD algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung2-3038948-large.gif
2020,9262846,FIGURE 3.,T4-C4 channel data of (a) BECTS and (b) TLE patients and their first five IMFs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung3ab-3038948-large.gif
2020,9262846,FIGURE 4.,"Boxplot of coefficient variation, fluctuation index, skewness, kurtosis of T3-C3 channel of (a) IMF1, (b) IMF2, (c) IMF3, (d) IMF4, and (e) IMF5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung4abcde-3038948-large.gif
2020,9262846,FIGURE 5.,"Boxplot of coefficient variation, fluctuation index, skewness, kurtosis of T4-C4 channel of (a) IMF1, (b) IMF2, (c) IMF3, (d) IMF4, and (e) IMF5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung5abcde-3038948-large.gif
2020,9262846,FIGURE 6.,Flowchart of the proposed HML algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung6-3038948-large.gif
2020,9262846,FIGURE 7.,Comparison of accuracy and computation time according to the number of decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung7-3038948-large.gif
2020,9262846,FIGURE 8.,Comparison of PCA and SVM-RFE performance with classification accuracy and computation time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9262846/chung8-3038948-large.gif
2020,9112710,Fig. 1.,SFP process of WR-ELM model for prediction of number of software faults.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar1-2996261-large.gif
2020,9112710,Fig. 2.,"Pred(l)
metric analysis for all modules on different
λ
values for (a) intra release prediction (for Camel 1.0 dataset) and (b) cross project fault prediction (for Synapse 1.0-Poi 2.0 dataset). (a) For Camel 1.0 dataset. (b) For Synapse 1.0-Poi 2.0 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar2-2996261-large.gif
2020,9112710,Fig. 3.,"Pred(l)
metric analysis for faulty modules on different
λ
values for (a) intra release prediction (for Camel 1.0 dataset) and (b) cross project fault prediction (for Synapse 1.0-Poi 2.0 dataset). (a) For Camel 1.0 dataset. (b) For Synapse 1.0-Poi 2.0 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar3-2996261-large.gif
2020,9112710,Fig. 4.,"ARE analysis on different
λ
values for (a) intra release prediction (for Camel 1.0 dataset) and (b) cross project fault prediction (for Synapse 1.0-Poi 2.0 dataset). (a) For Camel 1.0 dataset. (b) For Synapse 1.0-Poi 2.0 dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar4-2996261-large.gif
2020,9112710,Fig. 5.,"Hidden node analysis with
λ
values on different prediction scenarios.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar5-2996261-large.gif
2020,9112710,Fig. 6.,RE performance between the basic ELM and WR-ELM for (a) intra release prediction (Poi 2.0 dataset) and (b) cross project fault prediction (Ivy 2.0-Prop v452 dataset). (a) Poi 2.0 dataset. (b) Ivy 2.0-Prop v452 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar6-2996261-large.gif
2020,9112710,Fig. 7.,"AAE box plot analysis of different models for (a) intra release prediction, (b) inter release prediction, and (c) cross project fault prediction on PROMISE datasets. (a) Intra release prediction. (b) Inter release prediction. (c) Cross project fault prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar7-2996261-large.gif
2020,9112710,Fig. 8.,"ARE box plot analysis of different models for (a) intra release prediction, (b) inter release prediction, and (c) cross project fault prediction on PROMISE datasets. (a) Intra release prediction. (b) Inter release prediction. (c) Cross project fault prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/24/9274445/9112710/kumar8-2996261-large.gif
2020,9194237,FIGURE 1.,Illustration of PPDL Scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw1-3023084-large.gif
2020,9194237,FIGURE 2.,Classical PP Classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw2-3023084-large.gif
2020,9194237,FIGURE 3.,Activation Layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw3-3023084-large.gif
2020,9194237,FIGURE 4.,Pooling Layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw4-3023084-large.gif
2020,9194237,FIGURE 5.,Fully Connected Layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw5-3023084-large.gif
2020,9194237,FIGURE 6.,Dropout Layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw6-3023084-large.gif
2020,9194237,FIGURE 7.,Convolutional Layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw7-3023084-large.gif
2020,9194237,FIGURE 8.,GAN Structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw8-3023084-large.gif
2020,9194237,FIGURE 9.,Required Modification for PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw9-3023084-large.gif
2020,9194237,FIGURE 10.,Adversarial Model in PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw10-3023084-large.gif
2020,9194237,FIGURE 11.,Metrics for Surveyed PPDL Works.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw11-3023084-large.gif
2020,9194237,FIGURE 12.,Classification of PPDL Methods by Its Privacy Preserving Techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw12-3023084-large.gif
2020,9194237,FIGURE 13.,The Surveyed Paper of PPDL Since 2016.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw13-3023084-large.gif
2020,9194237,FIGURE 14.,The Structure of HE-based PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw14-3023084-large.gif
2020,9194237,FIGURE 15.,The Structure of Secure MPC-based PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw15-3023084-large.gif
2020,9194237,FIGURE 16.,The Structure of Secure Two-party Computation-based PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw16-3023084-large.gif
2020,9194237,FIGURE 17.,The Structure of Differential Privacy-based PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw17-3023084-large.gif
2020,9194237,FIGURE 18.,The Structure of Secure Enclaves-based PPDL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw18-3023084-large.gif
2020,9194237,FIGURE 19.,The Challenges and Weaknesses of State-of-the-Art PPDL Methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9194237/tanuw19-3023084-large.gif
2020,8967053,Fig. 1.,Block diagram of the implementation procedure in this letter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9062391/8967053/yang1-2968902-large.gif
2020,8967053,Fig. 2.,The structure of FNNs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9062391/8967053/yang2-2968902-large.gif
2020,8967053,Fig. 3.,One CNN structure used in this letter.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9062391/8967053/yang3-2968902-large.gif
2020,8967053,Fig. 4.,Stacking process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9062391/8967053/yang4-2968902-large.gif
2020,8967053,Fig. 5.,"Training loss of two training methods. SWATS is the method proposed in [16], customize refers to the combination of Adam and SGD that the first half of training adopts Adam while the second half uses SGD algorithm.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9062391/8967053/yang5-2968902-large.gif
2020,9212350,FIGURE 1.,Introduction of traditional machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai1-3028740-large.gif
2020,9212350,FIGURE 2.,Development track of machine vision based on CNNs. (a) LeNet-5 [84]. (b) AlexNet-5 [22]. (c) GoogLeNet [87]. (d) VGG-16 [92]. (e) Faster-RCNN [94]. (f) ResNet [97]. (g) YOLO [103]. (h) SSD [100]. (i) YOLOv3 [119]. (j) RetinaNet [111]. (k) CornerNet [112].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai2-3028740-large.gif
2020,9212350,FIGURE 3.,Comparison between supervised and unsupervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai3-3028740-large.gif
2020,9212350,FIGURE 4.,Process of self-supervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai4-3028740-large.gif
2020,9212350,FIGURE 5.,Mainstream reinforcement learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai5-3028740-large.gif
2020,9212350,FIGURE 6.,Tactile sensor schematic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai6-3028740-large.gif
2020,9212350,FIGURE 7.,Framework of visual-tactile fusion for object recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9212350/bai7-3028740-large.gif
2020,8889996,Fig. 1.,"Federated learning with a parameter server. Illustrated is one communication round of distributed SGD. (a) Clients synchronize with the server. (b) Clients compute a weight update independently based on their local data. (c) Clients upload their local weight updates to the server, where they are averaged to produce the new master model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek1abc-2944481-large.gif
2020,8889996,Fig. 2.,"Convergence speed when using different compression methods during the training of VGG11* 1 on CIFAR-10 and logistic regression on MNIST and Fashion-MNIST in a distributed setting with ten clients for i.i.d. and non-i.i.d. data. In the non-i.i.d. cases, every client only holds examples from exactly two respectively one of the ten classes in the data set. All compression methods suffer from degraded convergence speed in the non-i.i.d. situation, but sparse top-
k
is affected by far the least.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek2-2944481-large.gif
2020,8889996,Fig. 3.,"Left: distribution of values for
α
w
(1)
for the weight layer of logistic regression over the MNIST data set. Right: development of
α(k)
for increasing batch sizes. In the i.i.d. case, the batches are sampled randomly from the training data, while in the non-i.i.d. case, every batch contains samples from only exactly one class. For i.i.d. batches, the gradient sign becomes increasingly accurate with growing batch sizes. For non-i.i.d. batches of data, this is not the case. The gradient signs remain highly incongruent with the full-batch gradient, no matter how large the size of the batch.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek3ab-2944481-large.gif
2020,8889996,Fig. 4.,Effects of ternarization at different levels of upload and download sparsities. Displayed is the difference in final accuracy in % between a model trained with sparse updates and a model trained with sparse binarized updates. Positive numbers indicate better performance of the model trained with pure sparsity. VGG11 trained on CIFAR10 for 16 000 iterations with five clients holding i.i.d. and non-i.i.d. data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek4-2944481-large.gif
2020,8889996,Fig. 5.,Accuracy achieved by VGG11* when trained on CIFAR in a distributed setting with five clients for 16 000 iterations at different levels of upload and download sparsity. Sparsifying the updates for downstream communication reduces the final accuracy by at most 3% when compared to using only upload sparsity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek5-2944481-large.gif
2020,8889996,Algorithm 2,Efficient Federated Learning With Parameter Server Via STC,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek12-2944481-large.gif
2020,8889996,Fig. 6.,"Robustness of different compression methods to the non-i.i.d.-ness of client data on four different benchmarks. VGG11* trained on CIFAR. STC distinctively outperforms federated averaging on non-i.i.d. data. The learning environment is configured as described in Table III. Dashed lines signify that a momentum of
m=0.9
was used.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek6-2944481-large.gif
2020,8889996,Fig. 7.,Maximum accuracy achieved by the different compression methods when training VGG11* on CIFAR for 20 000 iterations at varying batch sizes in a federated learning environment with ten clients and full participation. Left: Every client holds data from exactly two different classes. Right: Every client holds an i.i.d. subset of data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek7ab-2944481-large.gif
2020,8889996,Fig. 8.,Validation accuracy achieved by VGG11* on CIFAR after 20 000 iterations of communication-efficient federated training with different compression methods. The relative client participation fraction is varied between 100% (5/5) and 5% (5/100). Left: Every client holds data from exactly two different classes. Right: Every client holds an i.i.d. subset of data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek8ab-2944481-large.gif
2020,8889996,Fig. 9.,"Validation accuracy achieved by VGG11* on CIFAR after 20 000 iterations of communication-efficient federated training with different compression methods. The training data are split among the client at different degrees of unbalancedness with
γ
varying between 0.9 and 1.0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek9-2944481-large.gif
2020,8889996,Fig. 10.,"Convergence speed of federated learning with compressed communication in terms of training iterations (left) and uploaded bits (right) on three different benchmarks (top to bottom) in an i.i.d. federated learning environment with 100 clients and 10% participation fraction. For better readability, the validation error curves are average-smoothed with a step size of five. On all benchmarks, STC requires the least amount of bits to converge to the target accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek10ab-2944481-large.gif
2020,8889996,Fig. 11.,Left: accuracy achieved by VGG11* on CIFAR after 20 000 iterations of federated training with federated averaging and STC for three different configurations of the learning environment. Right: upstream and downstream communication necessary to achieve a validation accuracy of 84% with federated averaging and STC on the CIFAR benchmark under i.i.d. data and a moderate batch-size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9184294/8889996/samek11ab-2944481-large.gif
2020,9144562,FIGURE 1.,"Frozen-Lake environment for the variational quantum DRL agent. In this frozen-lake environment, the RL agent is expected to go from the start location (S) to the goal location (G). There are several holes (H) on the way, and the agent should learn to avoid stepping into these hole locations. Furthermore, we set a negative reward for each step the agent takes. The agent is expected to learn the policy that going from S to G with the shortest path possible. In this work, we train the agents on three configurations of the frozen-lake environment shown in (a), (b) and (c) separately.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan1-3010470-large.gif
2020,9144562,FIGURE 2.,"Cognitive-Radio environment for the variational quantum DRL agent. In the cognitive-radio environment, the agent is expected to select a channel that is free of interference in each time step. For example, there is a primary user (PU) that will occupy a specific channel in each time step periodically. Our agent, which is the secondary user (SU), can only select the channels that are not occupied without the knowledge of the PU in advance. The agent is expected to learn the policy through the interaction with the environment.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan2-3010470-large.gif
2020,9144562,FIGURE 3.,"Cognitive-Radio environment with periodical channel-changing pattern for the variational quantum DRL agent. We provide three configurations for the cognitive-radio experiment; the first setting (a) is the main configuration for experiments on a different number of channels and experiments in noisy situations. The other two configurations in (b) and (c) are only tested in the case of 4 channels, the purpose of these additional experiments is to demonstrate that the proposed framework is generally applicable in different scenarios.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan3-3010470-large.gif
2020,9144562,FIGURE 4.,"Overview of variational quantum circuits for DRL. In this work, we study the capability of variational quantum circuits in performing DRL tasks. This DRL agent includes a quantum part and a classical part. Under current limitations on the scale of quantum machines and the capabilities of quantum simulations, we select frozen-lake and cognitive-radio environments for the proof-of-principle study. The proposed framework is rather general and is expected to solve complicated tasks when larger-scale quantum machines are available.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan4-3010470-large.gif
2020,9144562,FIGURE 5.,"Generic variational quantum circuit architecture for the deep
Q
network (VQ-DQN). The single-qubit gates
R
x
(θ)
and
R
z
(θ)
represent rotations along
x
-axis and
z
-axis by the given angle
θ
, respectively. The CNOT gates are used to entangle quantum states from each qubit and
R(α,β,γ)
represents the general single qubit unitary gate with three parameters. The parameters labeled
θ
i
and
ϕ
i
are for state preparation and are not subject to iterative optimization. Parameters labeled
α
i
,
β
i
and
γ
i
are the ones for iterative optimization. Note that the number of qubits can be adjusted to fit the problem of interest and the grouped box may repeat several times to increase the number of parameters, subject to the capacity and capability of the quantum machines used for the experiments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan5-3010470-large.gif
2020,9144562,FIGURE 6.,"Variational quantum circuits for the cognitive-radio experiments. The basic architecture is the same as the circuit used in the frozen-lake experiment. The main difference is that we have different number of qubits in order to fit the number of channels. In cognitive-radio experiments, all circuits are with two layers (grouped box repeated twice), regardless of the number of channels. In the 3-channel case, since the number of possible state is 32, which is greater than the number of computational basis of a 3-qubit system (which is 23 = 8). We use a 4-qubit system to accommodate the all possible states while the number of parameters is not increased.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan6-3010470-large.gif
2020,9144562,FIGURE 7.,"Performance of the variational quantum circuits for DQL on the frozen-lake experiment. Subfigures (a), (b), and (c) correspond to the results of the environment configurations (a), (b), and (c) in FIGURE III, respectively. Take subfigure (a) for example. The left panel of subfigure (a) shows that our proposed variational quantum circuits based DQL-agent reaches the optimal policy in the frozen-lake environment with a total reward of 0.9 at the 198th iteration. The gray area in the right panel of the subfigure represents the standard deviation of reward in each iteration during exploration with the standard reinforcement learning reproducible setting for stability. The mean and the standard deviation values of the average score (reward) are calculated from the scores (rewards) in the past 100 episodes. The policy learned via quantum circuit becomes more stable after the 301st iteration. The other two experiments in subfigures (b) and (c) demonstrate the similar pattern.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan7-3010470-large.gif
2020,9144562,FIGURE 8.,"Performance of the Variational quantum circuits for DQL on the cognitive-radio experiment. In the cognitive-radio experiments, we limit the maximum steps an agent can run to be 100, and the reward scheme is that for each correct choice of the channel, the agent will get a +1 reward and −1 for incorrect selection. The maximum reward an agent can achieve under this setting is 100. The top panels of the figure show that our proposed variational quantum circuits based DQL-agent reaches the optimal policy with a total reward of 100 in the 2-channel and 5-channel cases at only several iterations. For the cases of 3-channel and 4-channel, the agent also reaches near-optimal policy at only several iterations. The gray area in the bottom panels of the figure represents the standard deviation of reward in each iteration during exploration with the standard reinforcement learning reproducible setting for stability [41]. The mean and the standard deviation at each episode are calculated from the rewards(scores) in the past 100 episodes. The policy learned via variational quantum circuits becomes more stable after around 100 iterations for all the four cases.This part of experiment is tested on the configuration (a) shown in Fig. 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan8-3010470-large.gif
2020,9144562,FIGURE 9.,"Performance of the Variational quantum circuits for DQL on the cognitive-radio experiment. This part of experiment is tested on configurations (b) and (c) shown in Fig. 3, and the results are comparable to the results in FIGURE VI-C, which are tested on configuration (a) in Fig. 3. This demonstrates that our proposed quantum DRL (DQL) framework can be trained on different scenarios.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan9-3010470-large.gif
2020,9144562,FIGURE 10.,"Performance of the variational quantum circuit for DQL on cognitive-radio experiment conducted on the IBM Qiskit Aer quantum simulator with noise from the real quantum machine, ibmq-poughkeepsie. In this experiment, we investigated the robustness of the VQ-DQN against the noise in quantum machines. The device noise is downloaded from remote real quantum machines and then incorporated with the simulator software Qiskit Aer provided by IBM. This noisy backend then replaces the noiseless backend in previous experiments. The result of the experiment shows that our proposed variational quantum circuits based DQL or DQN is robust against noise in the current quantum machines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan10-3010470-large.gif
2020,9144562,FIGURE 11.,"Comparison of memory consumption in different learning schemes. The figure shows our proposed variational quantum circuits based DQL-agent has the quantum advantage in memory consumption compared to classical
Q
-learning and DQL (DQN). Specifically, in our cognitive-radio channel selection experiment, we set up four different testing environments with 2,3,4,5 possible channels, respectively. With the classical
Q
-learning, the number of parameters grow with
n
3
, and in DQL (DQN), the number of parameters grow with
n
2
. However, with our proposed variational quantum circuits based DQL (DQN), the number of parameters grows as
n×(3×2+1)
only.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9144562/goan11-3010470-large.gif
2020,8667368,Fig. 1.,"Flowchart of the human–machine annotation framework with support for various learning paradigms including semi-supervised learning (SSL), active learning (AL), dynamic active learning (DAL), cooperative learning (CL), and dynamic cooperative learning (DCL); the instances predicted with high confidence (
hc
) are automatically labelled by machine, whereas the instances with medium confidence (
mc
) are subject to human inspection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang1-2901499-large.gif
2020,8667368,Fig. 2.,"Pseudo-code description of the generic dynamic cooperative learning (DCL) algorithm, which combines semi-supervised learning (SSL) and dynamic active learning (DAL) by means of human-machine collaboration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang2-2901499-large.gif
2020,8667368,Fig. 3.,"Distribution of prosody scores in the (a) NC (training set
L
), (b) AUWL (unlabeled set
U
), and (c) C-AuDiT (test set
T
) datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang3abc-2901499-large.gif
2020,8667368,Fig. 4.,"Scatter diagram representing the correlation between the absolute prediction error and the estimated model uncertainty on the unlabeled set
U
(Spearman’s CC = 0.658).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang4-2901499-large.gif
2020,8667368,Fig. 5.,"CDF of
r
wg
measured on the ratings of the instances in the AUWL database.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang5-2901499-large.gif
2020,8667368,Fig. 6.,AL with the MC or LC query strategy versus SSL versus PL: performance in terms of the Spearman’s CC values on the C-AuDiT test set versus the number of labeled instances from the AUWL database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang6-2901499-large.gif
2020,8667368,Fig. 7.,"Comparison between dynamic cooperative learning (DCL)/dynamic active learning (DAL), which adapts the number of queries to an agreement level
α
, and standard cooperative learning (CL)/active learning (AL), which sets a fixed number of annotations per instance: Performance in terms of the Spearman’s CC values on the C-AuDiT test set vs the number of human labels. The medium confidence query strategy is used throughout.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/8964604/8667368/zhang7-2901499-large.gif
2020,9102451,Fig. 1.,"The figure shows the framework of CSS for NOMA, in which two PUs are transmitting simultaneously on the same frequency band with different power. When a SU needs to transmit signals, it first sends a request to FC. Then the FC commands all SUs in its area to perceive the states of PUs, and each SU sends the perceived energy signals to FC. Finally, the FC makes a final decision and returns it to the SU who has the data transmission needs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/9190045/9102451/shi1-2995594-large.gif
2020,9102451,Algorithm 1:,K-Means Clustering Algorithm for CSS,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/9190045/9102451/shi4-2995594-large.gif
2020,9102451,Algorithm 2:,EM Algorithm for GMM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/9190045/9102451/shi5-2995594-large.gif
2020,9102451,Fig. 2.,Decision model of DAG-SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/9190045/9102451/shi2-2995594-large.gif
2020,9102451,Fig. 3.,"Numerical results: (a) sensing accuracy of different algorithms versus the number of SUs; (b) sensing accuracy of different algorithms versus the number of SUs; (c) sensing accuracy of different algorithms versus average SNR of SUs; (d) sensing accuracy of different algorithms versus the ratio of power coefficients
Ω
1
/
Ω
2
; (e) training time and test time versus different algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7693/9190045/9102451/shi3abcde-2995594-large.gif
2020,8897080,Fig. 1.,"Overview of proposed high-order feature learning framework for multi-atlas based label fusion. The blue rectangles on the atlases represent the search region. The blue patches in the atlas images represent the selected candidate patches and the green patch in the target image represents the target patch. At stage (a), the mcRBM [26] model is used to extract the high-order patch-level features (i.e., mean feature and covariance feature) of brain MR images. At stage (b), a group-fused sparsity dictionary learning method is developed to jointly calculate the similarity (i.e., voting weight) between the target and candidate patches, based on the learned mean and covariance features as well as the original image intensity features. At stage (c), label fusion using the learned voting weights is performed for multi-atlas based ROI segmentation with brain MR images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun1a-2952079-large.gif
2020,8897080,Fig. 2.,"Architecture of mcRBM [26] used in this study. The orange line and green line represent twice and once connection, respectively. The mcRBM model only has the full connection between the input patch and hidden units, without any connection between the means hidden units and covariance hidden units. The covariance part has two layers, including the factor layer that connects twice to the input with the same filters, and the covariance layer that connects once to the factor layer. Also, the means part has one layer where the mean hidden units connect once to the input patch.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun2-2952079-large.gif
2020,8897080,Algorithm 1,High-Order Feature Learning Framework for Multi-Atlas Based Label Fusion,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun9-2952079-large.gif
2020,8897080,Fig. 3.,Visual illustration of surface distance between the segmentation results of different methods and ground truth on the left and right hippocampus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun3-2952079-large.gif
2020,8897080,Fig. 4.,"Segmentation results of 32 ROIs on the NIREP dataset achieved by LWV (yellow), PBM (cyan), JLF (blue), SPBM (orange) and our proposed HLF method (gray) in terms of DR values. Here, “$” and “*” denote that our method is significantly better than JLF and SPBM based on paired
t
-test (
p<0.05
), respectively. Meanwhile, our method is significantly better than the LWV and PBM in segmenting all ROIs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun4-2952079-large.gif
2020,8897080,Fig. 5.,"Segmentation result of 54 ROIs on the LONI-LPBA40 dataset achieved by LWV (yellow), PBM (cyan), JLF (blue), SPBM (orange) and our proposed HLF method (gray) in terms of DR values. “#”, “+”, “$” and “*” denote that our method is significantly better than LWV, PBM, JLF and SPBM based on paired
t
-test (
p<0.05
), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun5-2952079-large.gif
2020,8897080,Fig. 6.,"DR and ASD (
mm
) values achieved by the proposed HLF method using different numbers of atlases for hippocampus segmentation on the ADNI dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun6-2952079-large.gif
2020,8897080,Fig. 7.,"DR and ASD (
mm
) values achieved by the proposed HLF method, using different values of
λ
1
and and
λ
2
=0.01
for hippocampus segmentation on the ADNI dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun7-2952079-large.gif
2020,8897080,Fig. 8.,"DR and ASD (
mm
) values achieved by the proposed HLF method using
λ
1
=0.1
and different values of
λ
2
for hippocampus segmentation on the ADNI dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/8835130/8897080/sun8-2952079-large.gif
2020,9246504,FIGURE 1.,"The map of study area (a) two sites of study area are marked with black rectangles on Freeman-Durden decomposition RGB image where red is for double bounce, green is for volume and blue is for surface scattering components, (b) sample points are shown in red on the Google Earth image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar1ab-3035235-large.gif
2020,9246504,FIGURE 2.,"RGB images derived from preprocessed full scene Radarsat-2 data (a) 10 June 2015 and (b) 3 March 2016 where red is for HH polarization, green is for HV polarization, blue is for HH/HV, (c) 10 June 2015 and (d) 3 March 2016 where is red for double bounce, green is for volume and blue is for surface scattering components of Yamaguchi decomposition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar2abcd-3035235-large.gif
2020,9246504,FIGURE 3.,ANFIS architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar3-3035235-large.gif
2020,9246504,FIGURE 4.,"Scattering mechanisms (a) Surface scattering, (b) Double bounce scattering, (c) Volume scattering and (d) Helix scattering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar4abcd-3035235-large.gif
2020,9246504,FIGURE 5.,Flow chart of study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar5-3035235-large.gif
2020,9246504,FIGURE 6.,"The preprocessed Radarsat-2 SAR data scene which includes study area derived for 10 June 2015 using (a) sigma nought tecnique, (b) Yamaguchi, (c) van Zyl, (d) Freeman-Durden, (e) H/A/
α
and (f) Cloude decomposition models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar6abcdef-3035235-large.gif
2020,9246504,FIGURE 7.,"The preprocessed Radarsat-2 SAR data scene which includes study area derived for 3 March 3016 using (a) sigma nought tecnique, (b) Yamaguchi, (c) van Zyl, (d) Freeman-Durden, (e) H/A/
α
and (f) Cloude decomposition models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar7abcdef-3035235-large.gif
2020,9246504,FIGURE 8.,Soil moisture inversion using semiempirical models for 10 June 2015 Radarsat-2 data with Dubois (1995) model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar8-3035235-large.gif
2020,9246504,FIGURE 9.,Soil moisture inversion using semiempirical models for 3 March 2016 Radarsat-2 data with Oh (1992) model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar9-3035235-large.gif
2020,9246504,FIGURE 10.,"Soil moisture inversion using semiempirical models (a) 10 June 2015 Radarsat-2 data, (b) 3 March 2016 Radarsat-2 data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar10ab-3035235-large.gif
2020,9246504,FIGURE 11.,Soil moisture inversion using feature vector consists of Yamaguchi decomposition components by means of LS-SVM machine learning method for all samples of 10 June 2015.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar11-3035235-large.gif
2020,9246504,FIGURE 12.,Soil moisture inversion using feature vector consists of Yamaguchi decomposition components by means of LS-SVM machine learning method for all samples of 3 March 2016.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar12-3035235-large.gif
2020,9246504,FIGURE 13.,"RMSE (vol. %) values of soil moisture inversion using machine learning models from 10 June 2015 Radarsat-2 data for feature vector containing (a) Sigma nought values, (b) Yamaguchi decomposition components, (c) Generalized Freeman-Durden decomposition components, (d) van Zyl decomposition components, (e) Cloude decomposition components, (f) H/A/ \$\alpha \$ decomposition components.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar13abcdef-3035235-large.gif
2020,9246504,FIGURE 14.,"RMSE (vol. %) values of soil moisture inversion using machine learning models from 3 March 2016 Radarsat-2 data for feature vector containing (a) Sigma nought values, (b) Yamaguchi decomposition components, (c) Generalized Freeman Durden decomposition components, (d) van Zyl decomposition components, (e) Cloude decomposition components, (f) H/A/ \$\alpha \$ decomposition components.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9246504/acar14abcdef-3035235-large.gif
2020,9046761,FIGURE 1.,Actual thickness vs predicted thickness.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee1-2983188-large.gif
2020,9046761,FIGURE 2.,Thickness reduction concept by rolling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee2-2983188-large.gif
2020,9046761,FIGURE 3.,A machine learning model family.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee3-2983188-large.gif
2020,9046761,FIGURE 4.,Concept of data clustering based machine learning (DC-ML).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee4-2983188-large.gif
2020,9046761,FIGURE 5.,Prediction using an ML model family.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee5-2983188-large.gif
2020,9046761,Algorithm 1,Data Clustering based ML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee10-2983188-large.gif
2020,9046761,FIGURE 6.,Causal model for steel plate rolling factory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee6-2983188-large.gif
2020,9046761,FIGURE 7.,Overall average R2 score for roll force prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee7-2983188-large.gif
2020,9046761,FIGURE 8.,Overall average R2 score for plate thickness prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee8-2983188-large.gif
2020,9046761,FIGURE 9.,Overall average R2 scores for roll force prediction over clusters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046761/lee9-2983188-large.gif
2020,8859354,Fig. 1.,"Electrode structure, (a) conductive cloth electrode, (b) medical electrode.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao1-2945593-large.gif
2020,8859354,Fig. 2.,Experimental set-up of EIT gesture recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao2-2945593-large.gif
2020,8859354,Fig. 3.,Electrical model of electrode-skin contact impedance for gesture recognition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao3-2945593-large.gif
2020,8859354,Fig. 4.,"Three measured gestures in the experiments, Gesture 1, a fist, Gesture 2, palm open, Gesture 3, palm bent.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao4-2945593-large.gif
2020,8859354,Fig. 5.,Calculation method of the electrode-skin contact impedance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao5-2945593-large.gif
2020,8859354,Fig. 6.,Voltage comparison of three gestures among a series of electrode pair.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao6-2945593-large.gif
2020,8859354,Fig. 7.,Relationship between contact impedance and input voltage frequency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao7-2945593-large.gif
2020,8859354,Fig. 8.,Gesture recognition rate of three algorithms when using rectangular copper electrodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao8-2945593-large.gif
2020,8859354,Fig. 9.,Gesture recognition rate among six subjects with SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9108313/8859354/yao9-2945593-large.gif
2020,8786248,Fig. 1.,Overview of the proposed machine learning framework and the autoencoder-based semi-supervised learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar1-2932740-large.gif
2020,8786248,Fig. 2.,"Overview of the FAERS reports, drugs, drug pairs selection process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar2-2932740-large.gif
2020,8786248,Fig. 3.,"Confidence value matrix - feature and class label matrix for all drug pairs.
AE
columns indicate features (confidence values) extracted from adverse event reports; and the last column indicates class label of each drug pair.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar3-2932740-large.gif
2020,8786248,Fig. 4.,"Structure of an autoencoder with one hidden layer. The input
x
is mapped to the internal representation (or “code”)
h
via an encoder
f
; the code
h
is subsequently mapped to the output (or reconstruction)
x
^
via a decoder
g
. The objective is to have output
x
^
to approximate input
x
as close as possible by minimizing the mean squared errors between
x
and
x
^
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar4-2932740-large.gif
2020,8786248,Fig. 5.,Structure of a stacked autoencoder with three hidden layers. The resulting representation layer (“code”) of the first level autoencoder is used as inputs to train the second level autoencoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar5-2932740-large.gif
2020,8786248,Fig. 6.,Feature distributions for high-priority (positive class) and low-priority (negative class) DDIs. The subset of features shown in the figure represents around 10% of the total features used for model training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar6-2932740-large.gif
2020,8786248,Fig. 7.,"Learning curves of stacked autoencoders built with positive training samples
P
(
left
) and negative training samples
N
(
right
). Using Adam optimizer with learning rate = 0.001, decay = 1e-3, batch size = 64, and dropout rate = 0.3 (
left
) and 0.4 (
right
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar7-2932740-large.gif
2020,8786248,Fig. 8.,"Reconstruction performance for the testing set. The x-axis denotes the reconstruction errors via autoencoder built on positive set
P
; while the y-axis denotes the reconstruction errors via autoencoder built on negative set
N
. The
Δ
value is the difference between these two reconstruction errors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar8-2932740-large.gif
2020,8786248,Fig. 9.,"The selection of lower threshold
c
1
and the corresponding upper threshold
c
2
. The x-axis denotes the value of
c
1
represented as the
m
th
percentile of
Δ
i
; the value of
c
2
is fixed to be the
(100−m
)
th
percentiles to maintain the balance between reliable positives and negatives added to the DDI training set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar9-2932740-large.gif
2020,8786248,Fig. 10.,Performance (AUC) comparison using the proposed method and other methods on the benchmark dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/8949677/8786248/kumar10-2932740-large.gif
2020,8846235,Fig. 1.,(a) Proposed subthreshold voltage divider array PUF (b) nonlinearity in PUF array with 2 unit cells (c) comparator schematic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka1abc-2943121-large.gif
2020,8846235,Fig. 2.,Distribution of PUF differential output voltage for (a) when only 1 challenge input is ‘1’ (b) when all challenge inputs are ‘1’.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka2ab-2943121-large.gif
2020,8846235,Fig. 3.,Average simulated BER across supply voltage and temperature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka3-2943121-large.gif
2020,8846235,Fig. 4.,Die photo and layout.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka4-2943121-large.gif
2020,8846235,Fig. 5.,Measured normalized intra and inter-HD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka5-2943121-large.gif
2020,8846235,Fig. 6.,Average BER across supply voltage and temperature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka6-2943121-large.gif
2020,8846235,Fig. 7.,NIST randomness test results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka7-2943121-large.gif
2020,8846235,Fig. 8.,ML Prediction error vs training set size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka8-2943121-large.gif
2020,8846235,Fig. 9.,Measured secrecy capacity versus CRPs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9152175/8846235/venka9-2943121-large.gif
2020,9099439,Fig. 1.,Adversarial Machine Learning Cycle (AMLC).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg1-2968933-large.gif
2020,9099439,Fig. 2.,Attack types (* the scope of the paper).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg2-2968933-large.gif
2020,9099439,Fig. 3.,A generic system model of ML applications.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg3-2968933-large.gif
2020,9099439,Fig. 4.,Taxonomy of common ML algorithms in adversarial settings.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg4-2968933-large.gif
2020,9099439,Fig. 5.,Adversary's knowledge taxonomy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg5-2968933-large.gif
2020,9099439,Fig. 6.,A taxonomy of methods for generating adversarial samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg6-2968933-large.gif
2020,9099439,Fig. 7.,Clustering the state-of-the-art research on adversarial learning based on their ML algorithm and dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg7-2968933-large.gif
2020,9099439,Fig. 8.,Types of proactive defense strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7433297/9145895/9099439/sadeg8-2968933-large.gif
2020,8638802,Fig. 1.,Training data with (a) hybrid noise and (b) our model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang1ab-2894985-large.gif
2020,8638802,Algorithm 1:,Optimization Algorithm of HNOML,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang9-2894985-large.gif
2020,8638802,Fig. 2.,Example images used in our experiments. (a) Corel5k. (b) Pascal. (c) Espgame.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang2abc-2894985-large.gif
2020,8638802,Fig. 3.,"Robustness experiments with different types of noise. The first to third columns correspond to label noise, feature noise, and hybrid noise, respectively. (a)–(c), (d)–(f), and (g)–(i) correspond to Yeast, TMC, and Emotions, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang3abcdefghi-2894985-large.gif
2020,8638802,Fig. 4.,Example classification results on Corel5k.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang4-2894985-large.gif
2020,8638802,Fig. 5.,"Visualization of the label enrichment matrix
B
. (a) Scene. (b) Emotions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang5ab-2894985-large.gif
2020,8638802,Fig. 6.,Parameter tuning. (a) Label embedding. (b) Label enrichment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang6ab-2894985-large.gif
2020,8638802,Fig. 7.,Convergence experiment. (a) Pascal. (b) Espgame.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang7ab-2894985-large.gif
2020,8638802,Fig. 8.,Comparison of the proposed method (control algorithm) with other methods using the Bonferroni–Dunn test. (a) Hamming loss. (b) Ranking loss. (c) One-error. (d) Coverage. (e) Average precision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9089323/8638802/zhang8abcde-2894985-large.gif
2020,9144204,Fig. 1.,Filtered the spectral feature by MFCC processing in three domains.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong1-3010291-large.gif
2020,9144204,Fig. 2.,MLP structure for the classification of acoustic features from acquisition vehicle power transmission system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong2-3010291-large.gif
2020,9144204,Fig. 3.,Theoretical architecture of the linear support vector machine algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong3-3010291-large.gif
2020,9144204,Fig. 4.,"The experimental vehicle power transmission system used in this experiment, where the variety of 15 fault conditions was set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong4-3010291-large.gif
2020,9144204,Fig. 5.,Experimental structure and the analyzed method for vehicle power transmission system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong5-3010291-large.gif
2020,9144204,Fig. 6.,Microphone specification by the measurement VPTS operation signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong6-3010291-large.gif
2020,9144204,Fig. 7.,"The red line is the original signal for normal operation, and the blue line is the original signal for the first-order vibration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong7-3010291-large.gif
2020,9144204,Fig. 8.,Time domain diagram of the normal runup and first vibration original signals after MFCC enhanced filtering step.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong8-3010291-large.gif
2020,9144204,Fig. 9.,"The red line is the characteristic filtering of the vehicle engine normal runup in the normal condition, and the blue line is the filtering of the first-gear vibrated signal in the fault condition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong9-3010291-large.gif
2020,9144204,Fig. 10.,"Energy spectrum of the fifteen conditions of the sound signals after MFCC filtering in the experiment (a) MFCC filtering by two gain parameters a = 0.96, and a = 0.3 (b) Line graph represented by fifteen VPTS conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong10ab-3010291-large.gif
2020,9144204,Fig. 11.,Multiple hidden neurons neural networks structure for fault identification and classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong11-3010291-large.gif
2020,9144204,Fig. 12.,Using levenberg-marquardt optimizer function as training model (a) training performance (b) training state (c) error histogram (d) training regression (e) evaluated score.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong12abcde-3010291-large.gif
2020,9144204,Fig. 13.,Using SCG optimizer function with single hidden layer as training model (a) training performance (b) training state (c) error histogram (d) training regression (e) evaluated score.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong13abcde-3010291-large.gif
2020,9144204,Fig. 14.,Using SCG function with multiple hidden layers as training model (a) training performance (b) training state (c) error histogram (d) training regression (e) evaluated score.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong14abcde-3010291-large.gif
2020,9144204,Fig. 15.,New combination training model that the dimension reduction method with 5 hidden neurons of MLP structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong15-3010291-large.gif
2020,9144204,Fig. 16.,Three DNN training models by using PCA algorithm (a) 12 hidden neurons (b) 15 hidden neurons (c) 18 hidden neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9263090/9144204/gong16abc-3010291-large.gif
2020,9096623,Fig. 1.,Traditional KQIs-based hard decision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui1-2995160-large.gif
2020,9096623,Fig. 2.,Scenario description of QoE estimation using KQIs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui2-2995160-large.gif
2020,9096623,Fig. 3.,Data preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui3-2995160-large.gif
2020,9096623,Fig. 4.,KQIs samples statistical map after data preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui4-2995160-large.gif
2020,9096623,Fig. 5.,The proposed IPS-OCSVM framework of offline training and online detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui5-2995160-large.gif
2020,9096623,Fig. 6.,The proposed IPS-OCSVM vs. kernel functions. (a) Gaussian kernel. (b) Linear kernel. (c) Poly kernel. (d) Sigmoid kernel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui6-2995160-large.gif
2020,9096623,Fig. 7.,The proposed IPS-OCSVM vs. fluctuating thresholds.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9096623/gui7-2995160-large.gif
2020,8825553,Fig. 1.,"Parse tree for an example sentence: “He saw a bright light.” The table on the right illustrates the production rules used to derive the parse tree. The superscripts above the individual words denote their position within the sentence. Each dimension in a joint feature vector
ψ
corresponds to one type of production rules from a given grammar and the corresponding entries are counts of occurrences of the different rule types in a parse tree. Here, we use the tagset from the Penn Tree Bank [36] as node labels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8825553/bauer1-2934225-large.gif
2020,8825553,Fig. 2.,"Illustration of grammar binarization via binarization of trees. On the left is a simple example of a parse tree with four nodes. In the middle, we show its transformed version with five nodes based on the right-factored binarization. See [33] for more details on the binarization procedure. On the right, we show a copy of the binarized tree in the middle extended by additional parent annotation. In the label description of artificial nodes
A|C−D
:
A
before
|
denotes its parent and
C−D
the children of
A
in the original unbinarized tree, which are currently spanned by this artificial node. The superscript in each node denotes its parent label.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8825553/bauer2-2934225-large.gif
2020,8825553,Fig. 3.,"Illustration of the computational formula in (6). We consider a short phrase “a bright light” from an example sentence in Fig. 1. On the left, we have a corresponding part of the original unbinarized parse tree and on the right a binarized version with an additional parent annotation of the same noun phrase. The gray symbols denote either artificial nodes due to binarization or the parent annotation and will be removed after reversing the binarization. At the bottom, we give an example showing how the formula in (6) is applied during the inference to compute the values
Π
tp,fp
i,j,A
. Precisely, we consider the case with span boundaries
i=3,j=5
, the breakpoint
s=3
, and numbers of true and false positives
tp=4,fp=0
. The root symbol of the corresponding subtree is
NP
VP
. For simplicity, we also assume that the grammar contains only productions occurring in the parse tree example. Here,
y
¯
∗
denotes the original unbinarized ground-truth tree (on the left) and
y
∗
its binarized version (on the right). We can see that although our modified algorithm operates on the binarized trees, the corresponding loss statistics (
tp
,
fp
) are computed according to the structure of the unbinarized representation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8825553/bauer3-2934225-large.gif
2020,8825553,Fig. 4.,"Illustration of the loss difference (in
Δ
F
1
) on the test set when optimizing for
F
1
score on the original trees versus binary representation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8825553/bauer4-2934225-large.gif
2020,8825553,Fig. 5.,"Empirical running time for the inference algorithm solving the problem in (3) with
Δ
F
1
-loss. The graph represents an average computation time of the algorithm as a function of the sentence length. Note that we see only one graph since the deviation in time performance between the original algorithm in [1] and the modified version is not visible.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9133648/8825553/bauer5-2934225-large.gif
2020,8981961,FIGURE 1.,System model of the parameter server.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8981961/xiaop1-2971519-large.gif
2020,8981961,FIGURE 2.,System model of our DML-DIV Scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8981961/xiaop2-2971519-large.gif
2020,8981961,FIGURE 3.,Advertising recommendation application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8981961/xiaop3-2971519-large.gif
2020,8981961,FIGURE 4.,Comparison on time that data server generates proof.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8981961/xiaop4-2971519-large.gif
2020,8981961,FIGURE 5.,Comparison on TPA’s Verification time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8981961/xiaop5-2971519-large.gif
2020,8981961,FIGURE 6.,Timer per worker node spent on computing and waiting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8981961/xiaop6-2971519-large.gif
2020,9079640,Fig. 1.,Profiling for clustering algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang1-2990924-large.gif
2020,9079640,Fig. 2.,Profiling for deep neural networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang2-2990924-large.gif
2020,9079640,Fig. 3.,Profiling for collaborative filtering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang3-2990924-large.gif
2020,9079640,Fig. 4.,Profiling for genome sequencing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang4-2990924-large.gif
2020,9079640,Fig. 5.,Hot spot size for NetBench and MiBench.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang5-2990924-large.gif
2020,9079640,Fig. 6.,Accelerator architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang6-2990924-large.gif
2020,9079640,Fig. 7.,The out-of-order core architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang7-2990924-large.gif
2020,9079640,Fig. 8.,"Inter-task structural and data dependences representations, P1-3 represents Place, and T1-6 represents Transition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang8-2990924-large.gif
2020,9079640,Fig. 9.,Hardware structure of the OoO scheduler.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang9-2990924-large.gif
2020,9079640,Fig. 10.,Hierarchical framework of multi-execution flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang10-2990924-large.gif
2020,9079640,Fig. 11.,The accelerator design framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang11-2990924-large.gif
2020,9079640,Fig. 12.,Speedup of clustering algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang12-2990924-large.gif
2020,9079640,Fig. 13.,Speedup of K-means accelerators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang13-2990924-large.gif
2020,9079640,Fig. 14.,Speedup of genome sequencing accelerators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang14-2990924-large.gif
2020,9079640,Fig. 15.,Speedup of deep learning accelerators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang15-2990924-large.gif
2020,9079640,Fig. 16.,The speedup of collaborative filtering accelerators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang16-2990924-large.gif
2020,9079640,Fig. 17.,Performance comparison between three execution modes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang17-2990924-large.gif
2020,9079640,Fig. 18.,Trade-offs between computing time and data transfer time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang18-2990924-large.gif
2020,9079640,Fig. 19.,"Comparison of energy efficiency between our accelerators, CPU, and GPU implementations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9090388/9079640/wang19-2990924-large.gif
2020,9133591,FIGURE 1.,Flowchart of the EMD decomposition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian1-3007499-large.gif
2020,9133591,FIGURE 2.,EMD of a vibration signal of a mobile robotic roller bearing with inner-race fault.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian2-3007499-large.gif
2020,9133591,FIGURE 3.,Schematic of experimental mobile robotic equipment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian3-3007499-large.gif
2020,9133591,FIGURE 4.,Parallel EMD-SVM intelligent fault recognition model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian4-3007499-large.gif
2020,9133591,FIGURE 5.,Parallel EMD-SVM dataflow based on Spark big data cloud computing software framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian5-3007499-large.gif
2020,9133591,FIGURE 6.,Software architecture of Mesos.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian6-3007499-large.gif
2020,9133591,FIGURE 7.,Overall architecture of Spark running on Mesos as a big data cloud computing software framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian7-3007499-large.gif
2020,9133591,FIGURE 8.,Average classification accuracy curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian8-3007499-large.gif
2020,9133591,FIGURE 9.,Average accuracy (%) of parallel machine learning classification method under different training set sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian9-3007499-large.gif
2020,9133591,FIGURE 10.,Average recognition time (s) of the parallel machine learning classification methods under different training set sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian10-3007499-large.gif
2020,9133591,FIGURE 11.,Average recognition time for different cluster sizes in different big data cloud computing frameworks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian11-3007499-large.gif
2020,9133591,FIGURE 12.,Average recognition time (s) of different big data cloud computing frameworks under different input data sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian12-3007499-large.gif
2020,9133591,FIGURE 13.,Average recognition time (s) in the different distributed deployment modes of Spark under different cluster sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian13-3007499-large.gif
2020,9133591,FIGURE 14.,Average recognition time (s) in the different distributed deployment modes of Spark under different input data sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9133591/xian14-3007499-large.gif
2020,9050733,FIGURE 1.,The framework for identifying influential nodes in complex networks based on machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao1-2984286-large.gif
2020,9050733,FIGURE 2.,The training data constructed from node i.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao2-2984286-large.gif
2020,9050733,FIGURE 3.,The distribution of the labels of nodes in the network through SIR experiments and the calculation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao3-2984286-large.gif
2020,9050733,FIGURE 4.,The position of node i in the rank list of nodes according to the degree centrality value.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao4-2984286-large.gif
2020,9050733,FIGURE 5.,The corresponding position of node i in the labels’ distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao5-2984286-large.gif
2020,9050733,FIGURE 6.,The performance in CA-GrQc network as both the training network and the testing network. CA-GrQc network contains 5242 nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao6-2984286-large.gif
2020,9050733,FIGURE 7.,The performance in blogs network. blogs network contains 1224 nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao7-2984286-large.gif
2020,9050733,FIGURE 8.,The performance in soc-sign-bitcoin-alpha network. soc-sign-bitcoin-alpha network contains 3783 nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao8-2984286-large.gif
2020,9050733,FIGURE 9.,The performance in Wiki-Vote network. Wiki-Vote network contains 7115 nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao9-2984286-large.gif
2020,9050733,FIGURE 10.,The performance in CA-HepTh network. CA-HepTh network contains 9877 nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9050733/zhao10-2984286-large.gif
2020,8945334,Fig. 1.,IoV architecture-based edge computing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9128058/8945334/wang1-2962844-large.gif
2020,8945334,Fig. 2.,Concurrent data and similar images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9128058/8945334/wang2-2962844-large.gif
2020,8945334,Fig. 3.,Accuracy of three hashing algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9128058/8945334/wang3-2962844-large.gif
2020,8945334,Fig. 4.,Accuracy of two compression modes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9128058/8945334/wang4-2962844-large.gif
2020,8945334,Fig. 5.,Process of data augmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9128058/8945334/wang5-2962844-large.gif
2020,8945334,Fig. 6.,Performance of edge-based preprocessing methods. (a) Baselines and accuracy of image fingerprint (b) Recognition accuracy of three semi-supervised algorithms (c) Comparison of the different methods in recognition accuracy (d) Comparison of the different methods in delay.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9128058/8945334/wang6-2962844-large.gif
2020,9057416,FIGURE 1.,"Work flow to achieve objective: After collecting the data from healthy individuals, data is validated and pilot testing is performed. Further dataset is collected and its internal consistency is checked, issue of class imbalance is resolved. Data undergoes training and testing with ML models as SVM, KNN, ANN, Naïve Bayes, Decision tree. Models are further tuned to improve the performances using hyper parameters and finally results of all ML models are compared.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa1-2985717-large.gif
2020,9057416,FIGURE 2.,"Hyper plane separating data points: Distance form data points to separator is r,
ρ
is margin(width) between separator of classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa2-2985717-large.gif
2020,9057416,FIGURE 3.,Basic Structure of ANN with 5 hidden layers is shown where 28 features are given at input layer and Ayurveda dsoha type is received at output layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa3-2985717-large.gif
2020,9057416,FIGURE 4.,Evolution of decision tree algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa4-2985717-large.gif
2020,9057416,FIGURE 5.,RMSE of implemented machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa5-2985717-large.gif
2020,9057416,FIGURE 6.,Precision of implemented machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa6-2985717-large.gif
2020,9057416,FIGURE 7.,Recall values of implemented machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa7-2985717-large.gif
2020,9057416,FIGURE 8.,F-Score of implemented machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa8-2985717-large.gif
2020,9057416,FIGURE 9.,Accuracy of implemented machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9057416/madaa9-2985717-large.gif
2020,9072548,Fig. 1.,Schematic diagram of proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin1-2988393-large.gif
2020,9072548,Fig. 2.,Data distribution of suicide ideation vs. psychological stress.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin2-2988393-large.gif
2020,9072548,Fig. 3.,The flowchart of the proposed method. The input data are pre-processed by mini-max normalization and then partitioned to training and test sets for 10-fold cross validation. The synthetic minority over-sampling technique (SMOTE) for the minority data (those with suicide ideations) in training set is performed to balance the majority data of those without suicide ideations. The six proposed machine learning algorithms are individually trained by optimizing the corresponding hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin3-2988393-large.gif
2020,9072548,Fig. 4.,Data partition of 10-fold cross validation for training and test sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin4-2988393-large.gif
2020,9072548,Fig. 5.,Illustration of logistic regression used in the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin5-2988393-large.gif
2020,9072548,Fig. 6.,Illustration of decision tree used in the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin6-2988393-large.gif
2020,9072548,Fig. 7.,Illustration of multilayer perceptron used in the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin7-2988393-large.gif
2020,9072548,Fig. 8.,Receiver Operating Characteristic Curves and Precision Recall Curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin8-2988393-large.gif
2020,9072548,Fig. 9.,"A set of learning curves for each of the six machine learning methods. (a) Logistic Regressionn (b) Logistic Regression (c) Random Forest, (d) Gradient Boosting Decision Tree, (e) Support Vector Machine, (f) Multilayer Perceptron.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin9-2988393-large.gif
2020,9072548,Fig. 10.,Feature Importance for the machine learning methods in the proposed algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9130988/9072548/lin10-2988393-large.gif
2020,8955892,FIGURE 1.,a) Map of Pakistan and location of GB b) main districts of GB c) Selected study area.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan1ab-2965768-large.gif
2020,8955892,FIGURE 2.,"Flowchart of methodology to classify images into glaciers, debris-covered glaciers, and non-glacier area using three machine learning algorithms.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan2-2965768-large.gif
2020,8955892,FIGURE 3.,"(a). The focused area of study, from where the training and test dataset were extracted. (b). The reference image for the study areas showing three classes: glaciers, debris-covered glacier and non-glacier areas.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan3ab-2965768-large.gif
2020,8955892,FIGURE 4.,"Surface reflectance information for (a) land (non-glacier areas), (b) glacier, and (c) debris-covered glaciated areas.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan4abc-2965768-large.gif
2020,8955892,FIGURE 5.,Two points (A and B) and a line to observe change in land surface temperature with respect to elevation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan5-2965768-large.gif
2020,8955892,FIGURE 6.,Change in LST in winter and summer with the change in elevation between two points ‘A’ and ‘B’.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan6-2965768-large.gif
2020,8955892,FIGURE 7.,Snap shot of the first five rows (five points) with their corresponding column names from dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan7-2965768-large.gif
2020,8955892,FIGURE 8.,Classification of our selected sample area. a) classification by RF (b) classification by ANN (c) classification by SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan8abc-2965768-large.gif
2020,8955892,FIGURE 9.,Comparison of referenced datasets with their classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/8955892/khan9-2965768-large.gif
2020,9272728,FIGURE 1.,(a) Experimental setup of the RSS based VLP system. (b) Proposed VLP scheme using different unit cells to cover the whole indoor areas. AWG: arbitrary waveform generator; RTO: real-time oscilloscope; PD: photo-detector. (c) Photograph of the test-bed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow1abc-3041192-large.gif
2020,9272728,FIGURE 2.,Process of retrieving the RSS positioning information and the optical IDs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow2-3041192-large.gif
2020,9272728,FIGURE 3.,(a) Positions of training data (blue) and testing data (red) observed from the top. (b) Positions of the training data plane (blue) and testing data plane (red) observed from the side.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow3ab-3041192-large.gif
2020,9272728,FIGURE 4.,"One example of (a) the measured FFT spectrum of the received signal at the coordinate of (25, 16.24, 100). (b)-(d) The received IDs down-converted from three RF carrier frequencies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow4abcd-3041192-large.gif
2020,9272728,FIGURE 5.,"Experimental error distribution diagram of the 3D VLP system using LRML at vertical distances of (a) 95 cm, (b) 100 cm, (c) 115 cm and (d) 135 cm away from LED plane.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow5abcd-3041192-large.gif
2020,9272728,FIGURE 6.,"Experimental error distribution diagram of the 3D VLP system using LRML and SFDP at vertical distances of (a) 95 cm, (b) 100 cm, (c) 115 cm and (d) 135 cm away from LED plane.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow6abcd-3041192-large.gif
2020,9272728,FIGURE 7.,"Experimental error distribution diagram of the 3D VLP system using KRRML at vertical distances of (a) 95cm, (b) 100 cm, (c) 115 cm and (d) 135 cm away from LED plane.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow7abcd-3041192-large.gif
2020,9272728,FIGURE 8.,"Experimental error distribution diagram of the 3D VLP system using KRRML and SFDP at vertical distances of (a) 95cm, (b) 100 cm, (c) 115 cm and (d) 135 cm away from LED plane.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow8abcd-3041192-large.gif
2020,9272728,FIGURE 9.,"The experimental CDF against positioning error in horizontal direction at vertical distances of (a) 95 cm, (b) 100 cm, (c) 115cm and (d) 135 cm away from LEDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow9abcd-3041192-large.gif
2020,9272728,FIGURE 10.,"The experimental CDF of the vertical position error at vertical direction at (a) 95 cm, (b) 100 cm, (c) 115cm and (d) 135 cm away from LEDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9272728/chow10abcd-3041192-large.gif
2020,9105089,Fig. 1.,ELEMENT framework for multi-modal vessel segmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri1-2999257-large.gif
2020,9105089,Fig. 2.,Noiseless response of the Frangi filter (potential seed points).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri2-2999257-large.gif
2020,9105089,Fig. 3.,Pixel classification process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri3-2999257-large.gif
2020,9105089,Fig. 4.,ELEMENT segmentation results on images of the DRIVE dataset exhibiting retinal abnormalities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri4-2999257-large.gif
2020,9105089,Fig. 5.,ELEMENT segmentation results on images of the STARE dataset exhibiting retinal abnormalities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri5-2999257-large.gif
2020,9105089,Fig. 6.,"Retinal fluorescein angiograms, (a) and (e), their ground truth, (b) and (c), respectively, segmented by Rovira et al., [71], (c) and (g), respectively, and the ELEMENT framework, (d) and (h), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri6-2999257-large.gif
2020,9105089,Fig. 7.,"SLO images, (a) and (d), from the IOSTAR dataset. (b) and (e) show the ground truth of (a) and (d), respectively, while (c) and (f) are the results of ELEMENT's vessel segmentation, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri7-2999257-large.gif
2020,9105089,Fig. 8.,RC-SLO images and respective ELEMENT segmentation results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri8-2999257-large.gif
2020,9105089,Fig. 9.,ROC curves for the best and worst performances of ELEMENT on the retinal image datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9281055/9105089/rodri9-2999257-large.gif
2020,9113407,Fig. 1.,MmWave V2V communication with concurrent transmissions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9113407/he1-3001340-large.gif
2020,9113407,Fig. 2.,ASR of VANETs vs. V2V link density.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9113407/he2-3001340-large.gif
2020,9113407,Fig. 3.,Computational complexity comparison.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/25/9166810/9113407/he3-3001340-large.gif
2020,9062481,FIGURE 1.,Example EDA signal with two skin conductance responses (SCRs). The horizontal (red) dotted line on the first SCR represents the SCR amplitude. The vertical (red) dotted line on the first SCR represents the SCR duration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9062481/gjore1-2986810-large.gif
2020,9062481,FIGURE 2.,Two types of DL fusion approaches that are used in the study: early fusion and mid-fusion. The spectro-temporal ResNet (STRNet) is a special case of the mid-fusion approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9062481/gjore2-2986810-large.gif
2020,9062481,FIGURE 3.,"Distributions of the most informative features, namely, the features with the smallest p-value, for each type of driving session (ED, SD, CD, FDL, FDN) and overall.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9062481/gjore3-2986810-large.gif
2020,9062481,FIGURE 4.,"Precision-recall curves of the best-performing window classifier (blue) and session classifier (orange) that are built with XGB. AP denotes the average precision, which is defined as
∑
n
(R
n
−
R
n−1
)/
P
n
, where
R
n
and
P
n
are the recall and precision, respectively, for the
n
th
decision threshold.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9062481/gjore4-2986810-large.gif
2020,9099302,FIGURE 1.,Proposed workflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta1-2997311-large.gif
2020,9099302,FIGURE 2.,Death prediction by LR for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta2-2997311-large.gif
2020,9099302,FIGURE 3.,Death prediction by LASSO for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta3-2997311-large.gif
2020,9099302,FIGURE 4.,Death prediction by SVM for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta4-2997311-large.gif
2020,9099302,FIGURE 5.,Death prediction by ES for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta5-2997311-large.gif
2020,9099302,FIGURE 6.,New infected confirm cases prediction by LR for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta6-2997311-large.gif
2020,9099302,FIGURE 7.,New infected confirm cases prediction by LASSO for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta7-2997311-large.gif
2020,9099302,FIGURE 8.,New infected confirm cases prediction by SVM for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta8-2997311-large.gif
2020,9099302,FIGURE 9.,New infected confirm cases prediction by ES for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta9-2997311-large.gif
2020,9099302,FIGURE 10.,Recovery rate prediction by LR for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta10-2997311-large.gif
2020,9099302,FIGURE 11.,Recovery rate prediction by LASSO for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta11-2997311-large.gif
2020,9099302,FIGURE 12.,Recovery rate prediction by SVM for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta12-2997311-large.gif
2020,9099302,FIGURE 13.,Recovery rate prediction by ES for the upcoming 10 days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta13-2997311-large.gif
2020,9099302,FIGURE 14.,Mortality rate after 5 days of this study prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta14-2997311-large.gif
2020,9099302,FIGURE 15.,Recovery rate after 5 days of this study prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta15-2997311-large.gif
2020,9099302,FIGURE 16.,"Comparison between death rate, recovery rate and confirm case rate after 5 days of this study prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta16-2997311-large.gif
2020,9099302,FIGURE 17.,Ratio between recovery rate and death rate after 5 days of this study prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta17-2997311-large.gif
2020,9099302,FIGURE 18.,"ES performances on death rate, recovery rate and new confirmed case with 10–15 days intervals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta18-2997311-large.gif
2020,9099302,FIGURE 19.,All models predictions form 1/22/2020 to 4/6/2020 and real situation form 1/22/2020 to 4/6/2020.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta19-2997311-large.gif
2020,9099302,FIGURE 20.,Prediction intervals using LR for new confirmed forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta20-2997311-large.gif
2020,9099302,FIGURE 21.,Prediction intervals using LR for death rate forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta21-2997311-large.gif
2020,9099302,FIGURE 22.,Prediction intervals using LR for recovery rate forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9099302/rusta22-2997311-large.gif
2020,8664197,Fig. 1.,"The multi-view framework of the proposed method (supposed that the number of views is
m=3
). The training stage mainly emphasizes the view-specific weight learning cross different classes and the testing stage focuses on the strategy of category partition integrating multiple views.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9108388/8664197/han1-2904256-large.gif
2020,8664197,Fig. 2.,"Comparison of OURS and other methods on MSRCv1, Caltech101-7, SUN-01, Yale, ORL, and Notting-Hill datasets using all features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9108388/8664197/han2-2904256-large.gif
2020,8664197,Fig. 3.,"The confusion matrices of the proposed method on MSRCv1, Caltech101-7, Yale, and Notting-Hill datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9108388/8664197/han3-2904256-large.gif
2021,9079476,Fig. 1.,Composition of dialog system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song1-2985588-large.gif
2021,9079476,Fig. 2.,Flow chart of building dialog model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song2-2985588-large.gif
2021,9079476,Fig. 3.,Classification of dialog model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song3-2985588-large.gif
2021,9079476,Fig. 4.,Traditional ML algorithms used in dialog model (SVD is singular value decomposition).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song4-2985588-large.gif
2021,9079476,Fig. 5.,"DL algorithms used in dialog model (deep neural networks contain deep CNN, RNN, DBN, and other deep neural networks).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song5-2985588-large.gif
2021,9079476,Fig. 6.,RL algorithms used in dialog model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song6-2985588-large.gif
2021,9079476,Fig. 7.,Summary of the use of evaluation metrics in IR-based dialog models. The abscissa is the evaluation metric. The ordinate is occurrence frequencies of evaluation metrics appearing in articles. The legend on the right is the reference number.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song7-2985588-large.gif
2021,9079476,Fig. 8.,Summary of the use of evaluation metrics in generation-based dialog models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song8-2985588-large.gif
2021,9079476,Fig. 9.,Emotional chatting memory (ECM) model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song9-2985588-large.gif
2021,9079476,Fig. 10.,Classification of evaluation metrics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9079476/song10-2985588-large.gif
2021,8959387,Fig. 1.,Machining configuration variables in machining operations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li1-2961714-large.gif
2021,8959387,Fig. 2.,Structure of the CNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li2-2961714-large.gif
2021,8959387,Fig. 3.,Structure of the SAE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li3-2961714-large.gif
2021,8959387,Fig. 4.,Structure of the DBN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li4-2961714-large.gif
2021,8959387,Fig. 5.,Hardware platform and information flow in data acquirement platform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li5-2961714-large.gif
2021,8959387,Fig. 6.,Effects of the number of training samples on (a)–(c) prediction accuracy and (d) training time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li6abcd-2961714-large.gif
2021,8959387,Fig. 7.,Graphical representation of the CNN structure implemented in Case 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li7-2961714-large.gif
2021,8959387,Fig. 8.,Data reduction of (a) temperature data and (b) vibration data using PCA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li8ab-2961714-large.gif
2021,8959387,Fig. 9.,Frequency distribution and P–P plots of measured and simulated energy efficiency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li9-2961714-large.gif
2021,8959387,Fig. 10.,Effects of the number of training samples on (a) prediction accuracy and (b) training time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li10ab-2961714-large.gif
2021,8959387,Fig. 11.,Effects of (a) spindle motor aging and (b) tool wear on power consumption.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8856/9397439/8959387/li11ab-2961714-large.gif
2021,8972542,Fig. 1.,Machine learning applications on EEG have been developed based on supervised and unsupervised learning in the literature. Supervised learning is categorized to classification and regression which produce discrete and continuous accordingly. Unsupervised learning is categorized to clustering and dimensionality reduction which produce discrete and continuous accordingly.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse1-2969915-large.gif
2021,8972542,Fig. 2.,"The overall steps for EEG analysis by machine learning include preprocessing, feature extraction, feature selection, model training, model testing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse2-2969915-large.gif
2021,8972542,Fig. 3.,Higher dimension kernel separation. The kernel trick involves the transformation of the existing algorithm from a lower dimensional data set to a higher one.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse3-2969915-large.gif
2021,8972542,Fig. 4.,"Feedforward Neural Network. There are two directions for information flows, forward propagation and backpropagation. Forward propagation is used in the prediction time while backpropagation is used for adjusting the weights to minimize loss.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse4-2969915-large.gif
2021,8972542,Fig. 5.,"A radial basis function network is an ANN which uses radial basis functions as activation functions. A linear combination of radial basis functions of the inputs and the parameters of neurons is used for the output of the network. These structures have many applications such as time series prediction, classification, and function approximation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse5-2969915-large.gif
2021,8972542,Fig. 6.,Recurrent Neural Network where connections between nodes form a directed graph along a temporal sequence. It makes previous outputs to be used as inputs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse6-2969915-large.gif
2021,8972542,Fig. 7.,Example for decision tree technique to determine a health condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse7-2969915-large.gif
2021,8972542,Fig. 8.,Random Forest is an ensemble learning method which is used mostly for classification and regression. It operates by creating a multitude of decision trees on various sub-samples of the dataset and uses majority voting or averaging for finding output. This model improves the accuracy of prediction and can control over-fitting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse8-2969915-large.gif
2021,8972542,Fig. 9.,"Example for Fuzzy System. For a Fuzzy system to work effectively, the following features and components needs to be assured of performance: 1. Fuzzy sets, 2. Fuzzy Rules, 3. Fuzzy Logic Inference, 4. Fuzzy Score.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse9-2969915-large.gif
2021,8972542,Fig. 10.,"General K-means classification. K-means works based on using an algorithm to locate a partition in order to minimize the error between a cluster's empirical mean and points within. Using these K clusters, K-means tries to minimize the summation of the squared errors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse10-2969915-large.gif
2021,8972542,Fig. 11.,Operation of Reinforcement Learning. Software agents must to take suitable actions in an environment to maximize reward in a particular situation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/8972542/hosse11-2969915-large.gif
2021,9347828,Fig. 1.,"The predictor resides inside the evaluation module and is, hence, isolated from the search engine.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr1-3056950-large.gif
2021,9347828,Fig. 2.,Runtimes of DecisionTable.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr2-3056950-large.gif
2021,9347828,Fig. 3.,Learning curves of the three considered regressors for train time (left) and prediction time (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr3-3056950-large.gif
2021,9347828,Fig. 4.,"Runtime prediction performances of a random forest with 100 trees on datasets of size
50000×1000
. All 170 source datasets are considered, and a range of different parametrizations is considered for each base algorithm. Left and right column: Prediction results for training times and prediction times respectively. Rows group the ground truth runtimes into different bins in order to ease interpretation of the boxplots. Colors indicate degrees of acceptability for the prediction error.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr4-3056950-large.gif
2021,9347828,Fig. 5.,Top figure: Number of rejected executions. Bottom figures: (1) + (2) Number of correctly/incorrectly rejected executions relative to the number of total rejects. (3) + (4) number of correctly/incorrectly permitted executions relative to the number of total accepts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr5-3056950-large.gif
2021,9347828,Fig. 6.,"Absolute (Abs) and relative (Rel) comparison of prediction performances between the standard model
M
θ
and the models
M
−θ
for default parametrization (Def) and models
M
θ+d
for posteriors (Pos). Blue indicates improvements, white is neutral, and red indicates deteriorations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr6-3056950-large.gif
2021,9347828,Fig. 7.,Errors in the predicted number of features after pre-processor application with linear regression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr7-3056950-large.gif
2021,9347828,Fig. 8.,"Spy-wrapped base learners
b
1
,..,
b
k
leak feature transformations of the meta-learner and base learner runtimes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr8-3056950-large.gif
2021,9347828,Fig. 9.,Top 2 plots: Ratios of base learner train/prediction time and total meta-learner train/prediction time respectively. Bottom 2 plots: Abs/rel. difference of total true meta-learner runtime and the time predicted by (2) (in seconds).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr9-3056950-large.gif
2021,9347828,Fig. 10.,"Wasted computation times, numbers of successful evaluations, and hours spent in successful evaluations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9506969/9347828/mohr10-3056950-large.gif
2021,9095268,Fig. 1.,"(a) Total or apparent power
S
N
of the electrical network comprising of the active power
P
N
or the dissipated power and the reactive power
Q
N
or the power used for energy storage. The goal of the proposed resonant learning framework: (b) minimizing active power
P
N
during learning and ensuring
P
N
=0
, post-learning or steady state, and (c) maintaining
Q
N
=0
in learning and post-learning phases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr1abc-2984267-large.gif
2021,9095268,Fig. 2.,"Equivalent network model comprising of
N
electrical nodes, with an inductive and a capacitive element associated with each of the nodes. (a) Learning is equivalent to changing the values of inductive and capacitive elements. (b) In steady state, the network is driven into electrical resonance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr2-2984267-large.gif
2021,9095268,Fig. 3.,Illustration showing that operating in the complex domain allows different possible learning trajectories from an initial state to the final steady state. Regularization with respect to the phase factor could then be used to select the trajectory with an optimal active-power dissipation profile and results in limit cycle oscillations in steady state. The circles indicate the constant magnitude loci.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr3-2984267-large.gif
2021,9095268,Fig. 4.,"Cost function
L
1
plotted for different values of the hyperparameter
β
. (a)
β=0
. (b)
β=1
. (c)
β=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr4abc-2984267-large.gif
2021,9095268,Fig. 5.,"Comparison of the resonant optimization model (
M
r
) with its non-resonant variant (
M
nr
) for a quadratic objective function
L
N
shown in (19). For this experiment,
N=5
, input frequency
ω=π/10
, and the mean response is estimated over ten trials with random initializations of
V
i
,
I
i
,
ϕ
i
∀i=1,…,5
. (a) Comparison of the time evolution of
H
N
for
M
nr
and
M
r
. (b) Comparison of the time evolution of
D
N
(
∑
N
i=1
|
V
i
|
2
|
I
i
|
2
for
M
nr
and
∑
N
i=1
|
V
i
|
2
|
I
i
|
2
cos
2
ϕ
i
for
M
r
). For all the curves, the line indicates the mean response, while the shaded region shows the standard deviation about the mean across the ten trials. The true dissipation (
∑
N
n=1
|
V
i
||
I
i
|
for
M
nr
and
∑
N
n=1
|
V
i
||
I
i
|cos
ϕ
i
for
M
r
) over the ten trials is also shown. Phasor representations of the
LC
tank voltages and currents for a single trial. (c) Initial configuration and (d) final configuration for
M
r
. For
M
r
,
β=1
for all the trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr5-2984267-large.gif
2021,9095268,Fig. 6.,"Comparison of the performance of the resonant model
M
r
for different choices of annealing schedule for
β
(
N=5,ω=π/10
). (a) Time evolution of
H
N
(inset shows a zoomed-in view of the cost evolution in the transient phase). (b) Time evolution of
D
N
. (c) Time evolution of
β
. In all the cases, the optimization process starts after
0.1a.u
. from the onset of the simulation. The curves corresponding to
β=1
denotes the case when
β
takes a constant value from
t=0.1a.u
.,
β=logistic
corresponds to the cases when
β
is slowly increased following a logistic curve from
t=0.1a.u
. and takes on a maximum value of
β=1
, and
β=switching
corresponds to the case when
β
switches from a minimum value(=0) to a maximum value(=1) at
t=0.3 a.u
., after the system has converged to the optimal solution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr6abc-2984267-large.gif
2021,9095268,Fig. 7.,"Comparison of the active and reactive power dissipated at each node for the non-resonant model
M
nr
and the resonant model
M
r
for the synthetic one-class SVM problem on a two-dimensional dataset (Dataset I), with
N=300,ν=0.1,ω=π/4
, and random initializations for
V
i
,
I
i
,
ϕ
i
∀i=1,…,N
. (a) and (c) Values of the active power metric (
=|
V
i
||
I
i
|
) at each node in the initial and final stages, respectively, for
M
nr
. (b) and (d) Values of the reactive power metric (=0) at each node in the initial and final stages, respectively, for
M
nr
. (e) and (g) Values of the active power metric (
=|
V
i
||
I
i
|cos
ϕ
i
) at each node in the initial and final stages, respectively, for
M
r
. (f) and (h) Values of the reactive power metric (
=|
V
i
||
I
i
|sin
ϕ
i
) at each node in the initial and final stages, respectively, for
M
r
. For both models,
K(⋅,⋅)
was chosen to be a Gaussian kernel with kernel parameter
σ=1
, and
β=1
throughout the optimization process for
M
r
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr7abcdefghi-2984267-large.gif
2021,9095268,Fig. 8.,"Comparison of the performance of the resonant model
M
r
for different choices of annealing schedules for
β
for the one-class SVM problem, on three different two-dimensional synthetic datasets for a simulation duration of
t=4a.u
. In all cases,
N=300,ν=0.1,ω=π/20
, and
K(⋅,⋅)
was chosen to be a Gaussian kernel with parameter values
σ=1,10
and 20 for synthetic Datasets I-III, respectively. In addition, for each dataset,
V
i
,
I
i
,
ϕ
i
were randomly initialized
∀i=1,…,N
. (a) Contour plot with the decision boundary around the data points and SVs. (b) Time evolution of
H
(inset shows a zoomed-in view of the cost evolution in the transient phase). (c) Time evolution of
D
. (d) Time evolution of
β
for different annealing schedules. The curves corresponding to
β=1
and 10 denote the cases when
β
takes a constant value throughout the simulation duration;
β=
logistic
1
and
β=
logistic
2
correspond to the cases when
β
is slowly increased following a logistic curve and takes on maximum values of
β
max
=1
and
β
max
=10
, respectively;
β=switching
corresponds to the case when
β
switches from a minimum value (
β
min
=0
) to a maximum value (
β
max
=10
) at
t=2a.u
., after the system has converged to the optimal solution. (e)–(h) Similar plots on Dataset II. (i)–(l) Plots corresponding to Dataset III.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr8abcdefghijkl-2984267-large.gif
2021,9095268,Fig. 9.,"Robustness to random initialization. Comparison of (a) time evolution of the cost
H
and (b) dissipation metric profile
D
(
∑
N
n=1
|
V
i
|
2
|
I
i
|
2
for the non-resonant model
M
nr
and
∑
N
n=1
|
V
i
|
2
|
I
i
|
2
cos
2
ϕ
i
for the resonant model
M
r
, respectively) for the synthetic one-class SVM problem on Dataset I for
N=300,ν=0.1
, and
ω=π/8
over ten random initializations for
V
i
,
I
i
,
ϕ
i
∀i=1,…,N
. The regularization parameter was chosen to be
β=1
for the entire simulation duration of
t=4a.u
. of the optimization process for
M
r
. For all the curves, the solid line indicates the mean response, whereas the shaded region shows the standard deviation about the mean across the trials.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr9ab-2984267-large.gif
2021,9095268,Fig. 10.,"Comparison of the dissipation profiles of the resonant model
M
r
for different choices of annealing schedules for
β
for different real-life benchmark datasets. (a) ‘Iris’. (b) ‘Heart’. (c) ‘Diabetes’. (d) ‘Ecoli’. (e) ‘Adulta3a’. (f) ‘Mammography’.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr10abcdef-2984267-large.gif
2021,9095268,Fig. 11.,"Circuit- and phasor-based representations for a one-class SVM problem. (a) Support vectors, corresponding to resonant
LC
tanks. (b) Interior points, corresponding to sinks/ground (
V
i
,
I
i
=0
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr11ab-2984267-large.gif
2021,9095268,Fig. 12.,"LC
tank resonator.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9365681/9095268/chakr12-2984267-large.gif
2021,9354058,Fig. 1.,Secure machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang1-3059108-large.gif
2021,9354058,Fig. 2.,Time breakdown for two-party computation. We put the MNIST dataset in one batch for illustration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang2-3059108-large.gif
2021,9354058,Fig. 3.,ParSecureML overview.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang3-3059108-large.gif
2021,9354058,Fig. 4.,Data processing in offline phase.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang4-3059108-large.gif
2021,9354058,Fig. 5.,"Pipeline to overlap PCIe data transmission and GPU computation.
D
represents the result of “
(−i)×E+
A
i
”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang5-3059108-large.gif
2021,9354058,Fig. 6.,Pipeline execution in ParSecureML. The operation (forward) and operation (backward) are conducted on GPUs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang6-3059108-large.gif
2021,9354058,Fig. 7.,"The performance of cuRAND on the GPU and MT19937 on the CPU. Matrix dimension
n
represents that the matrix size of
n×n
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang7-3059108-large.gif
2021,9354058,Fig. 8.,Time proportion of GEMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang8-3059108-large.gif
2021,9354058,Fig. 9.,Tensor core operation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang9-3059108-large.gif
2021,9354058,Fig. 10.,Overall performance speedups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang10-3059108-large.gif
2021,9354058,Fig. 11.,Online performance speedups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang11-3059108-large.gif
2021,9354058,Fig. 12.,Offline performance speedups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang12-3059108-large.gif
2021,9354058,Fig. 13.,Inference performance speedups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang13-3059108-large.gif
2021,9354058,Fig. 14.,CPU optimization benefit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang14-3059108-large.gif
2021,9354058,Fig. 15.,GPU optimization benefit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang15-3059108-large.gif
2021,9354058,Fig. 16.,Communication benefits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang16-3059108-large.gif
2021,9354058,Fig. 17.,Performance of different workload size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/71/9380848/9354058/zhang17-3059108-large.gif
2021,9252851,Fig. 1.,Application scenarios of CPS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat1-3036778-large.gif
2021,9252851,Fig. 2.,Organization of the tutorial.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat2-3036778-large.gif
2021,9252851,Fig. 3.,Figure showing types of ML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat3-3036778-large.gif
2021,9252851,Fig. 4.,Interactions between the agent and environment in a RL system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat4-3036778-large.gif
2021,9252851,Fig. 5.,Applications of ML in CPS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat5-3036778-large.gif
2021,9252851,Fig. 6.,Figure showing communications in a VANET.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat6-3036778-large.gif
2021,9252851,Fig. 7.,Illustration of Medical Cyber Physical System.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat7-3036778-large.gif
2021,9252851,Fig. 8.,Application of DNN in autonomous vehicles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat8-3036778-large.gif
2021,9252851,Fig. 9.,Figure showing the operation of a GAN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat9-3036778-large.gif
2021,9252851,Fig. 10.,Figure showing classification of attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat10-3036778-large.gif
2021,9252851,Fig. 11.,Illustration of adversarial attack on ML-enabled CPS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat11-3036778-large.gif
2021,9252851,Fig. 12.,Categorization of methods for defense against adversarial attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9361471/9252851/rawat12-3036778-large.gif
2021,9410293,Fig. 1.,"Incremental strain curve and feature collection for a given pixel
P(
X
1
,
Y
1
)
in the apical four-chamber view ventricular mask.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki1-3074808-large.gif
2021,9410293,Fig. 2.,"Multi-2D isochrones generated with different heuristic-based automated approaches on the example of an anterior LV paced canine taken from the testing dataset and compared to the manually generated ground truth isochrones. The multi-2D isochrones are shown in the four apical views (from left to right: 4, 3.5, 2 and 3-chamber view) for: a) Manual ground truth; b) Naive heuristic-based automated approach selecting always the first occurring ZCs; c) Rule-based automated approach selecting the ZCs satisfying conditions (1) and (2). ANT = anterior; LAT = lateral, POST = posterior; SEPT = septum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki2abc-3074808-large.gif
2021,9410293,Fig. 3.,"Precision-recall curves for the three ML models (Left column: logistic regression in red, Center: SVM in blue, and Right column: Random Forest in green). The curves are displayed for performances evaluated on: a) the human validation dataset; b) the human test dataset; and c) the canine test dataset. The ML models performance are shown for their tuned hyperparameters: elastic net L1-ratio = 0.5; kernel = radial basis function and n estimators = 200.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki3abc-3074808-large.gif
2021,9410293,Fig. 4.,"3D-rendered isochrones generated with different automated approaches on the example of an antero-lateral LV paced canine taken from the testing dataset compared to the manually generated isochrone. The 3D-rendered isochrones are shown for: a) Manual ground truth; b) Naive heuristic-based automated approach selecting always the first occurring ZCs; c) Rule-based heuristic automated approach selecting the ZCs satisfying conditions (1) and (2); d) ML model with logistic regression classifier; e) ML model with SVM classifier; and f) ML model with Random Forest classifier. The corresponding EnSite electroanatomic map (EAM) is included for a limited epicardial surface of the anterior LV, accessible with the mapping catheter in the chest cavity during the canine experiment. The middle row for (d-f) corresponds to the ML models results with the
2
nd
voting approach: probability threshold applied to the ZC candidates (4) and satisfying a recall >70%, while the bottom row for (e-f) sets the condition to a recall >40%. Another key difference for the very last row is the considerably larger amount of pixels the models were applied on
(P(
X
i
,
Y
i
)with i∈[1:2500])
, not previously used in the initial ground truth manual ZC selection. The ML model-based automated algorithms generated the isochrones with their tuned hyperparameters: elastic net L1-ratio = 0.5; kernel = radial basis function and n estimators = 200. ANT = anterior; LAT = lateral, POST = posterior; SEPT = septum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki4abcdef-3074808-large.gif
2021,9410293,Fig. 5.,"Incremental axial strain curves for 4 points in the LV antero-lateral wall of the apical 3.5-chamber view isochrone from the LV paced canine (Fig. 4). The circled region over the myocardial isochrone mask outlines the basal region of the antero-lateral wall which displayed a few discrepancies on the corresponding 3D-rendered isochrones (dashed boxes on Fig. 4-f) compared to the manually generated one (Fig. 4-a). This region also corresponds to the area of lower recall circled in red on Supplemental Figure 1 for the Random Forest model with the voting approaches that apply a probability threshold to the candidates, and consequently matches the area of lower spatial sampling in Supplemental Figure 2. The different zero-crossing candidates are displayed in red on the strain curves for the given 4 points (
P
i
) and the chosen candidates are listed in the Table at the bottom for each of the manual selection and three Random Forest approaches (no voting threshold, recall >70% and recall >40%). The Random Forest classifier selected a different ZC candidate than the manual ground truth for
P
3
and discarded point
P
2
altogether due to low voting confidence in the case of both approaches with the recall conditions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki5-3074808-large.gif
2021,9410293,Fig. 6.,Incremental axial strain curves for 4 points in the RV free wall (black dashed outline) from the apical 3.5-chamber view isochrone of the antero-lateral LV paced canine (Fig. 4 and Fig. 5). The different zero-crossing candidates are displayed in red on the curves. The circled region over the myocardial mask corresponds to the area of recovered precision from Supplemental Figure 1 for the Random Forest model with the 2nd voting approach: probability threshold applied to the ZC candidates while satisfying a given set recall value.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki6-3074808-large.gif
2021,9410293,Fig. 7.,"3D-rendered isochrones generated with different automated approaches on the example of a patient in sinus rhythm taken from the human testing dataset compared to the manually generated isochrone. The 3D-rendered isochrones are shown for: a) Manual ground truth; b) Naive heuristic-based automated approach selecting always the first occurring ZCs; c) Rule-based heuristic automated approach selecting the ZCs satisfying conditions (1) and (2); d) ML model with logistic regression classifier; e) ML model with SVM classifier; and f) ML model with Random Forest classifier. The middle row for (d-f) corresponds to the ML models results with the
2
nd
voting approach: probability threshold applied to the ZC candidates (4) and satisfying a recall >70%, while the bottom row for (e-f) sets the condition to a recall >40%. Another key difference for the very last row is the considerably larger amount of pixels the models were applied on
(P(
X
i
,
Y
i
)with i∈[1:2500])
, not previously used in the initial ground truth manual ZC selection. The ML model-based automated algorithms generated the isochrones with their tuned hyperparameters: elastic net L1-ratio = 0.5; kernel = radial basis function and n estimators = 200. ANT = anterior; LAT = lateral, POST = posterior; SEPT = septum.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9526230/9410293/melki7abcdef-3074808-large.gif
2021,9311173,FIGURE 1.,The overall framework of network intrusion detection model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9311173/wang1-3048198-large.gif
2021,9311173,FIGURE 2.,Use t-SNE to visualize NSL-KDD(a) and CSE-CIC-IDS2018(b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9311173/wang2ab-3048198-large.gif
2021,9311173,FIGURE 3.,"F1-Score of DSSTE algorithm with different scaling factor
K
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9311173/wang3ab-3048198-large.gif
2021,9311173,FIGURE 4.,Comparison of the performance of different sampling methods(Accuracy and F1-Score are the average of each classifier).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9311173/wang4-3048198-large.gif
2021,9311173,FIGURE 5.,Confusion matrix of CIC-IDS-2018 by DSSTE+minVGGNet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9311173/wang5-3048198-large.gif
2021,9423959,FIGURE 1.,E-learning features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn1-3077663-large.gif
2021,9423959,FIGURE 2.,ML classification for e-learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn2-3077663-large.gif
2021,9423959,FIGURE 3.,Student modeling in e-learning systems [49].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn3-3077663-large.gif
2021,9423959,FIGURE 4.,DNN predicting the student performance in e-learning environment [69].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn4-3077663-large.gif
2021,9423959,FIGURE 5.,Student performance prediction using clustering [82].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn5-3077663-large.gif
2021,9423959,FIGURE 6.,Data distillation using training the student model [50].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn6-3077663-large.gif
2021,9423959,FIGURE 7.,Comprehensive ML accuracy in e-learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9423959/shabn7-3077663-large.gif
2021,9364756,Fig. 1.,"Flowchart describing data processing and model training. (ICA: independent component analysis, PSD: power spectral density, SE: spectral entropy, PAC: phase-amplitude coupling, LDA: linear discriminant analysis, PCA: principal component analysis, FSFS: forward sequential feature selection, BSFS: backwards sequential feature selection).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9580687/9364756/ye2-3062502-large.gif
2021,9364756,Fig. 2.,"Performance of models trained with features selected by linear discriminant analysis (LDA). The figure shows distribution of accuracies for 1000 iterations of training using randomly labeled data (orange), true labeled data (blue), and independent dataset (green line) for models based on features selected by LDA. Black and green dotted lines show ZeroR benchmarks for cross-validation (CV) and independent validation (IV) respectively. All models trained with true labeled data performed significantly better than randomly labeled data at 10-10 confidence interval in 10-fold CV, except for SVM fine Gaussian models (two sample K-S test). (SVM: support vector machine, KNN: K-nearest neighbors).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9580687/9364756/ye4-3062502-large.gif
2021,9364756,Fig. 3.,"Comparison of performance of models for classifying patients with TBI history from normal subjects. (a) and (b) show boxplots of accuracy of models trained with features selected by different methods. Accuracy was evaluated with 10-fold CV and independent dataset respectively. Models trained with PCA selected features performed worst in both 10-fold CV and independent validation, while those trained with features selected by LDA performed best. Models trained with features selected by Statistics performed inferior to those with LDA in CV, however, their performance was comparable with LDA models when used to classify independent dataset. (c) and (d) compare the accuracy of models trained with input features including demographic information and those without demographic information (Demo: demographic). The majority models with demographic inputs appear to perform better than their counterparts. (e) and (f) compare the performance of models trained with features generated from artifact removed clean EEG data versus those from raw EEG. Though variability is present, most models trained with raw data performed slightly better than the corresponding clean data in CV. And performance of models from raw data was comparable with those from clean data for predicting independent dataset. Each line in (c) to (f) represents each algorithm. Dark lines in (c) and (e) indicate significant difference in two sample K-S test at 10-10 significance level. (* p < 0.05, ** p < 0.01, *** p < 0.001, One way ANOVA and post-hoc Tukey test in (a) and (b), Signed-rank test in (c) to (f).) (CV: cross-validation, Stats: statistics, LDA: linear discriminant analysis, FSFS: forward sequential feature selection, PCA: principal component analysis, BSFS: backwards sequential feature selection.).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9580687/9364756/ye6-3062502-large.gif
2021,9364756,Fig. 4.,"Comparison of performance of models for 3-class classification. (a) and (b) show boxplots of accuracy of models trained with features selected by different methods for classifying patients with TBI and stroke history and normal subjects. Accuracy was evaluated with 10-fold CV and independent dataset respectively. Models trained with BSFS and LDA selected features performed best in 10-fold CV. In IV, models trained with features selected by LDA, statistics, and FSFS showed comparable performance. (c) and (d) compare the accuracy of models trained with input features including demographic information and those without demographic information (Demo: demographic). The majority models with demographic inputs appear to perform better than their counterparts. (e) and (f) compare the performance of models trained with features generated from artifact removed clean EEG data versus those from raw EEG. The majority models built upon clean data performed significantly better than raw data. (* p < 0.05, ** p < 0.01, *** p < 0.001, One way ANOVA and post-hoc Tukey test in (a) and (b).) (CV: cross-validation, Stats: statistics, LDA: linear discriminant analysis, FSFS: forward sequential feature selection, PCA: principal component analysis, BSFS: backwards sequential feature selection).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9580687/9364756/ye7-3062502-large.gif
2021,9364756,Fig. 5.,"Relationship between IV and CV accuracies for two-class (a) and three-class (b) classification. The respective ZeroR benchmarks for CV and IV are shown as black lines. (CV: cross-validation, IV: independent validation, LDA: linear discriminant analysis, FSFS: forward sequential feature selection, PCA: principal component analysis, Stats: statistics, BSFS: backwards sequential feature selection.).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9580687/9364756/ye8-3062502-large.gif
2021,9364756,Fig. 6.,"Changes in clean EEG features. (a) shows the fraction of features selected by statistics, LDA, and FSFS out of total number of features in each type of features (i.e., 171 coherence and 19 relative PSD features in each frequency band) without consideration of channels for 2-class classification. (b) shows the fraction of selected features for 3-class classification. (c) shows the median z score for each type of features in normal, TBI and stroke subjects respectively. (d) shows the broadband coherence change from normal to TBI. Main panel shows the median z score of coherence coefficients of all channel pairs. Inset demonstrates the channel pairs with median z score higher than 0.5 or lower than -0.5. (e) shows the topographic map of relative PSD based on z scores. (f) indicates the z score of stroke broadband coherence to TBI. Inset shows the channel pairs with median z score higher than 0.5 or lower than -0.5. (g) shows the topographic map of relative PSD z score of stroke subjects to TBI. (LDA: linear discriminant analysis, FSFS: forward sequential feature selection, Stats: statistics, PAC: phase-amplitude coupling, PSD: power spectral density).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9580687/9364756/ye9-3062502-large.gif
2021,9106879,Fig. 1.,Percentage of equilibrium point deviation to create peripheral point.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-1-source-large.gif
2021,9106879,Fig. 2.,Samples and sub-samples in training process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-2-source-large.gif
2021,9106879,Fig. 3.,SVM confusion matrix in test process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-3-source-large.gif
2021,9106879,Fig. 4.,CART confusion matrix in test process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-4-source-large.gif
2021,9106879,Fig. 5.,SVM confusion matrix in test process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-5-source-large.gif
2021,9106879,Fig. 6.,Classification of created operation points in 11 scenarios into collusion and collusion-free classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-6-source-large.gif
2021,9106879,Fig. 7.,F1score curve in collision detection data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-7-source-large.gif
2021,9106879,Fig. 8.,Classification of created operation points for 10 scenarios in bilateral and multi-lateral collusion classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8685265/9334117/9106879/017-fig-8-source-large.gif
2021,9121314,Fig. 1.,"Overall network architecture for the proposed methods. We adopt ResNet50 as the backbone network. The architecture consists of three branches. The global branch mainly extracts the global features of pedestrians, the multiscale feature representation learning branch intends to explore local detailed features and spatial correlations. The RBFM branch aims to learn the discriminative and attentive local representation features. The whole architecture is trained by five losses functions: one triplet loss and four identification losses.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9645278/9121314/wu1-3003674-large.gif
2021,9121314,Fig. 2.,"Comparison of DropBlock, BDB, and RBFM methods on the feature maps within the same batch. (a) DropBlock. (b) BDB. (c) RBFM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9645278/9121314/wu2abc-3003674-large.gif
2021,9121314,Fig. 3.,(a) Traditional multiscale feature learning manner embedded the multiscale feature extraction modules. (b) Multiscale representation learning strategy in a granular lever for the PReID task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9645278/9121314/wu3ab-3003674-large.gif
2021,9121314,Fig. 4.,MFRL module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9645278/9121314/wu4-3003674-large.gif
2021,8998334,Fig. 1.,Taxonomy of multi-view representation learning literatures. (a) Joint representation fuses multiple views through concatenation. (b) Alignment representation aligns features by utilizing consensus property. (c) Shared and specific representation employs multiple feature extractors for each view and inherits the advantages of (a) and (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing1-2973634-large.gif
2021,8998334,Fig. 2.,"Overall architecture and alternative optimization of SMDDRL. SMDDRL contains two parts: Multi-view Deep Discriminant Representation Learning (MDDRL) network as well as deep metric learning and density clustering based semi-supervised learning framework. Building on the representation learning and classification backbone, MDDRL adds two components: a) deep metric learning for better discriminability; b) orthogonality and adversarial similarity constraints for disentangling shared and specific features. Semi-supervised learning framework contains deep metric learning (in MDDRL) and density peak clustering, and they are trained alternatively (Step1-Step 4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing2-2973634-large.gif
2021,8998334,Fig. 3.,(a) Architecture of MDDRL. (b) Details of multi-view deep representation learning with orthogonality and adversarial similarity constraints. (c) Details of adversarial similarity constraint and loss.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing3-2973634-large.gif
2021,8998334,Fig. 4.,"Representations learned with and without deep metric learning. Coordinates are collected from 525 WebKB samples. (a) is the result with Siamese network (
Margin=3.0
), (b) is the result without it. The results demonstrate that deep metric learning makes the decision boundaries clearer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing4-2973634-large.gif
2021,8998334,Fig. 5.,"Classification accuracy and F1-score on WebKB (a, c) and AD (b, d) datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing5-2973634-large.gif
2021,8998334,Fig. 6.,Image classification results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing6-2973634-large.gif
2021,8998334,Fig. 7.,Classification results on BBC dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing7-2973634-large.gif
2021,8998334,Fig. 8.,"Ablation study on WebKB, Noisy MNIST, and BBC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing8-2973634-large.gif
2021,8998334,Fig. 9.,"Comparison of the representations and losses obtained from the original contrastive loss (a, c) and the improved contrastive loss (b, d) on WebKB and Noisy MNIST. The
Margin
is set as 3.0 for WebKB, and 50.0 for Noisy MNIST.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing9-2973634-large.gif
2021,8998334,Fig. 10.,"Explanation of our improvement in Siamese Network from the perspective of margin theory. (a) Improved loss pulls samples from the same class to their class center, while traditional contrastive loss randomly pulls two samples from the same class together. (b) Margin distribution comparison between our improved loss and contrastive loss.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing10-2973634-large.gif
2021,8998334,Fig. 11.,"Influence of trade-off parameters
λ
1
and
λ
2
on WebKB, Noisy MNIST, and BBC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing11-2973634-large.gif
2021,8998334,Fig. 12.,Convergence analysis of SMDDRL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9448371/8998334/jing12-2973634-large.gif
2021,9357977,Fig. 1.,"Sensor axes orientation in reference to the participant. x is the superior–inferior axis, y is the medial-lateral axis, and z is the anterior–posterior axis. Roll describes the angle about the anterior–posterior axis, pitch describes the angle about the medial-lateral axis and yaw describes the angle about the inferior–superior axis.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9360217/9357977/espin1-3060376-large.gif
2021,9357977,Fig. 2.,"Estimation plot demonstrating the difference in model weighted average F1-score using LOSO and train/test split, training, and validation methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9360217/9357977/espin2-3060376-large.gif
2021,9357977,Fig. 3.,"Weighted average for each validation method and model type. (a) Precision score, (b) recall score, (c) F1-score, and (d) accuracy score.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9360217/9357977/espin3-3060376-large.gif
2021,9357977,Fig. 4.,Confusion matrix demonstrating classification performance for MLP-NN using LOSO training and validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9360217/9357977/espin4-3060376-large.gif
2021,9357977,Fig. 5.,Confusion matrix demonstrating classification performance for GSVM using 75/25 train/test split training and validation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9360217/9357977/espin5-3060376-large.gif
2021,9343347,Fig. 1.,Overview of the proposed framework for posture detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9370028/9343347/liaqa1-3055898-large.gif
2021,9343347,Fig. 2.,Proposed Hybrid Classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9370028/9343347/liaqa2-3055898-large.gif
2021,9143412,Fig. 1.,General BCI system architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv1-3010014-large.gif
2021,9143412,Fig. 2.,Three types of BCIs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv2-3010014-large.gif
2021,9143412,Fig. 3.,Correct projection of linear discriminant analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv3-3010014-large.gif
2021,9143412,Fig. 4.,Error projection of linear discriminant analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv4-3010014-large.gif
2021,9143412,Fig. 5.,EEG signal classification results in movement test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv5-3010014-large.gif
2021,9143412,Fig. 6.,Comparison of classification effects before and after CSP algorithm improvement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv6-3010014-large.gif
2021,9143412,Fig. 7.,ROC curve of classification result of subject P1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv7-3010014-large.gif
2021,9143412,Fig. 8.,ROC curve of classification result of subject P3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv8-3010014-large.gif
2021,9143412,Fig. 9.,ROC curve of classification result of subject P5.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv9-3010014-large.gif
2021,9143412,Fig. 10.,Comparison of classification accuracy of different algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv10-3010014-large.gif
2021,9143412,Fig. 11.,Comparison of classification accuracy of subject P1 under different training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv11-3010014-large.gif
2021,9143412,Fig. 12.,Comparison of classification accuracy of subject P2 under different training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv12-3010014-large.gif
2021,9143412,Fig. 13.,Comparison of classification accuracy of subject P3 under different training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv13-3010014-large.gif
2021,9143412,Fig. 14.,Comparison of classification accuracy of subject P4 under different training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv14-3010014-large.gif
2021,9143412,Fig. 15.,Comparison of classification accuracy of subject P5 under different training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv15-3010014-large.gif
2021,9143412,Fig. 16.,Comparison of average classification accuracy of five subjects under different training samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/9143412/lv16-3010014-large.gif
2021,8821368,Fig. 1.,"An example of a biological pathway, visualized using CellDesigner [4]. Reactions, that can also be interpreted as hyperedges, are displayed as small squares that link, among others, reactants, inhibitors, activators, and products of the process. The style of the edge determines the implication of the biological element, and the shape of the elements themselves designate their type: proteins, molecules, complexes, phenotypes, etc. (see details about the diagram legend on celldesigner.org).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels1-2938501-large.gif
2021,8821368,Fig. 2.,A simple example showing the impact of node duplication.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels2-2938501-large.gif
2021,8821368,Fig. 3.,Different representations of biological pathway graphs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels3-2938501-large.gif
2021,8821368,Fig. 4.,"Grid lattice with the number of hops from the node
n
displayed. The size of the sets
x
hops from
n
is
{4,8,12,16,20}
for
x∈[1,5]
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels4-2938501-large.gif
2021,8821368,Fig. 5.,TPR and FPR pairs found when applying the algorithm to an unseen PD-Map graph instance at various tuning settings (selected tuning settings are shown in the balloons).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels5-2938501-large.gif
2021,8821368,Fig. 6.,ROC curve for the trained and tuned SVM model tested on an unseen Human Metabolism (ReconMap) instance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels6-2938501-large.gif
2021,8821368,Fig. 7.,"Average ROC curve for the trained and tuned SVM model tested on 25 unseen Reactome pathway graph instances. The vertical bars represent minimum and maximum values, the gray transparent areas bounded by dots represent the percentile range from 25 to 75 percent.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels7-2938501-large.gif
2021,8821368,Fig. 8.,Survey graphs example and corresponding legend.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels8-2938501-large.gif
2021,8821368,Fig. 9.,Example pathway shown on the Minerva platform with node duplication plugin integration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels9-2938501-large.gif
2021,8821368,Fig. 10.,"Area under Curve (AUC) of the Receiver Operating Characteristic (ROC) curve originating from a single, highest iso-cost, point.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9446661/8821368/niels10-2938501-large.gif
2021,9353390,Fig. 1.,Block diagram of the proposed transfer learning-based framework for the COVID-19 detection using X-ray and CT images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams1-3054306-large.gif
2021,9353390,Fig. 2.,Two sample images from the X-ray data set. (a) Covid. (b) Normal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams2ab-3054306-large.gif
2021,9353390,Fig. 3.,Two sample images from the CT data set. (a) Covid. (b) Normal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams3ab-3054306-large.gif
2021,9353390,Fig. 4.,2-D representation of X-ray and CT images processed by VGG16 and PCA. (a) CT. (b) X-ray.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams4ab-3054306-large.gif
2021,9353390,Fig. 5.,Grad-CAMs and Heatmaps show how our model makes decision. (a) Grad-CAMs. (b) Heatmaps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams5ab-3054306-large.gif
2021,9353390,Fig. 6.,"Distribution of accuracy, sensitivity, and specificity associated with (a) CT and (b) X-ray data sets, respectively (the top results are for CT data set of our different classifiers).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams6ab-3054306-large.gif
2021,9353390,Fig. 7.,"ROC-AUC plots are shown for CT images using four architectures (VGG16, InceptionResNetV2, ResNet50, and DenseNet121) and eight classifiers. As can be seen, linear SVM and NN (MLP) are the bests and their AUC values are greater than others. Also, it is noted that the performance of classifiers closely depends on the quality of features extracted by the convolutional layers of four considered architectures. (a) VGG16. (b) InceptionResNetV2. (c) ResNet50. (d) DenseNet121.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams7abcd-3054306-large.gif
2021,9353390,Fig. 8.,Accuracy average in the 2-D space of the number of CNN parameters (millions) and the number of features (ten thousands). The size of each point is an indication of the classifier accuracy (mean value in 100 runs).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams8-3054306-large.gif
2021,9353390,Fig. 9.,AUC average in the 2-D space of the number of CNN parameters (millions) and the number of features (ten thousands). The size of each point is an indication of the AUC metric (mean value in 100 runs).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams9-3054306-large.gif
2021,9353390,Fig. 10.,"Uncertainty quantification using 20 individual neural networks working as an ensemble. They differ in the number of neurons in their hidden layer before applying a multilayer perceptron classifier. The darker the color, the higher the uncertainty level. Samples on dark parts of the plot have a high level of predictive uncertainty as the 20 models could not all agree on the predicted label. (a) VGG16. (b) InceptionResNetV2. (c) ResNet50. (d) DenseNet121. (e) VGG16. (f) InceptionResNetV2. (g) ResNet50. (h) DenseNet121.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9394817/9353390/shams10abcdefgh-3054306-large.gif
2021,8735810,Fig. 1.,"In practice weakly supervised learning may be not safe, i.e., it may degenerate the performance with the usage of weakly supervised data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9280439/8735810/li1-2922396-large.gif
2021,8735810,Fig. 2.,"Intuition of our proposal via the projection viewpoint. Intuitively, the proposal learns a projection of
f
0
onto a convex feasible set
Ω
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9280439/8735810/li2-2922396-large.gif
2021,8735810,Fig. 3.,Classification accuracy of compared methods with different numbers of noise ratio.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9280439/8735810/li3-2922396-large.gif
2021,9470971,Fig. 1.,"Transmission spectra of the (a) reagents, (b) trained shielding materials, and untrained shielding materials used in this article. The NIR detection beam intensities according to the attenuation rate of the THz wave are also shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503871/9597879/9470971/murat1-3094128-large.gif
2021,9470971,Fig. 2.,THz spectroscopic system using an injection-seeded THz parametric generator (is-TPG). (HWP: half wave plate; ECLD: external cavity laser diode; SOA: semiconductor optical amplifier; NIR: near-infrared.) .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503871/9597879/9470971/murat2-3094128-large.gif
2021,9470971,Fig. 3.,Typical transmission spectra of the reagents obtained through various shielding materials. The transmittance was calculated based on the NIR detection beam output and was not converted to THz-wave transmittance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503871/9597879/9470971/murat3-3094128-large.gif
2021,9470971,Fig. 4.,Example contribution ratios for each frequency obtained using the random forest algorithm. The top seven frequencies are in red color. Frequencies near the absorption peak make larger contributions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503871/9597879/9470971/murat4-3094128-large.gif
2021,9470971,Fig. 5.,Shielding materials and reagent samples used for the spectroscopic imaging measurements. Reagents were sandwiched between four kinds of shielding materials that attenuated the THz wave (to −65 dB at 1.5 THz). (a) Shielding materials and reagents. (b) Measured sample.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503871/9597879/9470971/murat5-3094128-large.gif
2021,9470971,Fig. 6.,"(a) Spectroscopic imaging results showing the spatial distribution of maltose, glucose, and lactose. The spatial resolution was about 1 mm and the measurement time for this image was less than 2 h. (b) Imaging results overlayed on the samples. (c) Comparison of the spectra obtained at point A in (b), which was misidentified as maltose, with the spectra of the shield and maltose. (d) Comparison of the spectra obtained at point B in (b), which was misidentified as a shield, with the spectra of the shield and glucose.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503871/9597879/9470971/murat6-3094128-large.gif
2021,9115837,Fig. 1.,"Privacy–cost tradeoff with
d=6
and
n=2000
applying different weighting schemes. (a) Output perturbation. (b) Objective perturbation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9420773/9115837/chang1ab-2996972-large.gif
2021,9115837,Fig. 2.,"Privacy–cost tradeoff with
d=6
and
n=10000
applying different weighting schemes. (a) Output perturbation. (b) Objective perturbation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9420773/9115837/chang2ab-2996972-large.gif
2021,9115837,Fig. 3.,"Privacy–cost tradeoff with
d=10
and
n=10000
applying different weighting schemes. (a) Output perturbation. (b) Objective perturbation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9420773/9115837/chang3ab-2996972-large.gif
2021,9115837,Fig. 4.,Privacy–cost tradeoff on the DCCC data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9420773/9115837/chang4-2996972-large.gif
2021,9115837,Fig. 5.,Privacy–cost tradeoff on the PS data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9420773/9115837/chang5-2996972-large.gif
2021,9536751,FIGURE 1.,XAI concept and taxonomy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr1-3112397-large.gif
2021,9536751,FIGURE 2.,Self-Organizing Map displayed in the output space (a) and in the input space adapted to 2D distribution of input points (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr2ab-3112397-large.gif
2021,9536751,FIGURE 3.,Cluster quality evaluation approach for K clusters using Silhouette Coefficient and Davies-Bouldin Index for different SOM map sizes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr3-3112397-large.gif
2021,9536751,FIGURE 4.,"Fidelity test, Experiment I: Changed the values of p% number of most important (active) features, p% number of randomly picked features, and p% number of least important (inactive) features and calculated the percentage of data points where the cluster label changes after changing %p feature out of all the feature. we checked the two cases; 1) What is the percentage of test data records where the cluster label can be swapped by at least one other cluster label (left), 2) What is the percentage of test data records where all other clusters can swap the cluster label (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr4ab-3112397-large.gif
2021,9536751,FIGURE 5.,The percentage of closest K number of features included in the most important feature list of the BMU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr5-3112397-large.gif
2021,9536751,FIGURE 6.,"Local Interpretability; Explanation for a single data record, features are ordered from ascending order based on feature wise distance to BMU.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr6-3112397-large.gif
2021,9536751,FIGURE 7.,"Global Interpretability; Feature behavior for ‘flag’ feature of KDD data set across clusters (SOM neurons were clustered into three categories, U-matrix visualize the distances between clusters and how well clusters are separated, the ‘flag’ feature value is different across clusters).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9536751/wickr7-3112397-large.gif
2021,9387268,Fig. 1.,Designed tapping machine in this article. (a) Simulation. (b) Actual.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng1ab-3067216-large.gif
2021,9387268,Fig. 2.,Tapping process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng2-3067216-large.gif
2021,9387268,Fig. 3.,Signal of the designed tapping machine. (a) Tapper r/min. (b) Tapper torque. (c) Cam r/min. (d) Cam torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng3abcd-3067216-large.gif
2021,9387268,Fig. 4.,Relationship between tapper speed and tapper torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng4-3067216-large.gif
2021,9387268,Fig. 5.,Automatic thread quality detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng5-3067216-large.gif
2021,9387268,Fig. 6.,Measurement results of the thread quality.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng6-3067216-large.gif
2021,9387268,Fig. 7.,Segmentation of the torque signal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng7-3067216-large.gif
2021,9387268,Fig. 8.,Structure of the feature extraction module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng8-3067216-large.gif
2021,9387268,Fig. 9.,Flattened and GAP layer. (a) Flattened layer. (b) GAP layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng9ab-3067216-large.gif
2021,9387268,Fig. 10.,Flowchart of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng10-3067216-large.gif
2021,9387268,Fig. 11.,t-SNE visualization. (a) Cam speed. (b) Cam torque. (c) Tapper speed. (d) Tapper torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng11abcd-3067216-large.gif
2021,9387268,Fig. 12.,t-SNE visualization of the extracted features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng12-3067216-large.gif
2021,9387268,Fig. 13.,Tapper torque during the tapping process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng13-3067216-large.gif
2021,9387268,Fig. 14.,Important area localization of the Class 2 thread.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng14-3067216-large.gif
2021,9387268,Fig. 15.,Important area localization of the Class 3 thread.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng15-3067216-large.gif
2021,9387268,Fig. 16.,ROC curve. (a) Training data. (b) Testing data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng16ab-3067216-large.gif
2021,9387268,Fig. 17.,One of the regression trees of RF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng17-3067216-large.gif
2021,9387268,Fig. 18.,Tapper signal at 443 r/min. (a) Taper speed. (b) Tapper torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9259274/9387268/perng18ab-3067216-large.gif
2021,9333572,FIGURE 1.,IBM SPSS model for kidney disease prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin1-3053763-large.gif
2021,9333572,FIGURE 2.,"Comparison of precision, recall and accuracy for all classifiers without feature selections.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin2-3053763-large.gif
2021,9333572,FIGURE 3.,Performance of Area under curve for all classifiers without feature selections.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin3-3053763-large.gif
2021,9333572,FIGURE 4.,Performance of F-Measures for all classifiers without feature selections.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin4-3053763-large.gif
2021,9333572,FIGURE 5.,Dataset Feature importance using CFS algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin5-3053763-large.gif
2021,9333572,FIGURE 6.,"Comparison of precision, recall and accuracy for all classifiers after correlation-based feature selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin6-3053763-large.gif
2021,9333572,FIGURE 7.,Performance of GINI index for all classifiers after correlation-based feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin7-3053763-large.gif
2021,9333572,FIGURE 8.,Performance of area under the curve for all classifiers after correlation-based feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin8-3053763-large.gif
2021,9333572,FIGURE 9.,Dataset feature importance using CFS algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin9-3053763-large.gif
2021,9333572,FIGURE 10.,"Comparison of precision, recall and accuracy for all classifiers after Wrapper feature selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin10-3053763-large.gif
2021,9333572,FIGURE 11.,Performance of GINI index for all classifiers after Wrapper feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin11-3053763-large.gif
2021,9333572,FIGURE 12.,Performance of area under the curve for all classifiers after Wrapper feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin12-3053763-large.gif
2021,9333572,FIGURE 13.,Dataset feature importance using LASSO regression algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin13-3053763-large.gif
2021,9333572,FIGURE 14.,"Comparison of precision, recall and accuracy for all classifiers after LASSO feature selection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin14-3053763-large.gif
2021,9333572,FIGURE 15.,Performance of GINI index for all classifiers after LASSO feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin15-3053763-large.gif
2021,9333572,FIGURE 16.,Performance of area under the curve for all classifiers after LASSO feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin16-3053763-large.gif
2021,9333572,FIGURE 17.,"Comparison of precision, recall and accuracy for all classifiers with SMOTE and selected features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin17-3053763-large.gif
2021,9333572,FIGURE 18.,Performance of AUC for all classifiers with SMOTE and selected features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin18-3053763-large.gif
2021,9333572,FIGURE 19.,Performance of F-Measure for all classifiers after LASSO feature selection and SMOTE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin19-3053763-large.gif
2021,9333572,FIGURE 20.,"Comparison of precision, recall and accuracy for all classifiers with SMOTE and full features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin20-3053763-large.gif
2021,9333572,FIGURE 21.,Performance of AUC for all classifiers with SMOTE and full features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin21-3053763-large.gif
2021,9333572,FIGURE 22.,Performance of F-Measure for all classifiers after LASSO feature selection and SMOTE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin22-3053763-large.gif
2021,9333572,FIGURE 23.,Comparison of all classifier models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin23-3053763-large.gif
2021,9333572,FIGURE 24.,Comparison of LSVM in different models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9333572/jasin24-3053763-large.gif
2021,9411650,Fig. 1.,"Architecture of RAML. The approximation process contains the real and imaginary parts. The input of RAML is other receivers within a wavelength around the damaged receiver, and its output is transferred to NMM–IEM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9475570/9411650/liu1-3074959-large.gif
2021,9411650,Fig. 2.,Composition of the training model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9475570/9411650/liu2-3074959-large.gif
2021,9411650,Fig. 3.,"All four cases at 5 GHz in [22] are used to test the RAML. The first column is the ground truth, where the position and the size of the scatterer are marked. From the second to the fifth column, the TM mode is used, while from the sixth to the ninth column, the TE mode is used. The second, fourth, sixth, and eighth columns are the results of the experimental data with no receiver damaged. The third, fifth, seventh, and ninth columns are the results of the experimental data with ten receivers damaged.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9475570/9411650/liu3-3074959-large.gif
2021,9411650,Fig. 4.,"Results of zeroing damaged data input to NMM and NMM–IEM. The first to third columns are the cases where 1, 5, and 10 receivers are damaged, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9475570/9411650/liu4-3074959-large.gif
2021,9411650,Fig. 5.,Model misfit versus the number of damaged receivers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9475570/9411650/liu5-3074959-large.gif
2021,9363896,FIGURE 1.,"Difference between AI, ML and DL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader1-3062484-large.gif
2021,9363896,FIGURE 2.,Difference between ML and DL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader2-3062484-large.gif
2021,9363896,FIGURE 3.,Classifications of ML and DL techniques to detect brain diseases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader3-3062484-large.gif
2021,9363896,FIGURE 4.,Paper organization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader4-3062484-large.gif
2021,9363896,FIGURE 5.,Classifications of feature extraction techniques used in brain diseases detection process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader5-3062484-large.gif
2021,9363896,FIGURE 6.,Article distributions with respect to ML/ DL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader6-3062484-large.gif
2021,9363896,FIGURE 7.,Article distributions with respect to different diseases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader7-3062484-large.gif
2021,9363896,FIGURE 8.,Different types of image modality and other data used in the different articles to detect AD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader8-3062484-large.gif
2021,9363896,FIGURE 9.,Different types of database used in the different articles to detect AD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader9-3062484-large.gif
2021,9363896,FIGURE 10.,Different types of image modality and other data used in the different articles to detect brain tumors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader10-3062484-large.gif
2021,9363896,FIGURE 11.,Different types of database used in the different articles to detect brain tumors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader11-3062484-large.gif
2021,9363896,FIGURE 12.,Different types of image modality and other data used in the different articles to detect epilepsy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader12-3062484-large.gif
2021,9363896,FIGURE 13.,Different types of database used in the different articles to detect epilepsy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader13-3062484-large.gif
2021,9363896,FIGURE 14.,Different types of image modality and other data used in the different articles to detect PD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader14-3062484-large.gif
2021,9363896,FIGURE 15.,Different types of database used in the different articles to detect PD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9363896/kader15-3062484-large.gif
2021,9369420,Fig. 1.,"Example of a nonlinear function of the input features, which produces some prediction. The function can be approximated locally as a linear model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek1-3060483-large.gif
2021,9369420,Fig. 2.,Input example predicted to have low compressive strength and a featurewise explanation of the prediction. Red and blue indicate positive and negative contributions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek2-3060483-large.gif
2021,9369420,Fig. 3.,"Two difficulties encountered when explaining DNNs. Left: shattered gradient effect causing gradients to be highly varying and too noisy to be used for explanation. Right: pathological minima in the function, making it difficult to search for meaningful reference points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek3ab-3060483-large.gif
2021,9369420,Fig. 4.,"Illustration of the LRP propagation procedure applied to a neural network. The prediction at the output is propagated backward in the network, using various propagation rules, until the input features are reached. The propagation flow is shown in red.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek4-3060483-large.gif
2021,9369420,Fig. 5.,"Examples of images from ImageNet [157] with classes “space bar,” “beacon/lighthouse,” “snow mobile,” “viaduct,” and “greater swiss mountain dog.” Images are correctly predicted by the VGG-16 [175] neural network and shown along with an explanation of the predictions. Different explanation methods lead to different qualities of explanation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek5-3060483-large.gif
2021,9369420,Fig. 6.,Pixel-flipping experiment for testing faithfulness of the explanation. We remove pixels found to be the most relevant by each explanation method and verify how quickly the output of the network decreases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek6-3060483-large.gif
2021,9369420,Fig. 7.,"Graphical illustration of the function
R
k
(a)
that DTD seeks to decompose on the input dimensions. Because
R
k
is complex, it is often replaced by an analytically more tractable model
R
ˆ
k
(a)
that only depends on local activations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek7-3060483-large.gif
2021,9369420,Fig. 8.,"Example of two images predicted to be similar, along with a BiLRP second-order attribution of their similarity score rendered as a bipartite graph. (figure is adapted from [42]). The explanation shows that the front part of the two planes jointly contributes to the predicted similarity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek8-3060483-large.gif
2021,9369420,Fig. 9.,Left: kernel k-means applied to a toy 2-D problem with three clusters. Red and blue in the background represent the positive and negative values of the logit function for a given cluster. Right: four-layer neural network equivalent of the kernel k-means logit score [85].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek9ab-3060483-large.gif
2021,9369420,Fig. 10.,Deep neural network to which we append a neuralized version of the log-likelihood ratio [126]. Considering the latter quantity instead of the DNN output leads to a different explanation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek10-3060483-large.gif
2021,9369420,Fig. 11.,Pooled analysis. Top: featurewise contributions for the prediction on three clusters of the Concrete Compressive Strength Data set [197]. Bottom: coarse-grained explanations obtained by pooling contributions on data clusters and groups of features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek11-3060483-large.gif
2021,9369420,Fig. 12.,"SpRAy analysis. Explanation of the predictions of the Concrete Compressive Strength Data set [197], displayed at coordinates corresponding to their t-SNE embedding. This analysis provides a visualization of the overall classifier’s strategy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek12-3060483-large.gif
2021,9369420,Fig. 13.,"Top: explanations produced by different methods on the VGG-16 age model. Results are shown for the output neurons associated with age group labels (0–2), (25–32), and (60+), respectively. Bottom: application of the layer-dependent LRP-CMP decomposition strategy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek13-3060483-large.gif
2021,9369420,Fig. 14.,LRP heatmaps demonstrating the effects of ImageNet [35] pretraining (middle) compared to additional IMDB-WIKI [153] pretraining (bottom). All heatmaps show the model decision with respect to age group (60+).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek14abc-3060483-large.gif
2021,9369420,Fig. 15.,"Explanations based on waveform representation of speech data. Correct prediction of female (top) and male (bottom) subjects. The waveform data are visualized as a scatter plot of 8000 discrete measurements, color-coded according to relevance attribution for the true class label.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek15-3060483-large.gif
2021,9369420,Fig. 16.,Left: spectrogram representation of digit “zero” spoken by female speaker “vp12.” Right: spectrogram representation of digit “zero” spoken by male speaker “vp2.” Relevance maps are shown with respect to the samples’ true classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek16ab-3060483-large.gif
2021,9369420,Fig. 17.,"Identifying mortality risk factors from explanations. (a) First-order SHAP values show that high age correlates with high mortality risk. The impact of other interacting factors on the age contribution to risk remains hidden. (b) The SHAP interaction matrix can be decomposed into the main effects (diagonal elements) and the interaction effects (off-diagonal elements). Here, the interaction effect shows that the risk factor “age” is largely modulated by the sex of the participant (figure is adapted from [115]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek17ab-3060483-large.gif
2021,9369420,Fig. 18.,SpRAy analysis of the predictions of a pretrained VGG-16 model on images of the class “garbage truck.” Top: low-dimensional embedding of the explained decisions for the class “garbage truck.” Points highlighted in red are outliers. Bottom: images and corresponding decisions for some of the points highlighted in red.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek18-3060483-large.gif
2021,9369420,Fig. 19.,"Analysis of the learning process of a deep model playing Atari Breakout. The curves show the development of the relative relevance of different game objects (ball, paddle, and tunnel) averaged over six runs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek19-3060483-large.gif
2021,9369420,Fig. 20.,"Different applications of explanation techniques in the sciences. (a) LRP heatmaps merged into a computationally predicted fluorescence image. Here, red identifies cancer, green shows lymphocytes, and blue is a stroma. Adapted from [22]. (b) Prediction of MRMS radar signal from satellite image and pixelwise explanation. Adapted from [43]. (c) Learned filter weights from the first convolutional layer of a deep neural network trained for galaxy morphology classification. Adapted from [207]. (d) Whole-brain fMRI volume is decoded using a DNN. The decoding decision is explained voxelwise to localize brain areas corresponding to the predicted cognitive state. Adapted from [186]. (e) Example of LRP relevance maps for a single EEG trial of an imagined movement (each class). The matrices indicate the relevance of each time point (abscissa) and EEG channel (ordinate). Below the matrix, the relevance information for two single time points is plotted as a scalp topography. Adapted from [180]. (f) Prediction of mortality from ECG time-series data. Red areas indicate the time steps that most strongly explain the prediction. Adapted from [146]. (g) Some medical conditions predicted from electronic health record (EHR) time series and explained in terms of input features. Blue/red indicates low/high value, and the circle size indicates feature relevance. Adapted from [106]. (h) Graph convolutional neural prediction of the molecule’s mutagenicity and attribution of individual atoms. Interpretability feedback reveals that the model has correctly identified molecular substructures known to interact with (human) DNA. Adapted from [144]. (i) Predicted atom score describing protein-ligand interaction is explained with CLRP (green corresponding to a more favorable score). Adapted from [71].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5/9369414/9369420/samek20abcdefghi-3060483-large.gif
2021,9364974,FIGURE 1.,"Interaction between the elements of dyslexia: brain abnormalities, phonological disorder, visual deficit, and auditory dysfunction. Dyslexia occurs due to abnormalities and dysfunctionalities of certain brain subsystems such as phonological, visual and auditory as child develops right from birth. It manifests as both cognitive and behaviour deficits when a child attains school age.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9364974/latee1-3062709-large.gif
2021,9364974,FIGURE 2.,Flow diagram of publication selection process for selecting 22 final articles according to PRISMA protocol.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9364974/latee2-3062709-large.gif
2021,9364974,FIGURE 3.,(a). Annual distribution of 84 articles thoroughly screened. (b). Distribution of machine learning usage by researchers of final 22 reviewed articles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9364974/latee3ab-3062709-large.gif
2021,9364974,FIGURE 4.,Distribution of literatures according to number of citations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9364974/latee4-3062709-large.gif
2021,9173736,Fig. 1.,"SVM hyperplane
H
.
H
+
and
H
−
are the “supporting” hyperplanes for the positive (filled circles) and negative (unfilled circles) instances, respectively. The margin is the area between
H
+
and
H
−
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul1-3015442-large.gif
2021,9173736,Fig. 2.,"PSVM hyperplane
H
.
H
+
and
H
−
are the “proximal” hyperplanes for the positive (filled circles) and negative (unfilled circles) instances, respectively. The margin is the area between
H
+
and
H
−
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul2-3015442-large.gif
2021,9173736,Fig. 3.,"MIL example: four positive bags (continuous boxes) and four negative ones (dashed boxes). The circles inside the bags are the instances. The “pure proximal” hyperplane
H
correctly separates the two classes of bags.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul3-3015442-large.gif
2021,9173736,Fig. 4.,"MIL example: eight positive bags (continuous boxes) and four negative ones (dashed boxes). The circles inside the bags are the instances. The “pure proximal” hyperplane
H
correctly separates the two classes of bags because, for each positive bag, at least an instance is classified as positive, and for all negative bags, all the instance are classified as negative.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul4-3015442-large.gif
2021,9173736,Fig. 5.,"MIL example: four positive bags (continuous boxes) and eight negative ones (dashed boxes). The circles inside the bags are the instances. The “pure PSVM” hyperplane
H
is not able to correctly separate the two classes of bags since some instances of the negative bags are classified as positive.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul5-3015442-large.gif
2021,9173736,Fig. 6.,"MIL example: four positive bags (continuous boxes) and four negative ones (dashed boxes). The circles inside the bags are the instances. The SPSVM hyperplane
H
, with
C=1
, correctly separates all the bags. The hyperplane
H
+
is a “proximal” hyperplane that clusters the instances of the positive bags, while
H
−
is a “supporting” hyperplane for the instances of the negative bags.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul6-3015442-large.gif
2021,9173736,Fig. 7.,"Elephant data set: average testing correctness in function of the ratio
m/k
. The dotted line is the corresponding trend line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul7-3015442-large.gif
2021,9173736,Fig. 8.,"Fox data set: average testing correctness in function of the ratio
m/k
. The dotted line is the corresponding trend line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul8-3015442-large.gif
2021,9173736,Fig. 9.,"Tiger data set: average testing correctness in function of the ratio
m/k
. The dotted line is the corresponding trend line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9173736/fudul9-3015442-large.gif
2021,9134853,Fig. 1.,General ML pipeline that maps an input to a label. The two main steps of the pipeline are (i) extraction of an intermediary feature space and (ii) label prediction using a classification or clustering algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/9134853/shamo1-3007816-large.gif
2021,9134853,Fig. 2.,"Visualization of the patient flow in a hospital: Patient is either admitted as an elective or emergency admission, monitored in ward stay(s) during consultant episode(s). Patient may transfer from one ward to another, or may change the consultant during the in-hospital stay. * Accident & Emergency patients may be admitted as inpatients or just discharged.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/9134853/shamo2-3007816-large.gif
2021,9134853,Fig. 3.,"Dataset sizes reported in the literature in ascending order from left (2008) to right (2019). The vertical axis represents the dataset size, in terms of the number of patient admissions, and the horizontal axis represents the reference number. There is an increase of six orders of magnitude between 2008 and 2019 in terms of dataset size, highlighting the increased availability of EHR data for ML research.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/9134853/shamo3-3007816-large.gif
2021,9134853,Fig. 4.,"Clinical outcome prediction models first extract a cohort of interest based on a specific patient inclusion and exclusion criteria, and then prepare the data for further downstream tasks by assessing the characteristics of the raw dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9333686/9134853/shamo4-3007816-large.gif
2021,9063675,Fig. 1.,"Performance of AUC-ELM versus
γ
and
n
h
for the Pima dataset, in which
n
h
denotes the number of hidden nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/9619612/9063675/wang1-2982226-large.gif
2021,9063675,Fig. 2.,AUC performances on the UCI datasets with respect to different percentages of labeled training data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221021/9619612/9063675/wang2abcdef-2982226-large.gif
2021,9335273,Fig. 1.,"Example of 4 extracted ROIs with the detected suspicious soft-tissue masses (lesions) in ROI center. (a), (b) 2 ROIs involving malignant lesions and (c), (d) 2 ROIs involving benign lesions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida1-3054248-large.gif
2021,9335273,Fig. 2.,"Example to illustrate lesion segmentation, which include (a) the original ROI, (b) absolute difference of ROI from low-pass filtered version, (c) combination of (a) and (b) which gives the suspicious regions better contrast to the background, (d) output of morphological filtering, (e) blob with the largest size is selected (a) binary version of the lesion), and (f) finally segmented lesion area. It is output of mapping (e) to (a).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida2-3054248-large.gif
2021,9335273,Fig 3.,Wavelet based feature extraction. Wavelet decomposition is applied three times to make the images compress as possible. Then PCA is adopted as another way of data compression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida3-3054248-large.gif
2021,9335273,Fig 4.,Illustration of the overall classification flow of the CAD scheme developed and tested in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida4-3054248-large.gif
2021,9335273,Fig 5.,A malignant case annotated by radiologists in both CC and MLO views. The annotated mass is squared in each view.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida5-3054248-large.gif
2021,9335273,Fig. 6.,A trend of the case-based classification AUC values generated by the SVM models trained using different number of features (NF) generated by the RPA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida6-3054248-large.gif
2021,9335273,Fig. 7.,Comparison of 10 ROC curves generated using 5 SVM models and 2 scoring (region and case-based) methods to classify between malignant and benign lesion regions or cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9519572/9335273/heida7-3054248-large.gif
2021,8884110,Fig.1:,"Study workflow. Blood samples were taken from three potential patients and measured using RT-FDC. The output in this case was 10 morpho-rheological features together with classification information of each single cell resulting from the fluorescence signal. P1-partion1 was used for training purpose, while P1-partion2, P2, and P3 were used for validation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9508625/8884110/canni1-2945762-large.gif
2021,8884110,Fig. 2.,"A) Discriminative network module detected by PC-corr and related with PC2 discrimination (cut-off
=0.6
). B) Image of a red blood cell flowing in the RT-FDC channel, including illustrations of “area” (bounded by the blue contour), “x-size,” and y-size.”",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9508625/8884110/canni2-2945762-large.gif
2021,8884110,Fig. 3.,"Mean precision performance of evaluated methods for an independent validation class-balanced scenario using 10,000 permutations of random uniform mRBC sampling. The bar color is associated with the number of features used to train the respective model. Light blue used two features; gray used five features; and blue used seven features. The red whiskers report the standard deviation. A) Performance in internal validation P1-partition 2 data. B) Performance in external validation P2 data. C) Performance in external validation P3 data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9508625/8884110/canni3-2945762-large.gif
2021,9410543,FIGURE 1.,Crime distribution over the selected regions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat1-3075140-large.gif
2021,9410543,FIGURE 2.,Visual representation of data of kidnapping and dacoits.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat2-3075140-large.gif
2021,9410543,FIGURE 3.,Proposed classifier for ensemble stack based crime prediction model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat3-3075140-large.gif
2021,9410543,FIGURE 4.,Structure of the proposed stack based crime prediction model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat4-3075140-large.gif
2021,9410543,FIGURE 5.,Tree representation of j48.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat5-3075140-large.gif
2021,9410543,FIGURE 6.,Bagging classifier model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat6-3075140-large.gif
2021,9410543,FIGURE 7.,Correlation matrix between true class and predicted class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat7-3075140-large.gif
2021,9410543,FIGURE 8.,Optical parameter tuning of the conventional learning on feature extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat8-3075140-large.gif
2021,9410543,FIGURE 9.,Graph representations with different outcomes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410543/kshat9-3075140-large.gif
2021,9238396,Fig. 1.,"Detailed development of VTE models using a conversational protocol between machine learning and epidemiological experts. The flow chart shows data-flow, VTE statistics and various domain expertise guided models trained. [A]: At first, all the cohort data were used to train a model. [B]: In the next step, we eliminated the data-leaking covariates to train a subsequent model. [C]: We identified cancer/non-cancer hospitalizations to train two subsequent models on in-cancer and out-cancer sub-populations. [D]: We introduced a temporal control using D-Dimer lab tests as an event in hospitalization timeline and train a model. [E]: Again, we identified cancer in-population and cancer out-population hospitalizations on temporal masked data to train models. [F]: We performed propensity-controlled sub-groups on D-Dimer masked data. [G]: Finally, we conducted dose-dependent analyses on D-Dimer masked data for aspirin and ondansetron.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9238396/swami1-3033405-large.gif
2021,9238396,Fig. 2.,Variations in model performance and RRR of drugs pre and post elimination of data-leaking covariates. A: AUROC curves of the model predictions before and after data-leaking covariates are eliminated shows a drop in accuracy. B: Changes in % RRR of drugs after data-leaking covariates are eliminated from the dataset show changes in causal inferences.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9238396/swami2-3033405-large.gif
2021,9354635,FIGURE 1.,Arabic country-dialects in the continuous language-area.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag1-3059504-large.gif
2021,9354635,FIGURE 2.,Most used author keywords in Arabic dialects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag2-3059504-large.gif
2021,9354635,FIGURE 3.,PRISMA flow diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag3-3059504-large.gif
2021,9354635,FIGURE 4.,Number of studies (%) addressing quality assessment criteria.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag4-3059504-large.gif
2021,9354635,FIGURE 5.,Distribution of studies per language mode.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag5-3059504-large.gif
2021,9354635,FIGURE 6.,Research per country/regional dialect.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag6-3059504-large.gif
2021,9354635,FIGURE 7.,Popular machine learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag7-3059504-large.gif
2021,9354635,FIGURE 8.,Features used in ML studies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag8-3059504-large.gif
2021,9354635,FIGURE 9.,Type of data-sources used in dialect identification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag9-3059504-large.gif
2021,9354635,FIGURE 10.,Distribution of Arabic dialects studies per publication year.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag10-3059504-large.gif
2021,9354635,FIGURE 11.,Corpora used in the reviewed papers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag11-3059504-large.gif
2021,9354635,FIGURE 12.,Popularity of various evaluation criteria.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag12-3059504-large.gif
2021,9354635,FIGURE 13.,Research Dissemination Venues.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9354635/elnag13-3059504-large.gif
2021,9529039,Fig. 1.,Experiment set-up for hyperspectral measurements.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9314330/9529039/xie1-3109951-large.gif
2021,9529039,Fig. 2.,Photograph of the oil samples. (a) Crude oil. (b) Diesel. (c) Lubricant. (d) Heavy diesel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9314330/9529039/xie2-3109951-large.gif
2021,9529039,Fig. 3.,Workflow of the four machine learning models. (a) Random forest. (b) Support vector machine. (c) Deep neural network. (d) DNN with differential pooling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9314330/9529039/xie3-3109951-large.gif
2021,9529039,Fig. 4.,Reflectance spectra collected under different slick thickness and wind conditions for four types of oil. (a) Crude oil. (b) Diesel. (c) Lubricant. (d) Heavy diesel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9314330/9529039/xie4-3109951-large.gif
2021,9529039,Fig. 5.,Reflectance spectra collected for 1.944 mm crude oil under different sunlight elevation angle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9314330/9529039/xie5-3109951-large.gif
2021,9529039,Fig. 6.,Confusion matrix of test results for the four machine learning models. (a) Random forest. (b) Support vector machine. (c) Deep neural network. (4) DNN with differential pooling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9314330/9529039/xie6-3109951-large.gif
2021,9410627,FIGURE 1.,Flow diagram of the article selection process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi1-3075159-large.gif
2021,9410627,FIGURE 2.,"Number of journal articles, conference papers and book chapters amongst others.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi2-3075159-large.gif
2021,9410627,FIGURE 3.,The number of reviewed articles with respect to the publication year.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi3-3075159-large.gif
2021,9410627,FIGURE 4.,Vegetable oils consumption from 2013/14 to 2019/2020 by oil categories (in a million metric tons) globally [29].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi4-3075159-large.gif
2021,9410627,FIGURE 5.,Major vegetable oils production from 2012/13 to 2019/2020 by categories (in a million metric tons) globally [30].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi5-3075159-large.gif
2021,9410627,FIGURE 6.,Major palm oil producers and exporters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi6-3075159-large.gif
2021,9410627,FIGURE 7.,"Palm oil consumption worldwide from 2015/2016 to 2019/2020 (in 1,000 metric tons) [37].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi7-3075159-large.gif
2021,9410627,FIGURE 8.,The general architecture of machine learning-based crop yield prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi8-3075159-large.gif
2021,9410627,FIGURE 9.,The popular feature set for crop yield prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi9-3075159-large.gif
2021,9410627,FIGURE 10.,Widely used prediction algorithms for crop yield forecasting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi10-3075159-large.gif
2021,9410627,FIGURE 11.,Popular performance evaluation metrics for crop yield prediction algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi11-3075159-large.gif
2021,9410627,FIGURE 12.,Widely used crop in the crop yield prediction research.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi12-3075159-large.gif
2021,9410627,FIGURE 13.,Prospective palm oil yield prediction architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9410627/rashi13-3075159-large.gif
2021,9409613,Fig. 1.,Sensor data capturing and signal processing flow of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9440008/9409613/vyas1-3074183-large.gif
2021,9409613,Fig. 2.,Flow of 1D-CNN-based cough event classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9440008/9409613/vyas2-3074183-large.gif
2021,9409613,Fig. 3.,"Positions on the human body where acceleration was measured for sitting, standing, and walking activities with and without coughing. (a) Axes coordinate. (b) Positions on the body (1–4) where human activity was measured using Samsung A20 phone-based accelerometer; Position on the head (5) where human activity was measured using a Mide accelerometer mounted into a worn headphone's earpiece. The smartphone was not used for head measurements since humans tend to move the phone away from the head when coughing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9440008/9409613/vyas3-3074183-large.gif
2021,9409613,Fig. 4.,(a) Hardware used to record acceleration from different body parts and human activities. (b) Headphone mounted mide sensor to measure acceleration by the ear in a worn ear or headphone,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9440008/9409613/vyas4-3074183-large.gif
2021,9409613,Fig. 5.,"Accelerometer x, y, and z of cough and noncough event, for sitting, standing, and walking activities at chest, stomach, shirt pocket, upper hand, and ear position; Fig. 5(a)–(e) are the readings of an average height (5 feet 2 inches) subject, and 5(f) and (g) are the readings of a short (3 feet 5 inches) subject.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9440008/9409613/vyas5-3074183-large.gif
2021,9409613,Fig. 6.,(a) Cough event classification accuracy at five positions. (b) SD difference of accelerometer data between a cough and noncough event for waking activity at five positions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9440008/9409613/vyas6-3074183-large.gif
2021,9501960,FIGURE 1.,AB-TRAP Framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei1-3101188-large.gif
2021,9501960,FIGURE 2.,LAN Environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei2-3101188-large.gif
2021,9501960,FIGURE 3.,Composing AB-TRAP dataset with bonafide data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei3-3101188-large.gif
2021,9501960,FIGURE 4.,Decision tree analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei4-3101188-large.gif
2021,9501960,FIGURE 5.,ROC/AUC analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei5-3101188-large.gif
2021,9501960,Listing 1,Pseudo-code from decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei11-3101188-large.gif
2021,9501960,FIGURE 6.,Cloud instances to be used as attacker/targets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei6-3101188-large.gif
2021,9501960,FIGURE 7.,Lifeline of the attack dataset creation for each attack.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei7-3101188-large.gif
2021,9501960,FIGURE 8.,ROC/AUC analysis for Internet case.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei8-3101188-large.gif
2021,9501960,FIGURE 9.,NIDS packet traversal path.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei9-3101188-large.gif
2021,9501960,FIGURE 10.,Performance evaluation for each ML Model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9501960/perei10-3101188-large.gif
2021,9257084,Fig. 1.,Proposed model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir1-3033423-large.gif
2021,9257084,Fig. 2.,Proposed framework model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir2-3033423-large.gif
2021,9257084,Fig. 3.,Example of the most frequent decision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir3-3033423-large.gif
2021,9257084,Fig. 4.,Example of the most frequent weighted decision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir4-3033423-large.gif
2021,9257084,Fig. 5.,Number of samples for each class in UNSW-NB-15.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir5-3033423-large.gif
2021,9257084,Fig. 6.,Accuracy rate per class for the proposed system applied to the decision tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir6-3033423-large.gif
2021,9257084,Fig. 7.,Accuracy rate per class for the proposed system applied to random forest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir7-3033423-large.gif
2021,9257084,Fig. 8.,"FPR and TNR of the proposed system applied to the decision tree and random forest under different time index configurations (1, 1100).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir8-3033423-large.gif
2021,9257084,Fig. 9.,"Average node’s classification decision of the proposed system applied to the decision tree and random forest after 1, 600, and 1100 time units. (a) Desision tree. (b) Random forest.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir9-3033423-large.gif
2021,9257084,Fig. 10.,Accuracy rate of the proposed approach applied to decision tree using the most frequent decision and weighted decision techniques compared to the original algorithm as a function of time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir10-3033423-large.gif
2021,9257084,Fig. 11.,"Accuracy rate, detection rate, and a false positive rate of proposed system compared to NB, AODF, EDM, CADF, and TANN using UNSW dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9645394/9257084/chkir11-3033423-large.gif
2021,9406127,Fig. 1.,A framework of PR-based myoelectric control in offline and online phases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv1-3073751-large.gif
2021,9406127,Fig. 2.,"Online performance validation. Subjects were divided into group T1 and group T2. T1 trained one trial, while T2 trained five trials. Online testing contained two parts: online testing with or without visual feedback.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv2-3073751-large.gif
2021,9406127,Fig. 3.,A framework of K-fold CV and TSV to evaluate the offline classification performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv3-3073751-large.gif
2021,9406127,Fig. 4.,"CEs estimated by CV and TSV with different training-trial numbers across ten days for able-bodied subjects. CEs are averaged across ten days for each subject. Error bars represent the standard deviation for different subjects. Symbols ‘**’ and ‘*’ represent there are significant differences between CV and TSV (‘**’:
p<0.01
, ‘*’:
p<0.05
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv4-3073751-large.gif
2021,9406127,Fig. 5.,"CEs estimated by CV and TSV with different training-trial numbers across ten days for the amputees. CEs are averaged across ten days for each subject. Error bars represent the standard deviation for different subjects. Symbols ‘**’ and ‘*’ represent there are significant differences between CV and TSV (‘**’:
p<0.001
, ‘*’:
p<0.01
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv5-3073751-large.gif
2021,9406127,Fig. 6.,User learning curves for able-bodied subjects (S1-S8). RIs are averaged across ten days and used to qualify the ability of user learning. Error bands represent the standard deviation of each subject’s RIs across ten days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv6-3073751-large.gif
2021,9406127,Fig. 7.,"Machine learning curves for able-bodied subjects (S1-S8). Training errors and testing errors are averaged across ten days for each subject, respectively. The solid red lines denote training errors and testing errors converge to values when the training-trial number increases. Error bands represent the standard deviation of each subject’s CEs across ten days.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv7-3073751-large.gif
2021,9406127,Fig. 8.,User learning curves for the amputees (A1-A3). RIs are averaged across ten days and used to qualify the ability of user learning. Error bands represent the standard deviation of each subject’s RIs across ten days.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv8-3073751-large.gif
2021,9406127,Fig. 9.,"Machine learning curves for the amputees (A1-A3). Training errors and testing errors are averaged across ten days for each subject, respectively. The solid red lines denote training errors and testing errors converge to values when the training-trial number increases. Error bands represent the standard deviation of each subject’s CEs across ten days.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv9-3073751-large.gif
2021,9406127,Fig. 10.,"Trends of normalized RIs (a) and CEs (b) along the course of twenty-trial training across ten days for able-bodied subjects. The green curve in (a) represents the exponential fit of the RI values. The solid red line in (b) denotes normalized training errors and testing errors converge to a value when the training-trial number increases. The results are averaged across subjects and ten days. Error bands represent the standard deviation of normalized RIs or CEs for different subjects. Symbol ‘*’ represents there is a significant difference between different training-trial numbers (
p<0.05
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv10ab-3073751-large.gif
2021,9406127,Fig. 11.,"Trends of normalized RIs (a) and CEs (b) along the course of twenty-trial training across ten days for the amputees. The green curve in (a) represents the exponential fit of the RI values. The solid red line in (b) denotes normalized training errors and testing errors converge to a value when the training-trial number increases. The results are averaged across subjects and ten days. Error bands represent the standard deviation of normalized RIs or CEs for different subjects. Symbol ‘*’ represents there is a significant difference between different training-trial numbers (
p<0.05
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv11ab-3073751-large.gif
2021,9406127,Fig. 12.,"CEs estimated by offline (TSV), offline (CV), online (NF) and online (VF) with different training-trial numbers. CEs are averaged across subjects. Symbols ‘**’ and ‘*’ represent there are significant differences among different validation methods (‘**’:
p<0.001
, ‘*’:
p<0.05
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv12-3073751-large.gif
2021,9406127,Fig. 13.,"Feature distribution visualization of S1, who performs wrist extension for twenty trials on the first day.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9406127/lv13-3073751-large.gif
2021,9508419,FIGURE 1.,Illustration of ensemble learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali1-3103119-large.gif
2021,9508419,FIGURE 2.,"Boxplots of energy consumption (left), session duration (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali2ab-3103119-large.gif
2021,9508419,FIGURE 3.,Outlier detection using isolation forest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali3-3103119-large.gif
2021,9508419,FIGURE 4.,Graphical representation of the proposed framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali4-3103119-large.gif
2021,9508419,FIGURE 5.,Top ten features for session duration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali5-3103119-large.gif
2021,9508419,FIGURE 6.,Top ten features for energy consumption.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali6-3103119-large.gif
2021,9508419,FIGURE 7.,Validation loss curve for session duration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali7-3103119-large.gif
2021,9508419,FIGURE 8.,Validation loss curve for energy consumption.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9508419/alali8-3103119-large.gif
2021,9233366,Fig. 1.,"Overview of categorization with illustration. Orange box: interpretability interface to demarcate the separation between interpretable information and the cognitive process required to understand them. Gray box: algorithm output/product that is proposed to provide interpretability. Black arrow: computing or comprehension process. The perceptive interpretability methods generate items that are usually considered immediately interpretable. On the other hand, methods that provide interpretability via mathematical structure generate outputs that require one more layer of cognitive processing interface before reaching the interpretable interface. The eyes and ear icons represent human senses interacting with items generated for interpretability.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9591206/9233366/tjoa1-3027314-large.gif
2021,9233366,Fig. 2.,"(a1) Using LIME to generate explanation for text classification. Headache and sneeze are assigned positive values. This means both factors have positive contribution to the model prediction “flu.” On the other hand, weight and no fatigue contribute negatively to the prediction. (a2) LIME is used to generate the super-pixels for the classification “cat.” (a3) ADC modality of a slice of MRI scan from ISLES 2017 segmentation competition. Reddish intensity region reflects a possible “explanation” to the choice of segmentation (segmentation not shown). (b) Optimized images that maximize the activation of a neuron in the indicated layers. In shallower layer, simple patterns activate neurons strongly while in deeper layer, more complex features such as dog faces and ears do. Figure (b) is obtained from https://distill.pub/2018/building-blocks/ with permission from Chris Olah.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9591206/9233366/tjoa2-3027314-large.gif
2021,9233366,Fig. 3.,"Overview on perceptive interpretability methods. (a) Saliency method with decomposition mechanism. The input which is an image of a cat is fed into the model for processing along the blue arrow. The resulting output and intermediate signals (green arrows) are decomposed and selectively picked for processing, hence providing information for the intermediate mechanism of the model in the form of (often) heatmappings, shown in red/orange/yellow colors. (b) Saliency method with sensitivity mechanism. The idea is to show how small changes to the input (black figures of birds and ducks) affect the information extracted for explainability (red silhouette). In this example, red regions indicate high relevance, which we sometimes observe at edges or boundary of objects, where gradients are high. (c) Signal method by inversion and optimization. Inverses of signals or data propagated in a model could possibly reveal more sensible information (see arrow labeled “inverse”). Adjusting input to optimize a particular signal (shown as the
i
th component of the function
f
1
) may provide us with
x
1
that reveals explainable information (see arrow labeled “optimization”). For illustration, we show that the probability of correctly predicting duck improves greatly once the head is changed to the head of a duck which the model recognizes. (d) Verbal interpretability is typically achieved by ensuring that the model is capable of providing humanly understable statements, such as the logical relation or the positive words shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9591206/9233366/tjoa3abcd-3027314-large.gif
2021,9233366,Fig. 4.,"(a1) TCAV [96] method finds the hyperplane CAV that separates concepts of interest. (a2) Accuracies of CAV applied to different layers supports the idea that deeper NN layers contain more complex concepts, and shallower layers contain simpler concepts. (b) SVCCA [97] finds the most significant subspace (direction) that contains the most information. The graph shows that as few as 25 directions out of 500 are enough to produce the accuracies of the full network. (c) t-SNE clusters images in meaningful arrangement, for example, dog images are close together. Figures (a1) and (a2) are used with permission from the authors Been Kim; figure (b) and (c) from Maithra Raghu and Jascha Sohl-dickstein.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9591206/9233366/tjoa4-3027314-large.gif
2021,9233366,Fig. 5.,"Overview of methods whose interpretability depend on interpreting underlying mathematical structure. (a) Predefined models. Modeling with clear, easily understandable model, such as linear model can help improve readability, and hence interpretability. On the other hand, using NN could obscure the meaning of input variables. (b) Feature extraction. Data, predicted values, signals, and parameters from a model are processed, transformed, and selectively picked to provide useful information. Mathematical knowledge is usually required to understand the resulting pattern. (c) Sensitivity. Models that rely on sensitivity, gradients, perturbations, and related concepts will try to account for how different data are differently represented. In the figure, the small changes transforming the bird to the duck can be traced along a map obtained using clustering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9591206/9233366/tjoa5abc-3027314-large.gif
2021,9233366,Fig. 6.,Overview of challenges and future prospects arranged in a Venn diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9591206/9233366/tjoa6-3027314-large.gif
2021,9617738,Fig. 1.,"(a) Schematic of the basic photonic microwave frequency estimation system. CW: continuous wave; EOM: electro-optic modulator; PC: polarization controller. OC: optical coupler. (b) optical spectra of (i) optical carrier, (ii) CS-DSB signal, (iii) (iv) filter pair (cosine shape).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/9592523/9617738/liu1abcdefghi-3128867-large.gif
2021,9617738,Fig. 2.,Simulated transmission curve comparison. (a) sinusoidal transmission curve (log scale); (b) triangular transmission curve (log scale); (c) attenuation slope comparison between sinusoidal (purple) and triangular (green) transmission function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/9592523/9617738/liu2abc-3128867-large.gif
2021,9617738,Fig. 3.,Performance comparison between the attenuation ratio slope in sinusoidal and triangular spectral response. (a) sinusoidal with fixed ER; (b) sinusoidal with fixed FSR; (c) triangular with fixed ER; (d) triangular with fixed FSR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/9592523/9617738/liu3abcd-3128867-large.gif
2021,9617738,Fig. 4.,"The proposed data-driven photonic microwave frequency estimation system with improved resolution and immunity to system nonlinearity. (a) Experimental setup. DFB: distributed feedback laser; EOM: electro-optic modulator; SG: signal generator; WS: optical wave shaper; OPM: optical power meter; MP: microprocessor; DNN: deep neural network; (b) structure of the designed DNN, and the overall block diagram showing the workflow of the proposed data-driven evaluation methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/9592523/9617738/liu4ab-3128867-large.gif
2021,9617738,Fig. 5.,"Examples of various measurement using one setting: (a) Measured optical spectra of CS-DSB signal (blue); complementary triangular transmission curves (orange solid and dash), transmission curves with tunable FSR and ER (purple dotted curves); (b) measured optical power at different RF frequency (RF power = 0 dBm, ER = 15dB and FSR = 0.05THz); (c) Model error distribution among train, validation, and test data; (d) model evaluation with R2 equal to 0.9994 (inset: training, validation and testing loss curves).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/9592523/9617738/liu5abcd-3128867-large.gif
2021,9617738,Fig. 6.,Performance evaluation: Estimated frequency and true frequency at different system settings. (a) the estimated frequency and true frequency; (b) preset RF power and measured optical power at complementary triangular spectral functions; (c) preset FSRs and ERs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/68/9592523/9617738/liu6abc-3128867-large.gif
2021,9314126,FIGURE 1.,"Process of underground metal target detection in the TEM system: (a)using a typical TEM detector to detect underground metal targets, and (b) the generation process of EMI response.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li1ab-3049308-large.gif
2021,9314126,FIGURE 2.,Process of the simulation platform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li2-3049308-large.gif
2021,9314126,FIGURE 3.,Accuracy heatmap of classification models (in columns) and dimensionality reduction methods (in rows) in material-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li3-3049308-large.gif
2021,9314126,FIGURE 4.,Accuracy heatmap of classification models (in columns) and dimensionality reduction methods (in rows) in shape-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li4-3049308-large.gif
2021,9314126,FIGURE 5.,The normalized confusion matrices in material-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li5-3049308-large.gif
2021,9314126,FIGURE 6.,The normalized confusion matrices in shape-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li6-3049308-large.gif
2021,9314126,FIGURE 7.,The average accuracy (in columns) for different dimensionality reduction methods under different SNR (in rows) in material-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li7-3049308-large.gif
2021,9314126,FIGURE 8.,The average accuracy (in columns) for different dimensionality reduction methods under different SNR (in rows) in shape-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li8-3049308-large.gif
2021,9314126,FIGURE 9.,The average accuracy (in columns) for different filter-based feature selection methods with different selected feature numbers (in rows) in material-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li9-3049308-large.gif
2021,9314126,FIGURE 10.,The average accuracy (in columns) for different filter-based feature selection methods with different selected feature numbers (in rows) in shape-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li10-3049308-large.gif
2021,9314126,FIGURE 11.,The NFTI coefficients (in columns) of feature sets obtained by different methods (in rows) in material-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li11-3049308-large.gif
2021,9314126,FIGURE 12.,The NFTI coefficients (in columns) of feature sets obtained by different methods (in rows) in shape-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li12-3049308-large.gif
2021,9314126,FIGURE 13.,The mutual information coefficients (in columns) of feature sets obtained by different methods (in rows) in material-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li13-3049308-large.gif
2021,9314126,FIGURE 14.,The mutual information coefficients (in columns) of feature sets obtained by different methods (in rows) in shape-based classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li14-3049308-large.gif
2021,9314126,FIGURE 15.,The feature number (in columns) corresponding to different dimensionality reduction methods (in rows).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li15-3049308-large.gif
2021,9314126,FIGURE 16.,The build time consumption (in columns) of ANN with different dimensionality reduction methods (in rows).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li16-3049308-large.gif
2021,9314126,FIGURE 17.,The classification time consumption (in columns) of ANN with different dimensionality reduction methods (in rows).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314126/li17-3049308-large.gif
2021,9312605,FIGURE 1.,The ratio for each class in CIC-IDS2017 and Kyoto2016 datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak1-3048900-large.gif
2021,9312605,FIGURE 2.,F1-score of RF for each sub-dataset obtained by equisized partitioning for Kyoto2016 dataset in February.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak2-3048900-large.gif
2021,9312605,FIGURE 3.,"Overall structure of two-level partitioning, where
S
max
is a control parameter for deciding the partitioned sub-dataset size.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak3-3048900-large.gif
2021,9312605,FIGURE 4.,Histograms of duration and count features for Kyoto2016 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak4-3048900-large.gif
2021,9312605,FIGURE 5.,Classification performance results according to the type of classifier algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak5-3048900-large.gif
2021,9312605,FIGURE 6.,Comparison of classification performance results between existing approaches and the proposed approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak6-3048900-large.gif
2021,9312605,FIGURE 7.,Comparison of F1-scores between existing approaches and the proposed approach for monthly data of Kyoto2016 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak7-3048900-large.gif
2021,9312605,FIGURE 8.,Example of the proposed service-aware two-level partitioning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9312605/pak8-3048900-large.gif
2021,9564057,Fig. 1.,Controller for interaction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu1-3118918-large.gif
2021,9564057,Fig. 2.,Physical setting of virtual supermarket.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu2-3118918-large.gif
2021,9564057,Fig. 3.,System design diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu3-3118918-large.gif
2021,9564057,Fig. 4.,VR supermarket environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu4ab-3118918-large.gif
2021,9564057,Fig. 5.,VR supermarket task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu5abc-3118918-large.gif
2021,9564057,Fig. 6.,Walking trajectory with top view.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu6-3118918-large.gif
2021,9564057,Fig. 7.,"Each model cross-validation accuracy box plot. Note. The x-axis is the model name and y-axis is the accuracy rate. This graph shows the maximum, minimum, median, and upper and lower quartiles of a set of data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu7-3118918-large.gif
2021,9564057,Fig. 8.,"Each model ROC curve graph on testing set.
N
ote: True positive rate = sensitivity, which represents how much ground truth is correctly judged when it is true. False positive rate represents how many correctly judged ground truths are false. The diagonal is the baseline.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9363468/9564057/wu8-3118918-large.gif
2021,9329053,Fig. 1.,"SR results on a real-world image captured by a digital single-lens reflex (DSLR) camera [1]. ESRGAN [4], RCAN [5], Deng [6] and IKC [7] are trained based on the synthetic data with bicubic degradation. RCAN-Real [5] and ZoomSR [1] are trained with non-aligned real LR-HR pairs. The result of RCAN-Real is blurry due to the RCAN model is directly trained on the misaligned images with
ℓ
1
or
ℓ
2
losses. Other methods use special loss, synthetic paired dataset or unpaired training strategy to deal with misalignment. The ZSSR [8], KMSR [2], Ji [3], Maedar [9], Lugmayr [10], CinCGAN [11] and ours are trained under unpaired setting. Our result contains more natural details and textures suffering from less blur and artifacts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong1abcdefghijklmno-3049951-large.gif
2021,9329053,Fig. 2.,Examples of misalignment between LR and HR images in real-world datasets. (a) and (b) are from SR-RGB dataset [1]. (c) is from RealSR dataset [12] after pre-registration. Misalignment issues are complex and extremely difficult to be removed completely.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong2abc-3049951-large.gif
2021,9329053,Fig. 3.,"Diagrams of the main ideas of five different paradigms applying for real-world SR. (a) and (b) show the paired learning-based SR methods, based on synthetic bicubic LR degradation and captured real-world LR-HR image pairs. (c), (d) and (e) show the unpaired learning-based methods. Before learning to do SR, they first learn to obtain paired supervision signals for SR in different ways. Different methods are used to alleviate the influence of domain gap. In the figures, we illustrate how the SR network
R(⋅)
is trained (left) and applied during inference. The aligned supervisions are indicated by “=”. “
≈
” is used to indicate the misaligned LR-HR pairs.
B(⋅)
and
G(⋅)
denote the bicubic degradation and the trainable generation network, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong3abcde-3049951-large.gif
2021,9329053,Fig. 4.,Visual examples of different domains of LR images. (a) The original LR image from [1] taken by a DSLR camera with a shorter focal length. (b) Local area from (a). (c) Bicubic synthetic image. (d) Generated image by our degradation generation network. The degradation in the realistic LR image in (b) is much more complex than the bicubic downsampling image in (c). Our generated network produces similar characteristics shown in (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong4abcd-3049951-large.gif
2021,9329053,Fig. 5.,"The proposed scheme consists of two major parts - a degradation generation network for generating LR images from real HR images and a degradation adaptive SR network for super-resolving real-world LR images. The data flow of the synthetic domain and realistic domain are denoted by red and blue lines, respectively. Firstly, we learn to generate
I
L
gen
for
I
H
real
by mimicking the degradation encoded in
{
I
L
real
}
. After obtaining the aligned image pair set
{(
I
L
gen
,
I
H
real
)}
, we train the real-world SR network
R(⋅)
under paired supervision.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong5-3049951-large.gif
2021,9329053,Fig. 6.,"Visual comparisons for 4
×
SR on the testing data from RealSR [12]. Local areas full of details and textures are zoom in. Our approach can produce better visual perceptual quality with more clean and natural textures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong6-3049951-large.gif
2021,9329053,Fig. 7.,"Visual comparisons for 3
×
SR on the testing data from City100 dataset [28]. The proposed network gets rid of blurry and artifacts, it produces high-quality super-resolved images, especially edges and textures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong7-3049951-large.gif
2021,9329053,Fig. 8.,"Visual comparisons for 4
×
SR on the testing data from SR-RGB dataset [1]. Our approach recovers more informative information and finer details. Misalignment exists in LR-HR pairs as no pre-registration is applied. The result of RCAN-Real is blurry due to the RCAN model is directly trained on the misaligned images with
ℓ
1
or
ℓ
2
losses. Other methods use special loss, synthetic paired dataset or unpaired training strategy to deal with misalignment. Note that shown real HR image contains misalignment (maybe in terms of color, illumination, and motion) to the underlying HR image of the LR observation, which is shown for reference.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong8-3049951-large.gif
2021,9329053,Fig. 9.,Mean opinion of the subjective evaluation for different SR methods. The longer bar indicates the larger variance of the scores.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong9-3049951-large.gif
2021,9329053,Fig. 10.,"Perception-distortion evaluation of SR algorithms based on RealSR dataset [12]. We plot different approaches on the perception-distortion plane. Perception is measured by LPIPS, PI and FID. Distortion is measured by PSNR. In all plots, the lower left corner is blank, revealing an unattainable region in the perception-distortion plane. Our method appears better perceptual quality with the expense of less distortion.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong10-3049951-large.gif
2021,9329053,Fig. 11.,"Visual results of our degradation generation network
G(⋅)
on RealSR dataset [12]. The bicubic degradation LR images, LR images generated by our method and the real LR images are illustrated. The LR images generated by our method can reflect characteristics of the realistic LR images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong11-3049951-large.gif
2021,9329053,Fig. 12.,Ablation study: SR results based on different variants of our degradation generation network. The examples are from RealSR testing set [12].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong12-3049951-large.gif
2021,9329053,Fig. 13.,Ablation study: SR results by using different training strategy and loss functions on our SR network. The examples are from RealSR testing set [12].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9263394/9329053/gong13-3049951-large.gif
2021,8908752,Fig. 1.,"An illustration of the MWC problem. The subscript of each vertex denotes the vertex index and the superscript denotes the vertex weight. The MWC of the given graph is {
v
3
4
,
v
4
5
,
v
3
7
} with total weight of 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9393625/8908752/sun1-2954827-large.gif
2021,8908752,Fig. 2.,"A comparison between different graph reduction techniques:
ml
,
f
c
,
f
r
,
f
b
,
f
d
and
f
w
. The horizontal axis represents the percentage of vertices selected by each method; and the vertical axis represents the best objective values generated by solving the subproblem formed by the selected vertices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9393625/8908752/sun2-2954827-large.gif
2021,8908752,Fig. 3.,"A comparison between MLPR
small
(trained on small graphs), MLPR
large
(trained on large graphs) and MLPR
none
(without any problem reduction) when incorporated with the 4 algorithms to solve the 11 large hard problem instances (
L
te
). The horizontal axis represents the graph index, and the vertical axis represents the mean of best objective values generated (
y
¯
) within the cutoff time (1000 seconds). For each method we sort
y
¯
from the 11 graphs in ascending order to generate the plots for easier visualization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9393625/8908752/sun3-2954827-large.gif
2021,9429227,FIGURE 1.,Machine learning general model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro1-3079639-large.gif
2021,9429227,FIGURE 2.,Deep neural network model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro2-3079639-large.gif
2021,9429227,FIGURE 3.,Perceptron model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro3-3079639-large.gif
2021,9429227,FIGURE 4.,Neural network abstraction for universal function approximation [38].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro4-3079639-large.gif
2021,9429227,FIGURE 5.,Training a neural network for solving optimization problems [38].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro5-3079639-large.gif
2021,9429227,FIGURE 6.,Activation functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro6-3079639-large.gif
2021,9429227,FIGURE 7.,Convolutional neural network model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro7-3079639-large.gif
2021,9429227,FIGURE 8.,Recurrent neural network model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro8-3079639-large.gif
2021,9429227,FIGURE 9.,Echo-State network model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro9-3079639-large.gif
2021,9429227,FIGURE 10.,Reinforcement learning model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro10-3079639-large.gif
2021,9429227,FIGURE 11.,Federated learning model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro11-3079639-large.gif
2021,9429227,FIGURE 12.,Hierarchical structure for solving optimization problems using FL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro12-3079639-large.gif
2021,9429227,FIGURE 13.,Performance analysis of DNN and WMMSE in terms of data-rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro13-3079639-large.gif
2021,9429227,FIGURE 14.,Evaluation of DROO convergence for 30 WDs and 30K time frames.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9429227/dahro14-3079639-large.gif
2021,8978961,Fig. 1.,"Schematic of the proposed system for lithofacies classification (B-ANN).
X
i
represents the
i
th input of the logging features,
g
i
is the
i
th neuron in ANN,
Pr(S|O)
is the ANN output probability of lithofacies (
S
), given well-logging observations (
O),
Q
i
is the latent discrete variable to be classified,
A
and
B
are the transition matrix and initial probability of lithofacies, respectively,
C
is the likelihood function that tells the distribution of
O
given
S
, and
Pr(S)
stands for the global proportion of
S
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng1-2968356-large.gif
2021,8978961,Fig. 2.,"Workflow for the classification strategy of lithofacies. Three depth points (1, 2, 3) are to be assigned with three possible lithofacies (
i
,
j
,
k
). The potential transitions between different lithofacies in the vertical direction are denoted by dashed lines. The final classified sequence of the lithofacies is demonstrated with solid lines (such as transitions of
a
ik
and
a
kj
) and filled circles (
i
,
k
,
j
in the upward vertical direction), which have the highest frequency at each depth point (
γ
d
(i))
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng2-2968356-large.gif
2021,8978961,Fig. 3.,"Crossplot of rock properties (Vp/Vs, AI), given each lithofacies (Shale, SH_Sand, and Sand). Marginal distributions are on the vertical slices.
Pr(⋅)
stands for the probability. High AI and low Vp/Vs are associated with Sand, whereas Shale is found with an opposite feature (low AI and high Vp/Vs). For SH_Sand, its property distribution highly overlaps with that of Sand.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng3-2968356-large.gif
2021,8978961,Fig. 4.,"Input features (AI and Vp/Vs) and predicted lithofacies by the proposed scheme (B-ANN) and ANN, together with the truth at the training well.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng4-2968356-large.gif
2021,8978961,Fig. 5.,"Input features (AI and Vp/Vs) and predicted lithofacies by the proposed scheme (B-ANN) and ANN, together with the truth at the blind well.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng5-2968356-large.gif
2021,8978961,Fig. 6.,"Precision–recall curves of each lithofacies for (a) training well and (b) blind well.
F1
is the harmonic mean of precision (
P
) and recall (
R
). The average precision-recall is calculated based on the whole lithofacies, which shows a general classification ability, rather than on the individual type.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng6ab-2968356-large.gif
2021,8978961,Fig. 7.,Posterior probabilities of each lithofacies at (a) training well and (b) blind well.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng7ab-2968356-large.gif
2021,8978961,Fig. 8.,Confusion matrices of (a) training well and (b) blind well.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9306020/8978961/feng8ab-2968356-large.gif
2021,9461805,FIGURE 1.,Proposed Methodology for Accurate Effort Prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva1-3091313-large.gif
2021,9461805,FIGURE 2.,Top 10 of the Techniques and Dataset used in the last 10 years.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva2-3091313-large.gif
2021,9461805,FIGURE 3.,Distribution of the effort value in the Desharnais database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva3-3091313-large.gif
2021,9461805,FIGURE 4.,Effort Value Distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva4-3091313-large.gif
2021,9461805,FIGURE 5.,Pearson Correlation Desharnais Dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva5-3091313-large.gif
2021,9461805,FIGURE 6.,Correlation coefficients between variables in the Desharnais database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva6-3091313-large.gif
2021,9461805,FIGURE 7.,Correlation Carvalho’s Work [11] Dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva7-3091313-large.gif
2021,9461805,FIGURE 8.,Boxplots of Mean Absolute Erro.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva8-3091313-large.gif
2021,9461805,FIGURE 9.,MAE comparison between ELM models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva9-3091313-large.gif
2021,9461805,FIGURE 10.,Comparison of time consumption of the applied methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9461805/carva10-3091313-large.gif
2021,9314000,FIGURE 1.,Predicting and intervening at-risk students at different percentages of the course length.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan1-3049446-large.gif
2021,9314000,FIGURE 2.,Heatmap showing correlation between demographics variables and final result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan2-3049446-large.gif
2021,9314000,FIGURE 3.,Heatmap showing correlation between demographics plus clickstream variables and final result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan3-3049446-large.gif
2021,9314000,FIGURE 4.,"Heatmap showing correlation between demographics, clickstream, and assessment variables with final result.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan4-3049446-large.gif
2021,9314000,FIGURE 5.,DFFNN confusion matrices when tested on different variables of OULAD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan5-3049446-large.gif
2021,9314000,FIGURE 6.,Normalized confusion matrix showing the DFFNN prediction for Fail and Pass classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan6-3049446-large.gif
2021,9314000,FIGURE 7.,Different types of triggers for students having different performance state.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan7-3049446-large.gif
2021,9314000,FIGURE 8.,Assessment score at 100% course vs sum of clicks at 100% course.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan8-3049446-large.gif
2021,9314000,FIGURE 9.,Late submission vs sum of clicks at 100% course.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan9-3049446-large.gif
2021,9314000,FIGURE 10.,Assessment score vs sum of clicks at 100% course.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan10-3049446-large.gif
2021,9314000,FIGURE 11.,Assessment performance at different percentage of course length.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan11-3049446-large.gif
2021,9314000,FIGURE 12.,Sum of clicks at different percentage of course length.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan12-3049446-large.gif
2021,9314000,FIGURE 13.,Students’ VLE interaction vs final grade positions at different percentage of course length.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan13-3049446-large.gif
2021,9314000,FIGURE 14.,"Relationship of Code module and imd_band with the final result where highest performance is observed in course BBB, FFF and lowest performance in course AAA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9314000/adnan14-3049446-large.gif
2021,9264700,Fig. 1.,Three-phase MMC circuit diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang1-3038064-large.gif
2021,9264700,Fig. 2.,(a) Traditional MPC for MMCs. (b) The proposed ML-based controller for MMCs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang2-3038064-large.gif
2021,9264700,Fig. 3.,Training and implementation procedures of ML MPC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang3-3038064-large.gif
2021,9264700,Fig. 4.,Picture of the experimental prototype.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang4-3038064-large.gif
2021,9264700,Fig. 5.,Experimental results of ML MPC and traditional MPC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang5-3038064-large.gif
2021,9264700,Fig. 6.,Output Current dynamic. (a1) Dynamic Performance of ML based controller (2A to 4A) (a2) Dynamic Performance of ML based controller (4A to 2A).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang6-3038064-large.gif
2021,9264700,Fig. 7.,Output Current frequency dynamic. (a1) Frequency Dynamic Performance of ML based controller (50 Hz to 20 Hz) (a2) Frequency Dynamic Performance of ML based controller (20 Hz to 40 Hz).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang7-3038064-large.gif
2021,9264700,Fig. 8.,Experimental Results of ML controller with different neuron numbers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang8-3038064-large.gif
2021,9264700,Fig. 9.,Results of ML controller with different size of data. (1) Output Current with 4860 data [A] (2) Output Current with 22500 data [A] (3) Output Current with 249018 data [A] (4) Output Current with 4889808 data [A] (5) Output Current with 31320432 data [A] (paper used data).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang9-3038064-large.gif
2021,9264700,Fig. 10.,Results when the output current references exceed the training range. (a1) Dynamic Performance of MPC (5.5A to 7A) (a2) Dynamic Performance of Feedforward Network (4A to 7A) (b1) Dynamic Performance of MPC (5.5A to 9A) (b2) Dynamic Performance of Feedforward Network (5.5A to 9A).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/41/9502023/9264700/wang10-3038064-large.gif
2021,9427236,Fig. 1.,"Illustration of a star network having one central station and three local workers
W
1
,
W
2
and
W
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho1-3078495-large.gif
2021,9427236,Algorithm 1,Communication-Efficient Distributed Dual Coordinate Ascent (CoCoA) [10],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho12-3078495-large.gif
2021,9427236,Fig. 2.,"Illustration of a tree-structured network, which has two layers. In the network, a central station (root node) has three direct child nodes
S
1
,
S
2
and
S
3
. Each node
S
i
has three direct child nodes
W
ij
,
j=1,2,3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho2-3078495-large.gif
2021,9427236,Fig. 3.,"Illustration of a subtree including a node
Q
on the
i
-th layer and its direct and indirect child nodes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho3-3078495-large.gif
2021,9427236,Algorithm 2,TreeDualMethod: Distributed Dual Coordinate Ascent for the Root Node Q on the Layer-0,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho13-3078495-large.gif
2021,9427236,Algorithm 3,"TreeDualMethod: Distributed Dual Coordinate Ascent for a General Tree Node Q on the Layer-
i,i=1,2,…,p−1",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho14-3078495-large.gif
2021,9427236,Procedure P.,TreeDualMethod: Distributed Dual Coordinate Ascent for a Leaf Node Q on the Layer-p,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho15-3078495-large.gif
2021,9427236,Fig. 4.,Illustration of the structure of the tree network factor in convergence analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho4-3078495-large.gif
2021,9427236,Fig. 5.,"Duality gap at the central node in a regression problem as the operation time of the algorithms goes. The distributed dual coordinate ascent in a tree network (red) and a star network (blue), i.e., CoCoA, are considered when the communication delay,
t
delay
, exists between the central node and its direct child nodes.
t
delay
=r×
t
lp
, where
t
lp
represents the computational time for one local iteration at a local worker, and
r
represents the delay severity level.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho5-3078495-large.gif
2021,9427236,Fig. 6.,"Duality gap at the central node in a classification problem as the operation time of the algorithms goes. The distributed dual coordinate ascent in a tree network (red solid line) and a star network (blue dotted line), i.e., CoCoA, are considered when the communication delay,
t
delay
, exists between the central node and its direct child nodes.
t
delay
=r×
t
lp
, where
t
lp
represents the computational time for one local iteration at a worker, and
r
represents the delay severity level.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho6ab-3078495-large.gif
2021,9427236,Fig. 7.,"(a) The objective value of (14), which is the convergence bound (or improvement), when the number of iterations
T
p
is varied from 1 to 2000, where
(C,K,δ,
t
total
,
t
lp
,
t
cp
)=(0.5,3,
1
/300,1,4×
10
−5
,3×
10
−5
)
and
t
delay
=r×
t
lp
. The red line represents the optimal number of local iterations to achieve the fastest convergence rate. (b) Optimal number of iterations to achieve the fastest convergence rate, when the parameters are the same as (a) and
r
is varied from 1 to 105.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho7ab-3078495-large.gif
2021,9427236,Fig. 8.,"(a) Duality gap when the delay severity
r
is 1. (b) Duality gap when the delay severity
r
is
10
5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho8ab-3078495-large.gif
2021,9427236,Fig. 9.,"Convergence bound over the whole tree network,
Θ
0
, by varying number of child nodes
K
with fixed other parameters
(p,
T
i
,
C
i
)=(3,40,0.9)
for all
i=0,1,…,p−1
, and
Θ
p
=0.5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho9ab-3078495-large.gif
2021,9427236,Fig. 10.,"Convergence bound over the whole tree network,
Θ
0
, by varying the number of layers
p
with fixed other parameters
(K,
C
i
)=(5,0.9)
for all
i=0,1,…,p−1
, and
Θ
p
=0.5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho10-3078495-large.gif
2021,9427236,Fig. 11.,"Optimal number of local iterations,
T
p
, by varying parameters. Except for the varying parameter, other parameters are fixed to
(C,K,δ,r)=(0.5,3,
1
/300,100)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/49/9457209/9427236/cho11abcd-3078495-large.gif
2021,9286431,FIGURE 1.,Visualizing the semantic relationships between words by Word2Vec word embeddings representation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar1-3043221-large.gif
2021,9286431,FIGURE 2.,The architecture of the pipeline for morbidity detection in clinical records using TF-IDF representations with CML and DL approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar2-3043221-large.gif
2021,9286431,FIGURE 3.,The architecture of DL models to use word embeddings representation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar3-3043221-large.gif
2021,9286431,FIGURE 4.,The architecture of DL models to use TF-IDF representation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar4-3043221-large.gif
2021,9286431,FIGURE 5.,The training time of CML models with different representations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar5-3043221-large.gif
2021,9286431,FIGURE 6.,The training time of DL models with different representations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar6-3043221-large.gif
2021,9286431,FIGURE 7.,Best performances of CML classifiers using embeddings with and without stopwords taken from Tables 9 and 10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar7-3043221-large.gif
2021,9286431,FIGURE 8.,Experimental results of CML and DL models with and without the employment of feature selection algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar8-3043221-large.gif
2021,9286431,FIGURE 9.,Experimental results of CML and DL models with word embeddings.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9286431/kumar9-3043221-large.gif
2021,9268187,Fig. 1.,"Illustration of diabetes management, along with the large amount of data produced by self-management and clinical EHRs. The data is processed by healthcare providers to develop deep learning algorithms for the new treatments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9497060/9268187/li1-3040225-large.gif
2021,9268187,Fig. 2.,"Visualization of ANNs and DNNs. DNNs have an increasing number of hidden layers embedding the variants of neural nodes and cells. Higher-level feature maps are computed by the deep models. There are five popular DNN architectures for diabetes: DMLP, CNN, RNN, AE, and RBM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9497060/9268187/li2-3040225-large.gif
2021,9268187,Fig. 3.,PRISMA flow of selection process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9497060/9268187/li3-3040225-large.gif
2021,9268187,Fig. 4.,Number of articles included in the collection grouped by the year of publication and application field. The orange dashed line indicates the number of citations in each year.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9497060/9268187/li4-3040225-large.gif
2021,9406331,Fig. 1.,"Comparison between classical SVM and NHSVM. The classical SVM has two parallel boundary hyperplanes, while the NHSVM has two nonparallel boundary hyperplanes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi1-3073812-large.gif
2021,9406331,Fig. 2.,Flowchart of the proposed MKL-NHSVM+ algorithm for BUS-based CAD with three CEUS images as multi-PI data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi2-3073812-large.gif
2021,9406331,Fig. 3.,"Example BUS and three CEUS images (AP, PVP, and DP) of ROIs from the benign and malignant liver tumors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi3-3073812-large.gif
2021,9406331,Fig. 4.,"ROC curves obtained by different algorithms for BUS-based CAD of liver cancers with different phases used as PI, respectively. (a) AP is used as PI, (b) PVP is used as PI, and (c) DP is used as PI.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi4-3073812-large.gif
2021,9406331,Fig. 5.,ROC curves obtained by different algorithms for BUS-based CAD of liver cancers with multi-view CEUS as PI.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi5-3073812-large.gif
2021,9406331,Fig. 6.,"Results obtained by different algorithms for BUS-based CAD of liver cancers with (a) AP and PVP, (b) AP and DP, and (c) PVP and DP as PI, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi6-3073812-large.gif
2021,9406331,Fig. 7.,"ROC curves obtained by different algorithms in BUS-based CAD of liver cancers with (a) AP and PVP, (b) AP and DP, and (c) PVP and DP as PI, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi7-3073812-large.gif
2021,9406331,Fig. 8.,"The accuracy results with different parameter settings (a) for varied
c
1
and
c
2
, and fixed
γ
, (b) for varied
c
1
and
γ
, and fixed
c
2
, and (c) for varied
c
2
and
γ
, and fixed
c
1
, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9406331/shi8-3073812-large.gif
2021,9381402,Fig. 1.,Flowchart of DQ-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8920/9502270/9381402/cao1-3067014-large.gif
2021,9382092,Fig. 1.,The smart pen for direct and continuous classification of anaesthetics in human serum.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass1-3067388-large.gif
2021,9382092,Fig. 2.,"The anesthesiologist follows up the infusion of anaesthetics in the patient body through the portable pen. The system includes a needle-shaped electrochemical sensor for propofol detection, a Quasi Digital (QD) potentiostat, on an embedded device, closed in a custom pen-shaped case, with Bluetooth; communication towards an external PC running a Machine Learning (ML)-based classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass2-3067388-large.gif
2021,9382092,Fig. 3.,"Circuit implementation of the QD potentiostat. The driver (on the left) converts the Pulse Width Modulated (PWM) input in the potential to be applied cross RE and WE, while the reader (on the right) converts the Faradaic current flowing through WE and CE into the QD output.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass3-3067388-large.gif
2021,9382092,Fig. 4.,Disposable pencil graphite needle-shaped sensor for detection of propofol in human serum.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass4-3067388-large.gif
2021,9382092,Fig. 5.,"Waveform of the current-to-QD conversion: the capacitor C
R2
is discharged according to the input current, after a time
T
off
, the comparator sets the QD signal when V(C
R2
) overcomes the lower threshold (V
thL
). After a time
T
on
, C
R2
is charged again to V
thH
and QD is reset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass5-3067388-large.gif
2021,9382092,Fig. 6.,"The embedded board includes on a single PCB a Bluetooth; antenna for communication, a MCU for processing, the QD potentiostat (driver and reader), a power manager, and the jack connector to host the sensor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass6-3067388-large.gif
2021,9382092,Fig. 7.,Needle-shape sensor results: in the voltammogram (a) the current peak shows an increase which is linearly proportional to the concentration of propofol. This is used to extract the calibration curve (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass7-3067388-large.gif
2021,9382092,Fig. 8.,Comparison between commercial lab instrument (a) and proposed QD potentiostat (b) in detection of propofol in its therapeutic range. The calibration curves (c) present few differences.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9440419/9382092/aiass8-3067388-large.gif
2021,9302752,Fig. 1.,The structure of the semi-supervised learning algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9373029/9302752/shen1-3046531-large.gif
2021,9302752,Fig. 2.,The CDF of the ranging error on testing data with different trained models and raw data with NLOS condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9373029/9302752/shen2-3046531-large.gif
2021,9302752,Fig. 3.,"The RMSE for the semi-supervised model with 1%, 3% and 5% labeled training samples and different unlabeled samples tested on 80% testing samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9373029/9302752/shen3-3046531-large.gif
2021,9302752,Fig. 4.,"The RMSE for the semi-supervised model with different
η
and
C
tested on mixed testing samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962382/9373029/9302752/shen4-3046531-large.gif
2021,9449664,FIGURE 1.,LVDC distribution system with DERs and SCADA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho1-3087914-large.gif
2021,9449664,FIGURE 2.,Real-time prototype distribution system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho2-3087914-large.gif
2021,9449664,FIGURE 3.,PV and wind hybrid DERs installed in real-time.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho3-3087914-large.gif
2021,9449664,FIGURE 4.,Flowchart of proposed algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho4-3087914-large.gif
2021,9449664,FIGURE 5.,Training data set for classification learner.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho5-3087914-large.gif
2021,9449664,FIGURE 6.,Confusion matrix of fine tree classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho6-3087914-large.gif
2021,9449664,FIGURE 7.,Confusion matrix of Linear SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho7-3087914-large.gif
2021,9449664,FIGURE 8.,Confusion matrix of quadratic SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho8-3087914-large.gif
2021,9449664,FIGURE 9.,Confusion matrix of Gaussian SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho9-3087914-large.gif
2021,9449664,FIGURE 10.,"Electrical parameters: voltage, current and power across the loads of distribution system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho10-3087914-large.gif
2021,9449664,FIGURE 11.,Electrical parameters across the loads of F3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho11-3087914-large.gif
2021,9449664,FIGURE 12.,"V, I and power across the loads of feeder F3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho12-3087914-large.gif
2021,9449664,FIGURE 13.,Comparative results of machine learning with AI based algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho13-3087914-large.gif
2021,9449664,FIGURE 14.,Comparative analysis of all algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9449664/mudho14-3087914-large.gif
2021,8946700,Fig. 1.,"An illustration of congruency in the saliency prediction task. Assuming training samples are provided in a sequential manner, an incongruency occurs since the food item is related to different saliency values across these samples. Here,
S
j
stands for sample
j={1,2,3}
,
w
i
is the weight at time step
i
,
Δ
w
S
j
is the weight update generated with
S
j
for
w
i
, and the arrows indicate updates for the model. Specifically,
Δ
w
S
j
=−η
g
S
j
where
η
is the learning rate and
g
S
j
is the gradient w.r.t.
S
j
. The update of
S
2
(i.e.,
Δ
w
S
2
) is congruent with
Δ
w
S
1
, whereas
Δ
w
S
3
is incongruent with
Δ
w
S
1
and
Δ
w
S
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo1-2963387-large.gif
2021,8946700,Fig. 2.,"An illustration of model training with the proposed DCL module. Here, 3 samples are observed in a sequential manner. The gradient generated by
S
3
is expected to be different with the gradients generated by
S
1
and
S
2
. Hence, to tackle the expected violation between the update
−η
g
S
3
and the accumulated update
−η
∑
2
i=1
g
S
i
, the proposed DCL method finds a corrected update
−η
g
~
S
3
(the pink arrow) by solving a quadratic programming problem (5). In this way, the angle between
−η
g
~
S
3
and
−η
∑
2
i=1
g
S
i
(the blue arrow), i.e.,
α
~
, is guaranteed to be equal to or less than
α
. Note that the gradient descent processes with or without the proposed DCL module is identical in the test phase.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo2-2963387-large.gif
2021,8946700,Fig. 3.,"An illustration of DCL constraints with two reference points
r
0
=
w
0
,
r
1
=
w
1
.
g
^
r
0
is the pink arrow while
g
^
r
1
is the green one. The colored dashed line indicates the border of feasible region with regards to
−
g
^
r
i
,i∈{0,1}
, since Constraint (6) forces
−η
g
~
k
to have an angle which is smaller than or equal to
90
∘
w.r.t.
g
^
r
0
and
g
^
r
1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo3-2963387-large.gif
2021,8946700,Fig. 4.,"An illustration to demonstrate the concept of the effective window. Given the spiral convergence path,
−η
g
^
10|
w
0
restricts the search direction and the minimum (i.e., the red star) and
w
11
are unreachable according to the search direction. In contrast,
w
11
can be reached along the search direction of
−η
g
^
10|
w
7
. To adaptively yield appropriate accumulated gradients that converge to the minimum, we define an effective window to periodically update the reference.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo4-2963387-large.gif
2021,8946700,Fig. 5.,"An example demonstrating the effect of the proposed DCL method on three optimizers, i.e., gradient descent (GD), RMSProp, and Adam. Given a problem
z=f(x,y)
, we use these optimization algorithms to compute the local minima, i.e.,
(
x
∗
,
y
∗
)
that yield the minimal
z
∗
. In the experiment, except the learning rate, the setting and hyperparameters are the same for ALGO and ALGO DCL, where ALGO={GD, RMSProp, Adam}. The proposed DCL method encourages the convergence paths to be as straight as possible.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo5-2963387-large.gif
2021,8946700,Fig. 6.,"An illustration demonstrating the difference between DCL (left) and GEM [33] (right). The search direction in DCL is determined by the accumulated gradient while the adjusted gradient (solid line) of GEM is optimized by avoiding the violation between the gradient (dashed line) and memory samples’ gradients (green line). Since the weights are iteratively updated and the memory samples are preserved, the direction of the adjusted gradient of the memory samples could be dynamically varying.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo6-2963387-large.gif
2021,8946700,Fig. 7.,"The congruencies (Cong.) generated by the given references (Ref.) and samples with the baseline ResNet-50 RMSP in Table 2. The cosine similarities (Sim.) between referred images and sample images are provided for comparison purposes. Source images and the corresponding ground-truths, i.e., fixation maps, are displayed along with the congruencies. The first and second block are the results of subset that contains persons in various scenes. The third block is examples of food subset. The rightmost block shows subset with mixed image categories, i.e., contain objects of various categories in various scenes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo7-2963387-large.gif
2021,8946700,Fig. 8.,The congruencies (Cong.) generated by the given references (Ref.) and samples with the baseline ResNet-101 SGD in Table 7. The images with its labels are displayed along with the congruencies. The cosine similarities (Sim.) between referred images and sample images are provided for comparison purposes. The first block is the results of the intra-similar-class subset consisting of images of tabby cat and Egyptian cat. The middle block is the results of the inter-class subset consisting of images of tabby cat and German shepherd dog. The value in bracket indicates number of images. The bottom block is the results of images of various labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo8-2963387-large.gif
2021,8946700,Fig. 9.,"Ablation study w.r.t. effective window size
β
w
and references number
N
r
. (a) and (b) are the experimental results on the SALICON validation set, while (c) and (d) are with the Tiny ImageNet validation set.
β
w
=∞
in (b) and
β
w
=50
in (d).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo9-2963387-large.gif
2021,8946700,Fig. 10.,"Congruencies along the epochs in saliency prediction learning, as defined in Eq. (4). The samples sequences for training models are determined by independent stochastic processes in Fig. 10a and 10b, while the permuted samples sequences are pre-determined and fixed for all models in Fig. 10c and 10d. The baseline, GEM, and DCL are ResNeXt-29 SGD, ResNeXt-29 SGD GEM, and ResNeXt-29 SGD DCL-
∞
-1 (see Table 6), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo10-2963387-large.gif
2021,8946700,Fig. 11.,The average congruencies over epochs in training on the three datasets for continual learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo11-2963387-large.gif
2021,8946700,Fig. 12.,"Analyses of the congruencies and magnitudes along the epochs in classification task, as defined in Eq. (4) and (20).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo12-2963387-large.gif
2021,8946700,Fig. 13.,"Training error vs. iteration on Tiny ImageNet with ResNet-101. (a) and (b) plot the mean and standard deviation of training errors at each epoch, respectively. Specifically, we show four representative curves of training error vs. iteration at epoch 1, 5, 10, and 15 in (c) – (f), respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo13-2963387-large.gif
2021,8946700,Fig. 14.,"Validation loss vs. epoch/task. In (c), the dashed blue curve indicates that the classification losses of GEM on Tiny ImageNet are all above 1.0 so they are not shown in the figure for clarity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9427901/8946700/luo14-2963387-large.gif
2021,9540268,Fig. 1.,Symbolic view of a closed-loop neural prosthesis. Multi-channel neural signals such as ECoG and LFP are recorded by cortical and deep-brain electrodes and sent to the implantable microchip. The on-chip biomarker extraction and ML processor detect the onset of symptoms and trigger a therapeutic neurostimulator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu1-3112756-large.gif
2021,9540268,Fig. 2.,"Standard and emerging electrodes for neural recording and stimulation via noninvasive, minimally-invasive, and invasive technologies; (a) Standard scalp-EEG electrodes. (b) The Epios subscalp EEG device for chronic epilepsy monitoring [39]. (c) Standard and high-density ECoG [40]. (d) Stereo-EEG leads [41]. (e) Clinical DBS (Medtronic's FDA-approved 3389, left), emerging directional DBS leads (8-channel direct STNAcute and 40-channel Medtronic-Sapiens, middle) and the Willsie and Dorval 1760-contact micro-DBS lead (right) [42].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu2-3112756-large.gif
2021,9540268,Fig. 3.,"Existing clinical or research-based closed-loop neuromodulation devices (with or without on-device ML); (a) The NeuroPace RNS device for epilepsy. (b) The AspireSR (Cyberonics, now known as LivaNova) device for epilepsy. (c) The Medtronic Percept PC device for movement disorders. (d) The Newronika AlphaDBS system for Parkinson's disease. (e) The DyNeuMo Mk-1 system for movement disorders.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu3-3112756-large.gif
2021,9540268,Fig. 4.,"Hardware architectures and chip micrographs of ML-embedded neural prostheses for epilepsy: (a) Linear dual-detector SVM classifier and closed-loop transcranial neurostimulator [13], (b) non-linear SVM-based seizure detector [14], (c) linear least square (LLS) classifier and closed-loop stimulator [17], (d) ridge regression classifier (RRC) and closed-loop stimulator [88]; (e) Gradient-boosted tree ensemble for seizure detection [12], (f) exponentially decaying-memory SVM and closed-loop stimulator [16], (g) AdaBoost decision tree classifier and closed-loop stimulator [29], (h) two-level coarse/fine classifier and closed-loop stimulator [89].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu4-3112756-large.gif
2021,9540268,Fig. 5.,"Hardware architectures and chip micrographs of ML-embedded neural prostheses for various applications: (a) Linear SVM for epilepsy [11], (b) ANN for migraine state detection [18], (c) DNN for Autism emotion detection [100], (d) CNN for emotion detection [101].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu5-3112756-large.gif
2021,9540268,Fig. 6.,"A DVTE with eight decision trees. Unlike conventional tree ensembles that uniformly set the maximum depth on all trees, the maximum depths in a DVTE are different (1–4). The internal and leaf nodes are shown in blue and black, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu6-3112756-large.gif
2021,9540268,Fig. 7.,"The outputs of decision trees in a DVTE. Latency is defined as the time difference between the expert-marked seizure onset and the state change of each tree's output.
d
is the maximum depth of each tree.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu7-3112756-large.gif
2021,9540268,Fig. 8.,"Performance comparison of DVTE and conventional tree ensemble with a maximum depth of 4. DVTE reduced the latency by 2.5× with a marginal performance reduction (
<
3% in sensitivity and
<
1% in specificity). Error bars indicate the standard errors across patients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu8-3112756-large.gif
2021,9540268,Fig. 9.,"Hardware cost as a function of the regularization coefficient
C
in DVTE. Large
C
imposes strong regularization and reduces the power/latency cost. The power cost was calculated as the average power consumption to extract features along the decision path. Latency was estimated as the average time to traverse a root-to-leaf decision path in the tree.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu9-3112756-large.gif
2021,9540268,Fig. 10.,"Seizure detection performance as a function of (a) power consumption and (b) latency. Shaded area indicates the standard errors across patients. The experiment was performed using DVTE and the following setting: 8 trees, depths varying from 1 to 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu10-3112756-large.gif
2021,9540268,Fig. 11.,"The number of extracted features in DVTE for different regularization coefficients. With greater
C
, the cost-aware model tends to use hardware-friendly features (e.g., LLN, Var). Features with longer windows (
δ,θ,α
) are also penalized in the cost-aware model. The power cost and latency for each
C
are shown in the legend, while the X-axis shows individual feature costs. For
C=0.01
, we achieved an average power cost of 268nW and latency of 0.52 s.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu11-3112756-large.gif
2021,9540268,Fig. 12.,"Hardware implementation of the proposed DVTE classifier: (a) system architecture, (b) layout, (c) area breakdown of the DVTE processor and a single decision tree, and (d) system power breakdown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu12-3112756-large.gif
2021,9540268,Fig. 13.,"Comparison of ResOT and axis-aligned tree ensemble on seizure and tremor detection tasks. The conventional gradient boosted ensemble (lightGBM [128]) and gradient boosting with power-efficient training (PEGB) were included. For PEGB, We used fixed-point thresholds and leaf weights, in contrast to floating points in lightGBM [105]. Cross-subject standard errors are shown by error bars.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu13-3112756-large.gif
2021,9540268,Fig. 14.,"The number of extracted features with different regularization coefficients in ResOT. With greater power- and latency-aware regularization terms, oblique trees prioritize low-power and low-latency features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu14-3112756-large.gif
2021,9540268,Fig. 15.,Parallel node evaluation scheme. (a) One internal node is evaluated per window. (b) Two layers (maximum 3 nodes) are concurrently evaluated per window. (c) All nodes are evaluated in parallel. The evaluated nodes are shown in color and bold lines represent the decision path. Node colors represent successive windows.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu15-3112756-large.gif
2021,9540268,Fig. 16.,"The power-latency trade-off with parallel node evaluation. With more nodes evaluated in parallel, latency is reduced at the cost of increased power consumption. Experiments were conducted with ResOT on epilepsy task.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu16-3112756-large.gif
2021,9540268,Fig. 17.,"(a) Interpretation of the tremor detection process using a tree ensemble and shapley additive explanations. Features plotted in red predicted an increased risk of tremor, while those in blue were associated with a low tremor risk. (b) Visualization of the seizure detection process in an oblique tree in one arbitrary patient. We show the percentage of samples visiting each node, and the required power and latency to evaluate each internal node. There are multiple “short paths” which allow dynamic early exiting. (c) Visualization of a cost-aware oblique tree, showing a significant reduction in the power cost of the root node.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4156126/9643424/9540268/zhu17-3112756-large.gif
2021,9468629,FIGURE 1.,The framework of the proposed multiclass prediction model for predicting final student grades.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam1-3093563-large.gif
2021,9468629,FIGURE 2.,Flowchart of the proposed multiclass prediction model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam2-3093563-large.gif
2021,9468629,FIGURE 3.,Mean and standard deviation of student’s final marks against student’s final grades achievement according to the taken courses.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam3-3093563-large.gif
2021,9468629,FIGURE 4.,Graph plot of student’s final marks distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam4-3093563-large.gif
2021,9468629,FIGURE 5.,Analysis of average grade point trend for ICS and CSA courses by yearly basis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam5-3093563-large.gif
2021,9468629,FIGURE 6.,Result of predictive model performance with SMOTE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam6-3093563-large.gif
2021,9468629,FIGURE 7.,Comparison of correctly classified by class without applied SMOTE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam7-3093563-large.gif
2021,9468629,FIGURE 8.,Comparison of correctly classified by class with applied SMOTE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam8-3093563-large.gif
2021,9468629,FIGURE 9.,Classification performance of accuracy and f-measure with different FS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam9-3093563-large.gif
2021,9468629,FIGURE 10.,Comparison of accuracy and f-measure of proposed SFS model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9468629/selam10-3093563-large.gif
2021,9174770,Fig. 1.,"Mapper algorithm. Here, the point cloud has the shape of a tree in
X=
R
2
. The filter function
f:X→Z
is given by the distance from the origin, and
Z=R
is covered by three overlapping intervals (blue, red, and green). Clustering each preimage leads to a covering
V
of the point cloud, whose nerve
N(V)
provides the graph shown on the right-hand side. The size of each vertex indicates the number of elements in the corresponding cluster, while edges connect clusters with common elements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel1-3015790-large.gif
2021,9174770,Fig. 2.,"Sketch of a fully connected feedforward neural network. During training, the incoming weights to a given neuron in the hidden or output layer evolve as a point in a space of dimension given by the number of neurons in the previous layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel2-3015790-large.gif
2021,9174770,Fig. 3.,"Evolution of weights for a neural network with one hidden layer, initialized at zero (colored by training step). Top: PCA projections. Bottom: learning graphs. Left: hidden layer. Right: output layer (digits are indicated).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel3-3015790-large.gif
2021,9174770,Fig. 4.,Evolution of the classification of test images of the digit 9. Horizontal: percentage of training. Vertical: number of classified test samples. The misclassification of nines as sevens loses its prominence around the time when the weights of neurons 7 and 9 branch off in the learning graph.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel4-3015790-large.gif
2021,9174770,Fig. 5.,"Evolution of the weights of a neural network with two hidden layers, initialized at tiny random values (colored by training step). Top: PCA projections. Bottom: learning graphs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel5-3015790-large.gif
2021,9174770,Fig. 6.,"Left: detail of the PCA projection of the evolution of the weights for the first hidden layer, as in the top left of Fig. 5. Several phases are visible: uniform evolution (blue), parallel evolution along a surface (green), and chaotic evolution (yellow). Right: learning graph representing the surface as a densely connected grid (filter function given by the first three PCA directions).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel6ab-3015790-large.gif
2021,9174770,Fig. 7.,"Top: norms of the weights per neuron for the two hidden layers and the output layer. The uniform increase is triggered by branching in the output layer. The branching is followed by a short period of reshuffling, before a converging period. Bottom: accuracy increases in jumps: 10% (chance), 40% and 80% (with more training steps; this would be followed by a slow increase toward 95%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel7-3015790-large.gif
2021,9174770,Fig. 8.,"Natural coordinates on the learning surface from Fig. 6, provided by differences in the weights of nearby neurons. The downward direction is the training time; the horizontal direction is the lateral spread of neurons across the surface. The last row shows a continuous transformation between the shapes of the digits 1, 3, and 0.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9505270/9174770/gabel8-3015790-large.gif
2021,9577211,Fig. 1.,Distribution frequency of each class of NSL-KDD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong1-3120804-large.gif
2021,9577211,Fig. 2.,Distribution frequency of each class of AWID.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong2-3120804-large.gif
2021,9577211,Fig. 3.,The interaction model between the agent and the environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong3-3120804-large.gif
2021,9577211,Fig. 4.,Dataset preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong4-3120804-large.gif
2021,9577211,Fig. 5.,Schematic diagram of SSDDQN model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong5-3120804-large.gif
2021,9577211,Fig. 6.,Current neural network structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong6-3120804-large.gif
2021,9577211,Fig. 7.,Training time of all models (NSL-KDD dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong7-3120804-large.gif
2021,9577211,Fig. 8.,Prediction time of all models (NSL-KDD dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong8-3120804-large.gif
2021,9577211,Fig. 9.,SSDDQN model performance (NSL-KDD dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong9-3120804-large.gif
2021,9577211,Fig. 10.,Confusion matrix of the DDQN model (NSL-KDD dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong10-3120804-large.gif
2021,9577211,Fig. 11.,Confusion matrix of the SSDDQN model (NSL-KDD dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong11-3120804-large.gif
2021,9577211,Fig. 12.,Confusion matrix of the DDQN model (AWID dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong12-3120804-large.gif
2021,9577211,Fig. 13.,Confusion matrix of the SSDDQN model (AWID dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong13-3120804-large.gif
2021,9577211,Fig. 14.,F1-Score metric changes with the line graph of the discount factor (AWID dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong14-3120804-large.gif
2021,9577211,Fig. 15.,Confusion matrix of the AE-DNN model (AWID dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong15-3120804-large.gif
2021,9577211,Fig. 16.,Confusion matrix of the SSDDQN with AE model (AWID dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong16-3120804-large.gif
2021,9577211,Fig. 17.,Confusion matrix of the SSDDQN with DNN model (AWID dataset).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4275028/9644641/9577211/dong17-3120804-large.gif
2021,9477439,FIGURE 1.,"Cross-section, magnetic flux density and flux lines of the machine under study.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba1-3095668-large.gif
2021,9477439,FIGURE 2.,Neural network structure [7].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba2-3095668-large.gif
2021,9477439,FIGURE 3.,Block diagrams for the motor dq reference frame model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba3-3095668-large.gif
2021,9477439,FIGURE 4.,Flux-linkage and linear ANN-based models (a) d- axis current (b) q-axis current and (c) electromagnetic torque waveforms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba4abc-3095668-large.gif
2021,9477439,FIGURE 5.,"(a) Equally distributed data, EDD, (441 points), (b) LHS data distribution (220 points) and (c) Sobol sequence (220 points).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba5abc-3095668-large.gif
2021,9477439,FIGURE 6.,Networks training relative errors expressed in [%].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba6-3095668-large.gif
2021,9477439,FIGURE 7.,The time dedicated to train the ANNs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba7-3095668-large.gif
2021,9477439,FIGURE 8.,"(a) electromagnetic torque and (b) instantaneous error of the FEM and ANN models obtained for different data samples distribution, at steady state.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba8ab-3095668-large.gif
2021,9477439,FIGURE 9.,(a) FEM vs ANN Torque comparison obtained for rated current and (b) Frequency spectrum of the electromagnetic torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba9ab-3095668-large.gif
2021,9477439,FIGURE 10.,(a) FEM vs ANN Torque comparison obtained for 5 times rated current and (b) Frequency spectrum of the electromagnetic torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba10ab-3095668-large.gif
2021,9477439,FIGURE 11.,(a) FEM vs ANN Torque comparison obtained for 8 times rated current and (b) Frequency spectrum of the electromagnetic torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba11ab-3095668-large.gif
2021,9477439,FIGURE 12.,The Matlab Simulink PMSM model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba12-3095668-large.gif
2021,9477439,FIGURE 13.,The laboratory test-bench setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba13-3095668-large.gif
2021,9477439,FIGURE 14.,"Simulated, measured and reference speed (a) and torque (b) variations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba14ab-3095668-large.gif
2021,9477439,FIGURE 15.,Simulated vs. measured dq machine currents.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9477439/ruba15-3095668-large.gif
2021,9423874,Fig. 1.,Parameterized geometry of the triode-type MIG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9439211/9423874/zhang1-3075166-large.gif
2021,9423874,Fig. 2.,Structure of artificial neural networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9439211/9423874/zhang2-3075166-large.gif
2021,9423874,Fig. 3.,Prediction accuracy of the artificial neural networks regression trees as the function of the generation number.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9439211/9423874/zhang3-3075166-large.gif
2021,9423874,Fig. 4.,(a) Dependence of the parameters to the beam lost rate and (b) structure of the main branch of the regression tree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9439211/9423874/zhang4ab-3075166-large.gif
2021,9423874,Fig. 5.,"(a) Dependence of the parameters to the alpha spread, (b) structure of the main branch of the alpha spread, and (c) alpha spread as the function of
X4
and
X13
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9439211/9423874/zhang5abc-3075166-large.gif
2021,9423874,Fig. 6.,"Prediction accuracies of the goal function by regression trees and neural networks with different tolerate errors
Δ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/16/9439211/9423874/zhang6-3075166-large.gif
2021,9551956,FIGURE 1.,1 round of a 4-branch GFS with 4-bit S-box.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh1-3116468-large.gif
2021,9551956,FIGURE 2.,Experiment 1 - Phase 1/Phase 2 flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh2-3116468-large.gif
2021,9551956,FIGURE 3.,Experiment 1 - Phase 3 flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh3-3116468-large.gif
2021,9551956,FIGURE 4.,Experiment 2 - Phase 1 flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh4-3116468-large.gif
2021,9551956,FIGURE 5.,Experiment 2 - Phase 2 flow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh5-3116468-large.gif
2021,9551956,FIGURE 6.,"ROC curve for 16-branch baseline experiment (AUROC
KNN
=0.989
, AUROC
DT
=0.969
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh6-3116468-large.gif
2021,9551956,FIGURE 7.,"ROC curve for TWINE (Round 1-8) experiment (AUROC
KNN
=0.818
, AUROC
DT
=0.659
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh7-3116468-large.gif
2021,9551956,FIGURE 8.,"ROC curve for TWINE (Round 9) experiment (AUROC
KNN
=0.934
, AUROC
DT
=0.781
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9551956/teh8-3116468-large.gif
2021,8950239,Fig. 1.,A 3D visualization of mTOR with Torin-2 (PDB id: 4JSX) inhibitor using Pymol software.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula1-2964203-large.gif
2021,8950239,Fig. 2.,A partial list of six known mTOR kinase inhibitors that are in clinical trial for cancer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula2-2964203-large.gif
2021,8950239,Fig. 3.,A visualization of the pipeline of our proposed approach for predicting mTOR inhibitors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula3-2964203-large.gif
2021,8950239,Fig. 4.,Misclassification error rate versus number of trees using default values of the mtry and ntree parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula4-2964203-large.gif
2021,8950239,Fig. 5.,Visualization of the OOB errors for different mtry values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula5-2964203-large.gif
2021,8950239,Fig. 6.,"Correlation plot of top-30 molecular descriptors ranked on the basis of (a) MDA, and (b) MDG values.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula6-2964203-large.gif
2021,8950239,Fig. 7.,"Performance evaluation plot of autoencoder using (a) MSE, (b) RMSE, (c) MAE, (d) SMAPE, (e)
R
2
, and number of nodes (#Nodes).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula7-2964203-large.gif
2021,8950239,Fig. 8.,(a) ROC and (b) PR curves with corresponding AUC values of NN classifier showing best performance over the validation dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9563006/8950239/abula8-2964203-large.gif
2021,9427525,FIGURE 1.,Nomadic relay node (NRN) cooperation in the downlink.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri1-3079171-large.gif
2021,9427525,FIGURE 2.,"The comparison between the proposed asymptotic ASER (21), and the exact ASER by inserting (9) into (20) for the AF-CNRS with BPSK modulation. We assume that \big ({E}{{{\big | {{h_{s,d}}} \big |}^{2}}} \big) = {E}\big ({{{\big | {{h_{s,r}}} \big |}^{2}}} \big) = {E}\big ({{{\big | {{h_{r,d}}} \big |}^{2}}} \big) = 1
and {P_{s}} = {P_{r}} ={1/2}
and for the channel with different values SCC and fading figures.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri2-3079171-large.gif
2021,9427525,FIGURE 3.,"The optimal distance ratio (
d
∗
=
d
∗
sr
/
d
sd
), versus power ratio (
r=
P
S
/P
) in the AF-CNRS over correlated Nakagami-
m
channel with
ν=2
and fading figure
m=1
(dashed line) and
m=2
(solid line) for the different
ρ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri3-3079171-large.gif
2021,9427525,FIGURE 4.,"The optimal power ratio (
r
∗
=
P
∗
S
/P
), versus distance ratio (
d=
d
sr
/
d
sd
) in the AF-CRS for Nakagami channel with PLE
ν=2
. The solid line for the fading parameter
m=1
and the dashed line for the
m=2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri4-3079171-large.gif
2021,9427525,FIGURE 5.,Block diagram of FNNs model used in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri5-3079171-large.gif
2021,9427525,FIGURE 6.,"Optimum relay location in AF-CRS versus
r=
P
s
/(
P
s
+
P
r
)
based on the ASER in the Nakagami channel with. For comparison, SER for a relay located at mid-point is also depicted. Solid line
ρ=0.9
and dashed line
ρ=0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri6-3079171-large.gif
2021,9427525,FIGURE 7.,"Average symbol error rate for various distance ratios,
d=
d
sr
/(
d
sr
+
d
rd
)
: Optimal power allocation (blue) and equal power allocation (red) (
v=3
,
m=1
). Solid line
ρ=0.9
and dashed line
ρ=0
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri7-3079171-large.gif
2021,9427525,FIGURE 8.,"The convergence behavior of the joint power-location optimization algorithm (Algorithm 2) for
ρ=0,0.5,0.9
, and
m=1,2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri8-3079171-large.gif
2021,9427525,FIGURE 9.,"The ASER versus total power in AF cooperative: (a)
r=0.5
,
d=0.5
(b)
r=0.5
,
d=
d
⋆
(c)
r=
r
⋆
,
d=0.5
(d)
r=
r
⋆
,
d=
d
⋆
. Assuming the
ν=3
,
m=0.5,1,2
, and
ρ=0,0.9
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri9abcd-3079171-large.gif
2021,9427525,FIGURE 10.,"The optimal power ratio (
r
∗
) and optimal distance ratio (
d
∗
) of the AF-CNRS in a correlated Nakagami channel with
m=1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9427525/amiri10-3079171-large.gif
2021,9381611,Fig. 1.,Proposed scheme for online probabilistic VSM estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/9460709/9381611/su1-3067150-large.gif
2021,9381611,Fig. 2.,Results of PI0.95 for VSM estimation using the proposed scheme (shown for 50 samples). (a) NREL-118. (b) TPS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/59/9460709/9381611/su2-3067150-large.gif
2021,9471848,FIGURE 1.,Flow chart of our training and testing scheme for machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9471848/chon1-3094175-large.gif
2021,9216518,Fig. 1:,"Several laminar and turbulent time slices of unsteady 2D vector fields from our public, numerically-simulated set of fluid flows. The bottom left number is the unique identifier of the flow in our data set. Here, color encodes vorticity and the dark lines correspond to the attracting hyperbolic Lagrangian coherent structures, estimated as ridges of the finite-time Lyapunov exponent.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-1-source-large.gif
2021,9216518,Fig. 2:,"Space-time visualization of a flow map interpolation from flow maps released from a
4×4
grid at time
t
0
in a 2D flow. For seed point
x
5
, the flow map is interpolated from grid vertices
x
1
,
x
2
, 
x
3
and
x
4
, resulting in estimate
ψ
^
τ
t
0
,S
(
x
5
)
(blue), which entails an interpolation error
E
τ
t
0
,S
(
x
5
)
(orange), compared to the ground truth
ϕ
τ
t
0
(
x
5
)
(red).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-2-source-large.gif
2021,9216518,Fig. 3:,"Wavelet noise-based construction of a random vector field u(x), following Kolmogorov's energy spectrum. Note that each band w(x) is periodic and uses a different random seed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-3-source-large.gif
2021,9216518,Fig. 4:,"Quadratic, cubic and quartic B-spline interpolation of the lowest band. Vorticity is color-coded showing discontinuities for lower-order interpolations that negatively affect the flow simulation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-4-source-large.gif
2021,9216518,Fig. 5:,"The ESPCN architecture consists of two convolutional and one deconvolutional layer. A low-resolution flow map goes in and a high resolution flow map is predicted. The convolutional layers have filter size
f
1
and
f
2
, and compute
n
1
and
n
2
feature maps, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-5-source-large.gif
2021,9216518,Fig. 6:,"Varying the number of features
n
1
and
n
2
in the two convolutional layers of ESPCN for a fixed filter size of
f
1
=
f
2
=3
. Growing the feature size further does not pay off in terms of the remaining error residual compared to the increasing training time (in brackets).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-6-source-large.gif
2021,9216518,Fig. 7:,"Study of the filter sizes
f
1
and
f
2
of the two convolutional layers of ESPCN for a fixed number of features
n
1
=
n
2
=64
per layer. The filter size has a marginal effect on the residual, but affects the training time (in brackets) by a few hours.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-7-source-large.gif
2021,9216518,Fig. 8:,"Hyperparameter adjustment for SRCNN at an upsampling factor of
k=2
. We examine the performance of the stochastic gradient descent and the Adam optimizer, and we explore the choice of the loss function (MSE vs MAE). MSE with Adam optimizer performed best. The required training time is stated in brackets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-8-source-large.gif
2021,9216518,Fig. 9:,Quantitative comparison of SRCNN and ESPCN for varying integration durations at 2x upsampling. ESPCN performed better by a small margin. The required training time is in brackets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-9-source-large.gif
2021,9216518,Fig. 10:,"Error maps and FTLE comparisons for different fluid flows. In all cases, the flow map was traced from to = 0 up to
T
. The top row of each data set contains the overview of the 2x upsampling and the quantitative error measures. The close-ups compare the 2x and 4x upsampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-10-source-large.gif
2021,9216518,Fig. 11:,"Error plots of real-world data for varying integration durations with up-scaling factor 2x and 4x. Compared to the cubic baseline at 2x upsampling, SRCNN reduced the error residuals by 37.8% (FLOW #2000), 32.7% (BOUSSINESQ) and 39.4% (CYLINDER), whereas ESCPN reduced the error residuals by 47.9% (FLOW #2000), 39.4% (BOUSSINESQ) and 44.9% (CYLINDER) at
τ=10
. For 4x upsampling, SRCNN reduced the error residuals by 28.6% (FLOW #2000), 12.2% (BOUSSINESQ) and 30.9% (CYLINDER), whereas ESCPN reduced the error residuals by 42.9% (FLOW #2000), 35.6% (BOUSSINESQ) and 39.4% (CYLINDER) at
τ=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-11-source-large.gif
2021,9216518,Fig. 12:,"Error maps and FTLE comparisons for different fluid flows. In all cases, the flow map was traced from
t
0
=0
up to
τ
. The top row of each data set contains the overview of the 2x upsampling and the quantitative error measures. The close-ups compare the 2x and 4x upsampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-12-source-large.gif
2021,9216518,Fig. 13:,"Error plots of real-world data for varying integration durations with up-scaling factor 2x and 4x. Compared to the cubic baseline, SRCNN reduced the error residuals by 44.8% (Guadalupe), 30.6% (Ocean) and 42.4% (Double Cylinder), whereas ESCPN reduced the error residuals by 43.5% (Guadalupe), 31.7% (Ocean) and 47.2% (Double Cylinder) at
τ=10
. For 4x upsampling, SRCNN reduced the error residuals by 36.3% (Flow #2000), 29.6% (Boussinesq) and 30.9% (Cylinder), whereas ESCPN reduced the error residuals by 44.0% (Flow #2000), 44.4% (Boussinesq) and 47.6% (Cylinder) at
τ=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-13-source-large.gif
2021,9216518,Fig. 14:,"Comparison of × 4 up-sampling with adaptive flow map sampling using Sibson's Cl continuous interpolation [4] with 128 x 128 samples. In this example we show the FTLE of close-ups of the flow #2000 for
τ=3
. Networks with regular grid inputs are less efficient especially in regions without FTLE ridges.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/2945/9340023/9216518/27tvcg02-jakob-3028947-x-fig-14-source-large.gif
2021,9096542,Fig. 1.,"The overall architecture of the applied machine learning cycle. During model development, we train each model 10 times, each time the development sample (n = 95,158) is partitioned into ten approximately equally sized sub-samples (n = 9,515).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake1-2995836-large.gif
2021,9096542,Fig. 2.,"An overview of the applied hierarchical, stacked ensemble, and CCE models used to predict discharge destination.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake2-2995836-large.gif
2021,9096542,Fig. 3.,"(a) Comparison between the discrimination ability of APACHE IV score and (b) APACHE IV variables using XGB classifier. The macro-average of a metric weighs all classes equally, whereas the micro-average weighs classes based on their contribution (i.e. the number of instances) to the overall performance. Given the highly imbalanced nature of eICU-CRD, throughout this paper, we use the micro-average to report the average performance of the evaluation metrics.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake3-2995836-large.gif
2021,9096542,Fig. 4.,"Comparison of classification performance among the top-three prediction models. Figure (e) combines figures (a) and (b) and intuitively compares the smallest convex set that encloses all pairs of (precision, recall) for the top-three classifiers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake4-2995836-large.gif
2021,9096542,Fig. 5.,Performance of XGBoost on the held-out dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake5-2995836-large.gif
2021,9096542,Fig. 6.,"Comparison of classification performance using different models and imputation techniques, as measured by the area under the receiver operating characteristic curve (AUC) and Index of Balanced Accuracy (IBA).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake6-2995836-large.gif
2021,9096542,Fig. 7.,The distribution of length of stay for different discharge destinations. The vertical white line shows the median of length of stay for each category.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9371311/9096542/shake7-2995836-large.gif
2021,9352033,Fig. 1.,Classification of the federated learning papers based on the high-level challenge that they address.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb1-3058573-large.gif
2021,9352033,Fig. 2.,Percentage breakdown of the federated learning literature: The approaches that address statistical and communication efficiency challenges account for 50% of the existing literature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb2-3058573-large.gif
2021,9352033,Fig. 3.,Percentage breakdown of the federated learning literature based on the publication type: Most of the federated learning papers are published as preprints.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb3-3058573-large.gif
2021,9352033,Fig. 4.,"Federated Learning: The parameter server shares a global machine learning model with the client devices, which use the parameters of the global model to locally train a model on their local data and then share the model updates with the server.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb4-3058573-large.gif
2021,9352033,Fig. 5.,"The main difference between distributed learning and federated learning is that in the former concept workers are not supposed to receive any global model from the server at each iteration, while in the latter workers receive a new version of the global model and derive updated parameters. The main difference between ensemble learning and federated learning is that the objective of the former is learn from a mixture of models to enrich the learning process, while in the latter the objective is to improve one single global model using naturally distributed data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb5ab-3058573-large.gif
2021,9352033,Fig. 6.,"The main difference between MapReduce and federated learning is that workers in the former concept are cluster machines that are deployed by the task owner, while in the latter workers are automatous and non-technical. The main difference between federated learning and shared machine learning is that the former only solves problems with in-domain data while the latter relaxes this restriction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb6ab-3058573-large.gif
2021,9352033,Fig. 7.,Classification of the statistical approaches in federated learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb7-3058573-large.gif
2021,9352033,Fig. 8.,Active Learning: The training algorithm can actively query an oracle to learn the correct prediction for a given problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb8-3058573-large.gif
2021,9352033,Fig. 9.,Multi-Task Learning: Several related tasks are trained together and the representations are sharing among them.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb9ab-3058573-large.gif
2021,9352033,Fig. 10.,Transfer Learning: The machine learning model capitalizes on some patterns that were already learned through solving a previous problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb10-3058573-large.gif
2021,9352033,Fig. 11.,Knowledge Distillation: A large complex deep network is first trained and then a small network is trained with the help of that large model to replicate its results on resource-constrained devices.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb11-3058573-large.gif
2021,9352033,Fig. 12.,Classification of the communication efficiency approaches in federated learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb12-3058573-large.gif
2021,9352033,Fig. 13.,Classification of the client selection and scheduling approaches in federated learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb13-3058573-large.gif
2021,9352033,Fig. 14.,Classification of the security-oriented approaches in federated learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb14-3058573-large.gif
2021,9352033,Fig. 15.,Classification of the privacy-oriented approaches in federated learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb15-3058573-large.gif
2021,9352033,Fig. 16.,Classification of the service pricing approaches in federated learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9739/9438976/9352033/taleb16-3058573-large.gif
2021,9319401,Fig. 1.,Communication structure between a central learner and multiple data owners with private datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok1-3050603-large.gif
2021,9319401,Fig. 2.,"Percentile statistics of relative fitness of 100 runs of Algorithm 1 for learning lending-interest-rates versus the iteration number
k
for a learning horizon of
T=1,000
iterations with three choices of privacy budgets
ϵ
1
=
ϵ
2
=
ϵ
3
. The gray area illustrates the range of 25% to 75% percentiles and the black line shows the median of relative fitness.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok2-3050603-large.gif
2021,9319401,Fig. 3.,"Example of communication timing for the asynchronous learning in Algorithm 1 for learning lending-interest-rates, illustrating
i
k
versus the iteration number
k
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok3-3050603-large.gif
2021,9319401,Fig. 4.,"Relative fitness of Algorithm 1 for learning lending-interest-rates after
T=1,000
iterations versus the size of the datasets
n
1
=
n
2
=
n
3
and the privacy budgets
ϵ
1
=
ϵ
2
=
ϵ
3
. The mesh surface illustrates the bound in (11).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok4-3050603-large.gif
2021,9319401,Fig. 5.,"Relative fitness of Algorithm 1 for learning lending-interest-rates after
T=1,000
iterations versus the privacy budget [top] and the size of the datasets [bottom]. The solid line illustrates the bound in (11).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok5-3050603-large.gif
2021,9319401,Fig. 6.,"Relative fitness of Algorithm 1 for learning lending-interest-rates after
T=1,000
iterations,
E{ψ(
θ
L,T
)}
, versus the privacy budgets
ϵ
i
,
∀i
, and the number of collaborating data owners
N
. The solid gray surface shows the relative fitness of the non-private ML model
θ
∗
1
,
ψ(
θ
∗
1
)
, constructed based on only the private data of the first data owner. If the relative fitness of Algorithm 1 is smaller than the relative fitness of the non-private ML model
θ
∗
1
, collaboration benefits the first data owner (illustrated by the black region at the bottom of the figure).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok6-3050603-large.gif
2021,9319401,Fig. 7.,"Percentile statistics of relative fitness of 100 runs of Algorithm 1 for learning length of stay at hospital versus the iteration number
k
for a learning horizon of
T=1,000
iterations with three choices of privacy budgets
ϵ
1
=
ϵ
2
=
ϵ
3
. The gray area illustrates the range of 25% to 75% percentiles for the relative fitness and the black line shows the median of relative fitness.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok7-3050603-large.gif
2021,9319401,Fig. 8.,"Example of communication timing for the asynchronous learning in Algorithm 1 for learning length of stay at hospital, illustrating
i
k
versus the iteration number
k
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok8-3050603-large.gif
2021,9319401,Fig. 9.,"Relative fitness of Algorithm 1 for learning length of stay at hospital after
T=1,000
iterations versus the privacy budget
ϵ
i
,
∀i
. The solid line illustrates the bound in (11).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok9-3050603-large.gif
2021,9319401,Fig. 10.,"Relative fitness of Algorithm 1 for learning length of stay at hospital after
T=1,000
iterations,
E{ψ(
θ
L,T
)}
, for three choices of privacy budgets
ϵ
i
=0.1
(black line),
ϵ
i
=1
(dashed line),
ϵ
i
=10
(dash-dotted line). The markers show the relative fitness of the non-private ML model
θ
∗
i
,
ψ(
θ
∗
i
)
, constructed based on only the private data of the
i
-th data owner versus the size of the data set owned by the
i
-th data owner. For
ϵ=10
, eight hospitals benefit from collaboration. The relative fitness of the non-private ML model
θ
∗
i
for these eight hospitals are above the dash-dotted line.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok10-3050603-large.gif
2021,9319401,Fig. 11.,"Relative fitness of Algorithm 1 for detecting credit card fraud after
T=100
iterations versus the privacy budget [top] and the size of the datasets [bottom]. The solid line illustrates the bound in (11).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9319401/farok11ab-3050603-large.gif
2021,9237955,Fig. 1.,"Performance of the matched versus mismatched attacker in terms of average
P
MD
, when Bob applies the a) LLR test and b) combined test, with
α
(I)
=1
,
ρ
AE
=
ρ
EB
=0.1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig1ab-3033454-large.gif
2021,9237955,Fig. 2.,"Training times required for different NN algorithms with
α
(I)
=0.8
,
α
(II)
=0.9
, SNR(I) = 15dB, SNR(II) = 20dB and
M=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig2-3033454-large.gif
2021,9237955,Fig. 3.,"Probability of MD as a function of the spatial correlation
ρ
AE
, comparing NN and SVM algorithms, with
α
(I)
=1
,
α
(II)
=0.9
,
N=3
, SNR(I) = 15dB, SNR(II) = 20dB and
M=100
(training set dimension).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig3-3033454-large.gif
2021,9237955,Fig. 4.,"a) Probabilities of FA and MD and b)
g
MEAN
and
ACC
as function of the number of sub-carriers
N
, comparing 11NN and SVM algorithms, with
α
(I)
=1
and
α
(II)
=0.9
,
ρ
AE
=0.8
, SNR(I) = 15dB, SNR(II) = 20dB and
M=300
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig4ab-3033454-large.gif
2021,9237955,Fig. 5.,"Average probability of MD versus
ρ
AE
using clustering with respect to known instance labels, with
N=6
,
M=200
,
α
(I)
=
α
(II)
=1
, SNR(I) = 15dB, SNR(II) = 20dB, classification performed using a SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig5-3033454-large.gif
2021,9237955,Fig. 6.,"Probabilities of FA and MD as function of the number of sub-carriers
N
, 11NN, with
α
(II)
=0.9
,
ρ
AE
=0.8
, SNR(I) = 15dB, SNR(II) = 20dB and
M=100
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig6-3033454-large.gif
2021,9237955,Fig. 7.,"Probability of MD as function of
α
(I)
, LLR test, with different values of
α
(II)
,
ρ
AE
=0.1
,
P
FA
=
10
−1
, SNR(I) = 15dB, SNR(II) = 20dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig7-3033454-large.gif
2021,9237955,Fig. 8.,"Average MD probability
E[
P
MD
]
versus number of sub-carriers
N
, comparing statistical-based test methods, for different values of
α
(II)
,
α
(I)
=1
,
ρ
AE
=0.1
, with SNR(I) = 15dB and SNR(II) = 20dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig8-3033454-large.gif
2021,9237955,Fig. 9.,"Average probability of MD as function of the number of sub-carriers
N
, with
α
(I)
=
α
(II)
=1
,
ρ
AE
=0.8
, SNR(I) = 15dB, SNR(II) = 20dB and
M=100
, considering
P
FA
(SVM)=[1.39⋅
10
-3
,2.12⋅
10
-4
,6.85⋅
10
-5
,1.68⋅
10
-5
,5.55⋅
10
-6
,1.62⋅
10
-6
]
,
P
FA
(NN)=[6.76⋅
10
-4
,4.21⋅
10
-4
,<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
]
and
P
FA
(LLR)=
10
−4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig9-3033454-large.gif
2021,9237955,Fig. 10.,"Average probability of MD using LLR test and SVM algorithm, with
N=3
,
ρ
AE
=0.5
,
M=100
, SNR(II) = 20dB,
α
(I)
=
α
(II)
=1
.
P
FA
=[<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
,<
10
-6
,4.92⋅
10
−6
,7.73⋅
10
−5
,7.11⋅
10
−
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig10-3033454-large.gif
2021,9237955,Fig. 11.,"Average probability of MD using LLR test, J1NN and SVM algorithm, with
N=3
,
M=100
, SNR(I) = 15dB, SNR(II) = 20dB,
α
(I)
=1
,
α
(II)
=0.9
.
P
FA
=0.94
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig11-3033454-large.gif
2021,9237955,Fig. 12.,"a) Probabilities of FA and MD obtained by using a hybrid approach and a 11NN with Euclidean metric, and b) Probability of MD achieved by LLR test and hybrid approach, with
M=100
,
α
(I)
=1
,
α
(II)
=0.9
,
ρ
AE
=0.1
, SNR(I) = 15dB, SNR(II) = 20dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9151439/9237955/senig12ab-3033454-large.gif
2021,9171485,Fig. 1.,"Overview of the HILL system. The system is composed of three major parts including prediction part (linked by black arrows), human-in-the-loop part (linked by orange arrows) and the RL part (linked by purple arrows). In the feed-forward direction, the HILL system is easily used for identifying OOD data. In the back-forward manner, the whole system is not end-to-end differentiable. A RL algorithm is designed in the purple block to optimize it via policy gradient.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9475542/9171485/deng1-3011559-large.gif
2021,9171485,Fig. 2.,"Corresponding rewards for the out-of-sample detection process of REINFORCE training for HILL. Green (red) dot means the prediction is correct (respectively, wrong).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9475542/9171485/deng2-3011559-large.gif
2021,9316190,FIGURE 1.,Transition graph of a two-action Tsetlin Automaton.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr1-3049569-large.gif
2021,9316190,FIGURE 2.,The Integer Weighted Tsetlin Machine structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr2-3049569-large.gif
2021,9316190,FIGURE 3.,The complete learning process of the IWTM in a flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr3-3049569-large.gif
2021,9316190,FIGURE 4.,TM classification process for the Bankruptcy dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr4-3049569-large.gif
2021,9316190,FIGURE 5.,The number of literals included in different TM setups to work with Bankruptcy dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr5-3049569-large.gif
2021,9316190,FIGURE 6.,Sample complexity analysis for the Bankruptcy dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr6-3049569-large.gif
2021,9316190,FIGURE 7.,The number of literals included in different TM setups to work with Balance Scale dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr7-3049569-large.gif
2021,9316190,FIGURE 8.,Sample complexity analysis for the Balance Scale dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr8-3049569-large.gif
2021,9316190,FIGURE 9.,The number of literals included in different TM setups to work with Breast Cancer dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr9-3049569-large.gif
2021,9316190,FIGURE 10.,Sample complexity analysis for the Breast Cancer dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr10-3049569-large.gif
2021,9316190,FIGURE 11.,The number of literals included in different TM setups to work with the Liver Disorders dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr11-3049569-large.gif
2021,9316190,FIGURE 12.,Sample complexity analysis for the Liver Disorders dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr12-3049569-large.gif
2021,9316190,FIGURE 13.,The number of literals included in different TM setups to work with Heart Disease dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr13-3049569-large.gif
2021,9316190,FIGURE 14.,Sample complexity analysis for the Heart Disease dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9316190/abeyr14-3049569-large.gif
2021,9261088,Fig. 1.,Ordinal variable binarization under multiple scenarios. The best scenario is determined by solving a mixed-integer programming program.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar1-3038194-large.gif
2021,9261088,Fig. 2.,Overview of the experimental design.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar2-3038194-large.gif
2021,9261088,Fig. 3.,"Cross-validation AUC score as a function of
log(λ)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar3-3038194-large.gif
2021,9261088,Fig. 4.,"The fusion effects of the smoothing parameter on the discretized variables, using the “age” variable as an example. As the smoothing effect increases, the number of distinct coefficients of all age groups deceases from 5 levels to 4 levels, and finally reaches 3 levels. The adjacent age levels with same coefficients can be merged for a reduced model size.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar4-3038194-large.gif
2021,9261088,Fig. 5.,"Model size comparison between the MIP model and other generalized linear models. The average AUC scores for Elastic Net, Lasso and the MIP are 0.862, 0.864 and 0.863 respectively. The MIP model utilizes much less nonzero coefficients to achieve the same AUC performance as the other two methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar5-3038194-large.gif
2021,9261088,Fig. 6.,Optimal binarization results for two ordinal variables - Room Cleanliness and Room Quietness.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar6-3038194-large.gif
2021,9261088,Fig. 7.,Results of continuous variable discretization and coefficient learning for a numerical variable - Previous Hospital Visit. The number of previous hospital visit is measured by the previous admission count.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar7-3038194-large.gif
2021,9261088,Fig. 8.,"Satisfaction diagnosis for an individual unsatisfying patient. As an illustration example, top 10 feature are selected and ranked from highest to lowest based on their importance. The converted binary values represent patients’ perceptions of services and the corresponding survey responses. The feature contributions are evaluated using feature importance and values. The low level (i.e., value = 0) in top features are the potential root causes of patient dissatisfaction and should be considered as the primary focus of care quality improvements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9446651/9261088/kumar8-3038194-large.gif
2021,9446142,FIGURE 1.,Keratoconus versus healthy cornea.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri1-3086021-large.gif
2021,9446142,FIGURE 2.,Proposed machine learning approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri2-3086021-large.gif
2021,9446142,FIGURE 3.,Features ranking using a machine learning approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri3-3086021-large.gif
2021,9446142,FIGURE 4.,Scatter plot of anterior cornea curvature radius versus mean radius of cornea curvature of normal eyes and eyes with keratoconus.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri4-3086021-large.gif
2021,9446142,FIGURE 5.,ROC curve (Left) and the confusion matrix (Right) of the SVM machine learning applied on elevation parameters only.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri5ab-3086021-large.gif
2021,9446142,FIGURE 6.,"AUC curves for 3 classes (healthy eyes, keratoconus fruste and keratoconus) evaluated for elevation, pachymetry and topography datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri6-3086021-large.gif
2021,9446142,FIGURE 7.,"AUC curves for 5 classes (Healthy Eyes, fruste keratoconus stage I, keratoconus stage II, keratoconus stage III and keratoconus stage IV) including keratoconus severity level detection evaluated for elevation, pachymetry and topography datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri7-3086021-large.gif
2021,9446142,FIGURE 8.,AUC metrics for the selected corneal parameters determined using the developed hybrid method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri8-3086021-large.gif
2021,9446142,FIGURE 9.,Scatter plot for two of the selected corneal parameters (Minimum curvature radius of the cornea versus the eccentricity parameter of the cornea).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9446142/lavri9-3086021-large.gif
2021,9597532,FIGURE 1.,Few key categories of URL based features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga1-3124628-large.gif
2021,9597532,FIGURE 2.,Methodology proposed for building various machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga2-3124628-large.gif
2021,9597532,FIGURE 3.,URL Structure which is used for extracting features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga3-3124628-large.gif
2021,9597532,FIGURE 4.,Analysis of normal distribution of URL domain age using distplot visualization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga4-3124628-large.gif
2021,9597532,FIGURE 5.,Analysis of normal distribution of URL domain age using boxplot visualization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga5-3124628-large.gif
2021,9597532,FIGURE 6.,Part of the plot of the comparison of variant combinations between every feature using pair plot visualization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga6-3124628-large.gif
2021,9597532,Algorithm 1,Adaption of Best Features,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga22-3124628-large.gif
2021,9597532,FIGURE 8.,Proposed methodology using stacking classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga8-3124628-large.gif
2021,9597532,FIGURE 9.,Proposed architecture using voting classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga9-3124628-large.gif
2021,9597532,FIGURE 10.,Process of a middle layer of proposed architecture using voting classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga10-3124628-large.gif
2021,9597532,FIGURE 11.,ERG-SVC architecture elaboration using 2 classifier pairs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga11-3124628-large.gif
2021,9597532,FIGURE 12.,Comparison of accuracies for each classifier before and after HyperParameter Tuning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga12-3124628-large.gif
2021,9597532,FIGURE 13.,Proposed lightweight pre-processor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga13-3124628-large.gif
2021,9597532,FIGURE 14.,Observation of the accuracy value of Gradient Boost model change when “Loss Function” = “deviance” and “Criterion” with all options.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga14-3124628-large.gif
2021,9597532,FIGURE 15.,Observation of the accuracy value of Gradient Boost model change when Loss Function = “exponential” and “Criterion” with all options.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga15-3124628-large.gif
2021,9597532,FIGURE 16.,Observation of the ROC_AUC value of Gradient Boost model change when loss function = “deviance” and “Criterion” with all options.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga16-3124628-large.gif
2021,9597532,FIGURE 17.,Observation of the ROC_AUC value of Gradient Boost model change when loss function = “exponential” and “Criterion” with all options.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga17-3124628-large.gif
2021,9597532,FIGURE 18.,Elbow analysis using 46 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga18-3124628-large.gif
2021,9597532,FIGURE 19.,Elbow analysis using 22 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga19-3124628-large.gif
2021,9597532,FIGURE 20.,Silhouette analysis with 46 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga20-3124628-large.gif
2021,9597532,FIGURE 21.,Silhouette analysis with 22 features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9597532/halga21-3124628-large.gif
2021,9409628,Fig. 1.,mFIMMG dataset preprocessing: labeled and unlabeled samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9409628/berna1-3074206-large.gif
2021,9409628,Fig. 2.,Three different approaches. a) No-temporal: the temporal information is averaged across all time-windows; b) Stacked-temporal: the temporal information is preserved by concatenating longitudinally all the time windows; and c) Multitask-temporal: each time-window is treated as a separate task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9409628/berna2-3074206-large.gif
2021,9409628,Fig. 3.,SS-MTL: training experimental procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9409628/berna3-3074206-large.gif
2021,9409628,Fig. 4.,"MTL and SS-MTL approaches: Recall trend over the fraction of labeled training samples
x,y∈
Z
l
. In the legend, stars indicate that gender and age were included as predictors (Overall*), filled circles were not (Overall).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9409628/berna4-3074206-large.gif
2021,9409628,Fig. 5.,"Pseudolabel samples
x
˜
′
,
y
˜
′
∈
Z
u
˜
selected from SLA procedure (after random downsampling) over fraction of labeled training samples
x∈
Z
l
. In the legend, stars indicate that gender and age were included as predictors (Overall*), filled circles were not (Overall).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9559879/9409628/berna5-3074206-large.gif
2021,9627668,FIGURE 1.,The power and control circuits employed for the grid-connected PV system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr1-3130889-large.gif
2021,9627668,FIGURE 2.,"A typical grid-connected PV system consists of a PV array (
4×4
) with various types of PV array faults (permanent and temporary) followed by a two-stage converter to the utility grid.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr2-3130889-large.gif
2021,9627668,FIGURE 3.,The conventional protection systems in the PV array scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr3-3130889-large.gif
2021,9627668,FIGURE 4.,I-V characteristics of PV array under LL fault with different fault impedance values at STC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr4-3130889-large.gif
2021,9627668,FIGURE 5.,I-V characteristics of PV array under LL and OC faults using blocking diodes at STC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr5-3130889-large.gif
2021,9627668,FIGURE 6.,I-V characteristics of PV array under LL fault across one PV module (F4) at STC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr6-3130889-large.gif
2021,9627668,FIGURE 7.,I-V characteristics of PV array under LL fault across two PV modules (F5) at STC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr7-3130889-large.gif
2021,9627668,FIGURE 8.,PV array characteristics under 20% PS with/without bypass diodes at STC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr8-3130889-large.gif
2021,9627668,FIGURE 9.,The flow diagram of the proposed FDD algorithm framework for the PV array faults.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr9-3130889-large.gif
2021,9627668,FIGURE 10.,The workflow to obtain optimal supervised ML models for detection and diagnosis modules.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr10-3130889-large.gif
2021,9627668,FIGURE 11.,Overlapping the MPPs of a PV array at different faults cases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr11-3130889-large.gif
2021,9627668,FIGURE 12.,The utilized input quantities before applying standardization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr12-3130889-large.gif
2021,9627668,FIGURE 13.,The utilized input quantities after applying standardization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr13-3130889-large.gif
2021,9627668,FIGURE 14.,Workflow for hold-out and \$k\$ -fold cross-validation method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr14-3130889-large.gif
2021,9627668,FIGURE 15.,"The ML classifiers recruited for the FDD algorithm framework, namely, DT, KNN, and SVM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr15-3130889-large.gif
2021,9627668,FIGURE 16.,"Minimum Cross-Validation (CV) Error of the DT, KNN, and SVM models based on different splitting criteria, distance metrics, and Kernel functions, respectively during the Bayesian Optimization tuning process.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr16-3130889-large.gif
2021,9627668,FIGURE 17.,The labels recall accuracy at independent test datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr17-3130889-large.gif
2021,9627668,FIGURE 18.,The labels precision accuracy at independent test datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr18-3130889-large.gif
2021,9627668,FIGURE 19.,"The \$CM\$ for the worst diagnostic model, KNN ( \$\mathcal {M}_{14}\$ ).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr19-3130889-large.gif
2021,9627668,FIGURE 20.,"The \$CM\$ for the optimal diagnostic model, SVM ( \$\mathcal {M}_{22}\$ ).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr20-3130889-large.gif
2021,9627668,FIGURE 21.,"Experimental setup: 1) PV array, 2) reference PV module, 3) and 4) weather station transmitter and receiver appliances, respectively (for reference only), 5) data station, 6) fault impedance, 7) PV combiner and sensing circuit, 8) microcontroller board, 9) drive circuit, 10) power circuit, 11) boost converter, 12) DC-electronic load.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9627668/badr21-3130889-large.gif
2021,9388656,FIGURE 1.,Schematic diagram of the proposed LVDD prediction system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9388656/lin1-3069232-large.gif
2021,9388656,FIGURE 2.,Receiver operating characteristic curves and precision recall curves.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9388656/lin2-3069232-large.gif
2021,9388656,FIGURE 3.,Feature importance of random forest for the 26 ECG parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9388656/lin3-3069232-large.gif
2021,9178457,Fig. 1.,"Two unique procedures used to speed up ALDR+. (a) Batch decremental learning for S3VM: remove the chosen samples from the set
U
. (b) Batch incremental learning for S3VM: add the queried samples into the set
Q
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9525619/9178457/gu1ab-3016928-large.gif
2021,9178457,Fig. 2.,Illustrative example for querying discriminative and representative samples by our approach. (a) Binary classification. (b) Initial labeled samples. (c) After querying four samples. (d) After querying eight samples.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9525619/9178457/gu2abcd-3016928-large.gif
2021,9178457,Fig. 3.,Comparison of different active learning methods on benchmark data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9525619/9178457/gu3-3016928-large.gif
2021,9178457,Fig. 4.,Comparison of different active learning methods on real-world data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9525619/9178457/gu4-3016928-large.gif
2021,9178457,Fig. 5.,Performance comparison using different tradeoff parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9525619/9178457/gu5-3016928-large.gif
2021,9483945,Fig. 1.,"Polarization-imaging-based ML framework for quantitative diagnosis of cervical precancerous lesions. The schematic illustrates a fusion method of dual-modality information contained in the tissue sample’s Mueller matrix and H&E image, and (a)-(g) outlines the steps from the input of Mueller matrix image and H&E image of the cervical tissue sample to the output of PFP for obtaining CIN diagnostic indicators.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9629464/9483945/ma1abcdefg-3097200-large.gif
2021,9483945,Fig. 2.,"Cervical pathological samples and experimental setup. (a) Example H&E images of different cervical pathological tissues - Normal, CIN1, CIN2, and CIN3. (b) Photograph and schematic of the Mueller matrix microscope. P: polarizer. R: quarter-wave plate. (c) An example of Mueller matrix of the pathological section of CIN2. The element m11 represents the intensity image, and the other 15 elements are all normalized by m11. We subtract the identity matrix from the Mueller matrix for display using the color bar ranging from −0.05 to 0.05.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9629464/9483945/ma2abc-3097200-large.gif
2021,9483945,Fig. 3.,"Architecture of U-net for epithelial segmentation in cervical H&E images. Blue boxes correspond to multi-channel feature maps, and orange boxes represent copied feature maps. It consists of a contracting path and an expansive path (left side and right side respectively). This model has 3 levels in depth, and the filter sizes for the convolution layers in the 3 levels are 64, 128, and 256 respectively. The x-y size of the image is displayed at the lower left edge of the boxes. The arrows denote the different operations. A segmentation example in H&E image of a CIN2 pathological sample is presented.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9629464/9483945/ma3-3097200-large.gif
2021,9483945,Fig. 4.,"Statistical distance-based ML classifier. (a) Schematic of the statistical distance-based ML classifier for deriving a PFP for the diagnosis of cervical precancerous lesions. (b) Linear combination of the PBPs (represented by solid circles in different colors and shapes) is output as the PFP (represented by solid circle in black). The numbers above PBPs are the optimized coefficients obtained from the trained classifier, and the PFP can be acquired by a weighted sum of the PBPs with optimized coefficients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9629464/9483945/ma4ab-3097200-large.gif
2021,9483945,Fig. 5.,"Qualitative results of the polarization-imaging-based ML model. S1, S2 represent two samples taken from cervical precancerous tissue at each pathological stage - Normal, CIN1, CIN2, and CIN3. (a) Parts of H&E images of epithelial region of the whole cervical precancerous slides at different pathological stages. (b) Epithelium segmentation maps in the corresponding H&E images. (c) 2D images of PFP of the corresponding epithelial regions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9629464/9483945/ma5abc-3097200-large.gif
2021,9483945,Fig. 6.,"Statistical analysis of the derived PFP in the test samples. (a) PFP’s probability distribution of different cervical tissue samples - Normal (red line), CIN1 (blue line), CIN2 (green line), and CIN3 (orange line). The areas under the probability curves are normalized to 1, and the horizontal axis is divided into 50000 parts. (b) PFP’s three typical statistical features (mean, mode, and skewness) in the test samples - Normal (red triangle), CIN1 (blue circle), CIN2 (green square), and CIN3 (orange cross).
N
is the line of best fit obtained using PCA’s first component, which can be used for describing the features’ variations in the three-dimensional coordinate system. (c) Projections on
N
of three typical statistical features of cervical tissue samples - Normal (red solid circle), CIN1 (blue solid circle), CIN2 (green solid circle), and CIN3 (orange solid circle). P is the P-value between the two sets of experimental data in two adjacent pathological stages, which is obtained by the t-test (P < 0.05 is significant).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/9629464/9483945/ma6abc-3097200-large.gif
2021,9345704,FIGURE 1.,The ML-AIDS models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta1-3056614-large.gif
2021,9345704,FIGURE 2.,The ANN architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta2-3056614-large.gif
2021,9345704,FIGURE 3.,The DT architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta3-3056614-large.gif
2021,9345704,FIGURE 4.,The RF architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta4-3056614-large.gif
2021,9345704,FIGURE 5.,The CNN architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta5-3056614-large.gif
2021,9345704,FIGURE 6.,The SOM architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta6-3056614-large.gif
2021,9345704,FIGURE 7.,The usage of popular datasets in the literature.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta7-3056614-large.gif
2021,9345704,FIGURE 8.,The benchmarking methodology of ML-AIDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta8-3056614-large.gif
2021,9345704,FIGURE 9.,The overall accuracy and runtime performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta9-3056614-large.gif
2021,9345704,FIGURE 10.,The overall performance of the algorithms based on different types of attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta10-3056614-large.gif
2021,9345704,FIGURE 11.,Evaluation of the ML-AIDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9312710/9345704/mosta11-3056614-large.gif
2021,8967128,Fig. 1.,Protocols with possible attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar1-2968927-large.gif
2021,8967128,Fig. 2.,Approach followed in the proposed scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar2-2968927-large.gif
2021,8967128,Fig. 3.,Boosted linear model phases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar3-2968927-large.gif
2021,8967128,Fig. 4.,Standard deviations of PCs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar4-2968927-large.gif
2021,8967128,Fig. 5.,Spam score distribution by Bagged Model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar5-2968927-large.gif
2021,8967128,Fig. 6.,Spam score distribution by BGLM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar6-2968927-large.gif
2021,8967128,Fig. 7.,Spam score distribution by boosted linear model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar7-2968927-large.gif
2021,8967128,Fig. 8.,Spam score distribution by eXtreme gradient boosting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar8-2968927-large.gif
2021,8967128,Fig. 9.,Spam score distribution by GLM with stepwise feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar9-2968927-large.gif
2021,8967128,Fig. 10.,Transformations of PCs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar10-2968927-large.gif
2021,8967128,Fig. 11.,Features of smart home data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9424/9264783/8967128/kumar11-2968927-large.gif
2021,9110776,Fig. 1.,Comparison of the linearized proximal SVRG-ADMM and our ASVRG-ADMM algorithms for both SC and non-SC problems on the two data sets: a9a (top) and epsilon (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9600479/9110776/shang1-3000512-large.gif
2021,9110776,Fig. 2.,"Comparison of different stochastic ADMM methods for non-SC graph-guided fused Lasso problems on the four data sets. The
y
-axis represents the objective value minus the minimum value (top) or test loss (bottom), and the
x
-axis corresponds to the running time (seconds).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9600479/9110776/shang2-3000512-large.gif
2021,9110776,Fig. 3.,Comparison of the stochastic ADMM methods for SC graph-guided logistic regression problems on a9a (top) and w8a (bottom).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9600479/9110776/shang3-3000512-large.gif
2021,9110776,Fig. 4.,Accuracy comparison of multi-class classification on 20newsgroups: accuracy versus running time (left) or number of epochs (right).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9600479/9110776/shang4-3000512-large.gif
2021,9110776,Fig. 5.,"Comparison of all the methods for generalized graph-guided fused Lasso on a9a, where regularization parameters
λ
1
=
λ
2
=
10
−5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9600479/9110776/shang5-3000512-large.gif
2021,9110776,Fig. 6.,"Comparison of all the methods for multi-task learning problems on 20newsgroups, where the regularization parameter
λ
1
=
10
−4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9600479/9110776/shang6-3000512-large.gif
2022,9903420,FIGURE 1.,"Segmentation, classification & Object detection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil1-3209825-large.gif
2022,9903420,FIGURE 2.,2015-2022 year-wise publication count of learning styles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil2-3209825-large.gif
2022,9903420,FIGURE 3.,Outline of paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil3-3209825-large.gif
2022,9903420,FIGURE 4.,AI in computer vision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil4-3209825-large.gif
2022,9903420,FIGURE 5.,Evolution of machine learning styles-a brief history of machine learning styles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil5-3209825-large.gif
2022,9903420,FIGURE 6.,Framework of supervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil6-3209825-large.gif
2022,9903420,FIGURE 7.,Taxonomy of machine learning styles classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil7-3209825-large.gif
2022,9903420,FIGURE 8.,Framework of multiple instance learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil8-3209825-large.gif
2022,9903420,FIGURE 9.,Framework of active learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil9-3209825-large.gif
2022,9903420,FIGURE 10.,Meta-Learning framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil10-3209825-large.gif
2022,9903420,FIGURE 11.,Framework of multitask learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil11-3209825-large.gif
2022,9903420,FIGURE 12.,Framework of unsupervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil12-3209825-large.gif
2022,9903420,FIGURE 13.,Framework of self-supervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil13-3209825-large.gif
2022,9903420,FIGURE 14.,Constructive learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil14-3209825-large.gif
2022,9903420,FIGURE 15.,Association rule learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil15-3209825-large.gif
2022,9903420,FIGURE 16.,Framework of reinforcement learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil16-3209825-large.gif
2022,9903420,FIGURE 17.,Framework of semi-supervised learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil17-3209825-large.gif
2022,9903420,FIGURE 18.,Framework of feature learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil18-3209825-large.gif
2022,9903420,FIGURE 19.,Framework of transfer learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil19-3209825-large.gif
2022,9903420,FIGURE 20.,Ensemble learning framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil20-3209825-large.gif
2022,9903420,FIGURE 21.,Embedding-based zero-shot method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil21-3209825-large.gif
2022,9903420,FIGURE 22.,Zero-shot learning using generative model-based methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil22-3209825-large.gif
2022,9903420,FIGURE 23.,"On the Left: The user is pointing to an object, and on the Right: Process Flow.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil23ab-3209825-large.gif
2022,9903420,FIGURE 24.,Federated learning system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9903420/patil24-3209825-large.gif
2022,9275342,Fig. 1.,Classification boundaries of SVM and triangular membership function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu1-3032707-large.gif
2022,9275342,Fig. 2.,Classification boundaries of SVM and AFS membership function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu2-3032707-large.gif
2022,9275342,Fig. 3.,Framework of the Semantic SSL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu3-3032707-large.gif
2022,9275342,Fig. 4.,"Membership function of class description
ς
p
C
1
of each training step for the Iris dataset. (a) Step1: Iris-setosa
ς
1
C
1
:
m
3,1
. (b) Step2: Iris-setosa
ς
2
C
1
:
m
4,1
. (c) Step3: Iris-setosa
ς
3
C
1
:
m
3,1
+
m
4,1
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu4abc-3032707-large.gif
2022,9275342,Fig. 5.,"Membership function of class description
ς
p
C
2
of each training step for the Iris dataset. (a) Step1: Iris-versicolor
ς
1
C
2
:
m
3,2
. (b) Step2: Iris-versicolor
ς
2
C
2
:
m
4,2
. (c) Step3: Iris-versicolor
ς
3
C
2
:
m
3,2
+
m
4,2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu5abc-3032707-large.gif
2022,9275342,Fig. 6.,"Membership function of class description
ς
p
C
3
of each training step for the Iris dataset. (a) Step1: Iris-virginica
ς
1
C
3
:
m
3,3
. (b) Step2: Iris-virginica
ς
2
C
3
:
m
4,3
. (c) Step3: Iris-virginica
ς
3
C
3
:
m
3,3
+
m
4,3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu6abc-3032707-large.gif
2022,9275342,Fig. 7.,"Membership function of class description
ς
p
C
1
,
ς
p
C
2
, and
ς
p
C
3
for the Iris dataset in training step. (a) Step1: Semantic description
ς
1
C
1
,
ς
1
C
2
, and
ς
1
C
3
. (b) Step2: Semantic description
ς
2
C
1
,
ς
2
C
2
, and
ς
2
C
3
. (c) Step3: Semantic description
ς
3
C
1
,
ς
3
C
2
, and
ς
3
C
3
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu7abc-3032707-large.gif
2022,9275342,Fig. 8.,Two types of bending activity. (a) Bending 1. (b) Bending 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9796553/9275342/liu8ab-3032707-large.gif
2022,9852204,FIGURE 1.,Illustration of defensive techniques of machine learning in [13].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte1-3197299-large.gif
2022,9852204,FIGURE 2.,"Concept approach to detect an adversarial machine learning attack. Datasets are represented by circles (blue for legitimate and red for malicious), while actions are represented by squares (green color).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte2-3197299-large.gif
2022,9852204,FIGURE 3.,"Concept approach to execute an adversarial machine learning attack. Datasets are represented by circles (blue for legitimate and red for malicious), while actions are represented by squares (green color).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte3-3197299-large.gif
2022,9852204,FIGURE 4.,"Application of Algorithm 4 on a dataset of 2000 target objects sampled from a gaussian with mean [1, 1] and variance 4 and 100 negative examples sampled from a gaussian with mean [1, 1] and variance 5. (a) is the first iteration of the algorithm and (b) is the convergence at the 97th iteration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte4ab-3197299-large.gif
2022,9852204,FIGURE 5.,LLM rules comparison via rule viewers for DNS tunneling problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte5-3197299-large.gif
2022,9852204,FIGURE 6.,Adversarial region with inside for JSMA in DNS tunneling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte6-3197299-large.gif
2022,9852204,FIGURE 7.,Feature and value ranking for JSMA attack in DNS tunneling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte7-3197299-large.gif
2022,9852204,FIGURE 8.,"Adversarial Region obtained for FGSM attack in RUL dataset by perturbing the intervals thresholds for features
v
phi
and
m
N
c
with inside method (TPR = 0.81, FPR = 0.03, TNR = 0.97, FNR = 0.19).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte8-3197299-large.gif
2022,9852204,FIGURE 9.,"2D graph of the “adversarial region” (the red points are the attacked ones) with
d
0
(distance between cars) and
v
0
(initial platooning speed) as input features of the JSMA-platooning dataset. The star points are the SVs of the description, coloured referring their specific label.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte9-3197299-large.gif
2022,9852204,FIGURE 10.,"2D graph of the “adversarial region” (the red points are the attacked ones) with
s
OS2
(Skewness of operational setting 2) and
v
Nc
(Variance of physical core speed) as input features of the CW-RUL dataset. The star points are the SVs of the description, coloured referring their specific label.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte10-3197299-large.gif
2022,9852204,FIGURE 11.,"2D graph of the “adversarial region” (the red points are the attacked ones) with
m
Dt
(average interarrival time between query and answer packet over 1000 sample) and
m
Q
(average size of query packet) as input features of the JSMA-DNS dataset. The star points are the SVs of the description, coloured referring their specific label.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte11-3197299-large.gif
2022,9852204,FIGURE 12.,"Graphs of the processing time (denoted as CPU in the figure) as the dataset and algorithm change: first row DNS dataset, second row Platooning dataset (denoted with PLT in the plot captions), third row RUL dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9852204/narte12-3197299-large.gif
2022,9670709,Fig. 1.,Flowchart of the proposed Methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang1-3140433-large.gif
2022,9670709,Fig. 2.,"The pre-processing of the selected data. To uniquely identify each variable hash tables were created, containing unique hash values. Relationship description (One to One, One to Many, Many to Many and Unmapped).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang2-3140433-large.gif
2022,9670709,Fig. 3.,Generalized scenario of a 1-1 relationship among questions/relations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang3-3140433-large.gif
2022,9670709,Fig. 4.,A pruned dendrogram of the tree data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang4-3140433-large.gif
2022,9670709,Fig. 5.,"(a) A Self-organizing map eight (08) cluster resultant map. (b) After the training, the self-organizing map (SOM) plots a 12 × 12 grid.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang5-3140433-large.gif
2022,9670709,Fig. 6.,"A plot of
p(x)
and
q(x)
shows how well the sampling distribution covers
p(x)
for subgroup 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang6-3140433-large.gif
2022,9670709,Fig. 7.,"The transformation of input values as per the p(x) function resultant plots for subgroup 2. By shifting the mean of the sampling distribution, we can see the results of choosing a
q(x)
with a center of mass far away from
p(x)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9768790/9670709/wang7-3140433-large.gif
2022,9625792,Fig. 1.,A block diagram of the proposed automated distributed active learning (AutoDAL) system with two worker machines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen1-3129793-large.gif
2022,9625792,Fig. 2.,"Comparison of the accuracy using USDM [27], AER [28], Auto-WEKA [1], auto-sklearn [29], ASSL+US [19], DAL and the proposed AutoDAL-CME algorithms for five benchmark datasets with different percentages of labeled data varying from 0.1% to 20%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen2-3129793-large.gif
2022,9625792,Fig. 3.,"Comparison of the accuracy using USDM, AER, Auto-WEKA, auto-sklearn, ASSL+US, DAL and the proposed AutoDAL-CME and AutoDAL-SOAR algorithms for five benchmark datasets when the percentage of labeled data is 1%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen3-3129793-large.gif
2022,9625792,Fig. 4.,"The comparison of misclassification error versus the number of iterations for the proposed AutoDAL-CME and AutoDAL-SOAR algorithms, Auto-WEKA and auto-sklearn for benchmark datasets, where the horizontal axis represents the number of iterations and the vertical axis represents the misclassification error (%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen4-3129793-large.gif
2022,9625792,Fig. 5.,The performance comparison with MCLR[20] for the average precision and recall performance for five UCI datasets on evaluation of distributed active learning performance under the same 20 computing node.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen5-3129793-large.gif
2022,9625792,Fig. 6.,"The comparison of precision and recall curves for ECG signal classification on different classes including class S, V, F and Q ((a)-(d)), where AutoDAL-SOAR is compared to USDM, AER, Auto-WEKA, auto-sklearn and ASSL+US. For each method, the area under the curve (AUC) is reported as a measure for the imbalanced dataset. (e) evaluates the total average accuracy for different active learning methods with 95 percent confidence intervals and the supervised learning methods are implemented with half of data for training and the rest half for cross-validation. (f) demonstrates the average accuracy versus number of machines for the proposed AutoDAL-SOAR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen6-3129793-large.gif
2022,9625792,Fig. 7.,"The comparison of precision and recall curves for ECG signal classification on using five different loss functions under automatic hyperparameter tuning where for all the loss functions including the proposed two novel loss functions: CME and SOAR and three comparative loss functions: Expected error reduction loss, variance reduction loss and information density loss. The performance is reported by the averaging results for all the classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen7-3129793-large.gif
2022,9625792,Fig. 8.,The performance comparison with AOW-ELM [21] and MB-CB [22] for F1 score using area under the curve (F1AUC) vs the number of positive samples per negative samples for ECG dataset on evaluation on the performance of imbalanced dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen8-3129793-large.gif
2022,9625792,Fig. 9.,The performance comparison with autoencoder-based approach[33] for fraud detection using the area under learning curve (ALC) with 95% confidence intervals where the horizontal axis represents the label percent and the vertical axis presents the area under the curve (AUC).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen9-3129793-large.gif
2022,9625792,Fig. 10.,The performance comparison of automatic selection of active learning based model using F1AUC vs the number of normal transactions per fraud transaction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9940445/9625792/chen10-3129793-large.gif
2022,9458953,Fig. 1.,"General steps in machine learning methods with application of EEG-based cognitive workload recognition. (a) is the EEG data collection and preprocessing steps, including filter and artifacts removal steps. (b) is the classical machine learning steps, including feature extraction and selection from various domains, classification methods (e.g., SVM, LDA, and ANN), and performance evaluation metrics [accuracy (ACC), sensitivity (SEN), and specificity (SPE)]. (c) is the deep learning methods mainly based on neural networks (e.g., CNN, RNN, and DBN), where the data processing steps may be unnecessary for them when the model uses the raw data as inputs. In most cases, deep learning methods use computed features to learn temporal, spectral, and spatial information. Taking a three-class classification task as an example, the final result is a prediction of the cognitive workload with normal load, underload, and overload state.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9881572/9458953/zhou1abc-3090217-large.gif
2022,9173698,Fig. 1.,"The pipeline of our proposed few-shot learning method, including: (a) DNN pre-training on large-scale data, i.e., using the entire training dataset; and (b) meta-transfer learning (MTL) that learns the parameters of Scaling and Shifting (SS), on the basis of pre-trained feature extractor (Section 4.1). The learning process is scheduled by the proposed HT meta-batch (Section 4.2) and regularized by meta-gradient regularization (Section 4.3). In (c), it is meta-test on unseen task whose processing consists of a base-learner (classifier) Fine-Tuning (FT) stage and a final evaluation stage, described in the last paragraph in Section 3. Input data are along with arrows. Modules with names in bold get updated at corresponding phases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9173698/sun1-3018506-large.gif
2022,9173698,Fig. 2.,"Two kinds of meta operations on pre-trained weights. (a) Parameter-level Fine-Tuning (FT) is a conventional meta-train operation used in related works such as MAML [5], ProtoNets [33] and RelationNets [34]. Its update works for all neuron parameters,
W
and
b
. (b) Our neuron-level Scaling and Shifting (SS) operations in MTL. They reduce the number of learning parameters and avoid overfitting problems. In addition, they keep large-scale trained parameters (in yellow) frozen, preventing “catastrophic forgetting” [31], [32].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9173698/sun2-3018506-large.gif
2022,9173698,Fig. 3.,"The computation flow of online hard task sampling. During an HT meta-batch phase, the meta-training first goes through
K
random tasks then continues on re-sampled
K
′
hard tasks.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9173698/sun3-3018506-large.gif
2022,9173698,Fig. 4.,"The 5-way, 1-shot meta-validation accuracy plots on the FC100, using FT (pre-trained ResNet-12 and MAML [5]) and our MTL on different pre-trained networks. Red curve uses the original meta-batch [5] and others use our proposed HT meta-batch.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9173698/sun4-3018506-large.gif
2022,9755930,FIGURE 1.,Payment card authorisation process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan1-3166891-large.gif
2022,9755930,FIGURE 2.,Pooling layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan2-3166891-large.gif
2022,9755930,FIGURE 3.,CNN output layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan3-3166891-large.gif
2022,9755930,FIGURE 4.,Application of dropout over neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan4-3166891-large.gif
2022,9755930,FIGURE 5.,Class distribution of fraudulent and nonfraud transactions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan5-3166891-large.gif
2022,9755930,FIGURE 6.,Confusion metrics of machine learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan6-3166891-large.gif
2022,9755930,FIGURE 7.,The case count statistics for fraud and non-fraud transactions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan7-3166891-large.gif
2022,9755930,FIGURE 8.,Comparative analysis of machine learning algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan8-3166891-large.gif
2022,9755930,FIGURE 9.,Metrics of deep learning with epoch sizes as 35 and 14.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan9-3166891-large.gif
2022,9755930,FIGURE 10.,Area under the interpolated precision-recall curve.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan10-3166891-large.gif
2022,9755930,FIGURE 11.,Positive distribution of the data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan11-3166891-large.gif
2022,9755930,FIGURE 12.,Negative distribution of the data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan12-3166891-large.gif
2022,9755930,FIGURE 13.,Validation loss using zero bias and careful bias.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan13-3166891-large.gif
2022,9755930,FIGURE 14.,"Training and validation history of loss, precision Recall Accuracy (PRC), precisions and recall (Epoch size 35).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan14-3166891-large.gif
2022,9755930,FIGURE 15.,Training and validation history of accuracy and loss of CNN model using 100 epochs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan15-3166891-large.gif
2022,9755930,FIGURE 16.,Model accuracy when epoch sizes are 20 and 50.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan16-3166891-large.gif
2022,9755930,FIGURE 17.,Accuracy of the CNN model over number of layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9755930/khan17-3166891-large.gif
2022,9743305,Fig. 1,Application of SMOTE technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4620076/9928066/9743305/barth1-3162546-large.gif
2022,9676634,FIGURE 1.,Proposed fused model for diabetes prediction (FMDP).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9676634/khan1-3142097-large.gif
2022,9676634,FIGURE 2.,Proposed FMDP system rule surface.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9676634/khan2-3142097-large.gif
2022,9676634,FIGURE 3.,Proposed FMDP system result with diabetes (yes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9676634/khan3-3142097-large.gif
2022,9676634,FIGURE 4.,Proposed FMDP system result with diabetes (no).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9676634/khan4-3142097-large.gif
2022,9793641,FIGURE 1.,Grid based layout used in the simulation of cell outage detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9793641/kucuk1-3182014-large.gif
2022,9793641,FIGURE 2.,Dataset structure for the first use case.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9793641/kucuk2-3182014-large.gif
2022,9793641,FIGURE 3.,The ROC curves for the four datasets for the comparative analysis of modern and conventional machine learning with the results reported in the source paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9793641/kucuk3-3182014-large.gif
2022,9793641,FIGURE 4.,Area under curve (AUC) scores across different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9793641/kucuk4-3182014-large.gif
2022,9714370,Fig. 1.,Flowchart representation of our developed framework for the evaluation of conventional machine learning algorithms in the detection of sleep apnea from single-lead ECG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9717300/9714370/forou1-3151947-large.gif
2022,9714370,Fig. 2.,"Deep neural network architectures developed for the detection sleep apnea: (a) CNN, (b) DRNN, and (c) Hybrid CNN-DRNN framework.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9717300/9714370/forou2abc-3151947-large.gif
2022,9714370,Fig. 3.,HRV feature ranking for sleep apnea detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/19/9717300/9714370/forou3-3151947-large.gif
2022,9853541,FIGURE 1.,"Configuration of target CNC milling-turning machining center, Model S5, made by Zheng Feng CNC Technology Co. LTD.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang1-3197797-large.gif
2022,9853541,FIGURE 2.,Temperature key points on target machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang2-3197797-large.gif
2022,9853541,FIGURE 3.,Real cutting conditions for making the metal shell of a cell phone.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang3-3197797-large.gif
2022,9853541,FIGURE 4.,Temperature variations at thermal key points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang4-3197797-large.gif
2022,9853541,FIGURE 5.,Thermal drifts of milling and turning spindles in the Z-axis direction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang5-3197797-large.gif
2022,9853541,FIGURE 6.,Influence of temperature at thermal key points on milling-spindle deformations using OLS regression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang6-3197797-large.gif
2022,9853541,FIGURE 7.,Influennce of temperatures at thermal key points on turning-spindle deformations using Lasso regression.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang7-3197797-large.gif
2022,9853541,FIGURE 8.,Thermal error prediction results by DNN with selected features for the milling-spindle deformation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang8-3197797-large.gif
2022,9853541,FIGURE 9.,Thermal error prediction results by DNN with selected features for the turning-spindle deformation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang9-3197797-large.gif
2022,9853541,FIGURE 10.,Thermal error prediction results by SVR with selected feature points for the milling-spindle deformation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang10-3197797-large.gif
2022,9853541,FIGURE 11.,Thermal error prediction results by SVR with selected feature points for the turning-spindle deformation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang11-3197797-large.gif
2022,9853541,FIGURE 12.,Thermal error prediction results by RF with selected feature points for the milling-spindle deformation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang12-3197797-large.gif
2022,9853541,FIGURE 13.,Thermal error prediction results by RF regression with selected feature points for the turning-spindle deformation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9853541/wang13-3197797-large.gif
2022,9848788,FIGURE 1.,Structure of WBC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das1-3196037-large.gif
2022,9848788,FIGURE 2.,Healthy WBC and ALL subtypes: (a) Healthy and (b) L1; (c) L2 and (d) L3 -cells.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das2abcd-3196037-large.gif
2022,9848788,FIGURE 3.,Schematic of leukemia detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das3-3196037-large.gif
2022,9848788,FIGURE 4.,ALL segmentation technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das4-3196037-large.gif
2022,9848788,FIGURE 5.,Machine learning-based ALL classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das5-3196037-large.gif
2022,9848788,FIGURE 6.,Deep learning-based ALL classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das6-3196037-large.gif
2022,9848788,FIGURE 7.,Conventional CNN architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das7-3196037-large.gif
2022,9848788,FIGURE 8.,AlexNet architecture [91].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das8-3196037-large.gif
2022,9848788,FIGURE 9.,"Convolution blocks: (a) Block with two
3×3
, convolution-layers, (b) Block with three
3×3
, convolution-layers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das9ab-3196037-large.gif
2022,9848788,FIGURE 10.,Inception module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das10-3196037-large.gif
2022,9848788,FIGURE 11.,Residual learning block.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das11-3196037-large.gif
2022,9848788,FIGURE 12.,"Depth-wise separable convolution [128], [129].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das12-3196037-large.gif
2022,9848788,FIGURE 13.,MobileNetV2 blocks: (a) MVB1 and (b) MVB2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das13ab-3196037-large.gif
2022,9848788,FIGURE 14.,The architecture of an autoencoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das14-3196037-large.gif
2022,9848788,FIGURE 15.,Graphical representation of segmentation performance using BCCD dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das15-3196037-large.gif
2022,9848788,FIGURE 16.,Graphical representation of segmentation performance using ALLIDB2 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das16-3196037-large.gif
2022,9848788,FIGURE 17.,Graphical representation of machine learning-based classification performance in ALL-IDB1 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das17-3196037-large.gif
2022,9848788,FIGURE 18.,Graphical representation of deep learning-based classification performance in ALL-IDB1 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das18-3196037-large.gif
2022,9848788,FIGURE 19.,Graphical representation of deep learning-based classification performance with ALL-IDB2 dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9848788/das19-3196037-large.gif
2022,9387581,Fig. 1.,Tree diagram representing the models described in the article.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9686800/9387581/ponnu1-3069213-large.gif
2022,9387581,Fig. 2.,Modified SEIR Model with Control Measure [2].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9686800/9387581/ponnu2-3069213-large.gif
2022,9387581,Fig. 3.,SEIR Model with quarantine and death compartment [52].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9686800/9387581/ponnu3-3069213-large.gif
2022,9387581,Fig. 4.,Statistical Markov Model [55].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/9686800/9387581/ponnu4-3069213-large.gif
2022,9556566,Fig. 1.,Workflow of Delirium application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9556566/son1-3116967-large.gif
2022,9556566,Fig. 2.,Predictors’ frequency used to construct four predictive models during the 10-times stratified 2-fold cross-validation test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9556566/son2-3116967-large.gif
2022,9556566,Fig. 3.,Paired Wilcoxon signed-rank test for eight predictive models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9556566/son3-3116967-large.gif
2022,9382101,Fig. 1.,The diagram of gradient projection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou1-3067454-large.gif
2022,9382101,Fig. 2.,The system model of multi-objective machine learning based on edge computing network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou2-3067454-large.gif
2022,9382101,Fig. 3.,An end-to-end license plate recognition model architecture based on multi-objective machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou3-3067454-large.gif
2022,9382101,Fig. 4.,Accuracy of different methods on CCPD-Base.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou4-3067454-large.gif
2022,9382101,Fig. 5.,Accuracy of different methods on CCPD.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou5-3067454-large.gif
2022,9382101,Fig. 6.,Convergence curves of training loss function with different methods on CCPD-Base.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou6-3067454-large.gif
2022,9382101,Fig. 7.,Convergence curves of training loss function with different methods on CCPD-DB.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou7-3067454-large.gif
2022,9382101,Fig. 8.,Convergence curves of training loss function with different methods on CCPD-FN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou8-3067454-large.gif
2022,9382101,Fig. 9.,Convergence curves of training loss function with different methods on CCPD-Rotate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou9-3067454-large.gif
2022,9382101,Fig. 10.,Convergence curves of training loss function with different methods on CCPD-Tilt.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou10-3067454-large.gif
2022,9382101,Fig. 11.,Convergence curves of training loss function with different methods on CCPD-Weather.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou11-3067454-large.gif
2022,9382101,Fig. 12.,Convergence curves of training loss function with different methods on CCPD-Challenge.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9679301/9382101/zhou12-3067454-large.gif
2022,9439893,Fig. 1.,"The box-plots showing the brain-age delta followed by different regression algorithms on independent test sets. (A) CH individuals, (B) MCI patients, and (C) AD patients. Model 1 = Linear SVR, Model 2 = Quadratic SVR, Model 3 = Gaussian SVR, Model 4 = Ensemble Trees (Bag), Model 5 = Ensemble Trees (LSBoost), Model 6 = Linear Regression, Model 7 = Lasso Regression, Model 8 = Ridge Regression, Model 9 = Binary Decision Tree, Model 10 = Gaussian Regression (Kernel-Exponential), Model 11 = Gaussian Regression (Kernel-Squared Exponential), Model 12 = Gaussian Regression (Kernel-Matern32), Model 13 = Gaussian Regression (Kernel-Matern52), Model 14 = Gaussian Regression (Kernel-Rational-Quadratic), Model 15 = ETSVR (Kernel - Linear), Model 16 = Kernel Ridge Regression (Kernel-Linear), Model 17 = Nystrom Kernel Ridge Regression, Model 18 = DNNE, Model 19 = kNN (Weighted Mean), Model 20 = Neural Network (NN), Model 21 = RKNNWTSVR, Model 22 = LTSVR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9439893/ganai1-3083187-large.gif
2022,9138696,Fig. 1.,"Phase-transition phenomenon with respect to tolerance
ε
and sample size
m
given kernel size. (a)
n=20
. (b)
n=40
. (c)
n=60
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang1abc-2987810-large.gif
2022,9138696,Fig. 2.,"Phase-transition phenomenon with respect to tolerance
ε
and kernel size
n
given sample size. (a)
m=100
. (b)
m=250
. (c)
m=400
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang2abc-2987810-large.gif
2022,9138696,Fig. 3.,"(a) Contour plot of the test error when
m=500
with respect to the number of features and kernel parameters. (b) Testing error with respect to the number of features when
m=500
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang3ab-2987810-large.gif
2022,9138696,Fig. 4.,"LSF with different center generation mechanisms. (a), (d), and (g) LSF with Sobol centers; (b), (e), and (h) LSF with random centers (independent of samples); and (c), (f), and (i) LSF with random centers (depending on samples).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang4abcdefghi-2987810-large.gif
2022,9138696,Fig. 5.,"Comparisons of LSF with RLS, Nyström regularization, and randomized sketches on different sizes of samples in terms of (a) test error, (b) training time, and (c) number of features.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang5abc-2987810-large.gif
2022,9138696,Fig. 6.,"Comparisons of LSF with RLS, Nyström regularization, and randomized sketches on different sizes of samples in terms of test error for (a) inverse multiquadratic kernel and (b) thin-plate spline kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang6ab-2987810-large.gif
2022,9138696,Fig. 7.,"Contour plot of the test error when
m=500
with respect to the number of features and kernel parameters for (a) inverse multiquadratic kernel and (b) thin-plate spline kernel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang7ab-2987810-large.gif
2022,9138696,Fig. 8.,"Comparisons of LSF with LRF, DRLS, and G-Sketch on subsets of Million Song data in terms of (a) test error and (b) training time.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9749993/9138696/fang8ab-2987810-large.gif
2022,9870532,Figure 1.,An illustration of quantum-inspired machine learning and optimisation in 6G.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782711/9684258/9870532/duong1-3202876-large.gif
2022,9870532,Figure 2.,An example of quantum operation for wireless optimization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782711/9684258/9870532/duong2-3202876-large.gif
2022,9870532,Figure 3.,"A particular result of using quantum-based optimization for cell-free transmitter assignment. In the figure,
ζ
indicates the inter-beam interference factor [77].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782711/9684258/9870532/duong3-3202876-large.gif
2022,9870532,Figure 4.,"A particular result of using quantum-based optimization for NOMA beam assignment [78]. In the figure,
d
μ
denotes normalised user's distance to the base station.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782711/9684258/9870532/duong4-3202876-large.gif
2022,9870532,Figure 5.,An illustration of the applicability of quantum-inspired ML for 6G with observable security check points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782711/9684258/9870532/duong5-3202876-large.gif
2022,9090349,Fig. 1.,"A schematic representation of how to tune regularization parameters,
λ
1
for fields and
λ
2
for couplings. The average evolutionary energy per residue
ψ
N
¯
¯
¯
¯
¯
¯
¯
/L
of natural sequences and the ensemble average of evolutionary energy per residue
(
ψ
¯
−δ
ψ
2
)/L
in the Ising gauge are plotted by circle and plus markes, respectively, against one of the regularization parameters,
λ
1
N
eff
with
λ
2
=
λ
1
or
λ
2
N
eff
with
λ
1
=
λ
1,0
;
λ
1,0
and
λ
2,0
are the optimum values of
λ
1
and
λ
2
, respectively. The regularization model L2-GL1 and the modified Adam method are employed; see Tables 2 and 3 for the values of
λ
1,0
and
λ
2,0
. The upper and lower rows correspond to the figures for PF00595 and PF00153, respectively. The left four figures are for natural sequences of PF00595 and PF00153, and the right four figures are for the MCMC samples, MC1162 and MC1445 that are obtained in the Boltzmann machine learnings for PF00595 and PF00153, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz1-2993232-large.gif
2022,9090349,Fig. 2.,"Learning processes by the ModAdam, Adam, NAG, and RPROP-LR gradient-descent methods for PF00595. The averages of Kullback-Leibler divergences,
D
2
KL
for pairwise marginal distributions and
D
1
KL
for single-site marginal distributions, are drawn against iteration number in the learning processes.
D
2
KL
and
D
1
KL
for the Adam, NAG, and RPROP-LR are indicated by the upper and lower black lines in the left, middle, and right figures, respectively, and those for the ModAdam are shown in pink in all figures for comparison. The L2-L2 regularization model is employed. The values of hyper-parameters are listed in Table 4 as well as others.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz2-2993232-large.gif
2022,9090349,Fig. 3.,"Comparisons of the inferred couplings
J
ij
(
a
k
,
a
l
)
in the Ising gauge between the ModAdam and the other gradient-descent methods, Adam, NAG, and RPROP-LR, for PF00595. The abscissas correspond to the couplings inferred by the modified Adam, and the ordinates correspond to those by the Adam, NAG, and RPROP-LR in order from the left to the right. The regularization model L2-L2 is employed for all methods. The solid lines show the equal values between the ordinate and abscissa. The values of hyper-parameters are listed in Table 2. The overlapped points of
J
ij
(
a
k
,
a
l
)
in the units 0.001 are removed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz3-2993232-large.gif
2022,9090349,Fig. 4.,"Differences in the learning of coupling parameters,
J
ij
(
a
k
,
a
l
)
, between the ModAdam and Adam gradient-descent methods for PF00595. All
J
ij
(
a
k
,
a
l
)
where
(
a
k
,
a
l
)=
argmax
a
k
,
a
l
≠deletion
|
J
ij
(
a
k
,
a
l
)|
in the Ising gauge are plotted against the distance between
i
th and
j
th residues. The ModAdam and Adam methods are employed for the left and right figures, respectively. The regularization model L2-L2 is employed for both methods. The learning processes by both methods are shown in Fig. 2. Please notice that more strong couplings tend to be inferred for closely located residues pairs by the ModAdam method than by the Adam method. The values of hyper-parameters are listed in Table 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz4-2993232-large.gif
2022,9090349,Fig. 5.,"The profile of the average evolutionary energies along the learning process in the L2-L2 model by each gradient-descent method for PF00595. The average evolutionary energy per residue
ψ
N
¯
¯
¯
¯
¯
¯
¯
/L
of natural sequences and the ensemble average of evolutionary energy per residue in the Gaussian approximation
(
ψ
¯
−δ
ψ
2
)/L
in the Ising gauge are plotted every 10 iterations against iteration number in the learning by each of the ModAdam, NAG, Adam, and RPROP-LR in the order of the left to the right; the sample and ensemble averages are indicated by the upper and lower lines, respectively. The L2-L2 regularization model is employed. For the ModAdam in the leftmost figure, those for the first run of 1220 iterations and for the second run, which is conditioned to run by more than 2000 iterations, are plotted by dots and solid lines, respectively; they indistinguishably overlap.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz5-2993232-large.gif
2022,9090349,Fig. 6.,"The profiles of the average evolutionary energies along the leaning process in the L2-GL1 model by the ModAdam for PF00595 and PF00153. The average evolutionary energy per residue
ψ
N
¯
¯
¯
¯
¯
¯
¯
/L
of natural sequences and the ensemble average of evolutionary energy per residue in the Gaussian approximation
(
ψ
¯
−δ
ψ
2
)/L
in the Ising gauge are plotted every 10 iterations against iteration number in the learning by the ModAdam; the sample and ensemble averages are indicated by the upper and lower lines, respectively. The left and right figures are for PF00595 and PF00153, respectively. The L2-GL1 regularization model is employed. The values of the regularization parameters are listed in Tables 2 and 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz6-2993232-large.gif
2022,9090349,Fig. 7.,"Differences of inferred couplings
J
ij
(
a
k
,
a
l
)
among the regularization models for PF00153. All
J
ij
(
a
k
,
b
l
)
where
(
a
k
,
a
l
)=
argmax
a
k
,
a
l
≠deletion
|
J
ij
(
a
k
,
a
l
)|
in the Ising gauge are plotted against the distance between
i
th and
j
th residues. The regularization models L2-GL1, L2-L1, and L2-L2 are employed for the left, middle, and right figures, respectively. The protein family PF00153 is employed. The values of regularization parameters are listed in Table 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz7-2993232-large.gif
2022,9090349,Fig. 8.,"Comparisons of inferred couplings
J
ij
(
a
k
,
a
l
)
in the Ising gauge between the regularization models for PF00595. Both abscissa correspond to the couplings inferred by the L2-GL1. The ordinates in the left and right figures correspond to the couplings inferred by the L2-L1 and L2-L2 models, respectively. The values of regularization parameters are listed in Table 2. The solid lines show the equal values between the ordinate and abscissa. The overlapped points of
J
ij
(
a
k
,
a
l
)
in the units 0.001 are removed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz8-2993232-large.gif
2022,9090349,Fig. 9.,"Comparisons of inferred couplings
J
ij
(
a
k
,
a
l
)
in the Ising gauge between the regularization models for PF00153. Both abscissa correspond to the couplings inferred by the L2-GL1. The ordinates in the left and right figures correspond to the couplings inferred by the L2-L1 and L2-L2 models, respectively. The values of regularization parameters are listed in Table 3. The solid lines show the equal values between the ordinate and abscissa. The overlapped points of
J
ij
(
a
k
,
a
l
)
in the units 0.001 are removed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz9-2993232-large.gif
2022,9090349,Fig. 10.,"Precision of each regularization model in the contact predictions for PF00595 and PF00153. Precisions of contact predictions are compared between the regularization models. The ordinate of each figure corresponds to the precision of contact prediction, in which residue pairs are predicted as contacts in the decreasing order of contact score, and the number of predicted contacts is indicated as (the lowest rank of contact score)
/L
by the abscissa. Residues whose side chain centers are within 8 Å in the 3D protein structure are defined to be in contact; neighboring residue pairs along the sequence are included. The left and right figures are for the protein families PF00595 and PF00153, respectively. The solid, broken, and dotted lines correspond to the regularization models, L2-GL1, L2-L1, and L2-L2, respectively. The corrected Frobenius norm of couplings is employed for the contact score [10], [11].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz10-2993232-large.gif
2022,9090349,Fig. 11.,"Recoverabilities of the pairwise correlations of PF00595 and PF00153 by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method. The left and right figures are for PF00595 and PF00153, respectively;
D
KL
2
=0.0759
for PF00595 and
D
KL
2
=0.0318
for PF00153. The solid lines show the equal values between the ordinate and abscissa. The overlapped points of
C
ij
(
a
k
,
a
l
)
in the units 0.0001 are removed. See Tables 2 and 3 for the regularization parameters employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz11-2993232-large.gif
2022,9090349,Fig. 12.,"Recoverabilities of the pairwise correlations by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method for the protein-like sequences, the MCMC samples that are obtained by the same Boltzmann machine for PF00595 and PF00153. The MCMC samples obtained by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method for PF00595 and PF00153 are employed as protein-like sequences for which the Boltzmann machine learning with the same model and method is executed again in order to examine how precisely the marginals of protein-like sequences can be recovered. The marginals recovered by the Boltzmann machine learning for the MCMC samples are compared to those of the MCMC samples. The left and right figures are for the single-site probabilities and pairwise correlations, respectively. The solid lines show the equal values between the ordinate and abscissa. The overlapped points of
C
ij
(
a
k
,
a
l
)
in the units 0.0001 are removed. See Tables 2 and 3 for the regularization parameters employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz12-2993232-large.gif
2022,9090349,Fig. 13.,"Reproducibility of the fields and couplings in the Ising gauge by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method for the protein-like sequences, the MCMC samples that are obtained by the same Boltzmann machine for PF00595. The MCMC samples obtained by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method for PF00595 are employed as protein-like sequences for which the Boltzmann machine learning with the same model and method is executed again in order to examine how well the fields and couplings in the protein-like sequences can be reproduced. The fields and couplings inferred by the Boltzmann machine learning for the MCMC samples are plotted against the actual values of their interactions in the left and right figures, respectively. The solid lines show the equal values between the ordinate and abscissa. The overlapped points of
J
ij
(
a
k
,
a
l
)
in the units 0.001 are removed. See Table 2 for the regularization parameters employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz13-2993232-large.gif
2022,9090349,Fig. 14.,"Reproducibility of the fields and couplings in the Ising gauge by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method for the protein-like sequences, the MCMC samples that are obtained by the same Boltzmann machine for PF00153. The MCMC samples obtained by the Boltzmann machine learning with the L2-GL1 model and the ModAdam method for PF00153 are employed as protein-like sequences for which the Boltzmann machine learning with the same model and method is executed again in order to examine how well the fields and couplings in the protein-like sequences can be reproduced. The fields and couplings inferred by the Boltzmann machine learning for the MCMC samples are plotted against the actual values of their interactions in the left and right figures, respectively. The solid lines show the equal values between the ordinate and abscissa. The overlapped points of
J
ij
(
a
k
,
a
l
)
in the units 0.001 are removed. See Table 3 for the regularization parameters employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz14-2993232-large.gif
2022,9090349,Fig. 15.,"Comparisons of the histograms of inferred evolutionary energies in the Ising gauge between the target sequences (
ψ
N
) and the MCMC samples (
ψ
MC
) obtained by the Boltzmann machine learnings. In the upper left and right figures, the evolutionary energies (
ψ
N
) of the natural proteins are compared with those (
ψ
MC
) of the MCMC samples obtained by the Boltzmann machine learnings for PF00595 and PF00153, respectively. Sequences with no deletion for PF00595 and with no more than 2 deletions for PF00153 are employed; the effective numbers
M
eff
of sequences are 340.0 for PF00595, 139.8 for PF00153. The 162 samples with no deletion in MC1162 and 254 samples with no more than 3 deletions in MC1445 are employed. In the lower left and right figures, the evolutionary energies (
ψ
N
) of the protein-like sequences, MC1162 and of MC1445, are compared with those (
ψ
MC
) of the MCMC samples obtained by the Boltzmann machine learnings for them. The same regularization parameters as for the natural protein families are employed;
λ
2
=40.0/
N
eff
for PF00595 and
λ
2
=209/
N
eff
for PF00153, and
λ
1
=0.100/
N
eff
for both. The 118 samples with no deletion in the MCMC samples for MC1162 and 268 samples with no more than 3 in the MCMC samples for MC1445 are employed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz15-2993232-large.gif
2022,9090349,Fig. 16.,"Regression of the experimental values[65] of folding free energy changes (
ΔΔ
G
ND
) due to single amino acid substitutions on
Δ
ψ
N
(≃ΔΔ
ψ
ND
)
for the same types of substitutions in PF00595. The solid line shows the least-squares regression line through the origin with the slope, 0.400 kcal/mol, which is the estimates of
k
B
T
s
. The reflective correlation coefficient is equal to 0.92. The free energies are in kcal/mol units.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz16-2993232-large.gif
2022,9090349,Fig. 17.,"Relationships between
Δ
ψ
N
due to single nucleotide nonsynonymous substitutions and
ψ
N
of the homologous sequences in PF00595 and in PF00153. Each of the black plus and red cross marks corresponds to the mean and the standard deviation of
Δ
ψ
N
due to all types of single nucleotide nonsynonymous substitutions over all sites in each of the homologous sequences, respectively; the left and right figures are for PF00595 and PF00153, respectively. Representatives of unique sequences with no deletion for PF00595 and with no more than 2 deletions for PF00153, which are at least 20 percent different from each other, are employed; their numbers are 361 for PF00595 and 144 for PF00153. The solid lines show the regression lines for the mean and the standard deviation of
Δ
ψ
N
. The correlation and regression coefficients are listed in Table 5.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9090349/miyaz17-2993232-large.gif
2022,9852477,FIGURE 1.,"Distribution of BD 1, BD 2.1, BD 2.2, and BD 2.3 across PMUs of Interconnect A.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart1-3197553-large.gif
2022,9852477,FIGURE 2.,"Distribution of BD 1, BD 2.1, BD 2.2, and BD 2.3 across PMUs of Interconnect B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart2-3197553-large.gif
2022,9852477,FIGURE 3.,"Distribution of BD 1, BD 2.1, BD 2.2, and BD 2.3 across PMUs of Interconnect C.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart3-3197553-large.gif
2022,9852477,FIGURE 4.,Examples of days- and weeks-long voltage drops measured by 5 PMUs in interconnect C.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart4-3197553-large.gif
2022,9852477,FIGURE 5.,(a) Six-point statistical analysis of frequency measured by each PMU in Interconnect A training dataset across a two-year timespan. (b) Six-point statistical analysis of frequency measured by each PMU in Interconnect A training dataset across a two-year timespan (magnified).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart5ab-3197553-large.gif
2022,9852477,FIGURE 6.,(a) Six-point statistical analysis of frequency measured by each PMU in Interconnect B dataset across a two-year timespan. (b) Six-point statistical analysis of frequency measured by each PMU in Interconnect B dataset across a two-year timespan (magnified).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart6ab-3197553-large.gif
2022,9852477,FIGURE 7.,(a) Six-point statistical analysis of frequency measured by each PMU in Interconnect C dataset across a two-year timespan. (b) Six-point statistical analysis of frequency measured by each PMU in Interconnect C dataset across a two-year timespan (magnified).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart7ab-3197553-large.gif
2022,9852477,FIGURE 8.,(a) Six-point statistical analysis of current measured by each PMU in Interconnect A training dataset across a two-year timespan. (b) Six-point statistical analysis of current measured by each PMU in Interconnect A training dataset across a two-year timespan (magnified).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart8ab-3197553-large.gif
2022,9852477,FIGURE 9.,(a) Six-point statistical analysis of current measured by each PMU in Interconnect B training dataset across a two-year timespan. (b) Six-point statistical analysis of current measured by each PMU in Interconnect B training dataset across a two-year timespan (magnified).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart9ab-3197553-large.gif
2022,9852477,FIGURE 10.,(a) Six-point statistical analysis of current measured by each PMU in Interconnect C training dataset across a two-year timespan. (b) Six-point statistical analysis of current measured by each PMU in Interconnect C training dataset across a two-year timespan (magnified).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart10ab-3197553-large.gif
2022,9852477,FIGURE 11.,"Hypothetical dataset including imperfect match between event log and raw data resulting in unlabeled events, as well as temporal imprecision of labels in the event log.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8784343/9662292/9852477/hart11-3197553-large.gif
2022,9751143,Fig. 1.,Schematic illustration of quantum drift-diffusion NNs framework (up) and the residual learning neural networks (down).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/9919692/9751143/yang1-3158987-large.gif
2022,9751143,Fig. 2.,The visual images of initial condition 4.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/9919692/9751143/yang2-3158987-large.gif
2022,9751143,Fig. 3.,The visualization process of evolutionary for Eq. (27) with initial condition-4 (noise level is 0.9).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/9919692/9751143/yang3-3158987-large.gif
2022,9751143,Fig. 4.,Training loss curves in different situations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/9919692/9751143/yang4-3158987-large.gif
2022,9712274,FIGURE 1.,"Relation between Artificial Intelligence, Machine Learning, and Deep Learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw1-3151248-large.gif
2022,9712274,FIGURE 2.,Machine learning approaches and algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw2-3151248-large.gif
2022,9712274,FIGURE 3.,Deep learning approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw3-3151248-large.gif
2022,9712274,FIGURE 4.,NIDS versus HIDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw4-3151248-large.gif
2022,9712274,FIGURE 5.,Machine learning Vs. deep learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw5-3151248-large.gif
2022,9712274,FIGURE 6.,An overview of constructing ASNM datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw6-3151248-large.gif
2022,9712274,FIGURE 7.,OneM2M architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw7-3151248-large.gif
2022,9712274,FIGURE 8.,Sample of IDS deep learning model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw8-3151248-large.gif
2022,9712274,FIGURE 9.,Stacked NDAE classification model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw9-3151248-large.gif
2022,9712274,FIGURE 10.,IDS based on CNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw10-3151248-large.gif
2022,9712274,FIGURE 11.,DeepPTSD architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw11-3151248-large.gif
2022,9712274,FIGURE 12.,RNN and RNN-IDS architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw12-3151248-large.gif
2022,9712274,FIGURE 13.,Hierarchy of HAST-IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9712274/gunaw13-3151248-large.gif
2022,9829748,FIGURE 1.,"Distribution of amount (
$
per transaction) on Normal (0) and Fraudulent (1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu1-3190897-large.gif
2022,9829748,FIGURE 2.,Example of a two-group classification problem with support vectors highlighted.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu2-3190897-large.gif
2022,9829748,FIGURE 3.,Example of nonlinear support vector classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu3-3190897-large.gif
2022,9829748,FIGURE 4.,Example of kernel trick for nonlinear support vector classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu4-3190897-large.gif
2022,9829748,FIGURE 5.,Fraud detection framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu5-3190897-large.gif
2022,9829748,FIGURE 6.,AUROC curves of SVM-QUBO verses machine learning algorithms on ICCT dataset with no feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu6-3190897-large.gif
2022,9829748,FIGURE 7.,AUROC curves of SVM-QUBO verses machine learning algorithms on ICCT dataset with LASSO.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu7-3190897-large.gif
2022,9829748,FIGURE 8.,AUROC curves of SVM-QUBO verses machine learning algorithms on LOAN dataset without feature selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu8-3190897-large.gif
2022,9829748,FIGURE 9.,AUROC curves of SVM-QUBO verses machine learning algorithms on LOAN dataset with LASSO.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9829748/liu9-3190897-large.gif
2022,9509761,Fig. 1.,General architecture of ensemble techniques (adopted from [26]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat1-3103829-large.gif
2022,9509761,Fig. 2.,Ensemble process phases and their methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat2-3103829-large.gif
2022,9509761,Fig. 3.,Model bias and variance tradeoff(adopted from [13]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat3-3103829-large.gif
2022,9509761,Fig. 4.,Workflow for intelligent IDS using AMLS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat4-3103829-large.gif
2022,9509761,Fig. 5.,"Schematic diagram depicting a triple Stacking-based ensemble using a combination of PCA AML-IDS, 1-SVM AML-IDS, and 2-NN DL-IDS models.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat5-3103829-large.gif
2022,9509761,Fig. 6.,"Tuned PCA AML-IDS model for AUC and
F1
-score optimization using IoT_Botnet experimental run results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat6-3103829-large.gif
2022,9509761,Fig. 7.,Tuned 1-SVM AML-IDS model for AUC and \$F1\$ -Score optimization using IoT_Botnet data set experimental run results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat7-3103829-large.gif
2022,9509761,Fig. 8.,Tuned 1-SVM AML-IDS model for AUC and \$F1\$ -score optimization using IoT_Fridge data set experimental run results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat8-3103829-large.gif
2022,9509761,Fig. 9.,"Dual Stacking ROC: PCA and SVM using IoT_Botnet data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat9abc-3103829-large.gif
2022,9509761,Fig. 10.,"Dual Stacking ROC: PCA and NN using IoT_Botnet data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat10abc-3103829-large.gif
2022,9509761,Fig. 11.,"Triple Stacking ROC: PCA, SVM, and NN using IoT_Botnet data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat11abc-3103829-large.gif
2022,9509761,Fig. 12.,"Dual and Triple Stacking: graphed derived metrics for PCA and SVM, PCA and NN, and PCA, SVM, and NN using the IoT_Botnet data set and AUC, \$F1\$ -Score, and Accuracy optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat12-3103829-large.gif
2022,9509761,Fig. 13.,"Dual and Triple Stacking: graphed base metrics for PCA and SVM, PCA and NN, and PCA, SVM, and NN using the IoT_Botnet data set and AUC, \$F1\$ -Score, and Accuracy optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat13-3103829-large.gif
2022,9509761,Fig. 14.,"Dual Stacking: SVM and PCA using IoT_Botnet data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat14abc-3103829-large.gif
2022,9509761,Fig. 15.,"Dual Stacking: SVM and NN using IoT_Botnet data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat15abc-3103829-large.gif
2022,9509761,Fig. 16.,"Triple Stacking: SVM, PCA, and NN using IoT_Botnet data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat16abc-3103829-large.gif
2022,9509761,Fig. 17.,"Dual and Triple Stacking: graphed derived metrics for SVM and PCA, SVM and NN, and SVM, PCA, and NN using the IoT_Botnet data set and AUC, \$F1\$ -Score, and Accuracy optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat17-3103829-large.gif
2022,9509761,Fig. 18.,"Dual and Triple Stacking: graphed base metrics for SVM and PCA, SVM and NN, and SVM, PCA, and NN using the IoT_Botnet data set and AUC, \$F1\$ -Score, and Accuracy optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat18-3103829-large.gif
2022,9509761,Fig. 19.,"Dual Stacking: SVM and PCA using IoT_Fridge data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat19abc-3103829-large.gif
2022,9509761,Fig. 20.,"Dual Stacking: SVM and NN using IoT_Fridge data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat20abc-3103829-large.gif
2022,9509761,Fig. 21.,"Triple Stacking: SVM, PCA, and NN using IoT_Fridge data set and optimized using (a) AUC, (b) \$F1\$ -Score, and (c) Accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat21abc-3103829-large.gif
2022,9509761,Fig. 22.,"Dual and Triple Stacking: graphed derived metrics for SVM and PCA, SVM and NN, and SVM, PCA, and NN using the IoT_Fridge data set and AUC, \$F1\$ -Score, and Accuracy optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat22-3103829-large.gif
2022,9509761,Fig. 23.,"Dual and Triple Stacking: graphed base metrics for SVM and PCA, SVM and NN, and SVM, PCA, and NN using the IoT_Fridge data set and AUC, \$F1\$ -Score, and Accuracy optimization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9730117/9509761/rawat23-3103829-large.gif
2022,9798802,FIGURE 1.,"Main view of the dataset management interface. The list shows the names of datasets along with other information, e.g., number of documents, description. The three bars symbol on the left opens a menu of possible actions, e.g., go to browse and code mode, rename, delete, download.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli1ab-3184009-large.gif
2022,9798802,FIGURE 2.,"The “ browse and code” interface. The upper part lets the users select the classifiers to be used and select the browsing mode. The central part shows the content of the document to be classified. The lower part shows the classifiers in use, with any eventual suggestion or already assigned label. Classification is done by directly clicking/touching the labels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli2-3184009-large.gif
2022,9798802,FIGURE 3.,"Visualization of classifiers and label assignments in the browse and code mode, single-label classifier. Top to bottom: no suggestion shown and no label assigned by users; suggestion from the automatic classifier shown; classification made by users.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli3-3184009-large.gif
2022,9798802,FIGURE 4.,"Visualization of classifiers and label assignments in the browse and code mode, multi-label classifier. Top to bottom: no suggestions shown and no labels assigned by users; suggestion shown; partial classification by users; complete classification by users.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli4-3184009-large.gif
2022,9798802,FIGURE 5.,Live classification interface. Automatic suggestions and classification by users are shown similarly to the browse and code view. Any label assignment can be made or changed by clicking/touching on the label to be assigned. The text filter box allows the user to work on the subset of documents matching a given query.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli5-3184009-large.gif
2022,9798802,FIGURE 6.,The automatic classification interface lists ongoing and completed automatic classifications. Automatically classified documents can be then downloaded as a CSV formatted file.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli6-3184009-large.gif
2022,9798802,FIGURE 7.,"The interface for creation and management of classifiers. Similarly to the dataset interface (see Figure 1), it shows classifiers and their properties, each with a menu of action to modify them, e.g., rename/delete classifier, add/rename/delete labels, download training data or the classification model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli7-3184009-large.gif
2022,9798802,FIGURE 8.,Architecture of the system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli8-3184009-large.gif
2022,9798802,FIGURE 9.,"Learning curves for SVM, PA-1, and PA-R-
N
configurations using the active learning browsing mode. Simulation of 1,000 labeling steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli9-3184009-large.gif
2022,9798802,FIGURE 10.,"Time (seconds, logarithmic scale) required to update the classification models for SVM, PA-1, and PA-R- \$N\$ configurations using the active learning browsing mode. Simulation of 1,000 labeling steps.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli10-3184009-large.gif
2022,9798802,FIGURE 11.,"Learning curves for the transfer learning experiments on IMDB and Fine Foods datasets. Simulation of 1,000 labeling steps using Active browsing.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9798802/esuli11-3184009-large.gif
2022,9091845,Fig. 1.,"Different driving scenes (e.g., parking lots with varying structures).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang1-2987632-large.gif
2022,9091845,Fig. 2.,Distributions of (a) vehicle speed and (b) steering wheel angle according to two parking lots are different.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang2ab-2987632-large.gif
2022,9091845,Fig. 3.,"Framework of the proposed DTL method. Source and target data can be projected onto a common space by the discriminative distribution matching, which can reduce the MMD within classes, increase the distance across classes, and preserve the local manifold structure. Then, the majority voting classification based on source data can be used to iteratively refine the labels of target data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang3-2987632-large.gif
2022,9091845,Fig. 4.,(a) Original dataset and data transformation with (b) traditional distribution matching and (c) proposed discriminative matching.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang4abc-2987632-large.gif
2022,9091845,Algorithm 1:,DTL for Driving Pattern Recognition,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang13-2987632-large.gif
2022,9091845,Fig. 5.,Driving status features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang5-2987632-large.gif
2022,9091845,Fig. 6.,"(a) Intra-MMD and (b) inter-MMD of all samples on the dataset
ACDE→B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang6ab-2987632-large.gif
2022,9091845,Fig. 7.,"(a) Data visualization before transformation (intra-MMD, 10.3; inter-MMD, 1.6). (b) Data visualization after transformation (intra-MMD, 0.3; inter-MMD, 253.3) on the dataset
ACDE→B
. “
∘
” and “+” represent the source and target domains, respectively, different colors represent different categories.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang7ab-2987632-large.gif
2022,9091845,Fig. 8.,"Inter-MMD for 16 classes obtained from (a) TCA, (b) JDA, (c) STL, and (d) DTL on the dataset
ACDE→B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang8abcd-2987632-large.gif
2022,9091845,Fig. 9.,"Similarity matrices obtained from (a) TCA, (b) JDA, (c) STL, and (d) DTL embeddings on the dataset
ACDE→B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang9abcd-2987632-large.gif
2022,9091845,Fig. 10.,"Parameter sensitivity of DTL on three datasets (dashed lines indicate the best results from comparison methods). (a)
♯
subspace bases
k
. (b) Laplacian regularization parameter
β
. (c) Interclass regularization parameter
α
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang10abc-2987632-large.gif
2022,9091845,Fig. 11.,"Parameter sensitivity (
k
) of DTL on three datasets. (a)
A→B
. (b)
A→C
. (c)
A→D
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang11abc-2987632-large.gif
2022,9091845,Fig. 12.,"Convergence study of DTL on three datasets. (a)
A→D
. (b)
D→B
. (c)
ACDE→B
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9733099/9091845/yang12abc-2987632-large.gif
2022,9657494,Fig. 1.,"Growth of structures from cryo-EM experiments released per year in the PDB, with some significant structures solved by single-particle cryo-EM. (a) 2013 Structure of the TRPV1 icon channel. (b) 2020 Structure of the SARS-CoV-2 spike protein. (c) 2020 Structure of the RNA and remdesivir-bound RdRp complex.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang1abc-3131325-large.gif
2022,9657494,Fig. 2.,Workflow of structure determination of single-particle cryo-EM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang2-3131325-large.gif
2022,9657494,Fig. 3.,Architecture of the deep CNN used in DeepEM [59].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang3-3131325-large.gif
2022,9657494,Fig. 4.,(a) Simulated particles/nonparticles of a representative 3-D structure in the PARSED datasets. (b) Schematic representation of the PARSED model [61].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang4ab-3131325-large.gif
2022,9657494,Fig. 5.,"Differences in the clustering workflow of different clustering algorithms (a)
K
-means and (b) ML-based clustering.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang5ab-3131325-large.gif
2022,9657494,Fig. 6.,Process of projection matching and 3-D refinement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang6-3131325-large.gif
2022,9657494,Fig. 7.,Likelihood function and its trend with iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9702897/9657494/zhang7-3131325-large.gif
2022,9869838,FIGURE 1.,Methodology steps followed in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart1-3202896-large.gif
2022,9869838,FIGURE 2.,Number of articles published per year (line) and the average number of citations in each article (bars) (January 2000 to March 2022).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart2-3202896-large.gif
2022,9869838,FIGURE 3.,Number of articles applying ML in marketing published by JCR quartile every year (2008 to March 2022).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart3-3202896-large.gif
2022,9869838,FIGURE 4.,JCR journals with the highest number of published articles meeting the criteria (2008 to Mar 2022).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart4-3202896-large.gif
2022,9869838,FIGURE 5.,Classification of the most used types of learning and techniques in marketing research.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart5-3202896-large.gif
2022,9869838,FIGURE 6.,Distribution of the number of articles according to the type of ML (2008 to March 2022).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart6-3202896-large.gif
2022,9869838,FIGURE 7.,ML techniques percentage share use in solving marketing problems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart7-3202896-large.gif
2022,9869838,FIGURE 8.,"Percentage distribution of the number of articles published between 2008 and March 2022 by type of learning for each marketing problem (CB: consumer behavior, FC: forecasting, MS: market segmentation, RS: recommender system, TX: text and video analysis and context analysis).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart8-3202896-large.gif
2022,9869838,FIGURE 9.,Distribution of the number of articles by the main marketing problem they solve (2008 to March 2022).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9869838/duart9-3202896-large.gif
2022,9395220,Fig. 1.,Years of progress in the field of stereo vision and machine learning enable the estimation of depth maps of unprecedented quality from a) stereo or b) monocular images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893109/9395220/poggi1-3070917-large.gif
2022,9395220,Fig. 2.,"Overview of the most popular stereo datasets in literature, with examples of reference images and associated ground truth disparity. a) KITTI 2015 [9], b) Middlebury 2014 [8], c) ETH3D [10], d) Freiburg SceneFlow [11].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893109/9395220/poggi2-3070917-large.gif
2022,9395220,Fig. 3.,"Evolution of stereo algorithms. From left, reference image from KITTI 2015, disparity maps by SGM [26], MC-CNN-acrt [27] and DispNetC [11]. Learned matching costs outperform traditional pipelines, while end-to-end models perform even better in challenging regions (e.g., cars).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893109/9395220/poggi3-3070917-large.gif
2022,9395220,Fig. 4.,"Example of confidence estimation. From left to right, reference image from KITTI 2012 dataset, disparity map by MC-CNN-fst [27] raw algorithm and confidence estimation inferred by LGC-Net [95].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893109/9395220/poggi4-3070917-large.gif
2022,9395220,Fig. 5.,"Effects of domain-shift. On a KITTI 2015 stereo pair (top), a GWC-Net [89] instance trained on synthetic images [11] produces poor results (middle) on the road and in reflective surfaces. A short fine-tuning on KITTI 2012 dramatically improves the results (bottom).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893109/9395220/poggi5-3070917-large.gif
2022,9395220,Fig. 6.,"Evolution of stereo-supervised monocular depth estimation, showing results achieved through 2017 [94], 2018 [130] and 2019 [113].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893109/9395220/poggi6-3070917-large.gif
2022,9837043,FIGURE 1.,Class distribution of analyst directional prediction and realised stock directional movement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9837043/sidog1-3193141-large.gif
2022,9837043,FIGURE 2.,Proposed framework for stock price directional prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9837043/sidog2-3193141-large.gif
2022,9837043,FIGURE 3.,Feature importance for experiment 1.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9837043/sidog3-3193141-large.gif
2022,9837043,FIGURE 4.,Feature importance for experiment 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9837043/sidog4-3193141-large.gif
2022,9837043,FIGURE 5.,Feature importance for experiment 3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9837043/sidog5-3193141-large.gif
2022,9837043,FIGURE 6.,Feature importance for experiment 4.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9837043/sidog6-3193141-large.gif
2022,9863699,Fig. 1.,"Overview of the proposed pipeline. The Feature Extraction Module is composed of Mask R-CNN’s first layers trained offline on the FEATURE-TASK. The three sets of features for (i) region proposal (
F
r
), (ii) object detection (
F
d
), and (iii) instance segmentation (
F
s
) are fed to (i) the Online RPN, (ii) the Online Detection Module, and (iii) the Online Segmentation Module. At inference time, we substitute the final layers of the Mask R-CNN’s RPN with the Online RPN trained on the TARGET-TASK and, as in Mask R-CNN, the output of the Online Detection Module is fed as input to the RoI Align to compute the objects masks within the proposed bounding boxes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola1-3164331-large.gif
2022,9863699,Fig. 2.,"Online RPN. Given the feature map
F
r
, this is unrolled into
h×w
tensors of features of size
f
(
F
r
Unrolled). A subset of these features is chosen to train a FALKON classifier and four RLS regressors for each anchor.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola2-3164331-large.gif
2022,9863699,Fig. 3.,"Online segmentation. Given the feature map
F
s
associated to a RoI of class i, this is unrolled into
s×s
tensors of features of size
f
(
F
s
Unrolled) from which positive and negative features are sampled to train the
i
th
FALKON per-pixel classifier. Note that this procedure is performed for each RoI of the
N
classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola3-3164331-large.gif
2022,9863699,Fig. 4.,"Ours training protocol. We rely on the feature extraction layers of the Mask R-CNN pre-trained on the FEATURE-TASK to simultaneously extract
F
r
,
F
d
, and
F
s
. We then use these features to train the three online modules on the TARGET-TASK. The values on the arrows correspond to the training steps in Section III-D.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola4-3164331-large.gif
2022,9863699,Fig. 5.,"Ours Serial training protocol. We rely on the feature extraction layers of Mask R-CNN pre-trained on the FEATURE-TASK to extract
F
r
and we train the Online RPN on the TARGET-TASK. Then, we rely on the feature extraction layers of Mask R-CNN and on the Online RPN trained on the TARGET-TASK to extract
F
d
and
F
s
. Finally, we train the Online Detection Module and the Online Segmentation Module on the TARGET-TASK. The values on the arrows correspond to the training steps in Section VI-B.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola5-3164331-large.gif
2022,9863699,Fig. 6.,"Detection and segmentation mAPs for increasing number of Minibootstrap iterations for Ours and for increasing training time of the Mask R-CNN baselines, considering YCB-Video as TARGET-TASK. The plots show the average and the standard deviation of the accuracy obtained over three training sessions with the same parameters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola6-3164331-large.gif
2022,9863699,Fig. 7.,"We consider HO-3D as TARGET-TASK and we report the average and the standard deviation of the mAPs over three training sessions with the same parameters for increasing number of Minibootstrap iterations for Ours, and for increasing training time of Mask R-CNN (full), Mask R-CNN (output layers), and Mask R-CNN (store features).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola7-3164331-large.gif
2022,9863699,Fig. 8.,"Overview of the proposed robotic pipeline for online instance segmentation. At training time (solid arrows), a human teacher shows a new object to the robot, which automatically acquires the ground-truth annotations exploiting the depth information. Then, it extracts the features to train the online modules. At inference time (dashed arrows), the robot employs such modules to predict the masks of the images acquired by the camera.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola8-3164331-large.gif
2022,9863699,Fig. 9.,Predictions on test images from the incremental application deployed on the iCub.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola9-3164331-large.gif
2022,9863699,Fig. 10.,Dealing with false positives. Left image: an unknown object (a glass) is misclassified (as a masterchef). Center: training. The robot is provided with the correct label and a demonstration of the object. Right: after training the new object is correctly classified.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8860/9910236/9863699/ceola10-3164331-large.gif
2022,9832832,Fig. 1,Missing value imputation of sexual minority orientation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8964404/9832838/9832832/128-138-fig-1-source-large.gif
2022,9832832,Fig. 2,ROC curves of RF with different split strategies and different algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8964404/9832838/9832832/128-138-fig-2-source-large.gif
2022,9808122,Fig. 1.,Process overview for the design of a CWM method. Blocks with dashed lines represent the applied design/optimization methods and blocs with solid lines represent the final system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della1-3186625-large.gif
2022,9808122,Fig. 2.,Schematic representation of the signal processing and feature extraction processes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della2-3186625-large.gif
2022,9808122,Fig. 3.,Protocol of the experiment with the gamepad.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della3-3186625-large.gif
2022,9808122,Fig. 4.,Protocol of the experiment with FlyJacket.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della4-3186625-large.gif
2022,9808122,Fig. 5.,Cognitive workload perceived by participants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della5-3186625-large.gif
2022,9808122,Fig. 6.,Normalization methods impact on FDR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della6-3186625-large.gif
2022,9808122,Fig. 7.,Normalization methods impact on CWM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della7-3186625-large.gif
2022,9808122,Fig. 8.,"Best classifiers comparison on CV. Bigger markers denote the performance of the different models based on their corresponding cross-validated threshold or offset
b
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della8-3186625-large.gif
2022,9808122,Fig. 9.,Models performance comparison on a simulated online CWM (every 60 s). The first 180 s correspond to B task followed by F3M task for other 180 s.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9882959/9808122/della9-3186625-large.gif
2022,9709144,Fig. 1.,Methodological steps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9813470/9709144/oyebo1-3149862-large.gif
2022,9709144,Fig. 2.,Negative themes and the corresponding number of journal entries.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9813470/9709144/oyebo2-3149862-large.gif
2022,9709144,Fig. 3.,Positive themes and the corresponding number of journal entries.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9813470/9709144/oyebo3-3149862-large.gif
2022,9736635,Fig. 1.,The illustration of sensing modalities to be fused with sEMG in ML/DL based upper-limb motion estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9855434/9736635/bao1-3159792-large.gif
2022,9736635,Fig. 2.,Strategies to fuse sEMG and additional sensing modalities in ML/DL based upper-limb motion estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9855434/9736635/bao2-3159792-large.gif
2022,9736635,Fig. 3.,Applications of TL in myoelectric control. The arm position change and electrode shift are adapted from [87] and [88].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9855434/9736635/bao3-3159792-large.gif
2022,9736635,Fig. 4.,The typical structures of deep TL in upper-limb myoelectric control: (a) network-based deep TL; (b) feature-based deep TL. Note that unsupervised TL can be applied when the target labels/loss are unavailable in each structure. The discussion of supervised/unsupervised TL can be found in Section III.B3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9855434/9736635/bao4-3159792-large.gif
2022,9736635,Fig. 5.,"A typical workflow of confidence estimation for PR-based upper-limb motion estimation. To enhance the model reliability, confidences of classification results are estimated for a rejection or smoothing operation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9855434/9736635/bao5-3159792-large.gif
2022,9117188,Fig. 1.,"The three phases of the fairness-enhanced sampling framework: 1) where to sample, 2) how to sample and 3) how to train the model. Step 1 is to generate a new training dataset which consists of the original dataset and the pseudo labeled dataset. Step 2 is to construct multiple fair datasets through re-sampling. Step 3 is to train a model with each of the fair datasets through ensemble learning to produce the final predictions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9729922/9117188/zhu1-3002567-large.gif
2022,9117188,Fig. 2.,"The trade-off between accuracy (Red) and discrimination level (Blue). (a) LR in Health dataset; (b) SVM in Health dataset; (c) LR in Bank dataset; (d) SVM in Bank dataset; (e) LR in Adult dataset; (f) SVM in Adult dataset. The
X
-axis is the sample ratio
ρ
, which denotes that the percentage of
ρ
unlabeled data are sampled from the unlabeled dataset and then pseudo labeled for training.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9729922/9117188/zhu2-3002567-large.gif
2022,9117188,Fig. 3.,"The impact of ensemble learning on the accuracy (Red) and discrimination level (Blue) on (a) LR in Health dataset; (b) SVM in Health dataset; (c) LR in Bank dataset; (d) SVM in Bank dataset; (e) LR in Adult dataset; (f) SVM in Adult dataset. Initially, there is not obvious link between accuracy and discrimination level. However, as the ensemble size grows, the accuracy and discrimination level begin to converge. Each point is an average of 50 times.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9729922/9117188/zhu3-3002567-large.gif
2022,9117188,Fig. 4.,The impact of sample size on accuracy (Red) and discrimination level (Blue) on (a) LR in Health dataset; (b) SVM in Health dataset; (c) LR in Bank dataset; (d) SVM in Bank dataset; (e) LR in Adult dataset; (f) SVM in Adult dataset. An increasing in the sampling size leads to an increase in accuracy and may help to reduce discrimination level.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9729922/9117188/zhu4-3002567-large.gif
2022,9117188,Fig. 5.,"Comparison with original scheme (ORI), uniform sampling (US) and preferential sample (PS) with (a) LR in Health dataset; (b) SVM in Health dataset; (c) LR in Bank dataset; (d) SVM in Bank dataset; (e) LR in Adult dataset; (f) SVM in Adult dataset. With the fairness-enhanced sampling method (FS), discrimination decreases without much cost of accuracy or accuracy increases without much cost of discrimination.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9729922/9117188/zhu5-3002567-large.gif
2022,9761238,Fig. 1.,PMSM ITSC equivalent circuit model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh1-3169173-large.gif
2022,9761238,Fig. 2.,PMSM ITSC equivalent circuit model. (a) Stator part. (b) Experimental PMSM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh2ab-3169173-large.gif
2022,9761238,Fig. 3.,Short-circuit wiring diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh3-3169173-large.gif
2022,9761238,Fig. 4.,PMSM measurement setup. (a) Measurement setup. (b) Measurement schematic diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh4ab-3169173-large.gif
2022,9761238,Fig. 5.,Current waveforms for different ITSC levels at rated load. (a) ITSC level 1 (5%) at load 0.05 Nm. (b) ITSC level 2 (10%) at load 0.05 Nm. (c) ITSC level 3 (15%) at load 0.05 Nm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh5abc-3169173-large.gif
2022,9761238,Fig. 6.,Conversion of extracted data for SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh6-3169173-large.gif
2022,9761238,Fig. 7.,SVM training results with accuracy: (a) 99.75%; (b) 99.5%; and (c) 99.63%. (a) Dataset 20. (b) Dataset 100. (c) Dataset 200.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh7abc-3169173-large.gif
2022,9761238,Fig. 8.,Conversion of extracted data for CNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh8-3169173-large.gif
2022,9761238,Fig. 9.,CNN training results with accuracy: (a) 64%; (b) 97.5%; and (c) 98%. (a) Images 20. (b) Images 100. (c) Images 200.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh9abc-3169173-large.gif
2022,9761238,Fig. 10.,Comparison of prediction accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/20/9841496/9761238/hsieh10-3169173-large.gif
2022,9765963,FIGURE 1.,"Framework of transfer learning with lag, memory and classifiers aggregation for credit scoring (TLL).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham1-3171569-large.gif
2022,9765963,FIGURE 2.,Lending club dataset confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham2-3171569-large.gif
2022,9765963,FIGURE 3.,PPDai dataset confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham3-3171569-large.gif
2022,9765963,FIGURE 4.,German dataset confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham4-3171569-large.gif
2022,9765963,FIGURE 5.,Default dataset confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham5-3171569-large.gif
2022,9765963,FIGURE 6.,Accuracy result comparison of lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham6-3171569-large.gif
2022,9765963,FIGURE 7.,Accuracy result comparison of PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham7-3171569-large.gif
2022,9765963,FIGURE 8.,Accuracy result comparison of German dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham8-3171569-large.gif
2022,9765963,FIGURE 9.,Accuracy result comparison of Default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham9-3171569-large.gif
2022,9765963,FIGURE 10.,Accuracy club dataset time series comparison for Lending.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham10-3171569-large.gif
2022,9765963,FIGURE 11.,Accuracy time series comparison for German dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham11-3171569-large.gif
2022,9765963,FIGURE 12.,Accuracy time series comparison for Default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham12-3171569-large.gif
2022,9765963,FIGURE 13.,Accuracy time series comparison for PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham13-3171569-large.gif
2022,9765963,FIGURE 14.,Overall precision comparison result of Lending Club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham14-3171569-large.gif
2022,9765963,FIGURE 15.,Overall precision comparison result of PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham15-3171569-large.gif
2022,9765963,FIGURE 16.,Overall precision comparison result of German data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham16-3171569-large.gif
2022,9765963,FIGURE 17.,Overall precision comparison result of Default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham17-3171569-large.gif
2022,9765963,FIGURE 18.,Precision time series comparison for Lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham18-3171569-large.gif
2022,9765963,FIGURE 19.,Precision time series comparison for German dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham19-3171569-large.gif
2022,9765963,FIGURE 20.,Precision time series comparison for default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham20-3171569-large.gif
2022,9765963,FIGURE 21.,Precision time series comparison for PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham21-3171569-large.gif
2022,9765963,FIGURE 22.,Overall recall comparison result of Lending Club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham22-3171569-large.gif
2022,9765963,FIGURE 23.,Overall recall comparison result of PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham23-3171569-large.gif
2022,9765963,FIGURE 24.,Overall recall comparison result of German dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham24-3171569-large.gif
2022,9765963,FIGURE 25.,Overall recall comparison result of default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham25-3171569-large.gif
2022,9765963,FIGURE 26.,Recall time series comparison for lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham26-3171569-large.gif
2022,9765963,FIGURE 27.,Recall time series comparison for german dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham27-3171569-large.gif
2022,9765963,FIGURE 28.,Recall time series comparison for default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham28-3171569-large.gif
2022,9765963,FIGURE 29.,Recall time series comparison for PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham29-3171569-large.gif
2022,9765963,FIGURE 30.,Overall specificity comparison result of lending club data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham30-3171569-large.gif
2022,9765963,FIGURE 31.,Overall specificity comparison result of PPDai data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham31-3171569-large.gif
2022,9765963,FIGURE 32.,Overall specificity comparison result of german data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham32-3171569-large.gif
2022,9765963,FIGURE 33.,Overall specificity comparison result of default data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham33-3171569-large.gif
2022,9765963,FIGURE 34.,Specificity time series comparison for lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham34-3171569-large.gif
2022,9765963,FIGURE 35.,Specificity time series comparison for german dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham35-3171569-large.gif
2022,9765963,FIGURE 36.,Specificity time series comparison result for default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham36-3171569-large.gif
2022,9765963,FIGURE 37.,Specificity time series comparison result for PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham37-3171569-large.gif
2022,9765963,FIGURE 38.,Overall NPV comparison result of lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham38-3171569-large.gif
2022,9765963,FIGURE 39.,Overall NPV comparison result of PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham39-3171569-large.gif
2022,9765963,FIGURE 40.,Overall NPV comparison result of german dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham40-3171569-large.gif
2022,9765963,FIGURE 41.,Overall NPV comparison result of default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham41-3171569-large.gif
2022,9765963,FIGURE 42.,NPV time series comparison for lending Club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham42-3171569-large.gif
2022,9765963,FIGURE 43.,NPV time series comparison for german dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham43-3171569-large.gif
2022,9765963,FIGURE 44.,NPV time series comparison for default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham44-3171569-large.gif
2022,9765963,FIGURE 45.,NPV time series comparison for PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham45-3171569-large.gif
2022,9765963,FIGURE 46.,Overall F-Measure time series comparison for lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham46-3171569-large.gif
2022,9765963,FIGURE 47.,Overall F-Measure time series comparison for PPDai dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham47-3171569-large.gif
2022,9765963,FIGURE 48.,Overall F-Measure time series comparison for german dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham48-3171569-large.gif
2022,9765963,FIGURE 49.,Overall F-Measure time series comparison for Default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham49-3171569-large.gif
2022,9765963,FIGURE 50.,F-measure time series comparison for lending club dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham50-3171569-large.gif
2022,9765963,FIGURE 51.,F-measure time series comparison for german dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham51-3171569-large.gif
2022,9765963,FIGURE 52.,F-measure time series comparison for default dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9765963/moham52-3171569-large.gif
2022,9903303,Fig. 4.,Confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7782634/9898898/9903303/hashmi4-3209366-large.gif
2022,9737418,Fig. 1.,Cognitive optical network. EDFA: Erbium-doped fiber amplifier; ROADM: Reconfigurable optical add-drop multiplexer; WSS: Wavelength selective switch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb1-3160379-large.gif
2022,9737418,Fig. 2.,Cognitive lightpath QoT estimation tool.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb2-3160379-large.gif
2022,9737418,Fig. 3.,Evolution of SNR over time for 2 lightpaths. QoT threshold level set arbitrarily for illustration purposes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb3-3160379-large.gif
2022,9737418,Fig. 4.,Lightpath QoT forecasters based on recurrent neural networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb4-3160379-large.gif
2022,9737418,Fig. 5.,Multi-step prediction: (a) LSTM topology; (b) Encoder-Decoder LSTM; (c) Encoder-decoder LSTM with attention; (d) GRU.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb5-3160379-large.gif
2022,9737418,Fig. 6.,RMSE of the RNN models for forecast horizons ranging from 1 to 96 hours. From [28].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb6-3160379-large.gif
2022,9737418,Fig. 7.,Performance evaluation of the multivariate forecasting models: (a) RMSE; (b) AME. From [35].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb7-3160379-large.gif
2022,9737418,Fig. 8.,"Performance of the lightpath QoT forecasters, as obtained by executing the models on two different subsets of the MS and NASP datasets: (a) Subset 1; (b) Subset 2. From [29].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb8-3160379-large.gif
2022,9737418,Fig. 9.,Transfer learning process to forecast QoT of lightpath-2 using model pre-trained with lightpath-1 data. From [35].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb9-3160379-large.gif
2022,9737418,Fig. 10.,Performance evaluation (AME vs. forecast horizon) using KB-2 (lightpath carried in the same optical fiber on a portion of same route). From [35].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/50/9782885/9737418/tremb10-3160379-large.gif
2022,9994679,FIGURE 1.,Selected criteria for systematic literature review.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah1-3230983-large.gif
2022,9994679,FIGURE 2.,Article selection criteria for systematic literature review.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah2-3230983-large.gif
2022,9994679,FIGURE 3.,ENet architecture with bottleneck module.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah3-3230983-large.gif
2022,9994679,FIGURE 4.,DeepLabV3 Segmentation using Atrous convolutional.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah4-3230983-large.gif
2022,9994679,FIGURE 5.,BiseNet model of Semantic Segmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah5-3230983-large.gif
2022,9994679,FIGURE 6.,HarDNet model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah6-3230983-large.gif
2022,9994679,FIGURE 7.,GCNet layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah7-3230983-large.gif
2022,9994679,FIGURE 8.,Structure of EMA unit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah8-3230983-large.gif
2022,9994679,FIGURE 9.,a) Spatial wise Dynamic Affinity Modelling b) Channel wise dynamic affinity modelling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah9-3230983-large.gif
2022,9994679,FIGURE 10.,PP-LiteSeg working framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah10-3230983-large.gif
2022,9994679,FIGURE 11.,Overview of CDGCNet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah11-3230983-large.gif
2022,9994679,FIGURE 12.,DDRnets architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah12-3230983-large.gif
2022,9994679,FIGURE 13.,Working method for the proposed systematic literature review.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah13-3230983-large.gif
2022,9994679,FIGURE 14.,Frequency graph of selected papers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah14-3230983-large.gif
2022,9994679,FIGURE 15.,Heat map diagram of survey papers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9994679/shah15-3230983-large.gif
2022,9907008,FIGURE 1.,Data preparation workflow for machine learning-based IDS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding1-3211313-large.gif
2022,9907008,FIGURE 2.,Experiment workflow (a) Data processing (b) Model evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding2ab-3211313-large.gif
2022,9907008,FIGURE 3.,The pre-trained models’ performances when trained on unclean data and the overlapped rates of each dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding3-3211313-large.gif
2022,9907008,FIGURE 4.,"Cosine similarity distribution (a) Normal distribution group, (b) Negatively skewed distribution group.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding4ab-3211313-large.gif
2022,9907008,FIGURE 5.,"Performance comparison when trained with clean data versus with unclean data from ADFA-LD dataset (a) DT performance, (b) RF performance, (c) BERT performance, (d) GPT performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding5abcd-3211313-large.gif
2022,9907008,FIGURE 6.,"Performance comparison when trained with clean data versus with unclean data from the Login and Ps dataset (a) DT performance, (b) RF performance, (c) BERT performance, (d) GPT performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding6abcd-3211313-large.gif
2022,9907008,FIGURE 7.,"Performance comparison when trained with clean data versus with unclean data from the Stide dataset (a) DT performance, (b) RF performance, (c) BERT performance, (d) GPT performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding7abcd-3211313-large.gif
2022,9907008,FIGURE 8.,"Stable performances of the pre-trained models and the classic ML models when evaluated with an increasing duplication rate on the ADFA-LD dataset (a) Macro F1 score, (b) AUC score, (c) FPR, and (d) FNR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding8abcd-3211313-large.gif
2022,9907008,FIGURE 9.,"Deflated performances of the pre-trained models and the classic ML models when evaluated with an increasing duplication rate on the Inetd dataset (a) Macro F1 score, (b) AUC score, (c) FPR, and (d) FNR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding9abcd-3211313-large.gif
2022,9907008,FIGURE 10.,"Deflated performances of the pre-trained models and inflated performances of the classic ML models when evaluated with an increasing duplication rate on the Live Named dataset (a) Macro F1 score, (b) AUC score, (c) FPR, and (d) FNR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding10abcd-3211313-large.gif
2022,9907008,FIGURE 11.,"Unstable performances of both groups of models when evaluated with an increasing duplication rate on the Live Lpr dataset (a) Macro F1 score, (b) AUC score, (c) FPR, and (d) FNR.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding11abcd-3211313-large.gif
2022,9907008,FIGURE 12.,Cosine similarity is reduced in longer input sequence length from the Inetd dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9907008/ding12-3211313-large.gif
2022,9280414,Fig. 1.,"PCA applied to RNA-seq samples from TCGA and ARCHS4, after pre-processing the raw count data. The fact that PCA is not able to separate the data indicates that there may not be significant batch effects across them. ARCHS4 breast samples and TCGA all cancer samples (left); only Breast samples (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak1-3042309-large.gif
2022,9280414,Fig. 2.,"Architecture of the FFNN used for the experiments. The size of the hidden layers is either (300,100) or (100,20).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak2-3042309-large.gif
2022,9280414,Fig. 3.,Architecture of the FFNN used for semi-supervised learning. Outputs now include other tumor types in addition to breast cancer subtypes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak3-3042309-large.gif
2022,9280414,Fig. 4.,Using a Variational Autoencoder (VAE) for semi-supervised learning. 1) VAE network training; 2) Classifier training and encoder fine-tuning for breast cancer subtyping.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak4-3042309-large.gif
2022,9280414,Fig. 5.,"PCA of the latent representations obtained in two different steps of the VAE training procedure. After the unsupervised training with TCGA non-BRCA (left); after the supervised training of the stacked classifier with TCGA BRCA, while fine-tuning the encoder weights (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak5-3042309-large.gif
2022,9280414,Fig. 6.,"PCA of the latent representations obtained in two different steps of the VAE training procedure. After the unsupervised training with ARCHS4 breast tissue (left); after the supervised training of the stacked classifier with TCGA BRCA, while fine-tuning the encoder weights (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak6-3042309-large.gif
2022,9280414,Fig. 7.,"PCA of TCGA BRCA training data, with corresponding subtypes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9702518/9280414/canak7-3042309-large.gif
2022,9477031,Fig. 1.,"The illustration shows the adaptive progressive network framework when the
t
th task arrives. The network controller focuses on building a task network for current task
t
adaptively.(e.g., RCL uses reinforcement learning to determine the number of nodes or convolution kernels for expanding network while BOCL employs Bayesian optimization to conduct this.) The gray area in the left of illustration means that the task network parameters from task 1 to task
t−1
are fixed and only perform inference procedure. The information is passed to current
t
th task network through knowledge extractor . For example, the knowledge extractor can be simply implemented by connecting the feature maps from task network 1 to
t−1
directly to current task network
t
(e.g., in RCL) or can employ a attention mechanism to select the useful ones to pass (e.g., in BOCL).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu1-3095064-large.gif
2022,9477031,Fig. 2.,"(a) RCL adaptively expands each layer of the network when
t
th task arrives. (b) The controller implemented as a RNN to determine how many filters to add for the new task.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu2-3095064-large.gif
2022,9477031,Fig. 3.,"(a) BOCL expands the network adaptively when
t
th task arrives. (b) How Bayesian optimization works in our BOCL.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu3-3095064-large.gif
2022,9477031,Fig. 4.,(a) Average test accuracy for all datasets. (b) Number of parameters after all tasks finished. (c) Training time for different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu4-3095064-large.gif
2022,9477031,Fig. 5.,"Average test accuracy v.s. model complexity for RCL, DEN and PGN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu5-3095064-large.gif
2022,9477031,Fig. 6.,Test accuracy on the first task as more tasks are learned.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu6-3095064-large.gif
2022,9477031,Fig. 7.,Average test accuracy on all tasks as the number of trials increases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu7-3095064-large.gif
2022,9477031,Fig. 8.,"Experiments on the influence of the parameter
α
for evaluating the performance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893033/9477031/zhu8-3095064-large.gif
2022,9703375,FIGURE 1.,Android security framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple1-3149053-large.gif
2022,9703375,FIGURE 2.,Static binary matrix extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple2-3149053-large.gif
2022,9703375,FIGURE 3.,Taxonomy of android architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple3-3149053-large.gif
2022,9703375,FIGURE 4.,Reverse engineering APK files architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple4-3149053-large.gif
2022,9703375,FIGURE 5.,Taxonomy of android manifest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple5-3149053-large.gif
2022,9703375,FIGURE 6.,Machine learning process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple6-3149053-large.gif
2022,9703375,FIGURE 7.,Flow analysis of our research.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple7-3149053-large.gif
2022,9703375,FIGURE 8.,Proposed methodology of our system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple8-3149053-large.gif
2022,9703375,FIGURE 9.,Training model Processing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple9-3149053-large.gif
2022,9703375,FIGURE 10.,Graph of application threat increase by 5%.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple10-3149053-large.gif
2022,9703375,FIGURE 11.,Increase in android malware statistics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple11-3149053-large.gif
2022,9703375,FIGURE 12.,Third-party well-known dangerous apps increase from 2013 to 2020.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple12-3149053-large.gif
2022,9703375,FIGURE 13.,Boosting mechanism.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple13-3149053-large.gif
2022,9703375,FIGURE 14.,Representation of the modules of our program.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple14-3149053-large.gif
2022,9703375,FIGURE 15.,Program parameters and split functions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple15-3149053-large.gif
2022,9703375,FIGURE 16.,Fit and pred function for SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple16-3149053-large.gif
2022,9703375,FIGURE 17.,Predictive measures for AdaBoost.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple17-3149053-large.gif
2022,9703375,FIGURE 18.,Results stored to acc variable and plotted by plt.bar function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple18-3149053-large.gif
2022,9703375,FIGURE 19.,Models accuracy percentage w.r.t label.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple19-3149053-large.gif
2022,9703375,FIGURE 20.,Prediction function for SVM for testing data for the database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple20-3149053-large.gif
2022,9703375,FIGURE 21.,Prediction function for AdaBoost for testing data for the database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple21-3149053-large.gif
2022,9703375,FIGURE 22.,Output [1] representing the benign application (SVM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple22-3149053-large.gif
2022,9703375,FIGURE 23.,Output [0] representing the malware application (SVM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple23-3149053-large.gif
2022,9703375,FIGURE 24.,Output [1] representing the benign application (AdaBoost).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple24-3149053-large.gif
2022,9703375,FIGURE 25.,Output [0] representing the malware application (AdaBoost).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple25-3149053-large.gif
2022,9703375,FIGURE 26.,Orange entries for hon-harmful applications in AdaBoost.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple26-3149053-large.gif
2022,9703375,FIGURE 27.,Black entries for harmful applications in AdaBoost.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple27-3149053-large.gif
2022,9703375,FIGURE 28.,Orange entries for non-harmful applications in SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple28-3149053-large.gif
2022,9703375,FIGURE 29.,Black entries for Harmful applications in SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple29-3149053-large.gif
2022,9703375,FIGURE 30.,Comparative analysis of malicious and benign in Adaboost and SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9703375/maple30-3149053-large.gif
2022,9233921,Fig. 1.,Magic behind deep learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin1-3032422-large.gif
2022,9233921,Fig. 2.,Covering numbers of different sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin2-3032422-large.gif
2022,9233921,Fig. 3.,"The role of depth for approximating
t
2
using SGD.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin3-3032422-large.gif
2022,9233921,Fig. 4.,Bias-variance trade-off for ERM on deep nets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin4-3032422-large.gif
2022,9233921,Fig. 5.,Network architectures of various depths and widths.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin5-3032422-large.gif
2022,9233921,Fig. 6.,MSE curves of networks with various structures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin6-3032422-large.gif
2022,9233921,Fig. 7.,Networks with various width distributions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin7-3032422-large.gif
2022,9233921,Fig. 8.,Adaptivity of the feature to structures.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin8-3032422-large.gif
2022,9233921,Fig. 9.,Best results from networks of different depths.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin9-3032422-large.gif
2022,9233921,Fig. 10.,The generalization error result of deep nets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin10-3032422-large.gif
2022,9233921,Fig. 11.,Dense synthetic MMI map generated by (20).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin11-3032422-large.gif
2022,9233921,Fig. 12.,MSE curve of network during training on synthesis synthetic intensity dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin12-3032422-large.gif
2022,9233921,Fig. 13.,U.S. earthquake intensity data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9729045/9233921/lin13-3032422-large.gif
2022,9028182,Fig. 1.,"Human actions in the KTH (the first row), UCF11 (the second row), and HMDB (the third row). (a) Boxing. (b) Handclapping. (c) Jogging. (d) Running. (e) Basketball. (f) Biking. (g) Diving. (h) Golf. (i) BrushHair. (j) Catch. (k) Chew. (l) Clap.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang1abcdefghijkl-2973300-large.gif
2022,9028182,Fig. 2.,2-D DN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang2-2973300-large.gif
2022,9028182,Fig. 3.,"Sample of 3DDN with a single-hidden layer, where the inputs are four continuous frames in the RGB format (
K
2
=4
), and three representative filters (
K
1
=3
). Noted that
f
1
={
f
1
1
,…,
f
1
K
1
}
, and each
f
1
k
consists of
K
0
color channels
f
1
k,1
,…,
f
1
k,
K
0
(
K
0
=3)
. The deconvolution layer is a convolutional form of sparse coding that decomposes several continuous frames into feature
z
. The feature maps are convolved, respectively, together with filters and sum to reconstruct
Y
. The direction of arrow indicates that the filter is moving in this direction, showing that each feature is convolved with two filters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang3-2973300-large.gif
2022,9028182,Fig. 4.,"Visualization of a 3DDN model with two deconvolution layers. Each deconvolution layer is a convolution of sparse coding that decomposes the input frames into feature maps
z
(green) and learned filters
f
(red). In this example of 3DNN, the input video has four continuous frames in the RGB format, and two filters in each layer. The second deconvolution layer is conceptually identical to the first one, but has different channels. In practice, it can have many more filters and feature maps per layer dependent on specific problems.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang4-2973300-large.gif
2022,9028182,Fig. 5.,"Reconstructed frames in the KTH (the first row), UCF-101 (the second row), and HMDB (the third row). (a) Boxing. (b) Handclapping. (c) Handwaving. (d) Basketball. (e) Golf. (f) Horse. (g) Catch. (h) Hair. (i) Wave.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang5abcdefghi-2973300-large.gif
2022,9028182,Fig. 6.,Total loss against the number of training epoch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang6-2973300-large.gif
2022,9028182,Fig. 7.,"Classification accuracy for varied
λ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang7-2973300-large.gif
2022,9028182,Fig. 8.,Classification accuracy for varied the number of filters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang8-2973300-large.gif
2022,9028182,Fig. 9.,Classification accuracy for varied number of feature maps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang9-2973300-large.gif
2022,9028182,Fig. 10.,Two samples of video captioning from the MSVD dataset. (a) Person is cutting some meat. (b) Woman is boiling peppers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9678110/9028182/zhang10ab-2973300-large.gif
2022,9830835,FIGURE 1.,An example of a networked control system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9830835/sedgh1-3191343-large.gif
2022,9830835,FIGURE 2.,Event triggered learning diagram [15].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9830835/sedgh2-3191343-large.gif
2022,9830835,FIGURE 3.,NN-based ETC [17].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9830835/sedgh3-3191343-large.gif
2022,9830835,FIGURE 4.,Event-triggered Adaptive Critic Architecture. [49].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9830835/sedgh4-3191343-large.gif
2022,9830835,FIGURE 5.,Joint Learning of System and Network Model [124].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9830835/sedgh5-3191343-large.gif
2022,9845681,Fig. 1.,CRAV-Net system model for 5G-based IoV networks in an urban city scenario.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen1-3195425-large.gif
2022,9845681,Fig. 2.,"DL-based SS model architecture.
M=
Module, ASPP = Atrous spatial pyramid pooling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen2-3195425-large.gif
2022,9845681,Fig. 3.,SU vehicles stay time calculation in the CRAV-Net framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen3-3195425-large.gif
2022,9845681,Fig. 4.,Block diagram of the SVM classifier’s training and testing phases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen4-3195425-large.gif
2022,9845681,Fig. 5.,"Representation of true positives (
T
P
), true negatives (
T
N
), false positives (
F
P
), and false negatives (
F
N
) in the CRAV-Net framework.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen5-3195425-large.gif
2022,9845681,Fig. 6.,"Augmentation techniques: (a) original spectrogram, (b) reflection, (c) rotation, (d) scaling, (e) translation, and (f) multiple transformations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen6abcdef-3195425-large.gif
2022,9845681,Fig. 7.,Performance comparison of the DL-based SS model on OTA signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen7-3195425-large.gif
2022,9845681,Fig. 8.,"P
D
versus
P
FA
performance comparison of the DL-based SS model: (a) traditional methods and (b) DL-based methods.
P
D
versus
SNR
performance comparison of the DL-based SS model: (c) traditional methods and (d) DL-based methods.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9973413/9845681/chen8abcd-3195425-large.gif
2022,9875000,Fig. 1.,"Translation from transistor duty cycle to
Δ
V
th
[5]. The depicted curve is obtained from the physics-based BTI aging model [7] and configured for 14 nm FinFET at 0.8 V supply voltage. The steep increase near 100% duty cycle is due to the difference how stress is perceived in AC/DC operation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm1-3201431-large.gif
2022,9875000,Fig. 2.,"Due to the complexity of a full cell library, all data is split into smaller components, e.g., all delays correlated to one cell and timing arc. Per component, one estimator is trained to learn these targets as a function of the corresponding features of the cell. Shown exemplary values are rounded.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm2-3201431-large.gif
2022,9875000,Fig. 3.,"The complex information stored in the standard cell library is separated into multiple smaller components. This allows us to build and optimize an individual ML model for each cell component. For instance, all delays for different input-slew/output-load combinations for a single timing arc grouped into one component and thus learned by one ML component estimator.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm3-3201431-large.gif
2022,9875000,Fig. 4.,"The active learning paradigm. The training set is gradually enlarged with the most valuable samples from a large pool of unlabeled candidates. After selecting an unlabeled sample based on the uncertainty of the ML model, the oracle (in our case, SPICE simulations) generates the missing labels to complete the sample for the next iteration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm4-3201431-large.gif
2022,9875000,Fig. 5.,"Example demonstrating random sampling for a two-transistor inverter cell. The color gradient from blue to red reflects the order of selection. Since all samples are selected randomly, they roughly follow the
Δ
V
th
distribution as shown in Fig. 1.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm5-3201431-large.gif
2022,9875000,Fig. 6.,The greedy sampling process for a two-transistor inverter cell. Samples are selected as evenly distributed as the unlabeled pool allows.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm6-3201431-large.gif
2022,9875000,Fig. 7.,"Active learning for a two-transistor inverter cell. The first 10 samples are selected randomly. Afterwards, 8 committees vote and agree upon the subsequently selected samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm7-3201431-large.gif
2022,9875000,Fig. 8.,"Combining active learning and greedy sampling to prevent the selection of redundant samples when operating in batch mode. Per learning iteration, 10 samples are selected in one batch before estimators are retrained.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm8-3201431-large.gif
2022,9875000,Fig. 9.,"The ML architecture of our framework implementation. Layer-by-layer, more complex estimators are built by combining and abstracting simpler estimators away. Each oval shape indicates a class implementing the Scikit-learn estimator interface. The Custom Estimator is used for passive learning strategies, whereas active learning strategies require another layer of abstraction with committees and the voting among them.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm9-3201431-large.gif
2022,9875000,Fig. 10.,Average learning curves of all estimators with pre-configured hyperparameters for a training set size of 1000 samples. The “combined” curve selects kernel ridge (polynomial) or k-nearest neighbors regression for each component individually. All curves closely resemble the results presented in Fig. 3 in [3].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm10-3201431-large.gif
2022,9875000,Fig. 11.,"Average learning curves of all estimators using random sampling. Compared to the state-of-the-art “combined” approach, the periodical re-optimization of hyperparameters gives a significant performance boost while the training set is still relatively small.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm11-3201431-large.gif
2022,9875000,Fig. 12.,Comparing the performance of greedy and random sampling on a few example cells. The selected examples show the tendency of greedy sampling to perform better for cells with fewer numbers of transistors and worst for cells with larger numbers of transistors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm12-3201431-large.gif
2022,9875000,Fig. 13.,"Distribution of cell sizes among cells evaluated in this work. Although greedy sampling tends to work well for most of the cells, there are also a few larger cells for which random sampling yields better results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm13-3201431-large.gif
2022,9875000,Fig. 14.,"Comparing the performance of active learning strategies with and without batch-wise sampling. Our active learning approach using k-nearest neighbors (kNN) outperforms regular uncertainty sampling, even with a batch size of 10 samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm14-3201431-large.gif
2022,9875000,Fig. 15.,"For conventional active learning, increasing the batch size will also result in a slightly lower learning rate, even if both compared learners selected the same number of samples in total. However, for active learning with k-nearest neighbors, this disadvantage becomes almost insignificant.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm15-3201431-large.gif
2022,9875000,Fig. 16.,"Showcasing our active learning strategy on the same three examples as cells shown in Fig. 12. For the majority of cells, active learning performs better or on par with the passive learning strategies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm16-3201431-large.gif
2022,9875000,Fig. 17.,"Comparison of all presented learning strategies against the state-of-the-art approach. Although our learning strategies show comparable performances at first glace, there can be a considerable gap in required training samples, depending on the targeted accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm17-3201431-large.gif
2022,9875000,Fig. 18.,"Timing guardband estimations on a RISC-V “PULP” processor [25], evaluated with Static Timing Analysis (STA). While a timing guardband of
0 %
represents the nominal critical path delay of the circuit without the consideration of aging-induced degradations, a timing guardband of
100 %
reflects the circuit’s critical path delay at worst-case aging. Workloads are obtained from [26].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8919/9985434/9875000/klemm18-3201431-large.gif
2022,9709708,Figure 1.,Block diagram of the proposed prediction method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen1-3147376-large.gif
2022,9709708,Figure 2.,"March 08, 2012, 00:00, ACE-SIS Carbon and GOES-15 proton reconstructed hourly energy spectra with respect to the CREME96 SPE models, respectively. Dashed lines correspond to the corresponding extrapolation method flux fitting results. Squares correspond to the ACE-SIS hourly carbon flux. Triangles correspond to the hourly proton in the GOES low energy detector. Circles correspond to the hourly proton in the GOES HEPAD detector.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen2-3147376-large.gif
2022,9709708,Figure 3.,"March 6-11, 2012 SRAM hourly SEU rate estimated from GOES proton database. The particle flux for all of the lower to higher energy channels are shown, and all channels data are with good quality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen3-3147376-large.gif
2022,9709708,Figure 4.,"March 6-11, 2012, SRAM hourly SEU rate estimated from ACE-SIS heavy-ion database. The particle ion flux of He, C and Ni for the lower and higher energy channels are shown, and the data are with poor quality.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen4-3147376-large.gif
2022,9709708,Figure 5.,"R
2
scores (higher the better) for the selected regression models with varying history data length nh.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen5-3147376-large.gif
2022,9709708,Figure 6.,RMSE (lower the better) for the selected regression models with varying history data length nh.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen6-3147376-large.gif
2022,9709708,Figure 7.,Block diagram of the proposed online SEU prediction function parameter adjustment procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen7-3147376-large.gif
2022,9709708,Figure 8.,"R
2
  score for the online learning evaluation on the data set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen8-3147376-large.gif
2022,9709708,Figure 9.,Proposed hardware accelerator design with interface to external logic.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen9-3147376-large.gif
2022,9709708,Figure 10.,Decision flowchart for the hardware accelerator.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen10-3147376-large.gif
2022,9709708,Figure 11.,"Hardware accelerator SEU prediction performance for 2Gbit and 4 Mbit SRAM during large and small SPEs, respectively. The Equation (5) is the prediction function with history data length 17 and magnification factor 32. The Equation (9) represents the prediction with history data length 4 and magnification factor 1024.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen11-3147376-large.gif
2022,9709708,Figure 12.,"Hardware accelerator SEU prediction performance for 20 Mbit SRAM during a small SPE on Mar 08, 2011.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen12-3147376-large.gif
2022,9709708,Figure 13.,The gradient descent with contour plot respect to the Coef1 and Coef3 under the different learning rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen13-3147376-large.gif
2022,9709708,Figure 14.,The prediction performance for online learning from scratch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen14-3147376-large.gif
2022,9709708,Figure 15.,The prediction performance for online learning optimizes the offline prediction function on the new data set.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen15-3147376-large.gif
2022,9709708,Figure 16.,Decision flow for the optimal operation mode selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245516/9789501/9709708/chen16-3147376-large.gif
2022,9165021,Fig. 1.,Stacked architecture and the learning process in DCOT-LS-SVMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9778253/9165021/wang1-3008963-large.gif
2022,9165021,Fig. 2.,Changes of the classification performance of DCOT-LS-SVMs and IDCOT-LS-SVMs as the layer grows.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9778253/9165021/wang2-3008963-large.gif
2022,9501999,Fig. 1.,Data pre-processing steps.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa1-3100758-large.gif
2022,9501999,Fig. 2.,Pre-trained GoogLeNet layers and parameter details [23].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa2-3100758-large.gif
2022,9501999,Fig. 3.,Architecture of proposed model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa3-3100758-large.gif
2022,9501999,Fig. 4.,Training progress of the proposed model. (a) Proposed GoogLeNet with Softmax classifier. (b) Proposed GoogLeNet with SVM classifier. (c) Proposed GoogLeNet with K-NN classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa4-3100758-large.gif
2022,9501999,Fig. 5.,Training Loss of the proposed model. (a) Proposed GoogLeNet with Softmax classifier. (b) Proposed GoogLeNet with K-NN classifier. (c) Proposed GoogLeNet with SVM classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa5-3100758-large.gif
2022,9501999,Fig. 6.,"Confusion matrix of the proposed model. (a) proposed GoogLeNet using softmax classifier, (b) proposed GoogLeNet using SVM classifier and (c) proposed GoogLeNet using K-NN classifier.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa6-3100758-large.gif
2022,9501999,Fig. 7.,"ROC of proposed CNN. (a) Softmax, (b) SVM, and (c) K-NN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9729643/9501999/biswa7-3100758-large.gif
2022,9817116,Fig. 1.,Iterative learning based estimation for the photon-counting Poisson channel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9882525/9817116/chung1-3189030-large.gif
2022,9817116,Fig. 2.,Photon-counting Poisson channel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9882525/9817116/chung2-3189030-large.gif
2022,9817116,Fig. 3.,Estimation performance and mean squared error in the proposed technique relative to the number of iterations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9882525/9817116/chung3-3189030-large.gif
2022,9817116,Fig. 4.,Performance comparison of the proposed technique with the least square and the Viterbi detection technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9882525/9817116/chung4-3189030-large.gif
2022,9817116,Fig. 5.,Time-complexity analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4234/9882525/9817116/chung5-3189030-large.gif
2022,9392296,Fig. 1.,"Illustration of the curriculum learning (CL) concept (The fruit images are from [106]). CL is a training strategy for machine learning that trains from easier data to harder data, imitating human curricula. Specifically, CL initially trains the model on a small and easy subset. With the progress of the training, CL gradually introduces more harder examples into the subset, and finally trains the model on the whole training dataset. This CL strategy can improve both model performance and convergence rate, compared with direct training on the whole training dataset.
Q
t
here stands for a reweighting of the training data distribution
P
at the
t
th training epoch (See details in Section 2).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang1-3069908-large.gif
2022,9392296,Fig. 2.,"A categorization of CL methods and the corresponding illustrations. We divide the existing methods into predefined CL and automatic CL, the latter of which including Self-paced Learning, Transfer Teacher, RL Teacher and Other Automatic CL. As shown in the illustrations, most CL methods comply with the general framework of Difficulty Measurer
+
Training Scheduler in Section 4.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang2-3069908-large.gif
2022,9392296,Fig. 3.,"Illustration of the continuation method from [5], which is the essence of the CL [6]. It starts from optimizing a heavily smoothed version of the objective, and gradually moves to the target objective. Tracking the local minima throughout the training guides the model towards better parameter space and makes it more generalizable.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang3-3069908-large.gif
2022,9392296,Fig. 4.,"Illustration of the CL from the data distribution perspective [27]. The left part demonstrates the data distribution shifts from the easy subset (the solid curve, which is assumed to approximate the testing distribution
P
target
(x)
well) to the full training set
P
train
(x)
(the red dashed curve). The right part shows the corresponding weighting scheme to enable this distribution shift. The center peak of curves refers to the high-confidence clean data, while the tails refer to the noisy data in the distributions. As shown in the left part,
P
target
(x)
is cleaner than
P
train
(x)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang4-3069908-large.gif
2022,9392296,Fig. 5.,"Visualization of common continuous schedulers. The horizontal axis
t
stands for the training epoch number, and the vertical axis
λ
is the corresponding proportion of the easiest training data subset. Baseline is without curriculum and involves the whole training set from the beginning. The Baby Step scheduler is also visualized for comparison.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang5-3069908-large.gif
2022,9392296,Fig. 6.,"Visualization of functions of best example weight
v
∗
i
w.r.t. losses
l
i
(the
l
-
v
∗
functions) of the SP-regularizers in Table 5. The age parameter
λ
(the threshold for non-zero weights) for many of the functions are set as 0.8. The Huber, Cauchy, L1-L2, and Welsch belong to the implicit SP-regularizers in [16], which are not presented in the table.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang6-3069908-large.gif
2022,9392296,Fig. 7.,"Illustration of different machine learning paradigms from the perspective of data distribution. Different paradigms aim to solve different distribution discrepancies among training and testing data, while we see similar mechanisms among some of them, which help us understand their connections and may potentially inspire new methodologies. For curriculum learning, we illustrate Definition 2, and the curriculum can be both predefined and automatically learned. Note that
T
j
stands for different tasks, while
T
(i)
is the modified distribution at the
i
th step in training.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9893110/9392296/wang7-3069908-large.gif
2022,9380770,Fig. 1.,Architecture of BLS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang1-3061094-large.gif
2022,9380770,Algorithm 1:,Basic BLS [17],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang10-3061094-large.gif
2022,9380770,Fig. 2.,"Dynamic expansion of BLS, including the increment of additional enhancement nodes, feature mapping nodes, and input samples.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang2-3061094-large.gif
2022,9380770,Fig. 3.,Road map of BLSs. Milestone variants are shown in this figure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang3-3061094-large.gif
2022,9380770,Fig. 4.,Structure of CEBLS-dense: cascade of enhancement nodes with dense connections proposed by Zhang et al. [46].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang4-3061094-large.gif
2022,9380770,Fig. 5.,BLS structure with recurrent feature nodes proposed by Du et al. [63]. (a) Recurrent-BLS. (b) Long short-term memory (LSTM)-like architecture: gated-BLS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang5ab-3061094-large.gif
2022,9380770,Algorithm 2:,Recurrent-BLS [63],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang11-3061094-large.gif
2022,9380770,Algorithm 3:,LSTM-Like Architectures: Gated-BLS [63],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang12-3061094-large.gif
2022,9380770,Fig. 6.,Structure of an R-BLS proposed by Xu et al. [64].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang6-3061094-large.gif
2022,9380770,Fig. 7.,Two different architectures of convolutional BLS. (a) Cascade of convolution feature mapping nodes (CCFBLS). (b) Broad convolutional neural network (BCNN).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang7ab-3061094-large.gif
2022,9380770,Algorithm 4:,Weighted BLS [77],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang13-3061094-large.gif
2022,9380770,Fig. 8.,Three different architectures of multiview BLS. (a) MvBLS with shared enhancement features. (b) MvBLS2 with respective enhancement features. (c) MvBLS3 with both shared and respective enhancement features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang8abc-3061094-large.gif
2022,9380770,Fig. 9.,Structure of an robust manifold BLS (RM-BLS) proposed by Feng et al. [96].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang9-3061094-large.gif
2022,9380770,Algorithm 5:,SS-BLS [112],https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/9861395/9380770/zhang14-3061094-large.gif
2022,9594448,Fig. 1.,Schematic diagram of radio wave propagation problem.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li1-3123836-large.gif
2022,9594448,Fig. 2.,Schematic diagram of subdivision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li2-3123836-large.gif
2022,9594448,Fig. 3.,Schematic diagram of DD algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li3-3123836-large.gif
2022,9594448,Fig. 4.,Schematic diagram of wave propagation and parameters in SOM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li4-3123836-large.gif
2022,9594448,Fig. 5.,Flowchart of machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li5-3123836-large.gif
2022,9594448,Fig. 6.,Schematic diagram of calculation area and field value comparison. (a) Calculation range. (b) Label field by MoM. (c) Prediction field by 2W-PE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li6-3123836-large.gif
2022,9594448,Fig. 7.,Comparison of field strength in single obstacle environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li7-3123836-large.gif
2022,9594448,Fig. 8.,Comparison of field strength in two obstacles environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li8-3123836-large.gif
2022,9594448,Fig. 9.,Comparison of field strength distributions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9701835/9594448/li9-3123836-large.gif
2022,9865120,Fig. 1.,The process followed to develop the predictor of the effect of mutations in the transcription initiation codon.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9945616/9865120/ferna1-3200966-large.gif
2022,9865120,Fig. 2.,"Two feature selection techniques created for this work. For AskNGetN, each statistical test returns n features, and we keep the n most frequent. For AskNGetAll, each test also returns n features, but we keep each feature selected by at least one test.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9945616/9865120/ferna2-3200966-large.gif
2022,9865120,Fig. 3.,"Web form to perform predictions of mutations in the initiation codon with different input options: features, sequences, or Ensembl transcripts.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9945616/9865120/ferna3-3200966-large.gif
2022,9484718,Fig. 1.,"The generated CFG for the original sample and used for extracting graph-based features (graph size, centralities, etc.) for graph/program classification and malware detection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai1-3097296-large.gif
2022,9484718,Fig. 2.,"The CFG for the selected target sample generated and used for extracting graph-based features (graph size, centralities, etc.) for graph/program classification and malware detection.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai2-3097296-large.gif
2022,9484718,Fig. 3.,"The generated adversarial graph using GEA approach. Note that this graph is obtained logically by embedding the graph in Fig. 2 into the graph in Fig. 1, although indirectly done by injecting the code listings as highlighted in Listings 1, 2, and 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai3-3097296-large.gif
2022,9484718,Fig. 4.,"SGEA pattern extraction and AE generation process. SGEA uses CORK to extract discriminative subgraphs from each class. Then, the extracted subgraphs are embedded to generate the AEs. The process is terminated and the AE is returned upon successfully misclassifying the model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai4-3097296-large.gif
2022,9484718,Fig. 5.,"Sample of extracted discriminative subgraph from Gafgyt malicious family. Here, the graph size is 7, and the labels are arbitrary. Ideally, connecting this subgraph to a sample should lead the model to misclassify the sample into Gafgyt.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai5-3097296-large.gif
2022,9484718,Fig. 6.,"The internal architectural of the DNN used for the detection and classification tasks. The design consists of six fully connected layers, with dropout operations and softmax activation function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai6-3097296-large.gif
2022,9484718,Fig. 7.,"Internal design of the CNN architecture used for detection and classification task. Notice that 46@1x3, for example, refers to applying 46 filters each of size 1x3 on the input data. The design consists of four convolutional layers with maxpooling and dropout operations. Then, a dense layer of size 512 is used with a softmax activation function to output the model’s prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai7-3097296-large.gif
2022,9484718,Fig. 8.,"Confusion matrices of IoT malware classification systems. Here, each row represents the actual class, whereas, columns represents the predicted labels. Labels are Benign (B), Gafgyt (G), Mirai (M), and Tsunami (T).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai8-3097296-large.gif
2022,9484718,Fig. 9.,"GEA: Confusion matrices of IoT malware classification systems. Here, each row represents the actual class, whereas, columns represents the predicted labels. Labels are Benign (B), Gafgyt (G), Mirai (M), and Tsunami (T).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai9-3097296-large.gif
2022,9484718,Fig. 10.,"SGEA: Confusion matrices of IoT malware classification systems. Here, each row represents the actual class, whereas, columns represents the predicted labels. Labels are Benign (B), Gafgyt (G), Mirai (M), and Tsunami (T).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai10-3097296-large.gif
2022,9484718,Fig. 11.,"DL-FHMC system flow. First, corresponding CFGs of the IoT software are extracted, then, 23 algorithmic features are extracted from the CFGs. Afterward, an IoT malware detection system classifies samples into benign and malware, all malware samples are directed to IoT malware classification system, while benign samples are directed into suspicious behavior detection system (SBD) for further investigation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai11-3097296-large.gif
2022,9484718,Fig. 12.,"Suspicious behavior detection system design. The design consists of four modules, a subgraphs mining module to extract frequent subgraphs from three IoT malicious families. Afterward, the subgraphs are ranked by the pattern selection module, where the top 10,000 patterns of each malicious family are selected. Further, the CFG of each sample is redirected to the Suspicious Behavior Detector, and represented as a vector of size 30,000. The vector representation is fed to Suspicious Behavior Detector Model (SBDM) to be classified into benign and suspicious.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai12-3097296-large.gif
2022,9484718,Fig. 13.,DL-FHMC: RF-based suspicious behavior detector ROC performance over GEA and SGEA attacks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8858/9872146/9484718/mohai13-3097296-large.gif
2022,9451546,Fig. 1.,"Demonstration of sampling submatrices from matrices (The process described in Definition 3, which is also Steps 2 and 3 in Algorithm 3.). We sample columns from
X
to get
X
′
and sample rows from
X
′
to get
X
′′
. Note that
X
′
and
X
′′
are normalized such that
E[
X
′
X
′T
]=X
X
T
and
E[
X
′′T
X
′′
]=
X
′T
X
′
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9966944/9451546/huang1-3084467-large.gif
2022,9451546,Fig. 2.,"Quantum-inspired SVM algorithm. In the algorithm, the subsampling of
A
is implemented by subsampling the matrix
X
(Steps 1–3), which is called the indirect sampling technique. After the indirect sampling, we perform the spectral decomposition (Step 4). Then we estimate the approximation of the eigenvectors (
V
~
l
) of
A
(Step 5). Finally, we estimate the classification expression (Steps 6–8).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9966944/9451546/huang2-3084467-large.gif
2022,9451546,Fig. 3.,"Whole procedure of proving
∥
∥
V
~
Σ
−2
V
~
T
A−
I
m
∥
∥
≤(ϵ/2)
. Theorem 2 shows the difference among
A
and the subsampling outcomes
A
′
and
A
′′
. Theorem 3 shows the relation between
A
′
and
V
′′
l
. Theorem 4 shows the relation between
A
and
V
~
l
. Theorem 6 shows the final relation between
A
and
V
~
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9966944/9451546/huang3-3084467-large.gif
2022,9451546,Fig. 4.,"Average classification rate of quantum-inspired SVM algorithm with different parameters on the dataset with rank 1. Each point represents an average classification rate for 50 trials, and the error bar shows the standard deviation of the 50 trials. (a) Algorithm performance when the parameter
ϵ
is taken from 1 to 10. (b) Algorithm performance when the parameter
η
is taken from 0.1 to 1. (c) Algorithm performance when the parameter
b
is taken from 1 to 10.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9966944/9451546/huang4abc-3084467-large.gif
2022,9665717,Fig. 1.,"System and processing method overview. (a) General view of the placement of the sensors (bust, thigh and tibia), (b) placement of the sensors on the bust, (c) front view of the sensors on the thigh and tibia, (d) side view of the placement sensors on the thigh and tibia, (e) The raspberry pi 3+ and its reception module NRF24L01 +, (f) Breakdown of raw data from inertial sensors; (g) Realization of a complementary filter, (h) extracted data, (i) Data accumulation step before classification, (j) Preconditioning using MMSN, (k) Classification with an SVM with RBF kernel, (l) prediction associated movement.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9757031/9665717/mascr1abcdefghijkl-3139588-large.gif
2022,9665717,Fig. 2.,"Flow chart of the multi-mapping sphere normalization process. Blue (IMU1), Green (IMU2); Orange (IMU3). The number “12” is the amount of data inside the buffer of each sensor. The “fusion data” corresponds to the pitch, roll and yaw angles extracted from inertial data using a complementary filter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9757031/9665717/mascr2-3139588-large.gif
2022,9665717,Fig. 3.,Raw data (a) before and (b) after mapping onto an unit hypersphere using accelerometer data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9757031/9665717/mascr3ab-3139588-large.gif
2022,9665717,Fig. 4.,RBF SVM normalized confusion matrix of training/validation dataset using SN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9757031/9665717/mascr4-3139588-large.gif
2022,9665717,Fig. 5.,Real time prediction with subject 2 using MMSN and RBF-SVM. Read labels correspond to true labels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/9757031/9665717/mascr5-3139588-large.gif
2022,9183996,Fig. 1.,Proposed BiLRP method for explaining similarity. Produced explanations are in terms of pairs of input features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta1-3020738-large.gif
2022,9183996,Fig. 2.,Diagram of the map used by DTD to derive BiLRP propagation rules. The map connects activations at some layer to relevance in the layer above.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta2-3020738-large.gif
2022,9183996,Fig. 3.,"Illustration of our approach to compute BiLRP explanations:
A.
Input examples are mapped by the neural network up to the layer at which the similarity model is built.
B.
LRP is applied to all individual activations in this layer, and the resulting array of explanations is recombined into a single explanation of predicted similarity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta3-3020738-large.gif
2022,9183996,Fig. 4.,"Benchmark comparison on a toy example where we have ground-truth explanation of similarity. BiLRP performs better than all baselines, as measured by the average cosine similarity to the ground truth.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta4-3020738-large.gif
2022,9183996,Fig. 5.,"Effect of the BiLRP parameter
γ
on the average cosine similarity between the explanations and the ground truth.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta5-3020738-large.gif
2022,9183996,Fig. 6.,"Application of BiLRP to a dot-product similarity model built on VGG-16 features at layer 31. Top: BiLRP explanations on different pairs of input images from the Pascal VOC 2007 dataset. Red and blue color indicate positive and negative contributions to the similarity. (Details of the rendering procedure are given in Appendix E of the Supplement, available online.) Bottom: Effect of the BiLRP parameter
γ
on the explanation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta6-3020738-large.gif
2022,9183996,Fig. 7.,Application of BiLRP to study how VGG-16 similarity transfers to various datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta7-3020738-large.gif
2022,9183996,Fig. 8.,"Explanation of measured invariance at layer 31. Left: Similarity matrix associated to a selection of video clips. The diagonal band outlined in black contains the pairs of examples in
⟨⋅
⟩
local
. Right: BiLRP explanations for selected pairs from the diagonal band.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta8-3020738-large.gif
2022,9183996,Fig. 9.,"Pairs of illustrations from the Sphaera corpus, explained with BiLRP. The high similarity originates mainly from matching fixed features in the image rather than capturing the rotating elements.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta9-3020738-large.gif
2022,9183996,Fig. 10.,Pairs of illustrations from the Sphaera corpus and BiLRP explanation for the improved similarity model. Similarity captures rotating elements such as letters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta10-3020738-large.gif
2022,9183996,Fig. 11.,"A.
Collection of tables from the Sphaera Corpus [29] from which we extract two tables with identical content.
B.
Proposed ‘bigram network’ supporting the table similarity model.
C.
BiLRP explanations of predicted similarities between the two input tables.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9703108/9183996/monta11-3020738-large.gif
2022,9615015,Fig. 1.,The frame work of GPCRs interaction prediction based on deep transfer learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9976468/9615015/wu1-3128172-large.gif
2022,9615015,Fig. 2.,The first residue ALA and the 116th residue HIS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9976468/9615015/wu2-3128172-large.gif
2022,9615015,Fig. 3. The,first residue ALA and the 27th residue GLY.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9976468/9615015/wu3-3128172-large.gif
2022,9615015,Fig. 4.,Contact distribution of different proteins.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9976468/9615015/wu4-3128172-large.gif
2022,9615015,Fig. 5.,The 182th residue PHE and the 285th residue GLN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/9976468/9615015/wu5-3128172-large.gif
2022,9722870,Fig. 1.,"Schematic overview of applied data flow. For the result given in Fig. 5, the input data are extended by artificial EL images. For the results shown in Fig. 7, the test dataset is replaced by a larger set of field-measured EL images.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi1-3150725-large.gif
2022,9722870,Fig. 2.,"Top: Training dataset of 54 EL images separated into classes by expert choice (ground-truth). Class A serves as a reference with no obvious failure. Class B gives examples of the LeTID and class C shows examples of the PID. Classes B and C include images taken from the literature: in B: #17 [9] and #18 [4] and in C image #2 [10], #4 [11], and #6 [12]. Bottom: The balanced test dataset contains 18 images per class, including half-cell technology based solar panels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi2-3150725-large.gif
2022,9722870,Fig. 3.,One example EL image of each class of Fig. 2 and its evolution because of the Gaussian blurring that is adapted as preprocessing step in Section III-B. The size of the blurring kernel is varied as a portion of the y-dimension of the EL image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi3-3150725-large.gif
2022,9722870,Fig. 4.,(a) Cumulative explained variance as a function of the number of PCs of the PCA. Data obtained with the original EL images in black and results achieved by using blurred EL images given in orange lines. (b) Cumulative explained variance and explained variance as a function of the number of PCs. The bending point indicates the number of components needed to describe the original dataset. (c) Dependence of PC2 as a function of PC1 obtained after PCA. The colors indicate the failure classes of the ground-truth.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi4-3150725-large.gif
2022,9722870,Fig. 5.,"Replot of PC relationships shown in Fig. 4(c) with additional results of schematic EL image shown on the right side. The schematic EL images represent a homogenous variation of the contrast and a specific pattern variation, as an example a pattern similar to those of the PID failure is chosen.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi5-3150725-large.gif
2022,9722870,Fig. 6.,"(a) Confusion matrix for three-class classification, A = Reference, B = LeTID, C = PID, results of 18 test EL images solely used during kNN classification. (b) EL images as an example of the prediction. (c) Resulting metrics for each class with P denoting the precision, R the recall, and F1(-score) the harmonic mean of precision and recall.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi6-3150725-large.gif
2022,9722870,Fig. 7.,Simulation of field case study using 40 EL images from field measurement. Results of PCA separation shown in Fuchsia together with training data points already shown in Fig. 3. The mean value of the field case PCA is marked as a thicker symbol and used for the classification of the entire field dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5503869/9760484/9722870/bordi7-3150725-large.gif
2022,9817133,Fig. 1.,"GUS preparation and working mode. a. GUS working mechanism model. b. Preparation principle of LIG on NOMEX fabric substrate. c. Raman spectra of GUS. d. SEM of GUS. Scale bar, 10
μm
. e. Flexible display of graphene-based fabric. Scale bar, 1 cm. f. Positive and g. negative of GUS. Scale bar, 0.75 cm. h. Flow chart of GUS working mechanism.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/55/9866921/9817133/tao1-3189204-large.gif
2022,9817133,Fig. 2.,"Acoustic tests of GUS. a. SPL output versus frequency generated at different laser powers. b. SP output versus input power at 10 kHz, 20 kHz and 40 kHz. c. SP output versus test distance. d. SPL output stability of GUS.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/55/9866921/9817133/tao2-3189204-large.gif
2022,9817133,Fig. 3.,"GUS for information encryption. a, d and g. Original sound signals. b, e and h. Modulated to high frequency ultrasound signals. c, f and i. Demodulation to low frequency sound signals.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/55/9866921/9817133/tao3-3189204-large.gif
2022,9817133,Fig. 4.,"Machine learning enhanced information encryption. a. Spectrum of modulation to high frequency “Hold your fire”, “Cover me”, and “Follow me” speeches. b. The confusion matrix showing the classification number for speech recognition. c. Overall accuracy for speech recognition under various training iterations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/55/9866921/9817133/tao4-3189204-large.gif
2022,9849070,Fig. 1.,"Flow diagram of the proposed method. (a) Schematic diagram of the proposed SJ-BPNN divided into two channels for reconstructing relative permittivity and conductivity. Each channel has the same three-layer structure, namely an input layer, a hidden layer, and an output layer. (b) U-Net schematic diagram to enhance the imaging quality from SJ-BPNN. (c) MAPCHI scheme employed to enhance the resolution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9967023/9849070/xiao1-3196189-large.gif
2022,9849070,Fig. 2.,"3-D imaging results of the relative permittivity and conductivity of human brain obtained by SJ-BPNN (first column), U-Net (second column), and MAPCHI scheme (third column) versus the ground truth (fourth column).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9967023/9849070/xiao2-3196189-large.gif
2022,9849070,Fig. 3.,"For different noise levels, the 3-D imaging results of the relative permittivity and conductivity of human brain obtained by the MAPCHI scheme.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9967023/9849070/xiao3-3196189-large.gif
2022,9849070,Fig. 4.,"2-D slices of the relative permittivity and conductivity distribution of human brain with abnormal scatterers at z = 0.00 m under the noise-free and relative noise levels of −10, −20, −30, and −40 dB, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/9967023/9849070/xiao4-3196189-large.gif
2022,9899428,FIGURE 1.,"Scenario of this study: A model generator, which stores differentially private data, generates a high-quality machine learning model using the data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei1-3208715-large.gif
2022,9899428,FIGURE 2.,Relationship between split point and the weighted average of the mean squared error (MSE) when generating decision trees.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei2abcde-3208715-large.gif
2022,9899428,FIGURE 3.,Overview of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei3-3208715-large.gif
2022,9899428,FIGURE 4.,"An example of calculating
u
′
i,j
from
t
i,j
in Equation (24) where
b=4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei4-3208715-large.gif
2022,9899428,FIGURE 5.,"MSE results of synthetic datasets (
g=30
,
n=1000
.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei5abc-3208715-large.gif
2022,9899428,FIGURE 6.,"MSE results of synthetic datasets (
g=30
,
ϵ=1.0
.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei6abc-3208715-large.gif
2022,9899428,FIGURE 7.,MSE results with error bars of the proposed method with changes in the size of dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei7-3208715-large.gif
2022,9899428,FIGURE 8.,MSE results of real datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei8abcd-3208715-large.gif
2022,9899428,FIGURE 9.,Increased ratio of the MSE of the real datasets of various machine learning models. Negative values indicate that our method decreased the MSEs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei9abcd-3208715-large.gif
2022,9899428,FIGURE 10.,Increased ratio of the MAE of the real datasets of various machine learning models. Negative values indicate that our method decreased the MAEs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei10abcd-3208715-large.gif
2022,9899428,FIGURE 11.,"Calculated correlation value vs.
ϵ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9899428/sei11-3208715-large.gif
2022,9791366,Fig. 1.,Recording tools and setting for Timed Up and Go test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora1-3181252-large.gif
2022,9791366,Fig. 2.,Recorded RGB and skeletal data for the various subtasks of the TUG test using Kinect V.2 camera.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora2-3181252-large.gif
2022,9791366,Fig. 3.,Overall steps of the proposed algorithm for processing and analyzing the recorded skeletal data of TUG test for AD detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora3-3181252-large.gif
2022,9791366,Fig. 4.,"Detection of the sit-to-stand subtask based on the changes of the vertical displacement of the shoulder center joint and the angle
α
. The angle
α
is defined between the hip-shoulder center vector and a hypothetical horizontal plane going through the hip center.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora4-3181252-large.gif
2022,9791366,Fig. 5.,The changes of the position of the right and left feet in the z-direction during different TUG subtasks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora5-3181252-large.gif
2022,9791366,Fig. 6.,Distance between the right and left shoulder joints in the x-direction during TUG test.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora6-3181252-large.gif
2022,9791366,Fig. 7.,Detection of the stand-to-sit subtask based on the changes in the shoulder center joint in the y-direction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora7-3181252-large.gif
2022,9791366,Fig. 8.,Extracting various walking parameters from the walking subtask of the TUG test based on the changes in the position of the right and left foot joints in the z-direction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora8-3181252-large.gif
2022,9791366,Fig. 9.,"Comparison of different features from the TUG subtasks for HC vs. AD after adjusting for age and GDS score. The outliers are shown with circles and the significant difference is presented by an asterisk. The
p
-values are provided for each comparison.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora9-3181252-large.gif
2022,9791366,Fig. 10.,t-distributed stochastic neighbor embedding (t-SNE) of extracted data for AD and HC participants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora10-3181252-large.gif
2022,9791366,Fig. 11.,The ROC curves for detecting AD vs. HC using the top five significant features separately and all the selected features integrated via SVM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9791366/ghora11-3181252-large.gif
2022,9468397,Fig. 1.,Proposed approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9684248/9468397/leon1-3093096-large.gif
2022,9468397,Fig. 2.,Overview of the generic tool proposed. Optional phases and steps are represented framed by dashed lines. Phases and steps that are applied regardless of the data type are represented framed by solid lines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9684248/9468397/leon2-3093096-large.gif
2022,9468397,Fig. 3.,Spearman correlation between HRV features and the target variable (PMA).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9684248/9468397/leon3-3093096-large.gif
2022,9468397,Fig. 4.,Weekly average of the Tr_HVG (blue) and the LF_HF (red) features for the entire HRV population. The error bars represent the standard error of the mean (SEM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9684248/9468397/leon4-3093096-large.gif
2022,9468397,Fig. 5.,"PMA versus FMA for all infants in the HRV population, grouped by term.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9684248/9468397/leon5-3093096-large.gif
2022,9468397,Fig. 6.,"True PMA versus Estimated FMA for all infants in the RRV and bradycardia population, grouped by term.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9684248/9468397/leon6-3093096-large.gif
2022,9785995,Fig. 1.,Architecture of the domain-adaptation-based impedance inversion model. The number in parentheses indicates the number of filters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9651998/9785995/yoo1-3179492-large.gif
2022,9785995,Fig. 2.,Well locations in the Carnarvon Basin. The area around the Glencoe-1 well was used in the source domain; the area around the Mentorc-1 well was used in the target domain. The location of the arbitrary line for the seismic data used is also indicated.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9651998/9785995/yoo2-3179492-large.gif
2022,9785995,Fig. 3.,"Comparison of acoustic impedance prediction results at the blind well (Mentorc-1) obtained using (a) conventional pre-stack inversion method with the source well, Glencoe-1 well, (b) standard CNN, and (c) proposed domain adaptation model. Red lines indicate acoustic impedances predicted by the models; black lines indicate acoustic impedances computed from well log data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9651998/9785995/yoo3abc-3179492-large.gif
2022,9785995,Fig. 4.,"Arbitrary line seismic section and acoustic impedance results estimated from seismic data in the source and target domain areas. (a) Near-angle stacked seismic data ranged from 5° to 17°. Acoustic impedance values calculated using the proposed model are displayed at each source and target domain location. (b) Near-angle stacked seismic data for the area around Mentorc-1. Acoustic impedances for the area around Mentorc-1 predicted by (c) conventional pre-stack impedance inversion using only source well data, (d) standard CNN model, and (e) proposed domain adaptation model. (f) Near-angle stacked seismic data for the area around Glencoe-1. Acoustic impedances for the area around Glencoe-1 predicted by (g) conventional pre-stack impedance inversion using only source well data, (h) standard CNN model, and (i) proposed domain adaptation model. Acoustic impedance values computed using well log data are displayed at each well location.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8859/9651998/9785995/yoo4abcdefghi-3179492-large.gif
2022,9844718,FIGURE 1.,Quantum machine learning application in different healthcare domains.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes1-3195044-large.gif
2022,9844718,FIGURE 2.,Flow graph of paper selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes2-3195044-large.gif
2022,9844718,FIGURE 3.,Published research articles in QML and healthcare domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes3-3195044-large.gif
2022,9844718,FIGURE 4.,Mapped the published article in quantum machine learning and health-care domain as per country-wise.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes4-3195044-large.gif
2022,9844718,FIGURE 5.,Published journal and conference articles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes5-3195044-large.gif
2022,9844718,FIGURE 6.,(a) Classical and quantum state (b) Quantum superposition and entanglement [8].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes6ab-3195044-large.gif
2022,9844718,FIGURE 7.,Quantum tunneling [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes7-3195044-large.gif
2022,9844718,FIGURE 8.,Basic quantum computing circuit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes8-3195044-large.gif
2022,9844718,FIGURE 9.,Comparison between machine learning and quantum machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes9-3195044-large.gif
2022,9844718,FIGURE 10.,Intersection of machine learning and quantum information processing [4].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes10-3195044-large.gif
2022,9844718,FIGURE 11.,Quantum logic includes the quantum parts for the technology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes11-3195044-large.gif
2022,9844718,FIGURE 12.,Matrix of classical and quantum system [35].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes12-3195044-large.gif
2022,9844718,FIGURE 13.,Schematic circuit of variational quantum circuit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes13-3195044-large.gif
2022,9844718,FIGURE 14.,Schematic circuit of neural network quantum circuit.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes14-3195044-large.gif
2022,9844718,FIGURE 15.,Schematic circuit of quantum convolutional neural network circuit [75].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes15-3195044-large.gif
2022,9844718,FIGURE 16.,Quantum algorithms flowchart [109].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes16-3195044-large.gif
2022,9844718,FIGURE 17.,"(a) Color code fields of Biotechnology and (b) different domains of Biomedical [5], [124].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9844718/mahes17ab-3195044-large.gif
2022,9618666,Fig. 1.,Illustration and comparison of ML and KDML.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang1-3128597-large.gif
2022,9618666,Fig. 2.,Structure of Learning Module (LSTM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang2-3128597-large.gif
2022,9618666,Fig. 3.,"The mean square error of channel estimation(LS, LS+DFT, MMSE, MMSE+DFT, MLP, KDML) (Pilot density = 50%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang3-3128597-large.gif
2022,9618666,Fig. 4.,Mean square error for different pilot density.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang4-3128597-large.gif
2022,9618666,Fig. 5.,The test mean square error versus the number of epochs for training of LSTM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang5-3128597-large.gif
2022,9618666,Fig. 6.,Uplink transmission in massive MIMO systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang6-3128597-large.gif
2022,9618666,Fig. 7.,Network Structure of the DnCNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang7-3128597-large.gif
2022,9618666,Fig. 8.,NMSE performance comparison of the proposed method and other methods versus transmitted SNR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang8-3128597-large.gif
2022,9618666,Fig. 9.,"NMSE performance comparison versus
β
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang9-3128597-large.gif
2022,9618666,Fig. 10.,NMSE performance comparison versus number of antennas.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang10-3128597-large.gif
2022,9618666,Fig. 11.,NMSE of KDML versus the number of epochs for training of DnCNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687307/9790075/9618666/zhang11-3128597-large.gif
2022,9937069,Fig. 1.,Systematic review process adapted from [8].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu1-3219366-large.gif
2022,9937069,Fig. 2.,Stages in the systematic review process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu2-3219366-large.gif
2022,9937069,Fig. 3.,Distribution of the studies over publication year.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu3-3219366-large.gif
2022,9937069,Fig. 4.,Summary of ML algorithms mostly utilized in GNSS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu4-3219366-large.gif
2022,9937069,Fig. 5.,Distribution of the NN model's terminologies as used in the selected studies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu5-3219366-large.gif
2022,9937069,Fig. 6.,Datasets for ML utilization in GNSS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu6-3219366-large.gif
2022,9937069,Fig. 7.,"ML versus non-ML (bars above zero line indicate that ML models are more accurate, while the bars below zero line indicate that non-ML models are more accurate).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu7-3219366-large.gif
2022,9937069,Fig. 8.,Utilization of ML in GNSS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu8-3219366-large.gif
2022,9937069,Fig. 9.,"Comparison between ML models (bars above zero line indicate that models in vertical axis are more accurate, whereas the bars below zero line indicate that models in horizontal axis are more accurate).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7/9970423/9937069/siemu9-3219366-large.gif
2022,9631374,Fig. 1.,"Panel A: The experimental set up with the IMU outlined on the dorsum of the participant’s dominant foot and the insole for foot contact identification. Panel B: Flow of the data from collection to identification of patterns for analysis. Panel C: The real-world environment course, unlabeled sections indicate level ground locomotion. Day 1 contained two environmentally constrained transitions and walk to run and run to walk transitions (course in red). Day 2 of the trial consisted of only walk to run and run to walk transitions (course in blue).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9631374/hahn1-3131953-large.gif
2022,9631374,Fig. 2.,"These graphs present five time points before and after a gait event and the proportion of footfalls when a selected state output from the BP-AR-HMM occurs. The states that identify swing phase and for all locomotion modes other than running are states 5 and 6. The absence of state 5, 6 and 13 are indicative of IC. The identification of TO can be achieved with first occurrence states 2, 16.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9631374/hahn2-3131953-large.gif
2022,9631374,Fig. 3.,"Representative foot ground contacts for Level Ground Walking (A & B) and Running (C & D), along with identified impending IC and TO from heuristic rules and the BP-AR-HMM. Panels A and C show the angular velocity plotted in Black with the identified foot contacts in Blue. Panels B and D show output from the BP-AR-HMM in Green plotted with the angular velocity in Black. The states used for identification of gait events are labeled in panels C and D.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9631374/hahn3-3131953-large.gif
2022,9631374,Fig. 4.,"Representative foot ground contacts for Stair Ascent (A & B) and Descent (C & D), along with identified impending IC and TO from heuristic rules and the BP-AR-HMM. Panels A and C show the angular velocity plotted in Black with the identified foot contacts in Blue. Panels B and D show output from the BP-AR-HMM in Green plotted with the angular velocity in Black. The states used for identification of gait events are labeled in panels B and D.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9631374/hahn4-3131953-large.gif
2022,9631374,Fig. 5.,"Representative foot ground contacts Ramp Ascent (A & B) and Descent (C & D), along with identified impending IC and TO from heuristic rules and the BP-AR-HMM. Panels A and C show the angular velocity plotted in Black with the identified foot contacts in Blue. Panels B and D show output from the BP-AR-HMM in Green plotted with the angular velocity in Black. The states used for identification of gait events are labeled in panels B and D.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/9695946/9631374/hahn5-3131953-large.gif
2022,9740532,Fig. 1.,Training of the proposed SASDL method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782705/9698118/9740532/lee1-3161837-large.gif
2022,9740532,Fig. 2.,Testing of the proposed SASDL method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782705/9698118/9740532/lee2-3161837-large.gif
2022,9740532,Fig. 3.,Performance analysis of Good Detection Rate (GDR) %.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782705/9698118/9740532/lee3-3161837-large.gif
2022,9740532,Fig. 4.,Performance analysis of error rate (%).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782705/9698118/9740532/lee4-3161837-large.gif
2022,9984667,FIGURE 1.,Typical fall and ADL of young and elderly people.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu1-3229044-large.gif
2022,9984667,FIGURE 2.,Distribution of falls and ADL data in young and elderly people.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu2-3229044-large.gif
2022,9984667,FIGURE 3.,Box plots of falls and ADLs for ten characteristics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu3-3229044-large.gif
2022,9984667,FIGURE 4.,Workflow of the Fed-ELM Algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu4-3229044-large.gif
2022,9984667,FIGURE 5.,"The Sen, Spc and Acc of the three algorithms (ELM, OS-ELM and Fed-ELM) on different young individuals’ data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu5-3229044-large.gif
2022,9984667,FIGURE 6.,The Acc of the Fed-ELM on different data of the elderly individuals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu6-3229044-large.gif
2022,9984667,FIGURE 7.,Comparison of other methods with the method proposed in this paper.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu7-3229044-large.gif
2022,9984667,FIGURE 8.,Comparison of elderly fall detection methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9984667/yu8-3229044-large.gif
2022,9950270,FIGURE 1.,Weekly pattern of the daily waste data in city of ballarat.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna1-3221941-large.gif
2022,9950270,FIGURE 2.,Weekly pattern of the daily waste data in city of austin.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna2-3221941-large.gif
2022,9950270,FIGURE 3.,Leaf-wise tree growth in LightGBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna3-3221941-large.gif
2022,9950270,FIGURE 4.,MAPE of the chosen models with different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna4-3221941-large.gif
2022,9950270,FIGURE 5.,Actual vs predictions for ballarat.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna5-3221941-large.gif
2022,9950270,FIGURE 6.,Actual vs predictions for moratuwa-sri lanka.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna6-3221941-large.gif
2022,9950270,FIGURE 7.,Average training time for the models across all the datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna7-3221941-large.gif
2022,9950270,FIGURE 8.,Average MAPE of the models across all the datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9950270/ferna8-3221941-large.gif
2022,9352534,Fig. 1.,Attention comparisons. Blue: ideal attention weights. Red: empirical attention weights. The color depth indicates the weight. Example sentences are partially displayed due to the space limitation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9849214/9352534/ren1-3053633-large.gif
2022,9352534,Fig. 2.,Overall architecture of DRGA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9849214/9352534/ren2-3053633-large.gif
2022,9352534,Fig. 3.,"Influences of regulating factors. (a) Accuracy over
β
and
γ
. (b) Training time over
β
. (c) Accuracy over
L
0
−
−
√
. (d) Accuracy over
δ
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9849214/9352534/ren3abcd-3053633-large.gif
2022,9352534,Fig. 4.,25 most modified words on SUBJ.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9849214/9352534/ren4-3053633-large.gif
2022,9352534,Fig. 5.,"JS divergence gap (
Δ
JS) (
Y
-axis) between attention distribution against
Δα
of attention weight (
X
-axis) between the most important token
i
and random selected token
r
. The results at the topper row in blue color are from the raw attention model, and the results at the corresponding bottom row in green color are from the attention retrofitted by DRAG. (a) DiSAN-SST. (b) DiSAN-SUBJ. (c) IAN-Sem14. (d) IAN-Sem15. (e) Trm-WMT14. (f) Trm-WMT15. (g) DiSAN-SST. (h) DiSAN-SUBJ. (i) IAN-Sem14. (j) IAN-Sem15. (k) Trm-WMT14. (l) Trm-WMT15.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/9849214/9352534/ren5abcdefghijkl-3053633-large.gif
2022,9663544,Fig. 1.,Basic structure of ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham1-3128213-large.gif
2022,9663544,Fig. 2.,Schematic diagram of how to incorporate H-ELM into the forecast of the real-time and day-ahead DLR values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham2-3128213-large.gif
2022,9663544,Fig. 3.,Training results of networks used in regression format associated with first case study. (a) Multilayer perceptron. (b) Group method of data handling. (c) Support vector regression. (d) Back-propagation neural network. (e) Extreme learning machine. (f) Hierarchical extreme Learning machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham3-3128213-large.gif
2022,9663544,Fig. 4.,Test results and DLR forecasting using the used networks in the first case study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham4-3128213-large.gif
2022,9663544,Fig. 5.,Test results of networks used in regression format for the first case study. (a) Multilayer perceptron. (b) Group method of data handling. (c) Support vector regression. (d) Back-propagation neural network. (e) Extreme learning machine. (f) Hierarchical extreme Learning machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham5-3128213-large.gif
2022,9663544,Fig. 6.,Forecasted DLR in the first case study based on variables of air temperature and wind speed. (a) Multilayer perceptron. (b) Group method of data handling. (c) Support vector regression. (d) Back-propagation neural network. (e) Extreme learning machine. (f) Hierarchical extreme Learning machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham6-3128213-large.gif
2022,9663544,Fig. 7.,Variation of DLR profile for Ghadamgah.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham7-3128213-large.gif
2022,9663544,Fig. 8.,Dispersion of DLR concerning ambient variables.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham8-3128213-large.gif
2022,9663544,Fig. 9.,Training results of networks used in regression format associated with the second case study. (a) Multilayer perceptron. (b) Group method of data handling. (c) Support vector regression. (d) Back-propagation neural network. (e) Extreme learning machine. (f) Hierarchical extreme Learning machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham9-3128213-large.gif
2022,9663544,Fig. 10.,Test results and DLR forecasting using the used networks in the second case study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham10-3128213-large.gif
2022,9663544,Fig. 11.,Test results of networks used in regression format for the second case study. (a) Multilayer perceptron. (b) Group method of data handling. (c) Support vector regression. (d) Back-propagation neural network. (e) Extreme learning machine. (f) Hierarchical extreme Learning machine..,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham11-3128213-large.gif
2022,9663544,Fig. 12.,Forecasted DLR in the second case based on variables of air temperature and wind speed; (a) Multilayer perceptron. (b) Group method of data handling. (c) Support vector regression. (d) Back-propagation neural network. (e) Extreme learning machine. (f) Hierarchical extreme Learning machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9866865/9663544/moham12-3128213-large.gif
2022,9403894,Fig. 1.,Framework of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9403894/zhang1-3073056-large.gif
2022,9403894,Fig. 2.,ROC curve of different classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9403894/zhang2-3073056-large.gif
2022,9403894,Fig. 3.,Ten-fold cross validation on different indicators.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9403894/zhang3-3073056-large.gif
2022,9403894,Fig. 4.,Relationship between variables and predicted responses.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9757026/9403894/zhang4-3073056-large.gif
2022,9409768,Fig. 1.,ML-KELM structure diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo1-3073431-large.gif
2022,9409768,Fig. 2.,Schematic diagram of threshold function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo2-3073431-large.gif
2022,9409768,Fig. 3.,Cholesky parallel matrices decomposition in Spark.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo3-3073431-large.gif
2022,9409768,Fig. 4.,Sequence diagram of matrix decomposition on Spark platform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo4-3073431-large.gif
2022,9409768,Fig. 5.,Class-incremental learning for ML-KELM network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo5-3073431-large.gif
2022,9409768,Fig. 6.,Multi-label classification learning system framework based on Spark.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo6-3073431-large.gif
2022,9409768,Fig. 7.,ML-KELM algorithm offline modeling time with single server and multi-thread.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo7-3073431-large.gif
2022,9409768,Fig. 8.,ML-KELM algorithm offline modeling time with Spark cluster environment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo8-3073431-large.gif
2022,9409768,Fig. 9.,"Testing
P@2
with different training set and online block sizes with RCV1-V2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488902/9781297/9409768/guo9-3073431-large.gif
2022,9619917,Fig. 1.,"(a) Overview of the AEDAP system and its role with respect to the domains involved in the operation of an HR-EGG driven clinical diagnostic tool. A domain analysis of the critical areas is provided along with the purpose of each stage of the AEDAP system. (b) Stage-by-stage block diagram for the iterative AEDAP process. The execution of each iteration proceeds after definition of system input and operational parameters. Following system execution, the results are interpreted, hypotheses revised, and a new system parameter set is devised.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9761805/9619917/olson1-3129175-large.gif
2022,9619917,Fig. 2.,"Data flow diagram for Feature Importance Evaluation, Model Building, Model Selection, and Top Model Evaluation stages.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9761805/9619917/olson2-3129175-large.gif
2022,9619917,Fig. 3.,(a) Shows an overview of model performance across all iteration sets. (b) Shows the objective function scores for iteration set #8 where the number of predictors used varied from 2 to 50.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9761805/9619917/olson3-3129175-large.gif
2022,9619917,Fig. 4.,Uppermost pie chart gives breakdown of the ten features used in the top performing model by class and with respect to their ranked importance value. The bar chart on the bottom shows lists the ten features in descending order of importance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9761805/9619917/olson4-3129175-large.gif
2022,9619917,Fig. 5.,"The histogram (top) shows the resulting distribution of scores after evaluating the top model using the repeated holdout resampling procedure after 10K runs (see legend) and after evaluating the held-out test-set under the custom data split (vertical lines, color coordinated to upper left legend). The two curves on the bottom show the precision-recall and ROC curve performance of the top model using the held-out test set under the single chosen random seed during the standard model building process that was identified as optimal across the entire experiment according to the objective function.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9761805/9619917/olson5-3129175-large.gif
2022,9619917,Fig. 6.,"(a) Shows an overlay of the experimental HR-EGG signal (blue) and respective SOBI-ICA signal (red). (b) Shows a similar comparison using a Synthetic HR-EGG signal (blue) and respective synthetic SOBI-ICA signal (red-dotted). (c) Shows a comparison of the simulated gastric source (black-dotted), the synthetic SOBI-ICA signal (red-dotted), and the experimental SOBI-ICA signal (red-solid).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9761805/9619917/olson6-3129175-large.gif
2022,9783196,FIGURE 1.,"A schematic overview of the medical machine learning workflow throughout the model’s life cycle, some of its stakeholders, and some of its risks. The risks and possible solutions to mitigate them are discussed throughout this document.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9783196/peter1-3178382-large.gif
2022,9783196,FIGURE 2.,"Medical machine learning lies at the intersection of medicine and machine learning and is subject to regulations and ethical considerations regarding both fields. The key requirements that emerge from all of these are safety, robustness, reliability, privacy, security, transparency, explainability, and fairness. In order to perform responsible MML, social impact assessment is another key requirement, which is not addressed in this document.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9783196/peter2-3178382-large.gif
2022,9783196,FIGURE 3.,"(a) Models that perform well on the training dataset need not perform well on the test dataset, and those performing well on the test dataset still need not perform well in the real world, for reasons including dataset shift, spurious correlations, and model underspecification. Figure closely modeled after Geirhos et al. [116]. Independent identically distributed (i.i.d.), out of distribution (o.o.d.). (B) A model class with a too restrictive hypothesis space prevents convergence towards the true model. (C) A model class with inadequate inductive biases renders convergence towards the true model inefficient, requiring lots of data. (D) A model class must cover a sufficiently large hypothesis space that includes the true model, it must be equipped with adequate inductive biases and there must be sufficient data to enable convergence towards the true model. Figures (B)–(D) closely modeled after Wilson and Izmailov [300].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9783196/peter3-3178382-large.gif
2022,9783196,FIGURE 4.,"Many different communication schemes have been proposed for data collection, model training, and model deployment. All of them have merits and drawbacks—many of which will be discussed in section IV-A —and are preferable under different circumstances. Mechanisms for preserving privacy and security in these different schemes against various attacks have been proposed; they are discussed in section IV-B. Centralized training and deployment is the current standard scheme in ML. The depicted decentralized training scheme corresponds to (horizontal) federated learning; the depicted decentralized deployment scheme is often called “Edge AI. ” (For decentralized, e.g., federated, training, a central coordinator is not strictly necessary: completely decentralized approaches have been proposed as well.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9783196/peter4-3178382-large.gif
2022,9783196,FIGURE 5.,"A visual summary of known important problems with the classical ML workflow as well as potential solutions to these challenges. The lower path is not meant to be pursued in its entirety; instead, the requirements in each specific application scenario must be analyzed carefully and the appropriate building blocks selected. Neither the list of problems nor the list of solution building blocks is considered comprehensive.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9783196/peter5-3178382-large.gif
2022,9985459,FIGURE 1.,A typical structure of a three-phase 12/8 SRM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar1-3229043-large.gif
2022,9985459,FIGURE 2.,Ideal profile of flux linkage versus rotor position.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar2-3229043-large.gif
2022,9985459,FIGURE 3.,Ideal SRM rotor positions with respect to phase A shown on an 6/8 SRM (a) aligned position and (b) unaligned position.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar3ab-3229043-large.gif
2022,9985459,FIGURE 4.,Classification of various modeling techniques for SRMs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar4-3229043-large.gif
2022,9985459,FIGURE 5.,The basic flow chart of supervised learning techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar5-3229043-large.gif
2022,9985459,FIGURE 6.,The basic flow chart of unsupervised learning techniques.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar6-3229043-large.gif
2022,9985459,FIGURE 7.,The basic flow chart of reinforcement learning technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar7-3229043-large.gif
2022,9985459,FIGURE 8.,The basic structure of FNN with one hidden layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar8-3229043-large.gif
2022,9985459,FIGURE 9.,The basic structure of FNN with two hidden layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar9-3229043-large.gif
2022,9985459,FIGURE 10.,The basic structure of a single-layer FNN with ten hidden nodes to model a 6/4 SRM [54].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar10-3229043-large.gif
2022,9985459,FIGURE 11.,Rotor position estimation error profile at 400-rpm and 1000-rpm [54].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar11-3229043-large.gif
2022,9985459,FIGURE 12.,A simplified example of RBFNN structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar12-3229043-large.gif
2022,9985459,FIGURE 13.,A basic structure of RNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar13-3229043-large.gif
2022,9985459,FIGURE 14.,The simulation block diagram of a 3-phase SRM using ANFIS to model the torque and flux linkage characteristics [66].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar14-3229043-large.gif
2022,9985459,FIGURE 15.,The RBFN-AFS architecture proposed in [68] for modeling of torque and flux linkage of an SRM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar15-3229043-large.gif
2022,9985459,FIGURE 16.,The schematic diagram of SVM separating and marginal hyperplanes for the (a) classification and (b) regression tasks. \$\varepsilon \$ in (b) is the tolerable deviation between the hyperplane and the transformed data [69].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar16ab-3229043-large.gif
2022,9985459,FIGURE 17.,LSSVM input-output models (a) cascaded single-output models (b) multi-output model [83].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar17ab-3229043-large.gif
2022,9985459,FIGURE 18.,"Comparison of torque modeling error for LSSVM, LSSVM-GA, LSSVM-PSO and LSSVM-DE [90].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar18-3229043-large.gif
2022,9985459,FIGURE 19.,Dynamic Torque of MCC-LSSVR model compared to look-up table model at 6000-rpm [91].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar19-3229043-large.gif
2022,9985459,FIGURE 20.,Measured and RR-LSSVR Phase torque comparison at (a) 600-rpm and (b) 6000-rpm [65].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar20ab-3229043-large.gif
2022,9985459,FIGURE 21.,A flowchart of motor design optimization using ML-based algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar21-3229043-large.gif
2022,9985459,FIGURE 22.,A quarter model of the considered SRM showing the stator and rotor arc angles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar22-3229043-large.gif
2022,9985459,FIGURE 23.,The GRNN Input-output modeling system proposed in [102].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar23-3229043-large.gif
2022,9985459,FIGURE 24.,The feasible triangle of stator and rotor pole arc angles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar24-3229043-large.gif
2022,9985459,FIGURE 25.,Electromagnetic model of concentrated winding PMSM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar25-3229043-large.gif
2022,9985459,FIGURE 26.,Electromagnetic model of a synchronous reluctance machine.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9985459/omar26-3229043-large.gif
2022,9676410,Fig. 1.,"Sketch of the end-to-end system architecture proposed in this work, from motion capture data collection to robot locomotion control.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec1-3141658-large.gif
2022,9676410,Fig. 2.,Block diagram of the overall learning-based ADHERENT architecture proposed in this work.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec2-3141658-large.gif
2022,9676410,Fig. 3.,"Desired motion and facing directions from the joypad (left) define the desired future ground base trajectory (center). On the right, the user-specified future trajectory (grey) is blended with the future trajectory from the previous network prediction (magenta), leading to the desired future ground base trajectory (green) actually included in the next input to the network.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec3-3141658-large.gif
2022,9676410,Fig. 4.,"Top: Visualization of a mixed trajectory including forward (1-3), oriented-forward (4-6), side (7-11), and backward (12-15) walking, with smooth transitions between them and a final stop (16). Below each frame, the user inputs interactively shaping the trajectory are plotted from the robot’s local viewpoint (red: Desired motion direction; blue: Desired facing direction). Bottom: The same trajectory performed on iCub. Numbering highlights frame correspondences.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec4-3141658-large.gif
2022,9676410,Fig. 5.,"Simulated and experimental results for different combinations of
c
v
and
c
f
. Red and green areas denote failures and successes in simulation. The green line connects the most challenging successes on the real robot.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec5-3141658-large.gif
2022,9676410,Fig. 6.,ADHERENT-generated vs. human-retargeted joint trajectories for a set of representative joints during forward walking.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec6-3141658-large.gif
2022,9676410,Fig. 7.,Output of the Postural Extractor and measured joint positions with ADHERENT Postural (AP) vs. Fixed Postural (FP) for four upper-body joints.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec7-3141658-large.gif
2022,9676410,Fig. 8.,"Blending coefficients
θ
profiles with
K=4
expert weights for an articulated trajectory including standing (0-1 s), straight (1-4 s) and steered (4-6 s) forward walking, right-side (6-10 s), and left-side (10-15 s) walking.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7083369/9647862/9676410/vicec8-3141658-large.gif
2022,9867986,FIGURE 1.,Summarized diagram of available machine learning techniques and algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa1-3201882-large.gif
2022,9867986,FIGURE 2.,Illustration of the mechanics of the SVR algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa2-3201882-large.gif
2022,9867986,FIGURE 3.,Block diagram of the artificial neural network (ANN) algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa3-3201882-large.gif
2022,9867986,FIGURE 4.,Predicted vs actual fitness values of each ML-based algorithm for the training dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa4-3201882-large.gif
2022,9867986,FIGURE 5.,Fitness values vs the index for each ML-based algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa5-3201882-large.gif
2022,9867986,FIGURE 6.,Prediction error vs the index for each ML-based algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa6-3201882-large.gif
2022,9867986,FIGURE 7.,Absolute prediction error vs index for the ML-based algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa7-3201882-large.gif
2022,9867986,FIGURE 8.,Actual vs predicted fitness values for each ML-based algorithm for the test dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa8-3201882-large.gif
2022,9867986,FIGURE 9.,Training and testing datasets of (a) accuracy vs epochs and (b) loss vs epochs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9867986/solwa9ab-3201882-large.gif
2022,9729867,FIGURE 1.,"The 5 diabetic retinopathy stages, ranked by severity [7].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9729867/atwan1-3157632-large.gif
2022,9729867,FIGURE 2.,"Inter-grader inconsistencies illustrated. In the highlighted red area, columns represent a single fundus image and the rows represent the final grade provided by the ophthalmologist.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9729867/atwan2-3157632-large.gif
2022,9729867,FIGURE 3.,"A fundoscopic illustration of the retina, showing Microaneurysms, Hemorrhages, and Exudates.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9729867/atwan3-3157632-large.gif
2022,9729867,FIGURE 4.,"Top chart shows the classification segment distribution across 11 supervised studies (inner circle), 3 self-supervised studies (middle circle), and 4 transformer studies (outer circle). Bottom chart shows evaluation metric usage distribution across 11 supervised studies (inner circle), 3 self-supervised studies (middle circle), and 4 transformer studies (outer circle).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9729867/atwan4-3157632-large.gif
2022,9729867,FIGURE 5.,"Distribution of studies in terms of model explainability, sorted by model type. As transformers evolve, more explainable models emerge.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9729867/atwan5-3157632-large.gif
2022,9936637,FIGURE 1.,"Machine learning as a service architecture, with and without privacy preservation. The area inside the dotted line in 1b represents the area in which the client can keep secrets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc1ab-3219049-large.gif
2022,9936637,FIGURE 2.,Scalar vs Single Instruction Multiple Data (SIMD) operations with 4 slots.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc2-3219049-large.gif
2022,9936637,FIGURE 3.,"A multi-layer perceptron with one hidden layer. On encrypted data, the ciphertext noise grows as computation progresses into the lower layers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc3-3219049-large.gif
2022,9936637,FIGURE 4.,"Calculating the output value of a single neuron using the inputs
x
i
, weights
w
i
, bias
b
, and activation function
f
. In most homomorphic encryption schemes, evaluating
f
is hard.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc4-3219049-large.gif
2022,9936637,FIGURE 5.,Connection between the input and the output for convolutions and pooling layers. The two differ in the function applied to values in the window.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc5-3219049-large.gif
2022,9936637,FIGURE 6.,Convolutions on inputs with multiple channels.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc6-3219049-large.gif
2022,9936637,FIGURE 7.,"Overview of a simple recurrent neural network (RNN) layer IV-Da “unrolled” along the time dimension
t
. The red dotted line in IV-Db shows the initial input value passing
x
0
through every unrolled layer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc7ab-3219049-large.gif
2022,9936637,FIGURE 8.,Internals of a long-short term memory (LSTM) cell with noise-increasing operations highlighted.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9936637/podsc8-3219049-large.gif
2022,9770332,Fig. 1.,Graphical abstract of this article.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen1-3172956-large.gif
2022,9770332,Fig. 2.,RVFL’s structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen2-3172956-large.gif
2022,9770332,Fig. 3.,Architecture of the edRVFL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen3-3172956-large.gif
2022,9770332,Fig. 4.,Forecasting performance versus the number of hidden layers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen4-3172956-large.gif
2022,9770332,Fig. 5.,MASE versus varying regularization strength.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen5-3172956-large.gif
2022,9770332,Fig. 6.,MASE versus varying hidde nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen6-3172956-large.gif
2022,9770332,Fig. 7.,"Nemenyi test results based on MASE. The critical distance is 3.712 and Friedman
p
values is 8.847e-9.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen7-3172956-large.gif
2022,9770332,Fig. 8.,Comparison of forecasts for communicable disease center.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/9910251/9770332/yuen8-3172956-large.gif
2022,9893801,FIGURE 1.,Environmental DNA metabarcoding.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9893801/kimur1-3207173-large.gif
2022,9893801,FIGURE 2.,Coding methods of the DNAs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9893801/kimur2-3207173-large.gif
2022,9893801,FIGURE 3.,Neural network and operation scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9893801/kimur3-3207173-large.gif
2022,9893801,FIGURE 4.,Accuracies for each coding method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9893801/kimur4ab-3207173-large.gif
2022,9893801,FIGURE 5.,Accuracies as functions of the epoch for each coding method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9893801/kimur5ab-3207173-large.gif
2022,9794643,FIGURE 1.,"Big data in GWAS. A pipeline is depicted schematically, beginning with SNPs generated by genotypes platforms, and ending with the development of analytical models using various big data analytics solutions and tools.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr1-3182543-large.gif
2022,9794643,FIGURE 2.,"Machine Learning types: a) Supervised learning, b) Unsupervised learning, c) Semi-supervised learning and d) reinforcement learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr2abcd-3182543-large.gif
2022,9794643,FIGURE 3.,Representation of an artificial neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr3-3182543-large.gif
2022,9794643,FIGURE 4.,An example of SVM model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr4-3182543-large.gif
2022,9794643,FIGURE 5.,Random forest in genetics application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr5-3182543-large.gif
2022,9794643,FIGURE 6.,A visual breakdown of publication selection based on PRISMA guidelines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr6-3182543-large.gif
2022,9794643,FIGURE 7.,Percentage of papers in each area are of study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr7-3182543-large.gif
2022,9794643,FIGURE 8.,Risk of bias of studies included in the review.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/9668973/9794643/alatr8-3182543-large.gif
2022,9618642,Fig. 1.,"Demonstration of our data poisoning attack model on federated machine learning, where different colors denote different nodes (devices), and there are
n
nodes in this federated learning system. Some nodes are injected by corrupted/poisoned data, while the models among some nodes could be trained with clean data.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun1-3128646-large.gif
2022,9618642,Fig. 2.,"Demonstration of three different data poisoning strategies: (a) direct attack (Left), (b) indirect attack (Middle), (c) and hybrid attack (Right), where the nodes marked with orange color are attacked.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun2abc-3128646-large.gif
2022,9618642,Fig. 3.,"Sample images from EndAD data set, where the first and the second rows are healthy images and lesion images, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun3-3128646-large.gif
2022,9618642,Fig. 4.,"Correlation matrices
Ω
of different data sets: (a) End_AD data set, (b) human activity data set, (c) landmine data set, and (d) parkinson data set, where the first half of nodes and the rest of nodes denote the
N
tar
and
N
sou
for the indirect attack scenario, respectively. The darker color indicates the higher correlation for each data set, and vice versa.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun4abcd-3128646-large.gif
2022,9618642,Fig. 5.,"Effect of ratio of injected data on (a) EndAD, (b) human activity, (c) landmine, and (d) parkinson-total data sets, where different lines denote different types of attacking strategies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun5abcd-3128646-large.gif
2022,9618642,Fig. 6.,Effect of number of target nodes and ratio of injected data on the human activity recognition data set: (a) direct attack and (b) indirect attack.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun6ab-3128646-large.gif
2022,9618642,Fig. 7.,"Effect of step size
η
of (4) on (a) human activity and (b) landmine data sets, where different lines denote different attacking strategies.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun7ab-3128646-large.gif
2022,9618642,Fig. 8.,"Convergence curves of our proposed AT2FL algorithm, where the classification errors are in (a) EndAD and (b) human activity data sets, and the primal suboptimality performances of the lower-level problem for (3) are in (c) EndAD and (d) human activity data sets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/9799556/9618642/sun8abcd-3128646-large.gif
2022,9372334,Fig. 1.,Deep transfer learning-based epileptic classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao1-3064228-large.gif
2022,9372334,Fig. 2.,AlexNet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao2-3064228-large.gif
2022,9372334,Fig. 3.,VGG19.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao3-3064228-large.gif
2022,9372334,Fig. 4.,Inception-v3.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao4-3064228-large.gif
2022,9372334,Fig. 5.,ResNet152.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao5-3064228-large.gif
2022,9372334,Fig. 6.,Inception-ResNet-v2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao6-3064228-large.gif
2022,9372334,Fig. 7.,Feature visualization extracted by VGG19 on the EEG MAS image.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao7-3064228-large.gif
2022,9372334,Fig. 8.,Fc learning block.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao8-3064228-large.gif
2022,9372334,Fig. 9.,Probabilities obtained in the Softmax layer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao9-3064228-large.gif
2022,9372334,Fig. 10.,Visualization comparisons on the Softmax layer probability feature distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao10-3064228-large.gif
2022,9372334,Fig. 11.,Confusion matrices on the 5-category CHB-MIT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao11-3064228-large.gif
2022,9372334,Fig. 12.,Confusion matrix comparisons to state-of-the-art algorithms on the 8-category CHB-MIT database.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao12-3064228-large.gif
2022,9372334,Fig. 13.,Box plots on the (a) CHB-MIT and (b) iNeuro databases.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274989/9793367/9372334/cao13ab-3064228-large.gif
2023,9672674,Fig. 1.,"Schematic overview of the systematization’s categories for AF sequence comparison in proximity-based machine learning: coding, feature generation and proximity measurement.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9672674/bohns1-3140873-large.gif
2023,9672674,Fig. 2.,"Demonstration of different ways to combine the modules coding, feature generation and proximity measurement to assess two sequences (dis-)similarity.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9672674/bohns2-3140873-large.gif
2023,9672674,Fig. 3.,Illustrating the possibility for nested codings and feature generations. The right-side example refers to the approach presented in [128].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9672674/bohns3-3140873-large.gif
2023,10168889,FIGURE 1.,Sequence diagram of meteorological factors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song1-3291146-large.gif
2023,10168889,FIGURE 2.,Schematic diagram of ELM structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song2-3291146-large.gif
2023,10168889,FIGURE 3.,GA-KELM algorithm flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song3-3291146-large.gif
2023,10168889,FIGURE 4.,Model architecture diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song4-3291146-large.gif
2023,10168889,FIGURE 5.,Before data processing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song5-3291146-large.gif
2023,10168889,FIGURE 6.,After data processing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song6-3291146-large.gif
2023,10168889,FIGURE 7.,End-to-end architecture of GA-KELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song7-3291146-large.gif
2023,10168889,FIGURE 8.,Results of the change in fitness for GA optimization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song8-3291146-large.gif
2023,10168889,FIGURE 9.,Annual air quality forecast comparison (Dataset A).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song9a-3291146-large.gif
2023,10168889,FIGURE 10.,Seasonal air quality forecast trends (Dataset B).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song10-3291146-large.gif
2023,10168889,FIGURE 11.,Monthly air quality forecast accuracy (Dataset C).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168889/song11-3291146-large.gif
2023,10092737,FIGURE 1.,(a) Human perceives circles and dodecagons to be similar. The prototype-based explanation method provides explanation by distance between a prototype and an input. (b) Previous methods provide different distances on similar inputs. (c) Our method provides similar distances on similar inputs and (d) the transition which shows the change of features through interpolation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee1abcd-3264794-large.gif
2023,10092737,FIGURE 2.,"Overall model architecture. Explanation space is made by the sampling process and latent space regularization in the distributional embedding layer. In explanation space, distances between embedded distribution
d
z
and the prototypes
d
i
are used as explanation with images of
μ
i
. The uncertainty layer is used to reduce the effect of uncertainty due to sampling.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee2-3264794-large.gif
2023,10092737,FIGURE 3.,Basic explanation: (a) Learned prototypes and distances between the input and prototypes. Additional explanation (b-c): (b) Transitions between input and prototypes. (c) Transitions between prototypes from similar shapes (bottom) to dissimilar shapes (top).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee3abc-3264794-large.gif
2023,10092737,FIGURE 4.,Top: Visualizations for the latent space of FashionMNIST to show class-specific clustering. Bottom: Histogram of the distance at latent space between two randomly sampled data. PDL shows various modes which means that the distances are distributed without a consistent criterion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee4-3264794-large.gif
2023,10092737,FIGURE 5.,Histograms of distance between the encoded embedding from a fixed instance and embeddings from randomly sampled instances in other classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee5-3264794-large.gif
2023,10092737,FIGURE 6.,"T-SNE visualizations for the embedded FashionMNIST with the proposed model. By distributional embedding layer (a) without disentanglement term, (b) with a disentangling method using a stronger constraint on the latent bottleneck [47], and (c) with disentanglement term.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee6abc-3264794-large.gif
2023,10092737,FIGURE 7.,The mean-median ratio of normalized distance change. We performed random perspective transformation from torchvision to distort the input while maintaining a similar shape.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee7-3264794-large.gif
2023,10092737,FIGURE 8.,"Explanation of DEES with 20 prototype distributions. (a) Prototypes and distance between input and prototype distributions in the latent space. The red distance is the distance from the prototypes in the T-shirt class (b) Transitions between input and prototypes. (c) Transitions between prototypes on PDL and DEES (Ours). PDL shows unrecognizable images in transitions, showing that they cannot provide clear information by the transition.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee8abc-3264794-large.gif
2023,10092737,FIGURE 9.,"Classification accuracy at number of prototype distributions
m=[2
, 3, 10, 20, 30, 40, 60, 100].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee9-3264794-large.gif
2023,10092737,FIGURE 10.,"Prototypes made with the proposed model using (a) KL-divergence, (b) JSD, and (c) JTD as distance measurements. T-SNE visualizations for their latent space with the proposed model using (d) KL-divergence, (e) JSD, and (f) JTD as distance measurements. Models are trained without disentanglement term with 1000 epochs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee10abcdef-3264794-large.gif
2023,10092737,FIGURE 11.,Example of interactive reconstruction experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee11-3264794-large.gif
2023,10092737,FIGURE 12.,Example of survey without relationships.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee12-3264794-large.gif
2023,10092737,FIGURE 13.,Example of survey with relationships.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee13-3264794-large.gif
2023,10092737,FIGURE 14.,Histograms of Euclidean distance between the encoded vector from a fixed instance and vectors from randomly sampled instances in other classes. These results are computed by the baseline model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee14-3264794-large.gif
2023,10092737,FIGURE 15.,Histograms of Wasserstein distance between encoded distribution from a fixed instance and distributions from randomly sampled instances in other classes. These results are computed by the proposed model with disentanglement term.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092737/lee15-3264794-large.gif
2023,10124747,FIGURE 1.,Statistical analysis of article publication trends from 2015 to 2022.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa1-3276439-large.gif
2023,10124747,FIGURE 2.,Taxonomy– PAL based on ML techniques to automatically identify LSs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa2-3276439-large.gif
2023,10124747,FIGURE 3.,Research development process of PAL based on ML techniques to classify LS automatically and dynamically [6].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa3-3276439-large.gif
2023,10124747,FIGURE 4.,Platforms that stimulated research on PAL based on ML to identify LSs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa4-3276439-large.gif
2023,10124747,FIGURE 5.,Frequency of learning style theories used in PAL to classify learners’ LSs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa5-3276439-large.gif
2023,10124747,FIGURE 6.,Frequency of learning style classification (ML) algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa6-3276439-large.gif
2023,10124747,FIGURE 7.,"Frequency of classification methods, combinations and comparisons.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa7-3276439-large.gif
2023,10124747,FIGURE 8.,Learning styles application in developing adaptive learning systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124747/essa8-3276439-large.gif
2023,10057473,Fig. 1.,The age distribution (years of birth) of our participants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9970396/10057473/gross1-3251842-large.gif
2023,10057473,Fig. 2.,Grouping our participants’ organizations according to size.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9970396/10057473/gross2-3251842-large.gif
2023,10057473,Fig. 3.,Self-reported AI maturity of participants’ organizations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9970396/10057473/gross3-3251842-large.gif
2023,10057473,Fig. 4.,Reported relevance for the all five attacks we tested. The Likert-scale provided had four items ranging from “irrelevant” to “very relevant.”,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10206/9970396/10057473/gross4-3251842-large.gif
2023,10151873,FIGURE 1.,Research paper selection methodology.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda1-3286344-large.gif
2023,10151873,FIGURE 2.,Steps involved for typical crime detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda2-3286344-large.gif
2023,10151873,FIGURE 3.,Research publication trends from 2018-2022.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda3-3286344-large.gif
2023,10151873,FIGURE 4.,Distribution of article’s page counts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda4-3286344-large.gif
2023,10151873,FIGURE 5.,Distribution of article’s citations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda5-3286344-large.gif
2023,10151873,FIGURE 6.,Distribution of research articles at various venues.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda6-3286344-large.gif
2023,10151873,FIGURE 7.,Word cloud on selected articles.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda7-3286344-large.gif
2023,10151873,FIGURE 8.,Distribution of neighborhood crime-related selected article’s technique types.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda8-3286344-large.gif
2023,10151873,FIGURE 9.,Distribution of neighborhood crime-related selected article’s technique’s classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda9-3286344-large.gif
2023,10151873,FIGURE 10.,Distribution of neighborhood Crime related selected article’s Technique classes Versus technique Type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda10-3286344-large.gif
2023,10151873,FIGURE 11.,Architecture flow of crime prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10151873/manda11-3286344-large.gif
2023,10124208,FIGURE 1.,"(a), (b) Back and the front side of the ear-hook with behind-the-ear EEG sensors; (c) The proposed device; (d) EEG measurement locations behind the ear.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung1abcd-3276244-large.gif
2023,10124208,FIGURE 2.,(a) First PCB part of the proposed device; (b) Second PCB part of the proposed device; (c) The block diagram of the proposed device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung2abc-3276244-large.gif
2023,10124208,FIGURE 3.,"(a) Experiment Protocol, (b) Feature extraction from the relative PSD using FFT.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung3ab-3276244-large.gif
2023,10124208,FIGURE 4.,(a) Power spectral densities (PSDs) during eyes-closed and eyes-opened resting state from On-Scalp and the behind-ear locations; (b) Use of TensorFlow and TensorFlow Lite to deploy a model on our embedded device.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung4ab-3276244-large.gif
2023,10124208,FIGURE 5.,(a) Proposed MLP model for on-chip classification; (b) Proposed 1D-CNN model for on-chip classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung5ab-3276244-large.gif
2023,10124208,FIGURE 6.,Functional block diagram of the proposed intelligent EEG embedded system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung6-3276244-large.gif
2023,10124208,FIGURE 7.,"Features extracted from three electrodes for two emotionals states: (a) electrode 1, (b) electrode 2, (c) electrode 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung7abc-3276244-large.gif
2023,10124208,FIGURE 8.,The average distribution of relative PSD from all electrodes over five frequency bands between 2 emotional states: (a) negative and (b) positive.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung8ab-3276244-large.gif
2023,10124208,FIGURE 9.,"Classfication results for MLP (a, c, e), and 1D-CNN (b, d, f) models on Subject 2.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung9-3276244-large.gif
2023,10124208,FIGURE 10.,(a) Data visualization using t-SNE and SVM; (b) Comparison of classification accuracy between male and female subjects.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung10ab-3276244-large.gif
2023,10124208,FIGURE 11.,(a) Process of using our real-time EEG-based embedded system; (b) Information about the user’s emotional state is displayed in the smartphone app.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10124208/chung11ab-3276244-large.gif
2023,10130155,FIGURE 1.,Technical road map.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi1-3278370-large.gif
2023,10130155,FIGURE 2.,Dataset descriptive statistics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi2-3278370-large.gif
2023,10130155,FIGURE 3.,Correlation matrix data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi3-3278370-large.gif
2023,10130155,FIGURE 4.,Correlation between predictor variables and learning effect.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi4-3278370-large.gif
2023,10130155,FIGURE 5.,The selection process of Decision Tree hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi5-3278370-large.gif
2023,10130155,FIGURE 6.,The selection process of KNN hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi6-3278370-large.gif
2023,10130155,FIGURE 7.,The selection process of Naive Bayes hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi7-3278370-large.gif
2023,10130155,FIGURE 8.,The selection process of Neural Net hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi8-3278370-large.gif
2023,10130155,FIGURE 9.,The selection process of Random Forest hyperparameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi9-3278370-large.gif
2023,10130155,FIGURE 10.,Performance comparison of learning effect prediction models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10130155/shi10-3278370-large.gif
2023,10155117,FIGURE 1.,Feature selection framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan1-3287326-large.gif
2023,10155117,FIGURE 2.,Defects prediction accuracy based on data sets using various classifiers (a) Accuracy on CM1; WFS and WOFS (b) Accuracy on JM1; WFS and WOFS (c) Accuracy on KC1; WFS and WOFS (d) Accuracy on KC2; WFS and WOFS (e) Accuracy on and WOFS PC1; WFS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan2abcde-3287326-large.gif
2023,10155117,FIGURE 3.,Defect prediction accuracy based on classifiers using various data sets (a) Accuracy of Multilayer Perceptron; WFS and WOFS (b) Accuracy of Lazy IBK; WFS and WOFS (c) Accuracy of Bayes Net; WFS and WOFS (d) Accuracy of Rule ZeroR; WFS and WOFS (e) Accuracy of J48; WFS and WOFS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan3abcde-3287326-large.gif
2023,10155117,FIGURE 4.,Defect prediction datasets accuracy on various classifiers WFS and WOFS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan4-3287326-large.gif
2023,10155117,FIGURE 5.,Defect prediction classifiers accuracy on various datasets WFS and WOFS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan5-3287326-large.gif
2023,10155117,FIGURE 6.,Results comparison on Classifiers basis using various data sets (a) Logistic Regression; Proposed (WFS) and AK (WOFS) (b) Random Forest; Proposed (WFS) and AK (WOFS) (c) Decision Stump; Proposed (WFS) and AK (WOFS) (d) Support Vector Machine; Proposed (WFS) and AK (WOFS).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan6abcd-3287326-large.gif
2023,10155117,FIGURE 7.,Comparison with Alsaeedi and Khan [50] results on classifiers basis using various data sets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan7-3287326-large.gif
2023,10155117,FIGURE 8.,Results comparison on data sets basis using various classifiers (a) JM1; Proposed (WFS) and AK (WOFS) (b) CM1; Proposed (WFS) and AK (WOFS) (c) KC2; Proposed (WFS) and AK (WOFS) (d) PC1; Proposed (WFS) and AK (WOFS).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan8abcd-3287326-large.gif
2023,10155117,FIGURE 9.,Comparison with Alsaeedi and Khan [50] results on data set basis using various classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan9-3287326-large.gif
2023,10155117,FIGURE 10.,T-Test using Minitab.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan10-3287326-large.gif
2023,10155117,FIGURE 11.,T-Test Result.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10155117/khan11-3287326-large.gif
2023,10015695,Fig. 1.,"Examples of captured inertial motions (i.e., linear accelerations and angular velocities) of command-related gestures (G1 through G8) and irrelevant gestures (G0).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim1-3236458-large.gif
2023,10015695,Fig. 2.,Customized software implemented in the smartwatch used in the experiment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim2-3236458-large.gif
2023,10015695,Fig. 3.,Structure of the messages used to send motion sensors data in real-time (fields' size are given in bytes).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim3-3236458-large.gif
2023,10015695,Fig. 4.,"Data segmentation with a custom build software showing a segmenting window. Once a specific user-intended gesture is segmented as highlighted in a red rectangle, the system additionally segments the adjacent windows by shifting the window by 5, 10, and 20 timesteps in both forward and backward directions in time and assigns them to the same class that user is assigned. Also, by shifting the window by 30 and 40 timesteps in both directions in time, the system also automatically produces the segmented data belong to in-between class (i.e., G9). Segmentation of G9 class highlighted in a blue rectangle is important to address issues from the in-transition gesture that should not trigger any of predefined gestures (i.e., G1 through G8).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim4-3236458-large.gif
2023,10015695,Fig. 5.,Examples of augmented using adjacent signals. The center of the segmentation window is highlighted by a dotted line only for visualization purposes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim5-3236458-large.gif
2023,10015695,Fig. 6.,"Examples of motions irrelevant to the proposed classification task. Note that we do not differentiate irrelevant gesture types but treat each as a single gesture type (i.e., G0). Unlike for other classes (G1 through G9), the segmentation process for G0 was accomplished with an automatic (not manual) sliding window.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim6-3236458-large.gif
2023,10015695,Fig. 7.,"Deep learning architectures adopted in this study. For the Conv1D model, we set the kernel size as five and the stride length of the convolution as one. All kinds of neural layers are stacked twice relatively to learn temporal dependencies that resided in the input signals. For simplicity, bidirectional LSTM and GRU is abbreviated as bLSTM and bGRU, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim7-3236458-large.gif
2023,10015695,Fig. 8.,"Confusion matrix based on deep learning classifiers. Here, row indicates actual classes,
y
y
, and column indicates predicted results,
y
y
^
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim8-3236458-large.gif
2023,10015695,Fig. 9.,"Visualizations of the learned features produced in the last hidden layer of each model on the test datasets using the t-SNE algorithm [36]. Compared to the results from Conv1D, gated RNN-based approaches exhibit more clustered distribution in the high dimensional latent space.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim9-3236458-large.gif
2023,10015695,Fig. 10.,"3D visualizations of the learned features produced in the last hidden layer of the bidirectional GRU (i.e., bGRU) model with varying azimuthal angles (30120, 210, and 300 degrees). Each point is colored according to the inferred classes,
y
y
^
∈
R
C
, similar to Fig. 9.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7274857/10136408/10015695/kim10-3236458-large.gif
2023,9655443,Algorithm 1:,DM-cSVM,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221036/10153804/9655443/zou5-3131424-large.gif
2023,9905918,Fig. 1.,Diagram of EMG electrode (green) and retro-reflective marker (blue) locations. Dashed lines indicate placement on the posterior section of the arm. PIP 2/5 = proximal interphalangeal joint of the 2nd and 5th digit; MCP 2/5 = metacarpophalangeal joint of the 2nd and 5th digit; ECRL = extensor carpi radialis longus; EDC = extensor digitorum communis; FCR = flexor carpi radialis; FDS = flexor digitorum superficialis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang1-3210892-large.gif
2023,9905918,Fig. 2.,The three tested arm postures for offline decoder evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang2-3210892-large.gif
2023,9905918,Fig. 3.,"Structures of the NARX network decoder in open-loop (top) and closed-loop (bottom) configurations. The decoder was calibrated first in open-loop and then in closed-loop configurations, but for real-time use, only the closed-loop configuration was tested.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang3-3210892-large.gif
2023,9905918,Fig. 4.,"Block diagram of the N2M2 decoder. Only the blue blocks and paths were active during real-time use. The measured joint positions, reward function, and critic were only active during calibration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang4-3210892-large.gif
2023,9905918,Fig. 5.,The 9 target postures (black) and base posture (blue) in the online postural matching task.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang5-3210892-large.gif
2023,9905918,Fig. 6.,"The Box and Blocks Test (a), Modified Page Turning Test (b), and Shelf Test (c).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang6-3210892-large.gif
2023,9905918,Fig. 7.,"Representative plots of simultaneous wrist (left) and MCP (right) joint angles estimated by the N2M2 (blue), MM (red), NN (orange), and MLP (purple) decoders for a trial completed by a non-disabled subject with self-selected movement velocities. Measured joint angles are represented by the black lines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang7-3210892-large.gif
2023,9905918,Fig. 8.,"Representative plots of simultaneous wrist (left) and MCP (right) joint angles estimated by the N2M2 (blue), MM (red), NN (orange), and MLP (purple)decoders for a trial completed by the transradial amputee subject with self-selected movement velocities. Measured joint angles are represented by the black lines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang8-3210892-large.gif
2023,9905918,Fig. 9.,"Average Pearson's correlation coefficient (r) between estimated and measured joint angles of the N2M2 (blue), MM (red), NN (orange), and MLP (purple) decoders for each of the 3 postures and shifted electrode locations. Error bars represent standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang9-3210892-large.gif
2023,9905918,Fig. 10.,"Average points and fitted lines for the Pearson's correlation coefficient (r) (top) and NRMSE (bottom) between estimated and measured wrist (left) and MCP (right) joint angles of the N2M2 (blue), MM (red), NN (orange), and MLP (purple) decoders for EMG data with added Gaussian noise with increasing standard deviation (σ).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang10-3210892-large.gif
2023,9905918,Fig. 11.,The average mean absolute jerk for joint angles estimated offline (excluding simulated noise cases) (a) and during the Virtual Posture Matching Test (b) for each decoder. Lower values indicate smoother estimated position trajectories. Error bars represent standard deviation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang11-3210892-large.gif
2023,9905918,Fig. 12.,"Average task completion percentage, task duration, normalized path length, and number of overshoots achieved by subjects using the N2M2 and NN decoders. Error bars represent standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9905918/huang12-3210892-large.gif
2023,10147346,Figure 1.,Disciplines of machine learning [80].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang1-3284717-large.gif
2023,10147346,Figure 2.,Broader scope around ML [80].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang2-3284717-large.gif
2023,10147346,Figure 3.,"Simplified block diagram on ML applications in a generic motor drive system. Every part of the motor drive control scheme could be ML-based, while also the entire control framework could be just one large ML model. The control depiction utilizing rotating
dq
coordinates is only of illustrative purpose as the ML-based method is not limited to this coordinate system.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang3-3284717-large.gif
2023,10147346,Figure 4.,Illustration of the rotor-flux-based MRAS components replaced by neural networks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang4-3284717-large.gif
2023,10147346,Figure 5.,Simplified block diagram on ML serving as different components of the SMO-based position observer based on the extended EMF model of PM machines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang5-3284717-large.gif
2023,10147346,Figure 6.,"(a) Schematic integration of an inverter model and inverter compensation scheme into electric machine drives. (b) Visualization of the utilized signals for the training process of the black box inverter model with time dependence: NN
M
and NN
C
represent the neural networks for the inverter model (16) and the inverter compensation scheme (17), respectively [200].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang6-3284717-large.gif
2023,10147346,Figure 7.,(a) Simplified schematic of the overall RL-based control and drive system structure partitioning the agent and environment. (b) Setup of the development process including the online RL remote rapid control prototyping toolchain [74].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang7-3284717-large.gif
2023,10147346,Figure 8.,"Overview of relevant embedded platform types available in the market, illustrating a simplified block diagram of their topology, providing indicative ranges for the theoretical peak TOPS performance and power consumption for each platform type, along with one representative device example. Adapted from [240].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang8-3284717-large.gif
2023,10147346,Figure 9.,FPGA-based SoC structure for the inference of ML models for motor control applications. Adapted from [244].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang9-3284717-large.gif
2023,10147346,Figure 10.,PYNQ—an open-source project from Xilinx that features an easy software interface and framework for rapid prototyping and development [250].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang10-3284717-large.gif
2023,10147346,Figure 11.,Example system with an integrated DPU [258].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782707/10008994/10147346/zhang11-3284717-large.gif
2023,10061162,FIGURE 1.,"The trade-off between a machine learning surrogate model and the original model. Once trained on data from the original model, the production of new data is likely to be significantly more efficient in terms of computation and time. However, as the machine learning model is produced using data from the original model, there will be some inevitable reduction in accuracy.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj1-3253429-large.gif
2023,10061162,FIGURE 2.,"A top down diagram of the AGR graphite core parmec model. Bricks are arranged into channels of two different types: fuel (blue) and interstitial (grey). Both types of channel are the same height, with fuel bricks being stacked seven high and the shorter interstitial bricks being stacked 12 high. The cracking status of all 1988 fuel bricks is included in the input features (whether the brick is cracked or not) of the surrogate machine learning model. For the output labels, only the earthquake response of the upper most interstitial brick (orange) is predicted by the surrogate machine learning model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj2-3253429-large.gif
2023,10061162,FIGURE 3.,"Visualisation of a 3-dimensional feature encoding. This example represents a single instance with each data-point representing a fuel brick. Yellow and black data points represent uncracked and cracked bricks, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj3-3253429-large.gif
2023,10061162,FIGURE 4.,"An example of image manipulation techniques to perform data augmentation. The base instance of an image depicting a bird is shown in image (a). The additional images show examples of two types of augmentation. Images (b), (c) and (d) show image (a) rotated by 90, 180 and 270 degrees, respectively. Similarly, images (e) and (f) show image (a) reflected about the vertical and horizontal axis, respectively. Despite being manipulated in this way, each image still effectively depicts an example of a bird and can be treated as such in the training of a machine learning model. Data augmentation can be used to expand a dataset without labelling additional examples, potentially improving model performance and reducing overfitting.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj4abcdef-3253429-large.gif
2023,10061162,FIGURE 5.,"The results from the best performing model produced via the method described in a previous research work [3]. This model is M3, the performance of which is listed in Table 1. The top and middle images both show the model prediction plotted as a function of the ground truth values. For comparison, the distribution of the dataset is shown in the bottom image. Notice that beyond the delineations shown in the top and middle images (0.2 & 0.6, respectively) there are fewer dataset examples, hence lower accuracy. TABLE 1
Performance of the Five Best Performing Models Produced Using the Method of [3]. Each model was evaluated using a dedicated testing set sequestered for this research work. The mean squared error (MSE) metric was used to evaluate each model against this testing set
Model Test Performance (MSE)
M1 9.28E-3
M2 9.25E-3
M3 9.22E-3
M4 9.68E-3
M5 9.48E-3
Performance of the Five Best Performing Models Produced Using the Method of [3]. Each model was evaluated using a dedicated testing set sequestered for this research work. The mean squared error (MSE) metric was used to evaluate each model against this testing set",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj5-3253429-large.gif
2023,10061162,FIGURE 6.,"An example of image manipulation techniques applied to Parmec data to facilitate data augmentation. i: a slice from the randomly generated feature inputs for an example instance: the dark blue spots represent intact fuel bricks, with the yellow spots being cracked bricks. This image is the equivalent of image (a) from Figure 4, i.e. it is an original, unaltered instance. Note also the position of the interstitial brick at the centre of the core (O) which is the source of the output labels (see Figure 2) ii: the same example instance as shown in (i), but it has been mirrored about the vertical centre-line. This image is the equivalent of image (e) from Figure 4. Note that the position of the central brick (O) is the same as it resides on the line about which the mirroring operation took place. iii: again, the same example as (i), but this time it has been rotated by 90 degrees - the equivalent of image (b) from Figure 4. In each case, the output label (O) effectively remains the same, as it represents the central brick of the core, about which the rotation or mirroring is performed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj6abcdefghijklmno-3253429-large.gif
2023,10061162,FIGURE 7.,"Parmec outputs for all bricks in the top level of the core. The colour of each brick data-point represents the average across the entire dataset (blue to red represents low to high, respectively). Note the symmetry about the vertical and horizontal centre-lines.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj7-3253429-large.gif
2023,10061162,FIGURE 8.,"Loss functions: visual comparison. Three loss functions are compared graphically. Blue: mean squared error, as values become more extreme, the loss value increases geometrically meaning that outliers heavily influence the calculated value. Red: mean absolute error, the loss value increases linearly as the input increases, reducing the effect of outliers. Green: Huber loss, a balance between the previous two loss functions mentioned previously. The loss function has a linear outer region and a central non-linear region.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj8-3253429-large.gif
2023,10061162,FIGURE 9.,"Adjustment factor as is utilised by Eq. (1). The normal distribution (Z) is centred on the mode (
μ
) and scaled by the standard deviation (
σ
) of the data distribution. This factor can be adjusted as per the right hand side of Eq. (1).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj9-3253429-large.gif
2023,10061162,FIGURE 10.,"Simplified model architecture used to compare augmentation approaches. The input layer requires a 3-dimensional tensor representing the bricks of the top three layers of the AGR core. This is followed by a convolutional layer of 16 nodes, each with a
3×3
window size. Two fully connected layers then follow, the first with 32 nodes, the second with 64 nodes. The first two layers have tanh and softplus [28] activation functions, respectively, with the second fully connected layer again using a tanh activation function. Each of these layers utilised a 20% dropout rate [10] during training. The output layer represents only a single value - displacement in the single brick during the earthquake. This model is highly simplified compared to that of [3].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj10-3253429-large.gif
2023,10061162,FIGURE 11.,"Visual Summary and Comparison of the Performance of the Optimal Model Produced During E1.0 (Left) & E2.4 (Right). The simplified model architecture shown in Figure 10 is trained on the original, unaugmented dataset. This process was repeated 32 times and evaluated against the test set. The predictions of the model with the lowest test loss are plotted against ground truth values and presented in four ways. In the top image, bounding lines are placed parallel to perfect prediction/ ground truth agreement line (black), demarcating a 10 & 20 percentage point margin. The lower three images split the data space into segments and quantify the proportion of samples which are correctly placed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj11ab-3253429-large.gif
2023,10061162,FIGURE 12.,"Visual summary of the performance of the optimal model produced during E4.1. The simplified model architecture shown in Figure 10 is trained on an expanded dataset which includes augmentation as described in subsection III-A. The custom loss function as per Eq. (1) & Eq. (2) is used during training with an
α
of value of 2. This process was repeated 32 times and evaluated against the test set. The predictions of the model with the lowest test loss are plotted against ground truth values and presented in four ways. In the top image, bounding lines are placed parallel to perfect prediction/ground truth agreement line (black), demarcating a 10 & 20 percentage point margin. The lower three images split the data space into segments and quantify the proportion of samples which are correctly placed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj12-3253429-large.gif
2023,10061162,FIGURE 13.,"Visual summary and comparison of the performance of the original model M3 (Left) & M3 After E5.3 (Right). The model obtained from [3] is trained using our custom loss function Eq. (1), with an
α
value of 2. This process was repeated six times and evaluated against the test set. The predictions of the model with the lowest test loss are plotted against ground truth values and presented in four ways. In the top image, bounding lines are placed parallel to perfect prediction/ground truth agreement line (black), demarcating a 10 & 20 percentage point margin. The lower three images split the data space into segments and quantify the proportion of samples which are correctly placed.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10061162/rhysj13ab-3253429-large.gif
2023,10015793,Fig. 1.,"(a) Conventional 1-D CNN for classification problem. (b) Prototype-based NN classifier architecture proposed by [28] where the trajectory of obtaining three ECs is overlaid in colored lines/arrows. Note that any NN can be used as an encoder and eight-layered 1-D CNN was chosen in our implementation where the same eight-layered 1-D CNN was also used for the conventional approach. The numbers of prototypes (
m
) and FC layers (
n
) were chosen from four to eight and one or two, respectively. (c) Examples on prototypical samples obtained from MNIST (left) and fashion-MNIST (right) datasets (both taken from [28]). The transformation (or reconstruction) process from prototype vectors in the latent space to prototype samples in the input space is shown as the left-hand side going blue diagonal (EC-1.1) in Fig. 1(b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh1abc-3236500-large.gif
2023,10015793,Fig. 2.,Map showing Vincent fields in the Exmouth sub-basin. Taken from [39].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh2-3236500-large.gif
2023,10015793,Fig. 3.,Supervised seismic facies classification workflow used in this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh3-3236500-large.gif
2023,10015793,Fig. 4.,"Formation evaluation results overlaid over well log data of Vincent-1, 2, and 3.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh4-3236500-large.gif
2023,10015793,Fig. 5.,Elastic properties estimated from seismic prestack inversion. Modified from [39].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh5-3236500-large.gif
2023,10015793,Fig. 6.,"Seismic facies classification results generated by (a) conventional CNN and (b) prototype NN, together with the well log interpreted facies used as labels during training.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh6ab-3236500-large.gif
2023,10015793,Fig. 7.,"Cross plots of training data and the learned prototypes decoded into the input data domain. The learned prototypes are marked with (a) indices of the prototype samples and (b) their prototypical facies. The left and right columns show the P-impedance versus Vp/Vs ratios and the Lambda-Rho versus Mu-Rho ratios, respectively. The plot in (b) corresponds to EC-1 in Fig. 1(b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh7ab-3236500-large.gif
2023,10015793,Fig. 8.,"Normalized squared
L
2
distances between the encoded surface seismic data and the eight prototype vectors in the latent space. The results in (a) and (b), (c)–(e), (f) and (g), and (h) refer to prototypical shale, wet-sand, oil-sand, and gas-sand samples, respectively. Indices (a)–(h) correspond to prototype samples 1, 4, 2, 5, 7, 3, 6, and 8, respectively. The plot in (b) corresponds to the EC-1 in Fig. 1(b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh8abcdefgh-3236500-large.gif
2023,10015793,Fig. 9.,"Normalized squared
L
2
distances between the surface seismic data and the decoded prototype vectors in the input domain. Indices (a)–(d) correspond to prototype samples 4, 5, 6, and 8, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10015793/noh9abcd-3236500-large.gif
2023,10121031,FIGURE 1.,Horizontal Privilege Escalation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin1-3273895-large.gif
2023,10121031,FIGURE 2.,Vertical Privilege Escalation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin2-3273895-large.gif
2023,10121031,FIGURE 3.,Privilege Escalation Attack Process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin3-3273895-large.gif
2023,10121031,FIGURE 4.,Random Forest Classifier for Insider threat classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin4-3273895-large.gif
2023,10121031,FIGURE 5.,Adaboost Classifier for Insider threat classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin5-3273895-large.gif
2023,10121031,FIGURE 6.,XGBoost Classifier for Insider threat classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin6-3273895-large.gif
2023,10121031,FIGURE 7.,LightGBM Classifier for Insider threat classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin7-3273895-large.gif
2023,10121031,FIGURE 8.,Overview of Privilege Escalation Attack Proposed Models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin8-3273895-large.gif
2023,10121031,FIGURE 9.,Flow chart of the proposed technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin9-3273895-large.gif
2023,10121031,FIGURE 10.,Features of Dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin10-3273895-large.gif
2023,10121031,FIGURE 11.,Demonstration of User Action from respective PC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin11-3273895-large.gif
2023,10121031,FIGURE 12.,Confusion Matrix of Random Forest.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin12-3273895-large.gif
2023,10121031,FIGURE 13.,Confusion Matrix of AdaBoost.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin13-3273895-large.gif
2023,10121031,FIGURE 14.,Confusion Matrix of XGBoost.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin14-3273895-large.gif
2023,10121031,FIGURE 15.,Confusion Matrix of LightGBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin15-3273895-large.gif
2023,10121031,FIGURE 16.,Heatmap of the specified features of dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin16-3273895-large.gif
2023,10121031,FIGURE 17.,Classification Report.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin17-3273895-large.gif
2023,10121031,FIGURE 18.,Different Algorithms for classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin18-3273895-large.gif
2023,10121031,FIGURE 19.,Recall score of proposed algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin19-3273895-large.gif
2023,10121031,FIGURE 20.,F1-Score of proposed algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin20-3273895-large.gif
2023,10121031,FIGURE 21.,Comparative Analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin21-3273895-large.gif
2023,10121031,FIGURE 22.,False Alarm Rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10121031/amin22-3273895-large.gif
2023,10093141,Fig. 1.,Flow diagram of the LHD subcooling system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana1-3264657-large.gif
2023,10093141,Fig. 2.,Schematic view of the network structure for the model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana2-3264657-large.gif
2023,10093141,Fig. 3.,Training data of the mass flow rate of subcooled He.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana3-3264657-large.gif
2023,10093141,Fig. 4.,"Training data of current values of the helical coil windings (a) H-I, (b) H-M, and (C) H-O.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana4-3264657-large.gif
2023,10093141,Fig. 5.,Training data of the temperatures at the coil case.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana5-3264657-large.gif
2023,10093141,Fig. 6.,Training data of temperatures at the coil outlet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana6-3264657-large.gif
2023,10093141,Fig. 7.,Input data for the model. (a) Trapezoidal current waveform and (b) staircase current waveform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana7-3264657-large.gif
2023,10093141,Fig. 8.,"Comparison between the prediction and measurement results for each set of input data, which is on 24/1/2020, including the trapezoidal current waveform.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana8-3264657-large.gif
2023,10093141,Fig. 9.,"Comparison between the prediction and measurement results for each set of input data, which is on 16/1/2020, including the staircase current waveform.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/77/10025583/10093141/obana9-3264657-large.gif
2023,9751352,Fig. 1.,The architecture of DFLM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9751352/he1-3165592-large.gif
2023,9751352,Fig. 2.,"The ten models(pre3,pre4,pre5,fine3,fine4,fine5,DeepBind,DanQ,ECLSTM,ECBLSTM) distribution of Auc across 69 ChIP-seq datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9751352/he2-3165592-large.gif
2023,9751352,Fig. 3.,"Six indicators(Auc, Acc, Mcc, Recall, Precision, Specificity) evaluation of the DFLM(pre3,pre4,pre5,fine3,fine4,fine5). (a) The performance of Auc in these DFLM. (b) The performance of Acc in these DFLM. (c) The performance of Mcc in these DFLM. (d) The performance of Recall in these DFLM. (e) The performance of Precision in these DFLMs. (f) The performance of Specificity in these DFLMs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9751352/he3-3165592-large.gif
2023,9751352,Fig. 4.,"The distribution of the ten models in the four different types Auc in complex, simple, small, and large. The green dataset represents the small dataset and the yellow dataset represents the large dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9751352/he4-3165592-large.gif
2023,9751352,Fig. 5.,Points are colored based on the first 2 bases of the 3-mer in our analysis. Visualization results of t-SNE with different encoding methods. (a) the t-SNE visualization one-hot. (b) the t-SNE visualization of pre-training 3-mer embedding representation with hg38. (c) the t-SNE visualization of fine-tuning 3-mer embedding representation with POLR2A dataset. (d) the t-SNE visualization of fine-tuning 3-mer embedding representation with KDM5A dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10036331/9751352/he5-3165592-large.gif
2023,10038667,FIGURE 1.,MIMO-NOMA system diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10038667/goudo1-3242917-large.gif
2023,10038667,FIGURE 2.,Classic ELM neural network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10038667/goudo2-3242917-large.gif
2023,10038667,FIGURE 3.,RC-ELM network training stage block diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10038667/goudo3-3242917-large.gif
2023,10038667,FIGURE 4.,BER vs. SNR for the classic ELM receiver as a function of different hidden layer neurons.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10038667/goudo4-3242917-large.gif
2023,10038667,FIGURE 5.,"BER for ELM and RC-ELM-based MIMO-NOMA receivers in training stage with (a)
α=
10
−2
(b)
α=
10
−3
(c)
α=
10
−4
(d)
α=
10
−5
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10038667/goudo5abcd-3242917-large.gif
2023,10038667,FIGURE 6.,"BER for MMSE, MLP, ELM and RC-ELM-based MIMO-NOMA receivers (in testing stage with (a)
α=
10
−2
(b)
α=
10
−3
(c)
α=
10
−4
(d)
α=
10
−5
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10038667/goudo6abcd-3242917-large.gif
2023,10158700,FIGURE 1.,Eclipse bug report 220151.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae1-3288156-large.gif
2023,10158700,FIGURE 2.,Life cycle of the bug report [7].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae2-3288156-large.gif
2023,10158700,FIGURE 3.,Nature-based prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae3-3288156-large.gif
2023,10158700,FIGURE 4.,Overall architecture for proposed model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae4-3288156-large.gif
2023,10158700,FIGURE 5.,Confusion matrices for two voting machine learning classifiers (without text augmentation).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae5-3288156-large.gif
2023,10158700,FIGURE 6.,Confusion matrices for base machine learning classifiers (without text augmentation).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae6-3288156-large.gif
2023,10158700,FIGURE 7.,Confusion matrices ensemble voting machine learning classifiers (with text augmentation).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae7-3288156-large.gif
2023,10158700,FIGURE 8.,Confusion matrices for base machine learning classifiers (with text augmentation).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae8-3288156-large.gif
2023,10158700,FIGURE 9.,F1-scores for each class without text augmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10158700/alsae9-3288156-large.gif
2023,9866523,Fig. 1.,"The scoring process of a single autoencoder model. The input data of the neural network is
X∈
R
n×d
, and the output is
X
^
∈
R
n×d
. When the model has reconstructed all the samples, calculate the distance
DS={DS
1
,D
S
2
,⋯,D
S
n
}∈
R
n×1
between
X
and
X
^
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10091711/9866523/shen1-3201295-large.gif
2023,9866523,Fig. 2.,"An overview of the ensemble deep learning method EnsDeepDP. In the first dashed frame of the figure gives an overview of the unsupervised stage. This stage consists of several neural networks with different structures, each model calculates the reconstruction
X
^
of
X
and the corresponding DS. The second dashed frame describes the selection process of DS. After that, the third dashed frame demonstrates the process of combining the selected DS with the original abundance. At last, a classifier is established on the composited features to get the final results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10091711/9866523/shen2-3201295-large.gif
2023,9866523,Fig. 3.,ROC curves of six datasets. EnsDeepDP is the method proposed in this paper. DSOnly represents the prediction result using classifiers only on the newly generated DS features. The curves of all methods are the average results from 30 different data partitions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10091711/9866523/shen3-3201295-large.gif
2023,9866523,Fig. 4.,"ROC curve with variance, the shaded part reflects the variance of the ROC curve, it shows the changes of ROC curve caused by 30 different data divisions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10091711/9866523/shen4-3201295-large.gif
2023,9866523,Fig. 5.,"The Venn diagrams for the numbers of selected features of XGBoost, CatBoost, EnsDeepDP_xgb, EnsDeepDP_catb of six datasets.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10091711/9866523/shen5-3201295-large.gif
2023,9866523,Fig. 6.,The change of the number of microbial biomarkers selected in the original features before and after the ensemble with DS features in six datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10091711/9866523/shen6-3201295-large.gif
2023,10049454,Fig. 1.,CHB-based three-level SAPF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v1-3244605-large.gif
2023,10049454,Fig. 2.,Proposed optimized ML-based current reference generation for SAPF.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v2-3244605-large.gif
2023,10049454,Fig. 3.,AutoML details. (a) Breakdown of the AutoML framework. (b) Overview of the Autokeras framework. (c) AutoML recommender MLP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v3abc-3244605-large.gif
2023,10049454,Fig. 4.,Performance of the proposed ANN model. (a) Predicted current model by using designed data. (b) Predicted current on system simulation data. (c) Designed data and system data combined.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v4abc-3244605-large.gif
2023,10049454,Fig. 5.,ML model-based SAPF simulation results. (a) Steady-state performance. (b) Load connection. (c) Load disconnection. (d) Unbalance load connection. (e) THD of the source current before the compensation. (f) THD of the source current after the harmonics mitigation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v5abcdef-3244605-large.gif
2023,10049454,Fig. 6.,SRF controller-based SAPF simulation results. (a) Nonlinear load disconnection. (b) Mitigated source current by the SRF controller and proposed ML. (c) Comparison of SRF and ML predicted fundamental current. (d) Partial nonlinear load disconnection under parameter variation (20% reduction in coupling inductance).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v6abcd-3244605-large.gif
2023,10049454,Fig. 7.,Experimental prototype developed in the laboratory.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v7-3244605-large.gif
2023,10049454,Fig. 8.,ML control-based SAPF experimental results. (a) Steady-state performance. (b) Load connection. (c) Unbalanced load connection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6245517/10149883/10049454/v8abc-3244605-large.gif
2023,9404874,Fig. 1.,An example of label enhancement.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng1-3073157-large.gif
2023,9404874,Fig. 2.,The flow chart of the proposed LESC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng2-3073157-large.gif
2023,9404874,Fig. 3.,The flow chart of the proposed gLESC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng3-3073157-large.gif
2023,9404874,Fig. 4.,"The flowchart of label recovery experiments conducted in this section. To be specific, the true label distributions are binarized to attain the logical labels first, then a LE method can be performed to obtain the recovered label distributions. Accordingly, we evaluate the recovered performance based on six frequently-used LDL evaluation measures [7].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng4-3073157-large.gif
2023,9404874,Fig. 5.,"Visualization of the ground-truth and recovered label distributions on the artificial dataset (regarded as RGB colors, best viewed in color).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng5-3073157-large.gif
2023,9404874,Fig. 6.,"CD diagrams of different LE methods on six measures, including Cheb, Canber, Clark, KL, Cosine, and Intersec. CD diagrams are calculated based on the Wilcoxon-Holm method [32]. Specifically, the method located on the right side is better that the method on the left side, and the line between two methods denotes that their recovery results are different within one critical difference.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng6-3073157-large.gif
2023,9404874,Fig. 7.,"Label recovery performance of LESC on SBU_3DFE, Yeast-alpha, and Yeast-cold in metrics of Cheb and Cosine. Specifically, different rows denote different values of
λ
1
, and different columns denote different values of
λ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng7-3073157-large.gif
2023,9404874,Fig. 8.,"Label recovery performance of gLESC on SBU_3DFE, Yeast-alpha, and Yeast-cold in metrics of Cheb and Cosine. Specifically, different rows denote different values of
λ
1
, and different columns denote different values of
λ
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9404874/zheng8-3073157-large.gif
2023,10149472,Figure 1.,Laminated bus bar with two apertures for terminals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel1-3285780-large.gif
2023,10149472,Figure 2.,Cross-section of two apertures for terminals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel2-3285780-large.gif
2023,10149472,Figure 3.,T-type equivalent circuit of laminated bus bar.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel3-3285780-large.gif
2023,10149472,Figure 4.,Split acquisition procedure of analysis datasets to realize online machine learning in which the machine learning model is sequentially updated.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel4-3285780-large.gif
2023,10149472,Figure 5.,"Algorithm of Gradient Boosting Decision Tree (GBDT) to achieve high prediction accuracy by decreasing the error between the predicted values
y
¯
and objective variable
y
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel5-3285780-large.gif
2023,10149472,Figure 6.,"Cross-validation, an evaluation methods to predict the accuracy of the machine learning model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel6-3285780-large.gif
2023,10149472,Figure 7.,Change in the analysis time of FEA with an increasing number of analysis points.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel7-3285780-large.gif
2023,10149472,Figure 8.,"Change in MRE and training time of
R
bus
with an increasing number of analysis points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel8-3285780-large.gif
2023,10149472,Figure 9.,"Change in MRE and training time of
L
bus
with an increasing number of analysis points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel9-3285780-large.gif
2023,10149472,Figure 10.,"Change in MRE and training time of
C
bus
with an increasing number of analysis points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel10-3285780-large.gif
2023,10149472,Figure 11.,"Detailed comparison between the predicted (horizontal axis) and the analyzed (vertical axis) of
L
bus
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel11-3285780-large.gif
2023,10149472,Figure 12.,Laminated bus bar for measurement using the impedance analyzer.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel12-3285780-large.gif
2023,10149472,Figure 13.,"Measurement environment for
R
bus
and
L
bus
using the impedance analyzer.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel13-3285780-large.gif
2023,10149472,Figure 14.,"Frequency characteristics of the predicted and measured
R
bus
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel14-3285780-large.gif
2023,10149472,Figure 15.,"Frequency characteristics of the predicted and measured
L
bus
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel15-3285780-large.gif
2023,10149472,Figure 16.,"Frequency characteristics of the predicted and measured
C
bus
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782709/10004734/10149472/ojpel16-3285780-large.gif
2023,9735300,Fig. 1.,Abstract concept of machine learning and deep learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba1-3159505-large.gif
2023,9735300,Fig. 2.,Example of a convolution operation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba2-3159505-large.gif
2023,9735300,Fig. 3.,Example of 2 × 2 max-pooling with stride = 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba3-3159505-large.gif
2023,9735300,Fig. 4.,Representation architecture of the proposed CNN model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba4-3159505-large.gif
2023,9735300,Fig. 5.,Schematic of using the proposed CNN model for ECG images of cardiac patients’ classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba5-3159505-large.gif
2023,9735300,Fig. 6.,Samples from the ECG images dataset. (a) NP. (b) AH. (c) MI. (d) H. MI.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba6-3159505-large.gif
2023,9735300,Fig. 7.,Sample from the ECG images dataset after performing cropping as a preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba7-3159505-large.gif
2023,9735300,Fig. 8.,Semantic of the confusion matrices for four classes results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba8-3159505-large.gif
2023,9735300,Fig. 9.,Training Progress for our proposed CNN model on the ECG images dataset in fold-1 (LR: 0.0001 and other hyperparameters are as in Table III).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba9-3159505-large.gif
2023,9735300,Fig. 10.,Confusion matrices of the proposed CNN model for classification of heart diseases in the ECG images dataset for each fold (RL: 0.0001 and other hyper-parameters are as in Table III).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9078688/10081152/9735300/abuba10-3159505-large.gif
2023,9976895,Algorithm 1,LAMA Algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn11-3225505-large.gif
2023,9976895,Algorithm 2,Mean and Variance Functions F and G,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn12-3225505-large.gif
2023,9976895,Algorithm 3,OAMP Algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn13-3225505-large.gif
2023,9976895,Algorithm 4,LAMANetMMSE Algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn14-3225505-large.gif
2023,9976895,Fig. 1.,"Number of multiplications required for preprocessing and detection for various detector types, antenna configurations, and modulation types. The
x
-axis shows the real-valued system dimensions of
H
and modulation type. (a) Number of multiplications in preprocessing performed once per channel coherence interval. LAMA-type detectors have no preprocessing and therefore are not shown in the log-scale plot. (b) Number of multiplications per channel use for the number of iterations
M
I
=10
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn1ab-3225505-large.gif
2023,9976895,Fig. 2.,Pipeline structure of the LAMANet accelerator design. The green numbers within each pipeline stage refer to the line number in the respective algorithm marked by the blue boxes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn2-3225505-large.gif
2023,9976895,Fig. 3.,Architecture of Cholesky-based MMSE matrix calculation for initializing the LAMANetMMSE detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn3-3225505-large.gif
2023,9976895,Fig. 4.,Architecture of the Jacobi-based eigenvalue decomposition calculation for initializing the LAMANetOpt detector.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn4-3225505-large.gif
2023,9976895,Fig. 5.,"Untrained detector performance on 20000 randomly taken channels from one drop (5120 channel realizations). The configuration of
M
R
=128
and
M
T
=32
for various modulations is shown.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn5-3225505-large.gif
2023,9976895,Fig. 6.,"Trained detector performances on a randomly selected channel for the configuration
M
R
=128
and
M
T
=32
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn6-3225505-large.gif
2023,9976895,Fig. 7.,"CDF of the trained detector performance over a total of 143 channel realizations for a configuration of
M
R
=64
and
M
T
=16
, QAM4. The measured SNR is
≈7
dB.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn7-3225505-large.gif
2023,9976895,Fig. 8.,"LAMANet performance metrics for various antenna and modulation configurations. The numbers indicate the number of antennas, and
M
R
and
M
T
are twice the indicated value: 1Normalized to one iteration and only supports QPSK, not QAM64. (a) Latency and throughput. (b) Resource utilization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn8ab-3225505-large.gif
2023,9976895,Fig. 9.,MMSE matrix inverse performance metrics for various antenna configurations. (a) Latency and throughput. (b) Resource utilization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn9ab-3225505-large.gif
2023,9976895,Fig. 10.,Eigenvalue decomposition performance metrics for various antenna configurations. (a) Latency and throughput. (b) Resource utilization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/92/10049803/9976895/brenn10ab-3225505-large.gif
2023,10049438,FIGURE 1.,Component of the power system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba1-3247193-large.gif
2023,10049438,FIGURE 2.,The architecture of a smart grid system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba2-3247193-large.gif
2023,10049438,FIGURE 3.,The workflow of the proposed model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba3-3247193-large.gif
2023,10049438,FIGURE 4.,The architecture of the RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba4-3247193-large.gif
2023,10049438,FIGURE 5.,The input path of an RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba5-3247193-large.gif
2023,10049438,FIGURE 6.,The reconstruction of RBM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba6-3247193-large.gif
2023,10049438,FIGURE 7.,Overview of the power system framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba7-3247193-large.gif
2023,10049438,FIGURE 8.,The accuracy of the conducted experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba8-3247193-large.gif
2023,10049438,FIGURE 9.,The precision of the conducted experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba9-3247193-large.gif
2023,10049438,FIGURE 10.,The recall score of the conducted experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba10-3247193-large.gif
2023,10049438,FIGURE 11.,The f1 score of the conducted experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049438/diaba11-3247193-large.gif
2023,10004567,Fig. 1.,"Examples of different levels of medical image labels, where the image-level class label in (b) contains only the lesion category. The bounding box label in (c) contains not only the lesion category but also a coarse location. The pixel-level label in (d) contains both the lesion category and location information of each pixel, which is strong image supervision. Though strong image supervisions are more informative, they are very expensive to obtain. The utilization of some easy-to-access image supervision is beneficial in practice.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/10004567/yan1abcd-3233405-large.gif
2023,10004567,Fig. 2.,"Illustration of the proposed Mixed Supervised Federated Learning (FedMix) framework. The local client update utilizes every available supervision for training. Based on this, an adaptive weight aggregation procedure is used for the global federated model update. Compared to existing methods, FedMix not only breaks through the constraint of a single level of image supervision but also can dynamically adjust the aggregation weight of each local client, achieving rich yet discriminative feature representations.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/10004567/yan2-3233405-large.gif
2023,10004567,Algorithm 1,Pseudocode of FedMix,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/10004567/yan5-3233405-large.gif
2023,10004567,Fig. 3.,"Exemplar qualitative results of different learning frameworks for breast tumor segmentation. The upper part (Rows 1 to 7): the raw images, the segmentation maps produced by local learning (LL), FedST and FedMix under semi-supervision (i. e.,[C1, C2, C3] = [
U
,
U
,
L
]), the segmentation maps of FedMix under mixed supervision (i. e.,[C1, C2, C3] = [
I
,
U
,
L
] and [C1, C2, C3] = [
B
,
B
,
L
]), and the manual annotations by experts respectively. The lower part (Rows 8 to 9): the segmentation maps obtained by federated learning under full pixel-level supervision using FedAvg and the proposed adaptive aggregation function respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/10004567/yan3-3233405-large.gif
2023,10004567,Fig. 4.,"Qualitative results of different learning frameworks for skin lesion segmentation. The upper part (Rows 1 to 6): the raw images, the segmentation maps produced by local learning (LL), FedST, FedMix under semi-supervision (i. e.,[C1, C2, C3, C4] = [
U
,
U
,
L
,
U
]), FedMix under mixed supervision (i. e.,[C1, C2, C3, C4] = [
B
,
B
,
L
,
B
]), and the expert annotations respectively. The lower part (Rows 7 to 8): the segmentation maps obtained by federated learning under the fully-supervised setting with FedAvg and the proposed adaptive aggregation function respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/10004567/yan4-3233405-large.gif
2023,10050743,Fig. 1.,"Flowchart of land use/land cover (LULC) classification for Xinjiang, China.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou1-3247624-large.gif
2023,10050743,Fig. 2.,"Pertinent locations in Xinjiang, China.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou2-3247624-large.gif
2023,10050743,Fig. 3.,Spatial consistency level between the five land cover products under the IGBP classification system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou3-3247624-large.gif
2023,10050743,Fig. 4.,"Proportion of machine learning training samples in different regions: ENF, evergreen needleleaf forests; DNF, deciduous needleleaf forest; DBF, deciduous broadleaf forest; MF, mixed forest; SL, shrublands; GL, grasslands; DL, dry land; CNM, cropland/natural vegetation mosaics; UB, urban and built-up lands; RL, rural land; IM, industrial and mining; PW, permanent wetlands; PS, permanent snow and ice; BL, bare lands; WB, water body.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou4-3247624-large.gif
2023,10050743,Fig. 5.,(a) PA and (b) UA of different LULC types obtained using various machine learning techniques for the whole Xinjiang. (c) PA and (d) UA for Northern Xinjiang. (e) PA and (f) UA for Tianshan. (g) PA and (h) UA for Southern Xinjiang.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou5-3247624-large.gif
2023,10050743,Fig. 6.,(a) PA and (b) UA of different LULC types obtained by ensemble learning for the whole Xinjiang. (c) PA and (d) UA for Northern Xinjiang. (e) PA and (f) UA for Tianshan. (g) PA and (h) UA for Southern Xinjiang.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou6-3247624-large.gif
2023,10050743,Fig. 7.,Spatial distribution of LULC in Xinjiang.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4609443/9973430/10050743/zhou7-3247624-large.gif
2023,10168911,FIGURE 1.,All cancer fact sheets - WHO; Data source - GLOBOCAN 2020 [11].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim1-3290997-large.gif
2023,10168911,FIGURE 2.,"The systematic review procedure by following PRISMA guideline [16,17].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim2-3290997-large.gif
2023,10168911,FIGURE 3.,Publication growth from January 2015 to March 2022.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim3-3290997-large.gif
2023,10168911,FIGURE 4.,GI malignancy detection in WCE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim4-3290997-large.gif
2023,10168911,FIGURE 5.,The KVASIR dataset’s sample images of eight different classes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim5-3290997-large.gif
2023,10168911,FIGURE 6.,The KVASIR dataset class distribution for Medico 2017.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim6-3290997-large.gif
2023,10168911,FIGURE 7.,Medico 2018 and BioMedia 2019 dataset classes and sample images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim7-3290997-large.gif
2023,10168911,FIGURE 8.,The BioMedia 2019 dataset class distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim8-3290997-large.gif
2023,10168911,FIGURE 9.,Hyper-KVASIR dataset’s classes and sample images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim9-3290997-large.gif
2023,10168911,FIGURE 10.,Hyper-KVASIR dataset class distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim10-3290997-large.gif
2023,10168911,FIGURE 11.,KVASIR Capsule dataset’s classes and sample images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim11-3290997-large.gif
2023,10168911,FIGURE 12.,KVASIR Capsule dataset class distribution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10168911/lim12-3290997-large.gif
2023,10044670,FIGURE 1.,Example of defective PCB patterns.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044670/isa1-3245093-large.gif
2023,10044670,FIGURE 2.,Examples of defective solder joints.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044670/isa2-3245093-large.gif
2023,10044670,FIGURE 3.,Research framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044670/isa3-3245093-large.gif
2023,10044670,FIGURE 4.,Research areas.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044670/isa4-3245093-large.gif
2023,10044670,FIGURE 5.,Focused review parts.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044670/isa5-3245093-large.gif
2023,10026819,FIGURE 1.,System model of a LoRa network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo1-3240308-large.gif
2023,10026819,FIGURE 2.,Classification accuracy of random forest (RF) algorithm for optimal power allocation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo2-3240308-large.gif
2023,10026819,FIGURE 3.,Convergence analysis of the proposed and the baseline RL algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo3-3240308-large.gif
2023,10026819,FIGURE 4.,EPP for EXP3s and proposed.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo4-3240308-large.gif
2023,10026819,FIGURE 5.,Resource distribution difference between EXP3s and the recommended algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo5ab-3240308-large.gif
2023,10026819,FIGURE 6.,Comparing the proposed algorithm with the ADR-based schemes for different cell radii for traffic load \$\lambda=0.83\$ .,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo6ab-3240308-large.gif
2023,10026819,FIGURE 7.,Comparing the proposed algorithm with the ADR-based schemes for different traffic loads for cell radius \$R = 4.5\$ km.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo7ab-3240308-large.gif
2023,10026819,FIGURE 8.,"Packet reception ratio (PRR) for different proposed algorithm variations, for \$N=500\$ EDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo8-3240308-large.gif
2023,10026819,FIGURE 9.,"Energy per packet (EPP) for different proposed algorithm variations, for \$N=500\$ EDs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo9-3240308-large.gif
2023,10026819,FIGURE 10.,PRR with 300 nodes and 50% mobile nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo10-3240308-large.gif
2023,10026819,FIGURE 11.,EPP with 300 nodes and 50% mobile nodes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10026819/mahmo11-3240308-large.gif
2023,10025016,Fig. 1.,Two stages of machine-learning—a classifier and a regression model—are required to obtain cloud-type predictions on datasets with low horizontal resolution.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps1-3237008-large.gif
2023,10025016,Fig. 2.,"Workflow schematic: 1) IResNet is trained on the CUMULO dataset and then applied on the unlabeled full-swath Moderate Resolution Imaging Spectroradiometer (MODIS) yielding the fully labeled CUMULO dataset; 2) RF regression model is trained on a coarse-grained version of this data to provide cloud class distributions; and 3) RF is applied to unseen data, allowing validation of the methods performance or evaluation of the target dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps2-3237008-large.gif
2023,10025016,Fig. 3.,"Schematic of the pixelwise classifier, which is a convolutional neural network trained on features from MODIS and one of eight cloud-type labels from CloudSat per pixel.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps3-3237008-large.gif
2023,10025016,Fig. 4.,Cloud type predictions for data with low horizontal resolution are obtained by coarse-graining high-resolution predictions as a basis to train a regression model predicting relative amounts of each cloud type for each coarse resolution grid cell.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps4-3237008-large.gif
2023,10025016,Fig. 5.,Method is validated by applying the trained regression model to data the model has not seen before. The predictions are then analyzed for physical consistency.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps5-3237008-large.gif
2023,10025016,Fig. 6.,"Joint density of the predicted and true (a) Ac and (b) Ns fractions from the CUMULO test set for a grid cell size of
(100km
)
2
, using the optimal set of features (see Section II-F). The color scale and the marginal histograms are logarithmic. The red line indicates the line of perfect correlation. The area between the dashed magenta lines indicates a deviation between ground truth and prediction of less than 0.1 and the area between the black lines indicates a deviation by less than a factor of two in either direction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps6ab-3237008-large.gif
2023,10025016,Fig. 7.,"Results for a model trained using features without (a) liquid/ice distinction (cwp, cer, cod, ptop, tsurf) and a model trained also without (b) cod, for comparison with Fig. 6.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps7-3237008-large.gif
2023,10025016,Fig. 8.,"Relative occurrence per class from CloudSat measurements (year 2008) from the 2B-CLDCLASS-LIDAR product per
2
∘
×
2
∘
grid cell.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps8-3237008-large.gif
2023,10025016,Fig. 9.,"Average class fractions for the predictions on coarse-grained ESA CCI data. The RF was trained on
(10km
)
2
grid cells and applied to
10×10
pixel grid cells. The results are projected onto a
1
∘
×
1
∘
grid. Many classes show similarities to the distributions in the CloudSat data (see Fig. 8), even though the location is not used as a feature.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps9-3237008-large.gif
2023,10025016,Fig. 10.,Time series of mean predicted class fractions from 1 June 2009 to 31 December 2011. The per-class mean is computed for all locations in the southern hemisphere where at least one instance of the respective cloud class fraction is within the 90th percentile of all predictions of this class. Note that July 2010 has been excluded due to faulty data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps10-3237008-large.gif
2023,10025016,Fig. 11.,"Difference between the mean predicted fractions and CloudSat per
2
∘
×
2
∘
grid cell for the relative amount of the classes with lowest/highest correlation, St [(a)
c
P
=0.18
]/Sc [(b)
c
P
=0.88
]. The color map is normalized to the range
[−m,m]
, where
m
is the maximum value for the class across both (CloudSat, predictions) distributions. Predictions obtained from RF trained on
(100 km
)
2
data and applied on
100×100
ESA CCI pixels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/36/10006360/10025016/kaps11ab-3237008-large.gif
2023,10105219,Fig. 1.,Modeling flow of the CPM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang1-3266840-large.gif
2023,10105219,Fig. 2.,Spatial distribution of the measured positions.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang2-3266840-large.gif
2023,10105219,Fig. 3.,Deviation of the ITU-R P.1546 and 2001 models. Deviation in (a) frequency dimension and (b) distance dimension.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang3ab-3266840-large.gif
2023,10105219,Fig. 4.,Map of weight against frequency and distance. (a) ITU-R P.1546 (group 1). (b) ITU-R P.1546 (group 2). (c) ITU-R P.1546 (group 3). (d) ITU-R P.1546 (group 4). (e) ITU-R P.1546 (group 5). (f) ITU-R P.1546 (group 6). (g) ITU-R P.2001 (group 1). (h) ITU-R P.2001 (group 2). (i) ITU-R P.2001 (group 3). (j) ITU-R P.2001 (group 4). (k) ITU-R P.2001 (group 5). (l) ITU-R P.2001 (group 6).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang4abcdefghijkl-3266840-large.gif
2023,10105219,Fig. 5.,Average RE of each validation group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang5-3266840-large.gif
2023,10105219,Fig. 6.,Difference between the maximal RE subtracting the minimal RE of each model in the six groups.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang6-3266840-large.gif
2023,10105219,Fig. 7.,RE in frequency and distance dimensions. RE in (a) frequency dimension and (b) distance dimension.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang7ab-3266840-large.gif
2023,10105219,Fig. 8.,RMSE in frequency and distance dimensions. RE in (a) frequency dimension and (b) distance dimension.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang8ab-3266840-large.gif
2023,10105219,Fig. 9.,Improvement percentages of the CPM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang9-3266840-large.gif
2023,10105219,Fig. 10.,"Deviation distribution of each model. Deviation distribution of (a) ITU-R P.1546, (b) ITU-R P.2001, and (c) CPM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang10abc-3266840-large.gif
2023,10105219,Fig. 11.,Average RE of each model in different propagation prediction scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang11-3266840-large.gif
2023,10105219,Fig. 12.,Average RE of each model in different terrain types.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8/10174790/10105219/yang12-3266840-large.gif
2023,9843947,Fig. 1.,ML job cluster.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen1-3190797-large.gif
2023,9843947,Fig. 2.,ML platform with data parallelism and model parallelism.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen2-3190797-large.gif
2023,9843947,Fig. 3.,Deep RL structure in MLF-RL.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen3-3190797-large.gif
2023,9843947,Fig. 4.,Validation loss in ML running of different ML algorithms.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen4-3190797-large.gif
2023,9843947,Algorithm 1,Algorithm Pseudocode of MLF-H,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen12-3190797-large.gif
2023,9843947,Algorithm 2,Algorithm Pseudocode of MLF-RL,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen13-3190797-large.gif
2023,9843947,Algorithm 3,Algorithm Pseudocode of MLF-C,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen14-3190797-large.gif
2023,9843947,Fig. 5.,Overall performance in real experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen5-3190797-large.gif
2023,9843947,Fig. 6.,Overall performance in large-scale simulation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen6-3190797-large.gif
2023,9843947,Fig. 7.,Urgency and deadline consideration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen7-3190797-large.gif
2023,9843947,Fig. 8.,Bandwidth consideration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen8-3190797-large.gif
2023,9843947,Fig. 9.,Effectiveness of system load control.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen9-3190797-large.gif
2023,9843947,Fig. 10.,System load reduction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen10-3190797-large.gif
2023,9843947,Fig. 11.,Effectiveness of OptS.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10045328/9843947/shen11-3190797-large.gif
2023,9514399,Fig. 1.,"Comparison between the basic reservoir computing framework and SpaRCe, which exploits the concepts of adaptable thresholds to introduce sparsity in the representation of the ESN. (a) Typical reservoir computing sampling paradigm for a time-series classification task. Time flows from top to bottom. The input signal
s(t)
is fed into the ESN through the input connectivity matrix
γ
W
in
. An ensemble of the activities
V(t)
across time is selected and concatenated to compose a vector
V
~
, which is then used to define the read-out. Typical choices of ESN activities used to define the read-out are
V
~
=V(T)
, where
T
denotes the final temporal step of the input sequence, or
V
~
=C({V(t)
}
∀t
)
, denoting the concatenation of all the reservoir dynamic across the temporal length of
s(t)
. Of course, while the first approach relies on the ESN intrinsic memory capacity, the latter case corresponds to a dimensionality expansion that contributes artificially to the memory of the system. Both these approaches and intermediate cases, i.e., where the dynamic of the reservoir across time is sampled with low frequency to compose the read-out, will be exploited in the tasks studied. (b) and (c) Comparison between the typical reservoir computing read-out (b) and SpaRCe model. Considering that
V
~
is used for the read-out, we define a threshold for each dimension of
V
~
. The result of this approach is the definition of different thresholds across time in the case where the representation is enriched through concatenation of the history of activities of the reservoir. In this way, the approach defined is general and can be applied regardless of the technique used to define the representation
V
~
for the read-out of the system. Furthermore, the initialization of different thresholds across time can be helpful to the case where there is temporal drift in the dataset, leading the activity
x
of the reservoir to exhibit a more stationary behavior across time. (b) ESN output
y=
W
o
V
~
is responsible for the classification process of the example sequence
s(t)
. In this paradigm, learning occurs exclusively on
W
o
. (c) Scheme of the SpaRCe model. Thresholds are introduced at the level of the
V
~
vector, leaving unaffected the dynamic of the reservoir and making the approach applicable to any physical or virtual reservoir model. Each threshold value is composed by a normalization term
P
n
(|
V
~
i
|)
, defined as the
n
th percentile of the activity distribution of the
i
th component across the data, plus an adaptable term
θ
~
i
(see text for more details). (d) and (e) Activity distributions of
V
~
and
x
correspondingly before the training process, where
θ
~
≈0
, for three example nodes. Each distribution is fit through two Gaussians for clarity purposes. The highlighted red region in (d) corresponds to the values for which the nodes would be active if the normalization mechanism proposed in (4) would be applied (percentile
P
50
in this case). From (e), it is clear that (4) also shifts the activity distributions acting as a normalization mechanism.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9514399/vasil1abcde-3102378-large.gif
2023,9514399,Fig. 2.,"SpaRCe read-out proposed leads to decreased variability in the ESN representation, to specialized responses, and to a interpretable learning rule that acts as feature selection mechanism. The learning rule for the thresholds is driven by the imbalance between two antagonist forces. (a) Distributions of the average activities of nodes across an example dataset (MNIST, Section II-C) for the standard ESN and for SpaRCe as the percentile of the normalization mechanism changes at the beginning of training (when
θ
~
=0
). SpaRCe decreases the variability of the activity distributions, acting as a normalization mechanism. (b) Frequency of active nodes for different starting sparsity levels and different number of classes for a classification task with ten classes (MNIST, Section II-C) before training (
θ
~
=0
). We note that
N
class
refers to the total number of classes to which a node responds. In particular, nodes that are active for
N
class
=10
are responding to all ten classes, while nodes that react for
N
class
=6
are responding to six classes only. As sparsity (
P
n
) increases, nodes respond to a smaller number of classes becoming more specialized. (c) Analysis of the two forces
Δ
θ
(1)
and
Δ
θ
(2)
involved in the learning rule for the thresholds. The positive
y
-axis shows a running average of
Δ
θ
(2)
, while the negative
y
-axis shows a running average of
Δ
θ
(1)
.
⟨⟩
indicates averaging across all neurons.
Δ
θ
(2)
increases (decreases) the threshold values for nodes that are equally (differently) contributing to the classification process.
Δ
θ
(1)
decreases (increases) the threshold values due to the positive (negative) contribution of the output weights connected to the correct output. Colors correspond to initial conditions (
P
60
,
P
95
). (d) Average cumulative change of a threshold in terms of percentile change due to adaptation of
θ
~
. If the starting level of sparsity is suboptimal and low (high), the average percentile change is positive (negative).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9514399/vasil2abcd-3102378-large.gif
2023,9514399,Fig. 3.,"Learning process modulates the network’s sparsity level toward an optimal percentage of active nodes. (a) Performance as a function of sparsity for different training instances of the model (a color represents a specific training time, which increases from top to bottom). For each instance, the results are fit with a second degree polynomial (
χ
2
=3×
10
−4
,
R
2
=0.98
), demonstrating the existence of an optimal percentage of active nodes, which is around 50% for the sequence classification task (note that the optimal sparsity level is task-dependent). The dashed line connecting the results for diverse training time highlights the change in the sparsity level achieved through the learning rule. (b) Performance as a function of sparsity and specialization. The best performance corresponds to the highest specialization values for all training instances, demonstrating the interpretability of the model.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9514399/vasil3ab-3102378-large.gif
2023,9514399,Fig. 4.,"SpaRCe algorithm increases the memory capacity of the ESN and the stability of the found solution. (a) Classification accuracy and root mean square error of the models for a case where the number of sequences to be classified is 192. Each minibatch corresponds to the presentation of 20 training samples. In this case, the performance of SpaRCe is related to a starting
P
n
=50
, while the HL has
N
h
=100
nodes. Dots correspond to the training instances in which the considered models solve the task (for the models that can solve the task), thus showing the training speed of the algorithms analyzed. (b) Performance of SpaRCe for three different starting sparsity levels (red and orange), for a standard ESN (black) and of HL (blue) for diverse
N
h
nodes. The
x
-axis reports the number of trainable parameters and is shown on a logarithmic scale. The graph shows how SpaRCe is able to reach good performance while maintaining a low number of trainable parameters. (c) Comparison of the root mean square error for the ESN and the ESN with thresholds as the external noise
σ
s
and the number of stimuli vary. The introduction of thresholds leads to a robust result. (d) Performance as the number of inputs to be classified increases, for the HL model with
N
h
=100
(blue), SpaRCe (red) with
P
n
=50
, and a standard ESN (black). SpaRCe and HL solve the tasks considered, but the latter uses a number of trainable parameters (reported along with the performance with the color scheme that reflects the referred model) that are two magnitudes higher than the first. The accuracy of the standard ESN drops considerably as the number of sequences increases. The inset shows the root mean square error for SpaRCe and HL, varying the starting condition
P
n
and the number of nodes
N
h
, respectively. The errors shown correspond to the training instance in which the fastest model reaches perfect classification accuracy. Numbers reflect the number of trainable parameters for example cases.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9514399/vasil4abcd-3102378-large.gif
2023,9514399,Fig. 5.,"SpaRCe model shows comparable performance to a 2/3 HL neural network on the MNIST dataset and accuracy comparable to more complex RNNs trained with BPTT on the psMNIST task. On the contrary, the standard ESN with online training leads to lesser performance. (a) Sizes of the dots reflect the percentage of active nodes (sparser network = smaller dots) in the network. Each minibatch corresponds to the presentation of 20 training samples. The abscissa of the inset figure is scaled logarithmically. (b) Performance of ESN with (red) and without (gray) threshold learning on the three tasks analyzed, measured by accuracy and convergence time (C.T.). The network with the SpaRCe model outperforms the standard ESN read-out on all the benchmarks, but the contribution of the thresholds decreases as the task becomes more complex. This can be understood by considering that the increasing complexity of the tasks from left to right of the graph arises from a greater demand for the network’s ability to understand long-term dependencies. This aspect depends on the system dynamics and is not strongly related to threshold learning. Furthermore, the SpaRCe model converges about five times faster than an ESN without thresholds.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9514399/vasil5ab-3102378-large.gif
2023,9514399,Fig. 6.,"SpaRCe helps to prevent catastrophic forgetting on the two analyzed benchmarks. Different data points correspond to diverse repetitions of the experiment. (a) Results on the MNIST dataset in the sequential classes paradigm as the starting fraction of active nodes varies (see text). (b) Same as (a), but for the permuted datasets paradigm. (c) Performance as a function of the starting percentage of active nodes and the number of tasks that are learned in the catastrophic forgetting simulations for the sequential classes task. (d) Same as (c), but for the permuted datasets task. (c) and (d) The surface is a cubic interpolation of the accuracy as the number of datasets and the starting sparsity level vary. The path shows the best performing sparsity levels across various numbers of tasks; its movement from right to left demonstrates the necessity of adopting an increasing level of sparsity as the number of datasets increases and the memory of previous tasks becomes more relevant.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9514399/vasil6abcd-3102378-large.gif
2023,10014533,Fig. 1.,Illustration for scattering by a 2-D object with arbitrary cross-section.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/10116031/10014533/xia1-3235928-large.gif
2023,10014533,Fig. 2.,Some examples of pentagons in the dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/10116031/10014533/xia2-3235928-large.gif
2023,10014533,Fig. 3.,"MLP architecture to extract the WCs. (a) To extract
C
TM
n
for TM polarization. (b) To extract
C
TE
n
for TE polarization.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/10116031/10014533/xia3-3235928-large.gif
2023,10014533,Fig. 4.,Comparison of bistatic RCS predicted by MLP with those calculated by MoM. (a) Cross-section of the scatterer. (b) RCS for TM polarization. (c) RCS for TE polarization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/10116031/10014533/xia4-3235928-large.gif
2023,10014533,Fig. 5.,Example in predicting RCS out of the dataset. (a) Cross-section of the scatterer. (b) RCS for TM polarization. (c) RCS for TE polarization.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7727/10116031/10014533/xia5-3235928-large.gif
2023,10058970,Fig. 1.,Convergence curves of the four competing methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/97/10036333/10058970/cao1-3253053-large.gif
2023,9941067,Fig. 1.,"Traffic light classification question: distribution of answers (
n=
104).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike1-3218013-large.gif
2023,9941067,Fig. 2.,"Traffic light classification explanations: distribution of answer categories (
n=
98).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike2-3218013-large.gif
2023,9941067,Fig. 3.,"Traffic light classification: answers (
x
-axis) versus explanation types (
y
-axis) (
n=
98).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike3-3218013-large.gif
2023,9941067,Fig. 4.,"Carcinoma classification question: distribution of answers (
n=
101).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike4-3218013-large.gif
2023,9941067,Fig. 5.,"Carcinoma classification explanations: distribution of answer categories (
n=
88).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike5-3218013-large.gif
2023,9941067,Fig. 6.,"Carcinoma classification question answers versus explanation types (
n=
88).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike6-3218013-large.gif
2023,9941067,Fig. 7.,"Traffic light question explanation types versus carcinoma classification question explanation types (
n=
88). Rows represents the students’ type of answer to the traffic light classification question and columns represents the students answers to the carcinoma classification question.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/13/10142034/9941067/mike7-3218013-large.gif
2023,9882005,Fig. 1.,Performance degradation: bin packing vs. standalone.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao1-3202529-large.gif
2023,9882005,Fig. 2.,Pair-wise interference (darker color indicates more severe interference).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao2-3202529-large.gif
2023,9882005,Fig. 3.,Placement under different schemes (diamonds represent parameter server and worker in job 1; squares represent those of job 2).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao3-3202529-large.gif
2023,9882005,Fig. 4.,Normalized training speed under 3 schemes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao4-3202529-large.gif
2023,9882005,Fig. 5.,Speed with increasing # of co-located CTC jobs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao5-3202529-large.gif
2023,9882005,Fig. 6.,Harmony workflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao6-3202529-large.gif
2023,9882005,Fig. 7.,DRL architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao7-3202529-large.gif
2023,9882005,Fig. 8.,Inference workflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao8-3202529-large.gif
2023,9882005,Fig. 9.,Reward prediction model architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao9-3202529-large.gif
2023,9882005,Fig. 10.,Performance comparison under different job arrival patterns.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao10-3202529-large.gif
2023,9882005,Fig. 11.,Performance comparison of each job type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao11-3202529-large.gif
2023,9882005,Fig. 12.,Sensitive analysis of scheduling interval.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao12-3202529-large.gif
2023,9882005,Fig. 13.,Training speed prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao13-3202529-large.gif
2023,9882005,Fig. 14.,Performance comparison under different job arrival patterns over 30 machines.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao14-3202529-large.gif
2023,9882005,Fig. 15.,Deep dive.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/90/10103748/9882005/bao15-3202529-large.gif
2023,9429985,Fig. 1.,"Information flow in informed machine learning. The informed machine learning pipeline requires a hybrid information source with two components: Data and prior knowledge. In conventional machine learning knowledge is used for data preprocessing and feature engineering, but this process is deeply intertwined with the learning pipeline (*). In contrast, in informed machine learning prior knowledge comes from an independent source, is given by formal representations (e.g., by knowledge graphs, simulation results, or logic rules), and is explicitly integrated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru1-3079836-large.gif
2023,9429985,Fig. 2.,"Taxonomy of informed machine learning. This taxonomy serves as a classification framework for informed machine learning and structures approaches according to the three above analysis questions about the knowledge source, knowledge representation and knowledge integration. Based on a comparative and iterative literature survey, we identified for each dimension a set of elements that represent a spectrum of different approaches. The size of the elements reflects the relative count of papers. We combine the taxonomy with a Sankey diagram in which the paths connect the elements across the three dimensions and illustrate the approaches that we found in the analyzed papers. The broader the path, the more papers we found for that approach. Main paths (at least four or more papers with the same approach across all dimensions) are highlighted in darker grey and represent central approaches of informed machine learning.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru2-3079836-large.gif
2023,9429985,Fig. 3.,Knowledge representations and learning tasks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru3-3079836-large.gif
2023,9429985,Fig. 4.,Knowledge integration and its goals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru4-3079836-large.gif
2023,9429985,Fig. 5.,Information flow for synthetic training data from simulations.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru5-3079836-large.gif
2023,9429985,Fig. 6.,Steps of rules-to-network translation [53]. Simple example for integrating rules into a KBANN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru6-3079836-large.gif
2023,9429985,Fig. 7.,"Illustrative application example of using neural networks and knowledge graphs for image classification, similar as in [15]. The image (from the COCO dataset) shows a pedestrian cross walk.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/9973432/9429985/vonru7-3079836-large.gif
2023,10015000,FIGURE 1.,Schematic diagram of the system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid1-3236002-large.gif
2023,10015000,FIGURE 2.,Types of smart wearable.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid2-3236002-large.gif
2023,10015000,FIGURE 3.,Complete workflow from data collection to deployment in ML application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid3-3236002-large.gif
2023,10015000,FIGURE 4.,Hardware implementation flowchart.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid4-3236002-large.gif
2023,10015000,FIGURE 5.,Design diagram of “Sense O’clock”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid5-3236002-large.gif
2023,10015000,FIGURE 6.,PCB design of “Sense O’clock”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid6-3236002-large.gif
2023,10015000,FIGURE 7.,Framework for the software implementation part of “Sense O’Clock”.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid7-3236002-large.gif
2023,10015000,FIGURE 8.,Transfer of data to “MedAi” Application via BLE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid8-3236002-large.gif
2023,10015000,FIGURE 9.,MedAi framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid9-3236002-large.gif
2023,10015000,FIGURE 10.,Attribute heatmap.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid10-3236002-large.gif
2023,10015000,FIGURE 11.,Machine learning workflow.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid11-3236002-large.gif
2023,10015000,FIGURE 12.,Process of machine learning embedding in android application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid12-3236002-large.gif
2023,10015000,FIGURE 13.,Workflow of embedding process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid13-3236002-large.gif
2023,10015000,FIGURE 14.,Confusion matrix of entire dataset on the application of RF algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid14-3236002-large.gif
2023,10015000,FIGURE 15.,Interfaces of android application.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid15-3236002-large.gif
2023,10015000,FIGURE 16.,Learning steps from units to whole.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015000/whaid16-3236002-large.gif
2023,9994748,Fig. 1.,"Illustration of FPSL for a multi-label classification task on chest X-ray images. Here, each client (data node) is annotated for only one thoracic disease. We use this simple example to convey the main concept of the problem of interest (in practice, each client could be partially labeled for multiple classes). In this scenario, we only know whether each image in the first data node has infiltration but have no knowledge on the other three diseases. To ensure data government, only model weights and the metadata (e.glet@tokeneonedot statistics) of the local data can be communicated between each data node and the parameter server (see Sec. IV for a formal description). The goal of FPSL is to utilize the four partially labeled datasets stored in the different data nodes to train the model of interest in the parameter server.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/9994748/dong1-3231017-large.gif
2023,9994748,Fig. 2.,Visual illustration of multi-site CXRs. (a) is diagnosed with cardiomegaly and emphysema. (b) is tuberculosis positive. (c) is COVID-19 positive.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/9994748/dong2abc-3231017-large.gif
2023,9994748,Fig. 3.,"Sensitivity of the overall performance (AUROC) to the size of the local dataset (
n
). The overall performance is measured by the mean AUROC over
C=6
classes in
K=4
clients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/9994748/dong3-3231017-large.gif
2023,9994748,Fig. 4.,"Impact of extreme clients on the overall performance (AUROC).
n
denotes the size of the local dataset in the extreme clients, while each common client has 1000 partially labeled examples. Two out of four clients are the extreme ones. The overall performance is measured by the mean AUROC over
C=6
classes in
K=4
clients.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10168838/9994748/dong4-3231017-large.gif
2023,10145774,Fig. 1.,Process of the obscurity-quantified curriculum learning for machine translation evaluation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/9970249/10145774/wong1-3282105-large.gif
2023,10145774,Fig. 2.,The obscurity distribution on the training tuples measured by different quantified obscurity perspectives.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/9970249/10145774/wong2-3282105-large.gif
2023,10145774,Fig. 3.,Correlation with human judgments at segment level of models trained on different total number of baby steps for overall WMT2019 and WMT2020 DARR data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570655/9970249/10145774/wong3-3282105-large.gif
2023,9965435,Fig. 1.,Number of publications related to XGP over the years since 1995.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei1-3225509-large.gif
2023,9965435,Fig. 2.,"Tradeoff between learning performance and explainability of some representative machine learning models, adapted from [50].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei2-3225509-large.gif
2023,9965435,Fig. 3.,"Main stages in solving a machine learning problem where interpretability is important, adapted from [51].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei3-3225509-large.gif
2023,9965435,Fig. 4.,Taxonomy of the XGP approaches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei4-3225509-large.gif
2023,9965435,Fig. 5.,"Common strategies to reduce GP model size, and to which stages in the GP framework they can be applied.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei5-3225509-large.gif
2023,9965435,Fig. 6.,"One-point crossover to control model size. The thick lines show the common structures of the parents. The subtree
neg(y)
from parent 1 and
y−x
from parent 2 are swapped, as they inherit the same structure of the upper part of the parents.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei6-3225509-large.gif
2023,9965435,Fig. 7.,Example GP tree generated from the Leukemia dataset [154].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei7-3225509-large.gif
2023,9965435,Fig. 8.,Visualization of feature values and the class labels on the Leukemia dataset [154].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei8-3225509-large.gif
2023,9965435,Fig. 9.,"Multiple feature construction with a multitree representation for a 3-class dataset, where
C
F
1
,
C
F
2
, and
C
F
3
are the three constructed features from the three trees [170], and each corresponds to one class.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei9-3225509-large.gif
2023,9965435,Fig. 10.,Example multilayer model structure [181].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei10-3225509-large.gif
2023,9965435,Fig. 11.,Examples GP programs for image classification [180]. (a) Example program template and (b) actual program.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei11ab-3225509-large.gif
2023,9965435,Fig. 12.,Canonical form of GP model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4235/10138708/9965435/mei12-3225509-large.gif
2023,10092758,FIGURE 1.,The flowchart expressing the progress of clustering.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park1-3264584-large.gif
2023,10092758,FIGURE 2.,Process of prediction and estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park2-3264584-large.gif
2023,10092758,FIGURE 3.,Data distribution of each dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park3-3264584-large.gif
2023,10092758,FIGURE 4.,Evaluation accuracy achieved on MNIST.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park4-3264584-large.gif
2023,10092758,FIGURE 5.,Evaluation accuracy achieved on FMNIST.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park5-3264584-large.gif
2023,10092758,FIGURE 6.,Evaluation accuracy achieved on CIFAR-10.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park6-3264584-large.gif
2023,10092758,FIGURE 7.,Comparison between K-FL and baselines in inference cross-entropy loss and communication cost space.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10092758/park7-3264584-large.gif
2023,10058201,FIGURE 1.,URL presentation based on HTTP.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto1-3252366-large.gif
2023,10058201,FIGURE 2.,Detection of phishing URLs and structure of proposed approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto2-3252366-large.gif
2023,10058201,FIGURE 3.,Classification of phishing URLs proposed based on designed methodological structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto3-3252366-large.gif
2023,10058201,FIGURE 4.,"Dataset presentation according to number of classes phishing and legitimate, where (1) presents phishing and (0) legitimate URLs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto4-3252366-large.gif
2023,10058201,FIGURE 5.,Proposed approach based on canopy feature selection with LR+SVC+DT (LSD) ensemble learning model using grid search hyperparameter tuning and cross fold validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto5-3252366-large.gif
2023,10058201,FIGURE 6.,Experimental results of the decision tree model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto6-3252366-large.gif
2023,10058201,FIGURE 7.,Experimental results of the naive bayes model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto7-3252366-large.gif
2023,10058201,FIGURE 8.,Experimental results of the linear regression model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto8-3252366-large.gif
2023,10058201,FIGURE 9.,Experimental results of the K-neighbors classifier model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto9-3252366-large.gif
2023,10058201,FIGURE 10.,Experimental results of the support vector machine model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto10-3252366-large.gif
2023,10058201,FIGURE 11.,Experimental results of the random forest model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto11-3252366-large.gif
2023,10058201,FIGURE 12.,Experimental results of the gradient boosting model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto12-3252366-large.gif
2023,10058201,FIGURE 13.,Experimental results of the hybrid (LR+SVC+DT) model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto13-3252366-large.gif
2023,10058201,FIGURE 14.,Comparative analyses of the experimental results of the proposed approach with applied machine models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058201/musto14-3252366-large.gif
2023,10171358,FIGURE 1.,"Graphical representation of the proposed architecture. Data are firstly fed into a VAE. Then, using original and reconstructed signals, after a feature extraction stage, data are fed into a OC-SVM for being classified as damaged or not.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla1-3291674-large.gif
2023,10171358,FIGURE 2.,Architecture of a Variational Autoencoder.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla2-3291674-large.gif
2023,10171358,FIGURE 3.,"Graphical representation of an hyper-sphere fitted using a OC-SVM where
v=0.1
(a) and
v=0.5
(b) on data described by two features
x
1
and
x
2
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla3ab-3291674-large.gif
2023,10171358,FIGURE 4.,Photo of the experimental setup [78].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla4-3291674-large.gif
2023,10171358,FIGURE 5.,Location and direction of the sensors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla5-3291674-large.gif
2023,10171358,FIGURE 6.,Damage scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla6-3291674-large.gif
2023,10171358,FIGURE 7.,"Graphical representation illustrating the data distributions for each sensor in every damage case of the benchmark dataset. Box plots were utilized to represent these distributions, and it can be observed that the distributions are not only significantly overlapping but also similar in the majority of the cases, suggesting that distinguishing between the damages may require additional analysis beyond examining the data alone.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla7-3291674-large.gif
2023,10171358,FIGURE 8.,Graphical representation of a OC-SVM fitted on undamaged (Case 1) training data (a) and tested on undamaged testing data (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla8ab-3291674-large.gif
2023,10171358,FIGURE 9.,Graphical representation of damage-sensitive features extracted from sensor 12 (a) and sensor 2 (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla9ab-3291674-large.gif
2023,10171358,FIGURE 10.,"Graphical comparison of
Po
D
avg
values obtained using VAE (blue) and AE (orange).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla10-3291674-large.gif
2023,10171358,FIGURE 11.,"Graphical representation of an undamaged (i.e., Case 1) (a) and a damaged (i.e., Case 7) (b) signal reconstructed by a VAE.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla11ab-3291674-large.gif
2023,10171358,FIGURE 12.,Graphical representation of latent representations for each case using t-SNE.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla12-3291674-large.gif
2023,10171358,FIGURE 13.,An investigation into the influence of noise factors in two distinct scenarios: when noise is initially present during the training stage (a); when noise emerges over time following the completion of the training stage (b).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla13ab-3291674-large.gif
2023,10171358,FIGURE 14.,The influence of noise on the representation of the first singular value of decomposed PSD for healthy (1) and damaged cases (4 and 9).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla14-3291674-large.gif
2023,10171358,FIGURE 15.,"An investigation into the influence of the frame size
s
on both our proposed method (a) and the numerosity of the dataset (b).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10171358/polla15ab-3291674-large.gif
2023,10002856,Figure 1.,Adaptive Ensemble learning model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/10102490/10002856/irfan1-3230820-large.gif
2023,10002856,Figure 2.,Overview of the working flow of the proposed approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/10102490/10002856/irfan2-3230820-large.gif
2023,10002856,Figure 3.,"Percentage improvements in accuracy, precision, recall, and F1-score.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/10102490/10002856/irfan3-3230820-large.gif
2023,10002856,Figure 4.,Correlation matrix for selected 18 cognitive features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/10102490/10002856/irfan4-3230820-large.gif
2023,10002856,Figure 5.,Confusion matrix for adaptive voting.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/10102490/10002856/irfan5-3230820-large.gif
2023,10002856,Figure 6.,ROC curves of various ML models for selected cognitive features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/46/10102490/10002856/irfan6-3230820-large.gif
2023,10006828,FIGURE 1.,Proposed DELM framework for detection of malaria.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer1-3234279-large.gif
2023,10006828,FIGURE 2.,Label analysis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer2-3234279-large.gif
2023,10006828,FIGURE 3.,Pre-processed images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer3-3234279-large.gif
2023,10006828,FIGURE 4.,Proposed CNN for features extraction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer4-3234279-large.gif
2023,10006828,FIGURE 5.,Proposed DELM for classification.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer5-3234279-large.gif
2023,10006828,FIGURE 6.,Confusion matrix of CNN with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer6-3234279-large.gif
2023,10006828,FIGURE 7.,Confusion matrix for CNN-ELM with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer7-3234279-large.gif
2023,10006828,FIGURE 8.,Confusion matrix for CNN-DELM with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer8-3234279-large.gif
2023,10006828,FIGURE 9.,Accuracy curve of CNN with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer9-3234279-large.gif
2023,10006828,FIGURE 10.,Loss curve of CNN with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer10-3234279-large.gif
2023,10006828,FIGURE 11.,ROC curve of CNN with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer11-3234279-large.gif
2023,10006828,FIGURE 12.,ROC curve of CNN-ELM with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer12-3234279-large.gif
2023,10006828,FIGURE 13.,ROC curve of CNN-DELM with original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer13-3234279-large.gif
2023,10006828,FIGURE 14.,Graphical comparison for original images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer14-3234279-large.gif
2023,10006828,FIGURE 15.,Confusion matrix of CNN with updated images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer15-3234279-large.gif
2023,10006828,FIGURE 16.,Confusion matrix for CNN-ELM with updated images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer16-3234279-large.gif
2023,10006828,FIGURE 17.,Confusion matrix for CNN-DELM with updated images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer17-3234279-large.gif
2023,10006828,FIGURE 18.,Accuracy curve of CNN with updated images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer18-3234279-large.gif
2023,10006828,FIGURE 19.,Loss curve of CNN with updated images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer19-3234279-large.gif
2023,10006828,FIGURE 20.,ROC curve of CNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer20-3234279-large.gif
2023,10006828,FIGURE 21.,ROC curve of CNN-ELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer21-3234279-large.gif
2023,10006828,FIGURE 22.,ROC curve of CNN-DELM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer22-3234279-large.gif
2023,10006828,FIGURE 23.,Graphical comparison for updated images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer23-3234279-large.gif
2023,10006828,FIGURE 24.,Graphical comparison of CNN-DELM with CNN [24].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10006828/omaer24-3234279-large.gif
2023,10058180,Fig. 1.,Cross section of the considered PMSM (base design).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop1-3252404-large.gif
2023,10058180,Fig. 2.,"Parameterization of the cross section of the PMSM with 9+3 geometric parameters. Three are not shown:
φ
1
and
φ
2
defining the curved shape of the PMs, and the airgap width
g
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop2-3252404-large.gif
2023,10058180,Fig. 3.,"Architecture of a multilayer perceptron (MLP), with
M
input features,
D
output variables, and three hidden layers (figure adapted from [15]).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop3-3252404-large.gif
2023,10058180,Fig. 4.,"Distribution of the normalized targets, along with
min
,
mean
, and
max
(in green), median (in orange), and IQR (black thick line, 49.8%, 87.4%, 9.7%, and 20.4%, from left to right); for base design all four targets are 0%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop4-3252404-large.gif
2023,10058180,Fig. 5.,Learning curves of the ANN classification and regression models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop5-3252404-large.gif
2023,10058180,Fig. 6.,"Confusion matrix of the final evaluation of the tuned classification model. (a) Decision threshold = 0.5 (accuracy = 97%, F1 = 0.923, TPR = 98.8%, FPR = 10%). (b) Decision threshold = 0.994 (accuracy = 97%, F1 = 0.930, TPR = 96.3%, FPR = 0%).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop6-3252404-large.gif
2023,10058180,Fig. 7.,Receiver operating characteristic curve of the tuned classification model (the positive class is the feasible class).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop7-3252404-large.gif
2023,10058180,Fig. 8.,Pairwise Pearson correlation coefficients between targets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop8-3252404-large.gif
2023,10058180,Fig. 9.,"Feature values
X
1
to
X
11
(in % w.r.t. the base design, range
±10%
) for the start and ANN-optimized designs. Not shown in the spider plot: feature
X
12
,
α
t
, equal to 2.46
∘
(random search) and 4.95
∘
(ANN-optimized); for base design all 12 features are 0%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop9-3252404-large.gif
2023,10058180,Fig. 10.,"FE-obtained back EMF at 1 r/min (a) and cogging torque (b) as function of rotor position (in electrical degrees) for the base, random-search and optimized designs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8847244/10170752/10058180/pop10-3252404-large.gif
2023,10049614,FIGURE 1.,Work flow for the damage predicting system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain1-3247499-large.gif
2023,10049614,FIGURE 2.,Distribution of damage grade.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain2-3247499-large.gif
2023,10049614,FIGURE 3.,Distribution of damage grade by district.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain3-3247499-large.gif
2023,10049614,FIGURE 4.,Distribution of damage grade by land surface type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain4-3247499-large.gif
2023,10049614,FIGURE 5.,Distribution of damage grade by foundation type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain5-3247499-large.gif
2023,10049614,FIGURE 6.,Distribution of damage grade by roof type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain6-3247499-large.gif
2023,10049614,FIGURE 7.,Distribution of damage grade by ground floor type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain7-3247499-large.gif
2023,10049614,FIGURE 8.,Distribution of damage grade by other floor type.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain8-3247499-large.gif
2023,10049614,FIGURE 9.,Distribution of damage grade by position.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain9-3247499-large.gif
2023,10049614,FIGURE 10.,Distribution of damage grade by plan configuration.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain10-3247499-large.gif
2023,10049614,FIGURE 11.,Distribution of damage grade by condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain11-3247499-large.gif
2023,10049614,FIGURE 12.,The plots of empirical cumulative distribution for the Building’s age split by damage grade.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain12-3247499-large.gif
2023,10049614,FIGURE 13.,ECDF for building age.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain13-3247499-large.gif
2023,10049614,FIGURE 14.,Distribution of Plinth Area by Grade.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain14-3247499-large.gif
2023,10049614,FIGURE 15.,Comparison of rooms per building pre & post earthquake by damage grade.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain15-3247499-large.gif
2023,10049614,FIGURE 16.,The plot summarizes of the mean occurrence of each of the superstructure variables.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain16-3247499-large.gif
2023,10049614,FIGURE 17.,The overall procedure of the hybrid future selection methodology (HFSM).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain17-3247499-large.gif
2023,10049614,FIGURE 18.,The 3-layer BPNN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain18-3247499-large.gif
2023,10049614,FIGURE 19.,Damage classification (unsupervised) method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain19-3247499-large.gif
2023,10049614,FIGURE 20.,Hybrid machine learning technique (HMLT).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain20-3247499-large.gif
2023,10049614,FIGURE 21.,Anomaly detection results.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain21-3247499-large.gif
2023,10049614,FIGURE 22.,F1-Score value of proposed HMLT with other classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain22-3247499-large.gif
2023,10049614,FIGURE 23.,Accuracy of proposed HMLT with other classifiers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049614/jain23-3247499-large.gif
2023,9723003,Fig. 1.,Overview of unsupervised ECG analysis applications.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4664312/10007429/9723003/forou1-3154893-large.gif
2023,10097658,Fig. 1,Example of a palm tree infected by white scale.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-1-source-large.gif
2023,10097658,Fig. 2,System architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-2-source-large.gif
2023,10097658,Fig. 3,Initial number of samples per infestation degree.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-3-source-large.gif
2023,10097658,Fig. 4,Samples of healthy and infected leaflets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-4-source-large.gif
2023,10097658,Fig. 5,Distribution of images per class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-5-source-large.gif
2023,10097658,Fig. 6,"Confusion matrices of SVM classifier. 0–3 indicate healthy, low infestation degree, medium infestation degree, and high infestation degree, respectively. The color metric scale represents the number of prediction in the confusion matrices.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-6-source-large.gif
2023,10097658,Fig. 7,Confusion matrices of KNN classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-7-source-large.gif
2023,10097658,Fig. 8,Confusion matrices of RF classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-8-source-large.gif
2023,10097658,Fig. 9,Confusion matrices of lightgbm classifier.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8254253/10097649/10097658/263-272-fig-9-source-large.gif
2023,9226432,Fig. 1.,Rolling-based forecasting procedure (training stage) for one day.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li1-3027822-large.gif
2023,9226432,Fig. 2.,Plot of forecasting results for five models on station ID N89123.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li2-3027822-large.gif
2023,9226432,Fig. 3.,Plot of forecasting results for five models on station ID N88125.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li3-3027822-large.gif
2023,9226432,Fig. 4.,Plot of forecasting results for five models on station ID NN89819.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li4-3027822-large.gif
2023,9226432,Fig. 5.,Plot of forecasting results for five models on station ID N88425.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li5-3027822-large.gif
2023,9226432,Fig. 6.,Plot of forecasting results for five models on station ID N87923.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li6-3027822-large.gif
2023,9226432,Fig. 7.,Plot of forecasting results for five models on station ID N91007.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li7-3027822-large.gif
2023,9226432,Fig. 8.,"Plot of the forecasting performance of probabilistic R-ELANFIS compared with those of R-ELANFIS, RVR-CSA, SVRGA-SA, and NN-GA.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10091960/9226432/li8-3027822-large.gif
2023,9813497,Fig. 1.,Structure of CM and CU sketches.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang1-3185560-large.gif
2023,9813497,Fig. 2.,Structure of the FM sketch.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang2-3185560-large.gif
2023,9813497,Fig. 3.,Block diagram of our generic framework to augment sketches with machine learning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang3-3185560-large.gif
2023,9813497,Fig. 4.,Block diagram of the implementation of our machine learning framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang4-3185560-large.gif
2023,9813497,Fig. 5.,Absolute error versus sampling rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang5-3185560-large.gif
2023,9813497,Fig. 6.,Average absolute error versus scale rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang6-3185560-large.gif
2023,9813497,Fig. 7.,The time overhead of machine learning using different sampling rate.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang7-3185560-large.gif
2023,9813497,Fig. 8.,Training once versus. training always.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang8-3185560-large.gif
2023,9813497,Fig. 9.,AAE of CM sketches for different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang9-3185560-large.gif
2023,9813497,Fig. 10.,ARE of CM sketches for different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang10-3185560-large.gif
2023,9813497,Fig. 11.,AAE of CU sketches for different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang11-3185560-large.gif
2023,9813497,Fig. 12.,ARE of CU sketches for different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang12-3185560-large.gif
2023,9813497,Fig. 13.,AAE of CSM sketches for different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang13-3185560-large.gif
2023,9813497,Fig. 14.,ARE of CSM sketches for different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang14-3185560-large.gif
2023,9813497,Fig. 15.,AAE for the top-k flows on different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang15-3185560-large.gif
2023,9813497,Fig. 16.,ARE for the top-k flows on different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang16-3185560-large.gif
2023,9813497,Fig. 17.,"RE comparison (RE of FM sketch is
>30%
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang17-3185560-large.gif
2023,9813497,Fig. 18.,"ARE comparison (RE of FM sketch
<30%
).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/12/10065566/9813497/wang18-3185560-large.gif
2023,9924545,Fig. 1.,Overview of the algorithmic approach from IMU raw data of real-world gait and 4x10MWTs to fall risk prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10006388/9924545/ullri1-3215921-large.gif
2023,9924545,Fig. 2.,"Confusion matrices of the leave-one-participant-out cross-validation for the best feature selector-classifier combinations of the six experimental conditions (Tables V and VI). Top: Gait tests aggregated test-wise (a), daily (b), and participant-wise (c). Bottom: Real-world gait aggregated bout-wise (d), daily (e), and participant-wise (f). The numbers in the cells correspond to the respective number of classified participants.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10006388/9924545/ullri2-3215921-large.gif
2023,9527390,Fig. 1.,High-level view of TRCLA structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta1-3106705-large.gif
2023,9527390,Fig. 2.,TRCLA structure in detail.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta2-3106705-large.gif
2023,9527390,Fig. 3.,Source handling algorithm of TRCLA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta3-3106705-large.gif
2023,9527390,Fig. 4.,Threshold checking algorithm of TRCLA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta4-3106705-large.gif
2023,9527390,Fig. 5.,Model transfer algorithm of TRCLA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta5-3106705-large.gif
2023,9527390,Fig. 6.,Target training algorithm of TRCLA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta6-3106705-large.gif
2023,9527390,Fig. 7.,"Entropy analysis of
E
1
–
E
4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta7-3106705-large.gif
2023,9527390,Fig. 8.,"Entropy analysis of
E
5
–
E
9
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta8-3106705-large.gif
2023,9527390,Fig. 9.,"NT analysis of
E
1
–
E
4
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta9-3106705-large.gif
2023,9527390,Fig. 10.,"NT analysis of
E
5
–
E
9
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta10-3106705-large.gif
2023,9527390,Fig. 11.,NT in TL-CLA and TRCLA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10114428/9527390/basta11-3106705-large.gif
2023,10138551,FIGURE 1.,The proposed study methodology analysis for heart failure prediction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir1-3281484-large.gif
2023,10138551,FIGURE 2.,The dataset distribution analysis is based on the target column.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir2-3281484-large.gif
2023,10138551,FIGURE 3.,The heatmap-based correlation analysis of dataset features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir3-3281484-large.gif
2023,10138551,FIGURE 4.,The selection of attributes from the dataset by using the PCHF technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir4-3281484-large.gif
2023,10138551,FIGURE 5.,The bar chart-based results comparison analysis of the applied machine learning models without using the PCHF technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir5-3281484-large.gif
2023,10138551,FIGURE 6.,The bar chart-based results comparison analysis of the applied machine learning models using the PCHF technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir6-3281484-large.gif
2023,10138551,FIGURE 7.,The bar chart-based performance validation analysis of applied machine learning models using the k-fold technique.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10138551/munir7-3281484-large.gif
2023,9844144,Fig. 1.,Circuit architecture of IDWT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh1-3194811-large.gif
2023,9844144,Fig. 2.,Examples of the generated erroneous images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh2-3194811-large.gif
2023,9844144,Fig. 3.,Image quality distribution comparison between the exhaustive case and the sampled case.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh3-3194811-large.gif
2023,9844144,Fig. 4.,Accuracy of acceptable images corresponding to FSIMc values.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh4-3194811-large.gif
2023,9844144,Fig. 5.,Comparison of fraction of acceptable images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh5-3194811-large.gif
2023,9844144,Fig. 6.,Architecture of the proposed reliability enhancement scheme.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh6-3194811-large.gif
2023,9844144,Fig. 7.,Flow chart of the proposed test method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh7-3194811-large.gif
2023,9844144,Fig. 8.,Extreme values of erroneous images.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh8-3194811-large.gif
2023,9844144,Fig. 9.,Example of the proposed detection method for an erroneous pixel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh9-3194811-large.gif
2023,9844144,Fig. 10.,Hardware architecture.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/43/10077488/9844144/hsieh10-3194811-large.gif
2023,9807403,Fig. 1.,"(a) Hardware of EVB90640-41 evaluation board and mounted MLX90640ESF-BAA-000-TU infrared sensor, (b) the infrared board that had the size of 75 mm × 42 mm was installed on a two-story platform for overhead mount.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9998557/9807403/rezae1-3186313-large.gif
2023,9807403,Fig. 2.,"Background noise. (a) shows a scene in which a subject is sitting on a chair and a hot kettle is on the bed, (b) shows a background scene, (c) shows the corresponding overhead thermal image of “(a)”, (d) shows the corresponding side thermal image of “(b)”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9998557/9807403/rezae2-3186313-large.gif
2023,9807403,Fig. 3.,"Visualizing a frame of an occupant (target). (a) shows the scene recorded by a video camera, (b) shows the thermal image from the overhead sensor in which there is a bounding box around the target, (c) shows the segmented image in which background pixels are labelled as one, and target pixels are labelled as two.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9998557/9807403/rezae3-3186313-large.gif
2023,9807403,Fig. 4.,"Visualizing a frame while a participant is laid on the bed. (a) shows the scene recorded by a video camera, (b) shows the stereo thermal image concatenated horizontally with the side thermal image on the left and the overhead thermal image on the right, (c) shows the corresponding stereo grayscale image, (d) shows the corresponding stereo binary image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/9998557/9807403/rezae4-3186313-large.gif
2023,9921203,Fig. 1.,"Illustration of our method on simulations. (a) The system's hidden state variable
θ
i
(t)
colored according to the baseline value
θ
¯
i
. (b)–(c) The system's measurements
y
i
(t)=f(
θ
i
(t),
η
i
(t))∈
R
2
colored according to their (hidden) baseline values
θ
¯
i
. These measurements serve as the input data to our algorithm. (f) The one dimensional embedding
ψ
1
(i)
obtained by our suggested algorithm as a function of the state index. (e) The true baseline values
θ
¯
i
(“the ground truth”) as a function of the state index. (f) A scatter plot that present the relation between the embedding obtained by our suggested algorithm and the “ground truth”.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9921203/cohen1-3215092-large.gif
2023,9921203,Fig. 2.,"Illustration of our method on a mechanical system. (a) An example of the input data to the algorithm: measurements of the location of mass
m
2
over time. (b) A diagram of the mechanical system consisting of two coupled masses. (c) The two-dimensional embedding obtained by Algorithm 1 (our method) colored by the sum of the masses. (d) The two-dimensional embedding obtained by a modified Algorithm 1 colored by the sum of the masses, where the
ℓ
2
norm is used instead of the Mahalanobis distance (see text for details).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9921203/cohen2-3215092-large.gif
2023,9921203,Fig. 3.,"STN border detection obtained by our proposed method - Unsupervised State Variables approximation (USVA) - applied to a single trajectory. (a) The input data – time series of measurements of the neuronal activity along the pre-planned trajectory. We show 6 representative raw signal traces out of the 84 signal traces at various depths along the trajectory (a single DBS track) recorded from a Parkinson's disease patient. The different time series are colored according to the experts' labels: white matter before STN in blue, Dorso-Lateral Oscillatory Region (DLOR) in red, Ventro Medial Non-oscillatory Region (VMNR) in green, and white matter after STN in light blue. (b) The 1D embedding obtained by the USVA method. The Y-axis displays the approximate state variable value, i.e., the value of the most dominant eigenvector as a function of the Estimated Distance from Target (EDT). The STN entry and exit locations marked by a human expert are marked by red ‘x’. (c) Same as (b) but after pre-processing (smoothing) with a moving average window of size 3, and computing the difference between the medians of consecutive windows of size 5. (d) 2D embedding obtained by the USVA method. The embedded points are colored according to the expert's labels, where red points belong to depths in the DLOR and green points belong to depths in the VMNR. The X-axis indicates the value of the second most dominant eigenvector and the Y-axis indicates the corresponding value of the third most dominant eigenvector. (e) Same 2D embedding as in (d) but colored according to k-means as suggested in Algorithm 3. (f) A comparison between the detection results obtained by our USVA method (marked with ‘
⋄
’), HMM (marked with ‘+’), and the expert's labels (marked with ‘o’). Colors are the same as in Fig. 3(a).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9921203/cohen3-3215092-large.gif
2023,9921203,Fig. 4.,Comparison between our method and the HMM algorithm. The comparison is based on the experts' labels of 25 different trajectories of 16 different patients. The blue thick bars indicate the median error percentage of each task and the black thin bars indicate the interquartile range (IQR). (a) The median error in detecting the entry point to the STN region. (b) The median error in detecting the exit from the STN region. (c) The median overall error in detecting the STN region. (d) The median error in detecting the exit from the DLOR. (e) The median overall error in detecting the DLOR.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9921203/cohen4-3215092-large.gif
2023,9832795,Fig. 1.,"A schematic diagram of ZSL versus GZSL. Assume that the seen class contains samples of Otter and Tiger, while the unseen class contains samples of Polar bear and Zebra. (a) During the training phase, both GZSL and ZSL methods have access to the samples and semantic representations of the seen class. (b) During the test phase, ZSL can only recognize samples from the unseen class, while (c) GZSL is able to recognize samples from both seen and unseen classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp1-3191696-large.gif
2023,9832795,Fig. 2.,"A schematic view of Transductive and Inductive settings. In inductive setting, only the visual features and semantic representations of the seen classes (
A
and
B
) are available. While transductive setting, in addition to the seen class information, has access to the unlabelled visual samples of the unseen classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp2-3191696-large.gif
2023,9832795,Fig. 3.,A schematic view of different embedding spaces in GZSL (adapted from [58]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp3-3191696-large.gif
2023,9832795,Fig. 4.,A schematic view of the projection domain shift problem (adapted from [66]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp4-3191696-large.gif
2023,9832795,Fig. 5.,A schematic view of the bias concerning seen classes (source) in the semantic embedding space (adapted from [72]).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp5-3191696-large.gif
2023,9832795,Fig. 6.,"Embedding-based versus generative-based methods. The embedding based methods (a) lean an embedding space to project the visual and semantic features of seen classes into a common space. Then, the learned embedding space is used to perform recognition. In contrast, the generative-based methods (b) learn a generative model based on samples of seen classes conditioned on their semantic features. Then, the learned model is used to generate visual features for unseen classes using the semantic features of unseen classes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp6-3191696-large.gif
2023,9832795,Fig. 7.,The taxonomy of GZSL models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp7-3191696-large.gif
2023,9832795,Fig. 8.,"A schematic view of SRG [63]. First, the image prototype
f
k
and semantic prototype
e
k
of each class are reconstructed using the cluster center and (2), respectively. Then, the shared reconstruction coefficients between two spaces is learned. Finally, the learned SRG is used to synthesize class prototypes for unseen classes to perform prediction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp8-3191696-large.gif
2023,9832795,Fig. 9.,"A schematic of DAZLE [83]. After extracting image features of the
R
regions, the attention features of all attributes are computed using a dense attention mechanism. Then, the attention features are aligned with the attribute semantic vectors, in order to compute the score of attributes in the image.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp9-3191696-large.gif
2023,9832795,Fig. 10.,A general view of the f-CLSWGAN model [139]). It learns a generative model based on the visual features of seen classes conditioned on their corresponding semantic representations. It also uses classification loss to generate more discriminative visual features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp10-3191696-large.gif
2023,9832795,Fig. 11.,"A schematic view of CVAE-ZSL [165]. After concatenating the visual features and the semantic representations, it is passed through dense, dropout and dense layers. Then, another dense layer is used to output
μ
z
and
∑
z
. Next, a
z
is sampled from
N(
μ
x
i
,
Σ
x
i
)
and projected to the image space for reconstruction.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp11-3191696-large.gif
2023,9832795,Fig. 12.,"A general view of QFSL [44]. It first projects the visual features of the seen classes into a number of fixed points in the semantic space. Then, a bias loss function (12) is used to map the visual features of the unseen classes into other points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10061515/9832795/pourp12-3191696-large.gif
2023,9830620,Fig. 1.,"3-D volume on the bottom-right of the figure represents the seismic data after pseudo-deblending. The two images in the upper row show the pseudo-deblended data and its corresponding unblended data in the common-shot domain, where the blending noise is coherent. But in the common-receiver domain (the bottom-left image), the blending noise is incoherent.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang1-3188915-large.gif
2023,9830620,Fig. 2.,Example of the failed result by training with a traditional CNN with the blending loss. (a) Input noisy data. (b) Prediction after the convergence.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang2ab-3188915-large.gif
2023,9830620,Fig. 3.,"Basic idea of blind-trace method. Left and right images represent the input (blended) and prediction (deblended) data, respectively. To recover the red marked trace in the right image, only the information of the blue patches in input is used, ignoring the original noisy trace itself.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang3-3188915-large.gif
2023,9830620,Fig. 4.,"Structure of the blind-trace network. The red line marks a sample target trace. The blue and green patches indicate the areas used for predicting the deblended result. In the first layer after the blind-trace U-Net, the patches cover the target trace. And with padding and cropping operations, the red line is excluded and the model is fully blind-trace.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang4-3188915-large.gif
2023,9830620,Fig. 5.,Modified convolution layer (a) and max-pooling layer (b) in the blind-trace network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang5ab-3188915-large.gif
2023,9830620,Fig. 6.,"Illustration of the two-stage framework. The upper row is the first stage, i.e., training stage, and the bottom row shows the second tuning stage. The yellow BTNet indicates the model to be optimized for the training stage. After training, the parameters
θ
∗
are fixed and copied to the tuning stage. The predicted result
x
′
=
f
θ
∗
(x)
is used as the initial input to the tuning stage. In this stage, the yellow input
x
′
is to be optimized by the tuning loss defined in (10).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang6-3188915-large.gif
2023,9830620,Fig. 7.,"Result of the “simple” “alternate”-blended dataset. (a) Input noisy data and (b) unblended ground truth. (c) and (d) Predictions from conventional and the proposed methods, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang7abcd-3188915-large.gif
2023,9830620,Fig. 8.,"Training-stage result of the “alternate” blended SEAMII-unconventional data. (a)–(d) Blended data (input), unblended data (ground truth), and deblending results (prediction) from conventional and the proposed methods, respectively, in the common receiver domain. (e)–(h) Detail of the red-rectangle area in (a) accordingly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang8abcdefgh-3188915-large.gif
2023,9830620,Fig. 9.,"Training-stage result of the “half” blended SEAMII-unconventional data. (a)–(d) Blended data (input), unblended data (ground truth), and deblending results (prediction) from conventional and the proposed methods, respectively, in the common receiver domain. (e)–(h) Detail of the red-rectangle area in (a) accordingly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang9abcdefgh-3188915-large.gif
2023,9830620,Fig. 10.,"Example of the “alternate”-blended SEAMII-unconventional dataset (a) before (input) and (b) after (prediction) the amplitude balancing in the common receiver domain. (c)
log1p
function defined in (13).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang10abc-3188915-large.gif
2023,9830620,Fig. 11.,"Training-stage result of the “alternate” blended SEAMII-unconventional data in the common shot domain. (a)–(c) Blended data (input), unblended data (ground truth), and deblending result (prediction) of the proposed method, respectively. (d)–(f) Demonstrate the red-rectangle area in (a) accordingly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang11abcdef-3188915-large.gif
2023,9830620,Fig. 12.,"Training-stage result of the “half” blended SEAMII-unconventional data in the common shot domain. (a)–(c) Blended data (input), unblended data (ground truth), and deblending result (prediction) of the proposed method, respectively. (d)–(f) Demonstrate the red-rectangle area in (a) accordingly.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang12abcdef-3188915-large.gif
2023,9830620,Fig. 13.,"Result of the “continuous” blended BP2004 data. (a) Complete input and (b)–(g) emphasized area by the red rectangle. (b)–(d) Input
x
, ground truth
x
∗
, and training-stage deblending result (direct prediction) after
n=200
epochs training
x
′
=
f
θ
∗
(x)
, respectively. (d) and (e) Initial input and final prediction in the tuning stage. (f)
n=5
epochs training result. (g) Corresponding tuning result.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang13abcdefg-3188915-large.gif
2023,9830620,Fig. 14.,"Example trace comparison of Fig. 13(d) and (e). (a) Input and ground truth. (b) and (c) Training stage and tuning stage results, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10175014/9830620/wang14abcde-3188915-large.gif
2023,10058956,FIGURE 1.,Ice cream factory simulator overview.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon1-3252901-large.gif
2023,10058956,FIGURE 2.,"Graphical user interface that visualizes all six interconnected modules (Mixer, Pasteurizer, Homogenizer, AgeingCooling, DynamicFreezer, and Hardening). The box for Hardening module shows all parameters as an example.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon2-3252901-large.gif
2023,10058956,FIGURE 3.,Data plot from simulation that shows how different parameters change during one normal run.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon3-3252901-large.gif
2023,10058956,FIGURE 4.,Data plot from simulation that shows how different parameters change during one run where the temperature sensor is frozen. “Pasteurizer*” shows the actual temperature of the tank after the sensor is frozen.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon4-3252901-large.gif
2023,10058956,FIGURE 5.,Different types of injected anomalies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon5-3252901-large.gif
2023,10058956,FIGURE 6.,Percentage of instances per class.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon6-3252901-large.gif
2023,10058956,FIGURE 7.,Percentage of runs depending on (a) type of run and (b) module where the anomaly is injected.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon7ab-3252901-large.gif
2023,10058956,FIGURE 8.,Values of the parameters in which anomalies have been injected. Each blue circle represents one instance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon8-3252901-large.gif
2023,10058956,FIGURE 9.,Distribution per class of the parameters where the anomalies were injected. x-axis represent the possible values of the parameters while the y-axis show the frequency of which the values appear in the dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon9-3252901-large.gif
2023,10058956,FIGURE 10.,"Predictions of LR, DT, RF, and MLP for 4 full scenarios from the testing set for anomaly Detection. We are showing 1 for every 50 predictions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon10-3252901-large.gif
2023,10058956,FIGURE 11.,Confusion matrix and accuracy per class for each ML algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon11-3252901-large.gif
2023,10058956,FIGURE 12.,"Predictions of LR, DT, RF, and MLP for 4 full scenarios from the testing set for anomaly Classification. We are showing 1 for every 50 predictions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10058956/leon12-3252901-large.gif
2023,10061196,Fig. 1.,"The Overall Framework of Semi-supervised Optimal Transport with Self-paced Ensemble (SPSSOT), which consists of three main parts: (1) Feature Engineering to extract Sepsis-related features under the guidance of doctors; (2) Self-paced sampling to filter out “more contributing” samples from negative samples (no Sepsis, shown as circles), which will help to improve the performance of classifier. (3) DA (Semi-supervised domain adaptation): given all data with labels of source hospital (shown as red), little data with labels and most data without labels in target hospital (shown as blue), align two feature spaces via optimal transport theory and learn a better classifier to distinguish whether Sepsis will occur. Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding1-3253208-large.gif
2023,10061196,Fig. 2.,Semi-supervised Domain Adaptation with Optimal Transport (SSOT).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding2-3253208-large.gif
2023,10061196,Fig. 3.,The Core Idea of Self-paced Ensemble Based on SSOT (SPSSOT). There are three main steps: (1) Initialize SSOT according to Algorithm 1; (2) self-paced under-sampling from majority class in both domains to obtain balanced data; (3) get an additive model by iteration training. Best viewed in color.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding3-3253208-large.gif
2023,10061196,Fig. 4.,Ablation Study: compare SPSSOT with its variants.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding4-3253208-large.gif
2023,10061196,Fig. 5.,The convergence performance of SPSSOT and DeepJDOT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding5-3253208-large.gif
2023,10061196,Fig. 6.,Different percentages of labeled data in target domain.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding6-3253208-large.gif
2023,10061196,Fig. 7.,Different sampling percentages of source data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding7-3253208-large.gif
2023,10061196,Fig. 8.,"Parameter Sensitivity of MIMIC
→
Challenge: vary the four hyperparameters in the loss function and compare the results of the experiments.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding8-3253208-large.gif
2023,10061196,Fig. 9.,"Feature visualization. M
→
C means MIMIC
→
Challenge and C
→
M means Challenge
→
MIMIC. Different colors represent different classes (red: will have Sepsis, blue: will not have Sepsis), different shapes represent different domains (round: source domain, cross: target). Best viewed in color.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10144459/10061196/ding9-3253208-large.gif
2023,9956996,FIGURE 1.,MobileNetV2 special network bottleneck layer structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/9961067/9956996/guo1-3224021-large.gif
2023,9956996,FIGURE 2.,The architecture of RK-net.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/9961067/9956996/guo2-3224021-large.gif
2023,9956996,FIGURE 3.,The workflow of study. (A) Image conversion. (B) RK-net process. (C) Manual annotation process. (D) Manual screening process. (E) Training and testing of CRC diagnostic model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/9961067/9956996/guo3abcde-3224021-large.gif
2023,9956996,FIGURE 4.,Time cost for data processing and CRC diagnostic model training.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/9961067/9956996/guo4-3224021-large.gif
2023,9956996,FIGURE 5.,Training loss and accuracy of CRC diagnostic model with different datasets. (A) Model training loss. (B) Model training accuracy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221039/9961067/9956996/guo5ab-3224021-large.gif
2023,10049559,FIGURE 1.,Flowchart of this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa1-3247448-large.gif
2023,10049559,FIGURE 2.,Graphical representation of the SVM model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa2-3247448-large.gif
2023,10049559,FIGURE 3.,The Schematic system of the KNN model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa3-3247448-large.gif
2023,10049559,FIGURE 4.,Schematic diagram of SDT and EDT.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa4-3247448-large.gif
2023,10049559,FIGURE 5.,RF schematic diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa5-3247448-large.gif
2023,10049559,FIGURE 6.,Selected inputs using RF and its parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa6-3247448-large.gif
2023,10049559,FIGURE 7.,Iteration process of ML models’ optimization-Iowa dataset: (a) 70:30 training/testing partitioning; (b) 80:20 training/testing partitioning; (c) 90:10 training/testing partitioning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa7abc-3247448-large.gif
2023,10049559,FIGURE 8.,Iteration process of ML models’ optimization-Ohio dataset: (a) 70:30 training/testing partitioning; (b) 80:20 training/testing partitioning; (c) 90:10 training/testing partitioning.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa8abc-3247448-large.gif
2023,10049559,FIGURE 9.,Changes in average accuracy (%) and AUC across two datasets: (a) changes in accuracy; (b) changes in AUC.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa9ab-3247448-large.gif
2023,10049559,FIGURE 10.,A comparison between the average number of iterations required for training the optimized models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa10-3247448-large.gif
2023,10049559,FIGURE 11.,The average accuracy of various ML models developed in this study was optimized by BO and grid search.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10049559/aghaa11-3247448-large.gif
2023,9929436,Fig. 1.,Block diagram of the proposed eating-onset detection system.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9929436/aljla1-3217196-large.gif
2023,9929436,Fig. 2.,Drift corrected (upper) versus uncorrected (lower) velocity.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9929436/aljla2-3217196-large.gif
2023,9929436,Fig. 3.,"Pictorial representation of various signal properties used for feature extraction (horizontal axis represents the time in sampling units. (a) Usual patterns observed during bite action; (b) heading deviation of wrist depicted during a food intake maneuver; (c) distance traveled by the wrist for eating ground truth, and general gestures; (d) minimum energy instants during an eating episode; (e) slope regions and directions of roll and pitch signals; and (f) torsion and curvature energy calculation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9929436/aljla3-3217196-large.gif
2023,9929436,Fig. 4.,"Proposed evaluation scheme for TPs, FPs, and FNs.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9929436/aljla4-3217196-large.gif
2023,9929436,Fig. 5.,Smoking when walking (left) and eating with a spoon (right) [37].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/10/10078065/9929436/aljla5-3217196-large.gif
2023,10023486,FIGURE 1.,"Some apple images. (a) Real images, (b) Fake images generated with Pix-to-Pix GAN.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal1ab-3238570-large.gif
2023,10023486,FIGURE 2.,"Class distribution of Dataset A. (a) 3D class distribution, (b) 2D class distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal2ab-3238570-large.gif
2023,10023486,FIGURE 3.,"Class distribution of Dataset B. (a) 3D class distribution, (b) 2D class distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal3ab-3238570-large.gif
2023,10023486,FIGURE 4.,"Class distribution of Dataset C. (a) 3D class distribution, (b) 2D class distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal4ab-3238570-large.gif
2023,10023486,FIGURE 5.,"Class distribution of Dataset D. (a) 3D class distribution, (b) 2D class distribution.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal5ab-3238570-large.gif
2023,10023486,FIGURE 6.,Block diagram of the proposed hybrid CNN model. (I) Review of the proposed hybrid classification model. (II) Architecture of the best hybrid classification model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal6-3238570-large.gif
2023,10023486,FIGURE 7.,Used CNN architecture for this study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal7-3238570-large.gif
2023,10023486,FIGURE 8.,Confusion matrix.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal8-3238570-large.gif
2023,10023486,FIGURE 9.,Evaluation of KFold cross validation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal9-3238570-large.gif
2023,10023486,FIGURE 10.,Confusion matrix of CNN-SVM (linear) hybrid model for Dataset A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal10-3238570-large.gif
2023,10023486,FIGURE 11.,Confusion matrix of CNN-Gradient Boosting hybrid model for Dataset A.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal11-3238570-large.gif
2023,10023486,FIGURE 12.,Confusion matrix of CNN-SVM (linear) hybrid model for Dataset B.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal12-3238570-large.gif
2023,10023486,FIGURE 13.,Confusion matrix of CNN-SVM (linear) hybrid model for Dataset C.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal13-3238570-large.gif
2023,10023486,FIGURE 14.,Confusion matrix of CNN-SVM (linear) hybrid model for Dataset D.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10023486/bal14-3238570-large.gif
2023,10109271,Fig. 1.,Photography of the fuel tank under test in an attachment mode [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe1-3267280-large.gif
2023,10109271,Fig. 2.,"Fuel tank CAD [9]. At top, full CAD. At bottom, CAD without panels.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe2-3267280-large.gif
2023,10109271,Fig. 3.,Fuel tank FDTD mesh in an attachment mode [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe3-3267280-large.gif
2023,10109271,Fig. 4.,Illustration of the direct scheme with one cell shift.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe4-3267280-large.gif
2023,10109271,Fig. 5.,Flowchart of the post-treatment used in [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe5-3267280-large.gif
2023,10109271,Fig. 6.,Relative error averaged over frequencies for each fastener.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe6-3267280-large.gif
2023,10109271,Fig. 7.,"Relative error as a function of fastener number and frequency (top), as a function of fastener averaged over frequencies (bottom left), as a function of frequency averaged over fasteners (bottom right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe7-3267280-large.gif
2023,10109271,Fig. 8.,Illustration of the main parameters around the observation points (in red).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe8-3267280-large.gif
2023,10109271,Fig. 9.,"Relative error as a function of frequency (top), for fasteners 75 (left), 130 (middle), and 160 (right) and as a function of position (bottom), for frequencies 100 Hz (left), 500 kHz (middle), 990 kHz (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe9-3267280-large.gif
2023,10109271,Fig. 10.,"Predicted values versus ground truth for fasteners 75 (left), 130 (middle), 160 (right), and for frequencies 100 Hz (top), 500 kHz (middle), 990 kHz (bottom). The line corresponds to perfect predictions.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe10-3267280-large.gif
2023,10109271,Fig. 11.,"Predicted value versus ground truth for test sample 179 as a function of frequency (top) for fastener 75 (left), 130 (middle), 160 (right), and as a function of position (bottom) for frequency 100 Hz (left), 500 kHz (middle), 990 kHz (right).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe11-3267280-large.gif
2023,10109271,Fig. 12.,"SHAP plots for fasteners 75 (left), 130 (middle), 160 (right), and for frequencies 100 Hz (top), 500 kHz (middle), 990 kHz (bottom).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/15/10149542/10109271/monfe12-3267280-large.gif
2023,10002336,FIGURE 1.,Hybrid ensemble model with stacking approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj1-3232984-large.gif
2023,10002336,FIGURE 2.,Project progress diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj2-3232984-large.gif
2023,10002336,FIGURE 3.,Block diagram of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj3-3232984-large.gif
2023,10002336,FIGURE 4.,Distribution of gender and age of the students.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj4-3232984-large.gif
2023,10002336,FIGURE 5.,Comparison graph of students who want to go to university and their year-end grade averages.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj5-3232984-large.gif
2023,10002336,FIGURE 6.,Distribution of homework grades and year-end grades. (more: more than 50/less: less than 50).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj6-3232984-large.gif
2023,10002336,FIGURE 7.,Histogram of year-end average grades.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj7-3232984-large.gif
2023,10002336,FIGURE 8.,Heatmap shows that correlation matrix between features and year-end grade.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj8-3232984-large.gif
2023,10002336,FIGURE 9.,ROC curve of the hybrid model (when meta learner is SVM) for each fold and AUC scores.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj9-3232984-large.gif
2023,10002336,FIGURE 10.,Visualizing various metrics values of different meta-learners used in creating a hybrid model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10002336/alhaj10-3232984-large.gif
2023,9508768,Fig. 1.,"Overview of the proposed network. We randomly select trials regardless of the subjects for a mini-batch. After mapping an input
x
to a local feature
f
l
by a local encoder
E
l
, a point-wise convolutional layer
V
embeds it into a class-relevant feature
f
re
and a class-irrelevant feature
f
ir
with the help of mutual information estimator
M
. Through a GRL that multiplies a negative value during back-propagation,
M
estimates the mutual information between both class-relevant feature and class-irrelevant feature and the previous networks reduce the mutual information between them. Subsequently, a global encoder
E
g
operates on top of the class-relevant feature and then extracts global feature
f
g
. A classifier
C
takes the global feature
f
g
to identify the class label of an input EEG. Furthermore, to enhance the representational power of the global feature, we maximize mutual information between two features of
f
re
and
f
g
by using two networks, that is,
T
l
and
T
g
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9508768/suk1-3100583-large.gif
2023,9508768,Fig. 2.,Distributions of singular values of all kernel weights in encoders. We used the trained weights with GIST dataset [28] in Scenario II. (a) Deep ConvNet [12]. (b) EEGNet [9].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9508768/suk2ab-3100583-large.gif
2023,9508768,Fig. 3.,"Illustrative comparison of PSD maps and decision-relevance heatmaps [43] of input EEG signals from randomly selected test subjects’ data. The PSD and relevance scores were averaged along the frequency and time axis, respectively. We normalized each visualized topoplot in a range between 0 and 1. Whereas we used all subjects’ training data to train the network in Scenario I, the target subject’s EEG samples were not employed on training at all in Scenario II. (L: left-hand, R: right-hand). (a) Deep ConvNet [12] trained on GIST dataset [28]. (b) EEGNet [9] trained on KU dataset [29].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9508768/suk3ab-3100583-large.gif
2023,9508768,Fig. 4.,"t-SNE [44] of the class-irrelevant, class-relevant, and global features obtained from Subject 21 of KU dataset [29] in Scenarios I and II. We visualized all features trained using Deep ConvNet [12].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10036162/9508768/suk4-3100583-large.gif
2023,10036086,FIGURE 1.,"Target automaton
A
(SUL).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10036086/hanee1-3242124-large.gif
2023,10036086,FIGURE 2.,"Hypothesis automaton
M
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10036086/hanee2-3242124-large.gif
2023,10036086,FIGURE 3.,Evaluation framework.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10036086/hanee3-3242124-large.gif
2023,10036086,FIGURE 4.,Comparison of BDLIQ and ID algorithm with respect to queries.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10036086/hanee4-3242124-large.gif
2023,10036086,FIGURE 5.,Comparison of BDLIQ and ID algorithm with respect to learning time(ms).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10036086/hanee5-3242124-large.gif
2023,10122521,FIGURE 1.,Overview of research framework on the model development of cyberbullying detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10122521/varat1-3275130-large.gif
2023,10122521,FIGURE 2.,Process flow of replacing obfuscated profane and bad terms in text preprocessing.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10122521/varat2-3275130-large.gif
2023,10122521,FIGURE 3.,Process flow for generating (a) Elmo and nnlm sentence vector embeddings and (b) Word embeddings from bert model and its variant.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10122521/varat3ab-3275130-large.gif
2023,10122521,FIGURE 4.,A transfer learning approach to fine-tune PLMs for cyberbullying detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10122521/varat4-3275130-large.gif
2023,10122521,FIGURE 5.,Architecture of electra-small model that works in generator-discriminator model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10122521/varat5-3275130-large.gif
2023,10122521,FIGURE 6.,Screenshot of cyberbullying checker application developed with the fine-tuned DistilBert.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10122521/varat6-3275130-large.gif
2023,10107691,Fig. 1.,Equipment health quantitative assessment idea.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui1-3268462-large.gif
2023,10107691,Fig. 2.,Adaptive structure health quantitative assessment.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui2-3268462-large.gif
2023,10107691,Fig. 3.,Process of the EM-PASCAL approach.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui3-3268462-large.gif
2023,10107691,Fig. 4.,Network training of the minimum error pairwise comparison learning method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui4-3268462-large.gif
2023,10107691,Fig. 5.,Relative error at different numbers of nodes (in percentage).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui5-3268462-large.gif
2023,10107691,Fig. 6.,Results of the relative errors with the number of enhancement layers (in percentage).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui6-3268462-large.gif
2023,10107691,Fig. 7.,Comparative experimental results on accuracy using six methods for (a) and (b) GTC and (c) and (d) GT equipment (in percentage).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui7abcd-3268462-large.gif
2023,10107691,Fig. 8.,"GTC and GT health state true, assessed and predicted values (in percentage).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7361/10139852/10107691/cui8-3268462-large.gif
2023,9729603,Fig. 1.,"Illustration of inverse transform sampling for drawing a sample from
Cat(π)
. A sample from the Uniform distribution is converted to one of the categories/classes of the categorical distribution via its inverse cumulative density function. The chance that class
i
is being sampled equals
π
i
; the width of the corresponding box in this illustration.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb1-3157042-large.gif
2023,9729603,Fig. 2.,"Categorization of the different applications in deep learning in which Gumbel-based gradient estimators have been applied in order to facilitate training via backpropagation through discrete stochastic nodes. Visualizations in the bottom show where different models exhibit discretized data, indicated by the dashed gray boxes.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb2-3157042-large.gif
2023,9729603,Fig. 3.,"The Gumbel (double Exponential) distribution directly relates to the Uniform/Beta and Exponential distribution via a single, respectively double, negative logarithmic relation. The green ellipses indicate the probability distribution of the corresponding random variable in the cell. To prevent clutter, we here define
U:=
U
(i)
,
X:=
X
(i)
, and
G:=
G
(i)
, being an
i
th
independent standard Uniform, Exponential, and Gumbel random variable, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb3-3157042-large.gif
2023,9729603,Fig. 4.,"Drawing a sample from a categorical distribution can be done via different paths. The most suitable path depends on the context in which one would like to draw a sample. One can start from either the unnormalized log-probabilities
a
(and a Boltzmann temperature
T
), or the normalized probabilities
π
. The green ellipses in the upper right corner of certain nodes indicate that the node represents a random variable following the distribution indicated in the ellipse. The different partition functions
Z
D
all normalize their corresponding node, their input is omitted for readability reasons. The yellow square boxes are used to refer to certain paths in the text. A Jupyter Notebook that accompanies this figure is publicly available. 4",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb4-3157042-large.gif
2023,9729603,Fig. 5.,"Sampling Gumbels and the corresponding argmax
I
and maximum
M
, can be done in a bottom-up or top-down approach. The former finds the
(arg)max
from the perturbed logits, starting from the Gumbels. Top-down sampling, on the other hand, starts from either
M
and/or
I
, and conditionally samples the corresponding perturbed logits. The maximum and argmax are independent (
⊥⊥
), allowing for independent sampling of either of them in case only the other entity is known. The top-down procedure can be applied in parallel for the entire domain
D
(depicted) or sequentially (not visualized).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb5-3157042-large.gif
2023,9729603,Fig. 6.,"Overview of sampling algorithms for different scenarios: single (discrete or soft) or multiple sample(s) from (un)structured distributions. For all cases, both default/conventional and Gumbel-based algorithms are given. The green ellipses indicate the distribution from which samples of the indicated algorithms originate. In case of empty ellipses, it is unknown which distribution the samples follow. A superscript
z
indicates that the algorithm requires normalized probabilities, i.e., partition function
Z
should be known.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb6-3157042-large.gif
2023,9729603,Fig. 7.,"Illustration of Gumbel-top-
k
sampling. Figure adapted from [14]. Gumbel-top-
k
perturbs all logits with i.i.d. Gumbel samples, and returns the index of the
k
perturbed logits with the highest values. These indices yield
k
independent samples without replacement from
Cat(θ)
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10008914/9729603/huijb7-3157042-large.gif
2023,10065451,Figure 1.,"Circuits used in experiments to fit real-valued functions on
B
3
. (Top) Phase-embedding circuit. (Bottom) QRAC-embedding circuit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8924785/9998549/10065451/herma1-3255206-large.gif
2023,10065451,Figure 2.,"Simulator and experimental results obtained from using the (a) phase embedding with three qubits and (b) QRAC embedding with one qubit to fit the function
g
3
with
a
1
=1/2,
a
2
=−1/10,and
a
3
=1/4
. “Target” represents the exact outputs and Fourier spectrum of
g
3
. Both methods successfully fit the target function with high accuracy. (a) Phase Embedding. (b) QRAC Embedding.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8924785/9998549/10065451/herma2-3255206-large.gif
2023,10065451,Figure 3.,"Circuits used in experiments to fit real-valued functions on
B
6
. (Top) Phase-embedding circuit. (Bottom) QRAC-embedding circuit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8924785/9998549/10065451/herma3-3255206-large.gif
2023,10065451,Figure 4.,"Simulator and experimental results obtained from using the (a) phase embedding with six qubits and (b) QRAC embedding with two qubits to fit the function
g
6
with
d
1
=
d
2
=−1/5and
d
3
=
d
4
=1/10
. “Target” represents the exact outputs and Fourier spectrum of
g
6
. Both methods successfully fit the target function with high accuracy. (a) Phase Embedding. (b) QRAC Embedding.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8924785/9998549/10065451/herma4-3255206-large.gif
2023,10049417,Fig. 1.,Three possible BLE-based positioning scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho1-3247299-large.gif
2023,10049417,Fig. 2.,"RSSI analysis. (a) Phone 1: blue: 5 m, yellow: 3 m, green: 1 m. (b) Phone 2: gray: 5 m, orange: 3 m, blue: 1 m.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho2ab-3247299-large.gif
2023,10049417,Fig. 3.,Flow diagram of the hybrid learning method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho3-3247299-large.gif
2023,10049417,Fig. 4.,Example of cluster assignment for one RSSI record group.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho4-3247299-large.gif
2023,10049417,Fig. 5.,Model aggregation using GA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho5-3247299-large.gif
2023,10049417,Fig. 6.,Hybrid learning model.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho6-3247299-large.gif
2023,10049417,Fig. 7.,Experimental setup.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho7-3247299-large.gif
2023,10049417,Fig. 8.,"MAE for different phone pairs for the daily experiments (Four result groups: day 1, day 2, day 3, and day 4).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho8-3247299-large.gif
2023,10049417,Fig. 9.,Measurements from a broadcaster at the same distance by two different mobile phones.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho9-3247299-large.gif
2023,10049417,Fig. 10.,Average MAE for different methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho10-3247299-large.gif
2023,10049417,Fig. 11.,Cumulative distribution function of absolute error distance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho11-3247299-large.gif
2023,10049417,Fig. 12.,MAE and improvement of various methods.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho12-3247299-large.gif
2023,10049417,Fig. 13.,Tradeoff of flagging.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho13-3247299-large.gif
2023,10049417,Fig. 14.,Tradeoff of flagging for model aggregation/GA.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho14-3247299-large.gif
2023,10049417,Fig. 15.,Cluster patterns.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho15-3247299-large.gif
2023,10049417,Fig. 16.,Distance estimation accuracy—improvement percentage over baseline.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho16-3247299-large.gif
2023,10049417,Fig. 17.,Experimental setups for mobile ad-hoc positioning. (a) No obstacles. (b) Four chairs. (c) Six chairs. (d) Six chairs and two monitors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho17abcd-3247299-large.gif
2023,10049417,Fig. 18.,Positioning error (in meter) of the MAP experiment over different iterations. (a) No obstacles. (b) Four chairs. (c) Six chairs. (d) Six chairs and two monitors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10175002/10049417/ho18abcd-3247299-large.gif
2023,9930860,Fig. 1.,EPE visualization and its distribution budget for 5~7-nm logic node [5].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/10036056/9930860/dey1-3217326-large.gif
2023,9930860,Fig. 2.,EPE optimization flow [7].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/10036056/9930860/dey2-3217326-large.gif
2023,9930860,Fig. 3.,Diagram of PRISMA selection procedure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/10036056/9930860/dey3-3217326-large.gif
2023,9930860,Fig. 4.,Optimized lithographic masks with SRAFs and OPC patterns [40].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/10036056/9930860/dey4-3217326-large.gif
2023,9930860,Fig. 5.,The role of OPC in photolithography process [42].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/10036056/9930860/dey5-3217326-large.gif
2023,9930860,Fig. 6.,Illustration of a physics-informed ML algorithm [62].,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/66/10036056/9930860/dey6-3217326-large.gif
2023,9564257,Fig. 1.,"Total number of trips for each vehicle category. The
y
-axis is on a logarithmic scale. It is worth noting the numerous portion of unknown vehicles.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi1-3118834-large.gif
2023,9564257,Fig. 2.,"Total number of steps per road category. The
y
-axis is on a logarithmic scale. Ignoring the unknown streets, the order of magnitude of the occurrences by street is similar, but most of the steps of the trip fall on the highways.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi2-3118834-large.gif
2023,9564257,Fig. 3.,"Total number of trips and steps for each Private and Truck categories. The
y
-axis is on a logarithmic scale.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi3-3118834-large.gif
2023,9564257,Fig. 4.,"Number of occurrences with respect to (a) trips and (b) steps, respectively, grouped by duration and length. Short-to-mid range trips, both in space and time, are the most frequent in the whole data set. The steps distribution shows evident inhomogeneity both in duration and in distance.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi4ab-3118834-large.gif
2023,9564257,Fig. 5.,"Map location with the trips steps distribution, generated from the provided data set, in the metropolitan city of Bologna. The darker blue spots indicate a strong presence of trips steps. Large groups of trips can be found with strong density and concentration on highways and main freeways, while the trips on the other streets are less dense but more sparse than the large groups.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi5-3118834-large.gif
2023,9564257,Fig. 6.,"(a) Variation of the WCSS score by the number of clusters (
K
), with the best
K
, equal to 5 in this case, denoted by a vertical dashed line. (b) reports how the kneedle algorithm found the best
K
, with the data reported on a normalized scale.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi6ab-3118834-large.gif
2023,9564257,Fig. 7.,"Cluster distribution in the 3-D principal components space on each of the axis planes, with the projected features as arrows, whose amplitude has been overscaled for readability. The variance explained by the different three principal components is 32.41%, 17.92%, and 14.86%, respectively. The total variance explained in the 3-D principal components space sums up to 65.19%.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi7abc-3118834-large.gif
2023,9564257,Fig. 8.,"Distribution of (a) speed excesses, (b) speed excesses on adverse weather conditions, (c) total distance, and (d) median speed on each cluster. Plots (a), (b), and (d) are also divided for road type, blue for urban roads, orange for freeways, and green for highways. Plots (a) and (b) also report speed excesses by the vehicle type, namely, private vehicles on the first row and trucks on the second row. The black line represents the overall median.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi8abcd-3118834-large.gif
2023,9564257,Fig. 9.,"Quarterly median distribution of the features for each cluster. (a)–(d) represent the first quarter, second quarter, third quarter, and fourth quarter, respectively. The features are scaled between 0 and 1 to homogenize the representation. The features in the second quarter are differently distributed from the other quarters.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi9abcd-3118834-large.gif
2023,9564257,Fig. 10.,"Distribution of the trips in cluster 4 divided in the (a) first quarter, (b) second quarter, (c) third quarter, and (d) fourth quarter of the year. Darker blue spots indicate a stronger presence of trip steps. It is worth noting three distinct distributions: small presence but widespread in the first quarter and in the third quarter; very strong and widespread presence in the second quarter; and a distribution which is only relevant on the highways and in the town of Bologna in the fourth quarter.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6488907/10038283/9564257/prezi10abcd-3118834-large.gif
2023,10154613,Fig. 1.,"An illustration of the proposed reciprocal learning method. By modeling the imaging process of the shadow image, we prove that the main bottleneck of current unpaired data based methods is the lack of guidance by the shadow mask. Based on this, we design two complementary networks to learn shadow removal better. The top branch is a shadow detector to find where the shadow is, while the bottom branch is a shadow remover to remove the shadow. Note that the numbers in the circle indicate the learning orders.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie1-3285439-large.gif
2023,10154613,Fig. 2.,"An illustration of the proposed deep reciprocal network (DRNet) for shadow removal. There are three basic phases of reciprocal learning to train the DRNet. In phase 1 of initial learning, a shadow remover which contains shadow removal generator
G
f
0
and shadow synthesis generator
G
s
is trained using unpaired data. Then the pseudo shadow masks are produced by
G
f
0
, which are used to train shadow detector in phase 2. In phase 3 of shadow removal optimization, the concatenation of shadow images and shadow masks generated by the shadow detector is the input of the enhanced shadow removal generator
G
f
1
. In this way, the shadow detector and shadow remover guide each other to improve the overall restoration ability.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie2-3285439-large.gif
2023,10154613,Fig. 3.,"The data flow diagram of shadow removal optimization. In the generation phase, the shadow remover consists of a shadow removal generator
G
f
and a shadow synthesis generator
G
s
. Both generators use mask as a guide. In the discrimination phase, the shadow image discriminator
D
s
has a mask as input, while the clean image discriminator
D
f
has no mask input. Color-maintenance loss
l
color
is used to train the shadow removal generator
G
f
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie3-3285439-large.gif
2023,10154613,Fig. 4.,Visual comparisons of shadow removal. (a) and (b) are the shaow images and corresponding clean image; (c)-(e) are shadow removal results generated from existing methods. (f) are our results. MS represents MaskShadowGAN.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie4abcdef-3285439-large.gif
2023,10154613,Fig. 5.,Visual comparisons of shadow removal on the SRD dataset (first two rows) and the USR dataset (last two rows).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie5-3285439-large.gif
2023,10154613,Fig. 6.,"A visualization of BER on the ISTD test set with respect to different training iterations under direct training strategy (DT) and self-paced learning strategy (SPL) respectively. The dashed line represents DT, while the solid line represents SPL. A, S, and N means whole image, shadow region and non-shadow region respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie6-3285439-large.gif
2023,10154613,Fig. 7.,"Visual comparisons of shadow detection. (b), (c) and (d) are shadow masks produced by MaskShadowGAN (MS), direct training (DT) and self-paced learning (SPL) respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie7-3285439-large.gif
2023,10154613,Algorithm 1.,Self-Paced Shadow Detection Training Algorithm,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie11-3285439-large.gif
2023,10154613,Fig. 8.,"An illustration of the removal results using the different components of the proposed pipeline. M1 is our baseline (no mask input of
G
f
). M2 uses raw mask input generated by
G
f
in phase 1 without training a shadow detector. M3 uses the shadow mask generated by a shadow detector trained with the direct training strategy. M4 uses the shadow mask generated by a shadow detector trained with self-paced learning strategy. M5 and M6 use shadow attentive discriminator and color-maintenance loss respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie8-3285439-large.gif
2023,10154613,Fig. 9.,"An illustration of the attention region of the attentive discriminator and the original discriminator. (a), (b) are shadow images and guided shadow masks. (c) and (d) are fake shadow images and attention maps generated by the original discriminator. (e) and (f) are fake shadow images and attention maps generated by the shadow attention discriminator.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie9abcdef-3285439-large.gif
2023,10154613,Fig. 10.,An illustration of two failure cases. (a) shadow images; (b) estimated masks. (c) and (d) are shadow removal results and corresponding ground truth respectively.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/83/9991910/10154613/xie10abcd-3285439-large.gif
2023,10163789,FIGURE 1.,Schema of the experimental design.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste1-3289713-large.gif
2023,10163789,FIGURE 2.,"Clustering results (a) original image, (b) 5 clusters, and (c) 3 clusters using keras.app.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste2ab-3289713-large.gif
2023,10163789,FIGURE 3.,Example of results of clustering to main predefined colors.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste3-3289713-large.gif
2023,10163789,FIGURE 4.,Example of thresholding operation on a frame of “The Godfather” film.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste4-3289713-large.gif
2023,10163789,FIGURE 5.,Survey use diagram.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste5-3289713-large.gif
2023,10163789,FIGURE 6.,Start screen of the survey.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste6-3289713-large.gif
2023,10163789,FIGURE 7.,Operation of the (a) emotion and (b) color checkboxes in the survey.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste7ab-3289713-large.gif
2023,10163789,FIGURE 8.,Accuracy and model loss for RNN on frames of video fragments after clustering operation into RYB model-based colors and binary division of emotion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste8-3289713-large.gif
2023,10163789,FIGURE 9.,Accuracy and model loss for RNN on frames of video fragments after clustering operation into RYB model-based colors and binary division of emotion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste9-3289713-large.gif
2023,10163789,FIGURE 10.,Accuracy and model loss for RNN on frames of video fragments after clustering operation into RYB model-based colors and binary division of emotion.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10163789/koste10-3289713-large.gif
2023,9699417,Fig. 1.,"Meta-learning is an extension of hyper-parameter optimisation, where the meta-parameter
θ
is shared across all tasks. The solid arrows denote forward pass, while the dashed arrows indicate parameter inference, and rectangles illustrate the plate notations. The training subset
{(
x
(t)
ij
,
y
(t)
ij
)
}
m
(t)
i
j=1
of task
T
i
and the meta-parameter
θ
are used to learn the task-specific parameter
λ
i
, corresponding to the lower-level optimisation in (3). The obtained
λ
i
is then used to evaluate the error on the validation subset
{(
x
(v)
ij
,
y
(v)
ij
)
}
m
(v)
i
j=1
to learn the meta-parameter
θ
, corresponding to the upper-level optimisation in (3).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9970415/9699417/nguye1-3147798-large.gif
2023,9699417,Fig. 2.,"SImPa and MAML are compared in a regression problem when training is based on multi-modal data – half of the tasks are generated from sinusoidal functions, and the other half are from linear functions. The shaded area is the prediction made by SImPa
±
standard deviation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9970415/9699417/nguye2-3147798-large.gif
2023,9699417,Fig. 3.,Quantitative comparison between various probabilistic meta-learning approaches averaged over 1000 unseen tasks shows that SImPa has a comparable MSE error and the smallest calibration error.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9970415/9699417/nguye3-3147798-large.gif
2023,9699417,Fig. 4.,Calibration of the “standard” 4-block CNN trained with different meta-learning methods on 5-way 1-shot classification tasks on mini-ImageNet.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/9970415/9699417/nguye4-3147798-large.gif
2023,10015720,FIGURE 1.,"Example of a simple quantum circuit with three qubits. “H” refers to the Hadamard gate and “+” to a controlled X-gate. Finally, the states of qubits
q
0
and
q
1
are measured.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock1-3236409-large.gif
2023,10015720,FIGURE 2.,"Example of a quantum kernel based on the Pauli-feature-map. For simplicity, the figure only shows parts of the circuit.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock2-3236409-large.gif
2023,10015720,FIGURE 3.,Architecture of the variational quantum circuits used in the QNN experiments.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock3-3236409-large.gif
2023,10015720,FIGURE 4.,Quantum circuit demonstrating angle encoding with RY-gates.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock4-3236409-large.gif
2023,10015720,FIGURE 5.,Variational model with three parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock5-3236409-large.gif
2023,10015720,FIGURE 6.,"From the pairwise bivariate distributions of the Vlds dataset, it is apparent that a higher order function is required to separate them, indicating the difficulty of the dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock6-3236409-large.gif
2023,10015720,FIGURE 7.,"From the pairwise bivariate distributions of the custom dataset, it is apparent that a higher order function is required to separate them, indicating the difficulty of the dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock7-3236409-large.gif
2023,10015720,FIGURE 8.,"Pairwise bivariate distribution of the ad hoc dataset indicates a higher order partition function, indicating the difficulty of the dataset.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock8-3236409-large.gif
2023,10015720,FIGURE 9.,Accuracy of different quantum Support Vector Machines using three different feature maps (quantum kernels) with four different entanglement strategies on a quantum simulator for five different datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock9-3236409-large.gif
2023,10015720,FIGURE 10.,QNN circuit variant with 3 qubits and 1 layer used in our experiments. Referred to as q_circuit_01.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock10-3236409-large.gif
2023,10015720,FIGURE 11.,QNN circuit with 4 qubits and 1 layer used in our experiments. Referred to as q_circuit_02.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock11-3236409-large.gif
2023,10015720,FIGURE 12.,QNN circuit with 4 qubits and 1 layer used in our experiments. The qubit or layer count may vary depending on the input feature count or layer count setting. Referred to as q_circuit_03.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock12-3236409-large.gif
2023,10015720,FIGURE 13.,QNN circuit with 4 qubits and 1 layer used in our experiments. Referred to as q_circuit_04.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock13-3236409-large.gif
2023,10015720,FIGURE 14.,QNN circuit with 4 qubits and 1 layer used in our experiments. Referred to as q_circuit_05.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock14-3236409-large.gif
2023,10015720,FIGURE 15.,Accuracy comparison of 10 runs on a quantum simulator and a quantum computer over 5 datasets using 5 different quantum circuits and 4 different optimizers.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10015720/stock15-3236409-large.gif
2023,10129131,Fig. 1.,Overview of the SLR process.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara1-3277749-large.gif
2023,10129131,Fig. 2.,Percentage of papers included between January 2016 and January 2023.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara2-3277749-large.gif
2023,10129131,Fig. 3.,Prisma model depicting no. of records included and excluded.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara3-3277749-large.gif
2023,10129131,Fig. 4.,The location of sensors for the assessment of PD severity and tremor based on gait.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara4-3277749-large.gif
2023,10129131,Fig. 5.,The location of sensors for the assessment of PD severity and tremor based on the movement of the upper limbs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara5-3277749-large.gif
2023,10129131,Fig. 6.,"Pie chart representation of various deep learning models used in PD diagnosis based on gait, upper limb, speech and facial expressions modalities.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara6-3277749-large.gif
2023,10129131,Fig. 7.,"Diagrams representing the accuracy rates of utilized DL methods. The thin black line refers to the standard deviation of the accuracy, in the cases where same models were used for the same classification target.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7333/10031624/10129131/skara7-3277749-large.gif
2023,9466372,Fig. 1.,"Applying IS-free SARSA(
λ
) to the CVF estimation for value-based and actor-critic-based methods. (The left and right dashed boxes show the value-based SMIX(
λ
) and actor-critic-based SMIX(
λ
) algorithms, and the two solid boxes represent the modules involved in the centralized training and decentralized execution respectively. Each agent’s
Q
network (or
π
network) only has access to its own observation, and the centralized
Q
network (or critic network) aggregates all agents’ observation information.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10007736/9466372/tan1-3089493-large.gif
2023,9466372,Fig. 2.,"m
-step matrix game for
m=5
case.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10007736/9466372/tan2-3089493-large.gif
2023,9466372,Fig. 3.,"Average return of QMIX, SMIX(
λ
), and QTRAN on 5-step matrix game for 100 k training steps. (The mean and 95% confidence interval are shown across 20 independent runs.)",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10007736/9466372/tan3-3089493-large.gif
2023,9466372,Fig. 4.,"Test win rates for SMIX(
λ
), revised methods (COMA-R, VDN-R, IQL-R, QTRAN-R) and comparison methods (QMIX, COMA, VDN, IQL, QTRAN) in nine different scenarios. Revised methods are created by replacing the original methods’ CVF estimation procedures with the one adopted by SMIX(
λ
). The performance of our method and revised methods and plotted with a solid line, with their counterparts shown with dashed lines of the same color. The mean and 95% confidence interval are shown across ten independent runs. The legend in (a) applies across all plots. (a) 3 m. (b) 8 m. (c) 2s3z. (d) 3s5z. (e) 2m_vs_1z. (f) 2s_vs_1sc. (g) 3s_vs_3z. (h) 1c3s5z. (i) MMM.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10007736/9466372/tan4abcdefghi-3089493-large.gif
2023,9466372,Fig. 5.,"Sensitivity of SMIX(
λ
) to selected hyperparameters in two different scenarios. The mean and 95% confidence interval is shown across ten independent runs. [(a) and (f) Sensitivity of SMIX(
λ
) to the value of
λ
. (b) and (g) Results using n-step TD with different backup steps. (c) and (h) Comparison between SMIX(
λ
) and its ON-policy version. (d) and (i) Comparison of SMIX(
λ
) with/without OFF-policy correction. (e) and (j) Effects of nonnegative constraints where methods without nonnegative constraints are ended with suffix “-noabs.”]",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/10007736/9466372/tan5abcdefghij-3089493-large.gif
2023,10004504,FIGURE 1.,System model for relay selection in cooperative NOMA network.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10004504/alkha1-3233443-large.gif
2023,10004504,FIGURE 2.,"Average system throughput for several relay selection schemes, where
K=3
,
γ=2bps/Hz
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10004504/alkha2-3233443-large.gif
2023,10004504,FIGURE 3.,"Outage probability comparison between the improved two-stage max-min scheme and the proposed supervised learning based scheme, where
K=3
,
γ=2bps/Hz
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10004504/alkha3-3233443-large.gif
2023,10004504,Fig. 4.,"Impact of increasing the number of relays on the average system throughput of the improved two-stage max-min scheme and the proposed supervised learning based scheme, where
K={3,6}
,
γ=2bps/Hz
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10004504/alkha4-3233443-large.gif
2023,10004504,FIGURE 5.,"Impact of increasing the target data rate on the average system throughput of the improved two-stage max-min scheme and the proposed supervised learning based scheme, where
K=3
,
γ={1,3,5}bps/Hz
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10004504/alkha5-3233443-large.gif
2023,10004504,FIGURE 6.,"Impact of fixing the source transmit power at
P
t
=10
dB,
P
t
=15
dB and
P
t
=20
while changing the relays transmit power on the average system throughput of the improved two-stage max-min scheme and the proposed supervised learning based scheme, where
K=3
,
γ=2bps/Hz
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10004504/alkha6-3233443-large.gif
2023,9599369,Fig. 1.,Federated learning frameworks.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li1-3124599-large.gif
2023,9599369,Fig. 2.,Taxonomy of federated learning systems.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li2-3124599-large.gif
2023,9599369,Fig. 3.,The FATE system structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li3-3124599-large.gif
2023,9599369,Fig. 4.,The TFF system structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li4-3124599-large.gif
2023,9599369,Fig. 5.,The PaddleFL system structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li5-3124599-large.gif
2023,9599369,Fig. 6.,The FedML system structure.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li6-3124599-large.gif
2023,9599369,Fig. 7.,The design factors of FLSs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/69/10063074/9599369/li7-3124599-large.gif
2023,10035975,Fig. 1.,Schematic diagram of RF algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong1-3241995-large.gif
2023,10035975,Fig. 2.,Schematic diagram of BPNN algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong2-3241995-large.gif
2023,10035975,Fig. 3.,Schematic diagram of the Danjiangkou Reservoir control area.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong3-3241995-large.gif
2023,10035975,Fig. 4.,Wavelet analysis results of runoff data series in Danjiangkou Reservoir.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong4-3241995-large.gif
2023,10035975,Fig. 5.,Period distribution of inflow runoff of Danjiangkou Reservoir.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong5-3241995-large.gif
2023,10035975,Fig. 6.,M-K statistic curve of annual runoff in Danjiangkou Reservoir.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong6-3241995-large.gif
2023,10035975,Fig. 7.,Training results obtained using different models without correction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong7-3241995-large.gif
2023,10035975,Fig. 8.,Forecasting results obtained using different models without correction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong8-3241995-large.gif
2023,10035975,Fig. 9.,Training results obtained using different models based on the correction by periodic characteristics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong9-3241995-large.gif
2023,10035975,Fig. 10.,Forecasting results obtained using different models based on the correction by periodic characteristics.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong10-3241995-large.gif
2023,10035975,Fig. 11.,Training and forecasting results based on the improvement by mutation characteristics from 1969 to 1990.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong11-3241995-large.gif
2023,10035975,Fig. 12.,Training and forecasting results based on the improvement by mutation characteristics from 1991 to 2008.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong12-3241995-large.gif
2023,10035975,Fig. 13.,Training and forecasting results based on the improvement by mutation characteristics from 2012 to 2017.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong13-3241995-large.gif
2023,10035975,Fig. 14.,Comparison of different improvement schemes for different models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10035975/xiong14-3241995-large.gif
2023,9913996,Fig. 1.,Topology of DS-FMDPMM. (a) 3-D view. (b) Cross section view.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang1ab-3213083-large.gif
2023,9913996,Fig. 2.,Parametric model of DS-FMDPMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang2-3213083-large.gif
2023,9913996,Fig. 3.,Flowchart of proposed MLT-MLO method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang3-3213083-large.gif
2023,9913996,Fig. 4.,Sensitive analysis of the selected structural parameters.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang4-3213083-large.gif
2023,9913996,Fig. 5.,Flowchart of multiobjective optimization method combining SVR and NSGA-II.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang5-3213083-large.gif
2023,9913996,Fig. 6.,"Comparison of FE simulated and predicted results. (a)
T
avg
. (b)
T
rip
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang6ab-3213083-large.gif
2023,9913996,Fig. 7.,Training performance comparison of models I and II under different sample data quantities.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang7-3213083-large.gif
2023,9913996,Fig. 8.,Flowchart of NSGA-II.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang8-3213083-large.gif
2023,9913996,Fig. 9.,Pareto front plot based on NSGA-II.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang9-3213083-large.gif
2023,9913996,Fig. 10.,"Comparison of FE simulated and predicted results. (a)
T
avg
. (b)
T
rip
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang10ab-3213083-large.gif
2023,9913996,Fig. 11.,Pareto front plot based on NSGA-II.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang11-3213083-large.gif
2023,9913996,Fig. 12.,"Relationship of
T
avg
and
T
rip
with respect to the rotor parameters. (a)
T
avg
versus
h
rm
and
β
rm
. (b)
T
rip
versus
h
rm
and
β
rm
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang12a-3213083-large.gif
2023,9913996,Fig. 13.,Flux density distributions of initial and optimal DS-FMDPMMs. (a) Initial DS-FMDPMM. (b) Optimal DS-FMDPMM.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang13ab-3213083-large.gif
2023,9913996,Fig. 14.,Inner and outer air-gap flux densities of initial and optimal DS-FMDPMMs. (a) Waveforms of inner air-gap flux density. (b) Harmonic spectra of inner air-gap flux density. (c) Waveforms of outer air-gap flux density. (d) Harmonic spectra of outer air-gap flux density.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang14abcd-3213083-large.gif
2023,9913996,Fig. 15.,No-load back EMF comparison of initial and optimal DS-FMDPMMs. (a) Waveforms. (b) Harmonic spectra.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang15ab-3213083-large.gif
2023,9913996,Fig. 16.,Torque performance comparison of initial and optimal DS-FMDPMMs. (a) Cogging torque. (b) Rated torque. (c) Over-load torque.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang16abc-3213083-large.gif
2023,9913996,Fig. 17.,Thermal analysis of optimized DS-FMDPMM. (a) Rated load condition. (b) Over-load condition.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang17ab-3213083-large.gif
2023,9913996,Fig. 18.,Demagnetization analysis of DS-FMDPMM at the over-load condition. (a) SPs on the PMs. (b) Flux density distributions of SPs.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang18ab-3213083-large.gif
2023,9913996,Fig. 19.,Prototype of DS-FMDPMM. (a) Outer stator. (b) Inner stator. (c) Assembled stators. (d) Rotor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang19abcd-3213083-large.gif
2023,9913996,Fig. 20.,Experimental platform.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang20-3213083-large.gif
2023,9913996,Fig. 21.,Comparison of FE-simulated and measured no-load phase back EMF at 600 rpm. (a) Waveforms. (b) Harmonic spectra.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang21ab-3213083-large.gif
2023,9913996,Fig. 22.,"Comparison of FE-simulated and measured torque performances. (a) Rated torque waveforms. (b)
T
avg
against phase current.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6687316/10157995/9913996/fang22ab-3213083-large.gif
2023,9783144,Fig. 1.,"The Search Tree. Top: An example of a 2D line fitting problem, where red points (indexed by numbers) are outliers to be removed. Bottom: Tree structure, i.e., certain subsets of points called a basis (see Section 3.1 for definition) are explored by a globally optimal algorithm [13] (left) and by our unsupervised learning method (right), where red nodes correspond to outliers shown in the top figure and the sequences from top red nodes to bottom ones in the trees show the successive points to be removed. Both methods, determine a set of candidate points, a basis, to eliminate. What differs is the way these two algorithms decide to eliminate these basis points. A* uses a heuristic scoring, which may prioritise choices leading to exploring more of the tree, than the direct path. Our RL-based method learns to extract promising candidates and commits to removing those - avoiding some exploration costs. Explicitly, the admissible heuristic in A* method brings the search into some subparts (green line) before discovering the optimal solution (red line). Our agent learns to remove outliers by traversing from the initial state to the goal state in the minimal number of steps (the states are numbered based on the index of the removed point). Observe that, in this example (not guaranteed in general), both methods terminate at the same solution (i.e., both remove the same set of outliers).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon1-3178442-large.gif
2023,9783144,Fig. 2.,"Minimax Fitting and Basis. Illustration of a minimax fitting problem for a set of points in 2D. Blue dots represent measurements with largest residual, which form the basis set.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon2-3178442-large.gif
2023,9783144,Fig. 3.,"Visualization of states and actions. The initial state corresponds to the minimax fit of the original point set. The action, associated to a state, consists of removing a point in the basis and conduct minimax fit for the remaining points.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon3-3178442-large.gif
2023,9783144,Fig. 4.,"Structure of our proposed framework. Top row shows the (unrolled) operations (we use an instance of 2D line fitting as an example). Given a set of measurements, minimax is performed to obtain the initial state. Then, the state is encoded into a graph representation which is fed to the agent to predict the expected Q value (returns) of choosing each point in basis to eliminate. The agent then performs an action, receives a reward and moves to the next state. This process iterates until the agent reaches the goal state. Bottom row depicts the design of our network (agent).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon4-3178442-large.gif
2023,9783144,Fig. 5.,"2D line fitting on (a) 100 and (b) 200 synthetic data points with various outlier rates. Left: Distribution of differences between predicted consensus and global optimal solution (obtained using A*) with baseline models. Note: lower is better. Right: Run-time of our method compared to baseline models. Our method is very competitive for high number of points with high rate of outliers in terms of achieving optimal solution and less runtime. Note that A* is dropped out in the comparison of runtime for
N=200
since it is very slow in general for high number of outliers.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon5-3178442-large.gif
2023,9783144,Fig. 6.,3D plane fitting on (a) 100 and (b) 200 points with varying outlier rates. Left: Difference between predicted consensus and global optimal solution (using A*) with baseline models (lower is better). Right: Run-time of our method compared to baseline models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon6-3178442-large.gif
2023,9783144,Fig. 7.,"Comparison between different choices of reward functions (on training) using the (Left) ModelNet40 and (Right) KITTI datasets.
λ=0
corresponds to constant reward function: counting discarded points,
0<λ<∞
corresponds to reward function counting outliers and evaluating fitting residual (weighted with hyperparameter
λ
),
λ=∞
corresponds to reward function evaluating fitting residual only. Unless
λ
is too big (close to
∞
) or too small (close to 0), different choices of
λ
have relatively small impact on predicted results.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon7-3178442-large.gif
2023,9783144,Fig. 8.,Comparison of the performance of some variants of Deep Q learning adopted in training DDQN achieves the best performance for linearized fundamental matrix estimation on KITTI dataset while DQN-PER and DQN-Dueling Network have very similar performance.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon8-3178442-large.gif
2023,9783144,Fig. 9.,"Comparison of the performance of SuperGlue and our methods on ModelNet40 dataset. The distribution of consensus size per outlier rate is reported. “SuperGlue matches” are the counts before pruning technical outliers from those returned by SuperGlue. “SuperGlue inliers” are the counts after pruning. On average, our method clearly outperforms Superglue on outlier rates up to 40%. Our method never declares outliers to be inliers (and when we intervene to strip out the outliers Superglue has incorrectly classified as inliers (blue)), we still have much higher inlier counts (green). Even at 50% outliers, where Superglue does often return more true inliers than our method, more than half of the time it still returns few genuine inliers (horizontal bars are the median of the distibutions).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon9-3178442-large.gif
2023,9783144,Fig. 10.,"Linearized Fundamental Matrix Estimation on real datasets. Left: Consensus size (%). Right: Run times in log scale. Similar to the experiment on the ModelNet40 dataset, our method, on average, finds
2%−5%
higher consensus with affordable time (
∼1
second).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon10-3178442-large.gif
2023,9783144,Fig. 11.,Homography ftting on the KITTI dataset. Left: Consensus size (%) comparison. Right: Run time comparison in log scale.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon11-3178442-large.gif
2023,9783144,Fig. 12.,"Qualitative results of our method with different choices of reward function on the KITTI dataset. Note that we only draw the set of correspondences, represented by green lines, that the methods recognize as inliers. The dashed-red rectangular area highlights the mismatches. Left: Adaptive reward (
λ=1
); Middle: Constant reward (
λ=0
); Right: RANSAC.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/34/10036240/9783144/truon12-3178442-large.gif
2023,9910017,Fig. 1.,"An overview of the proposed network. The proposed network consists of an input layer, encoding module, rule module, and inference module. The nomenclatures we used in the diagram are described in Section III.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10006388/9910017/yao1-3211765-large.gif
2023,9910017,Fig. 2.,"Interpretation of a trained model on synthetic dataset 1 with
N=400
. (a) Visualization of four rules contributing to the positive class, which are summarized from the trained model. Rules are visualized in individual columns with each row corresponding to concept. For example, “x1_low” means “the value of
x
1
is low”. The contribution of individual concepts to individual rules are shown in color. (b) Membership functions for “low”, “medium”, and “high” concepts of
x
1
,
x
2
,
x
3
, and
x
4
in the encoding module, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10006388/9910017/yao2-3211765-large.gif
2023,9910017,Fig. 3.,"Interpretation of a trained model on synthetic dataset 2 with
N=400
.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10006388/9910017/yao3-3211765-large.gif
2023,9910017,Fig. 4.,Interpretation of a trained model on the heart failure dataset. (a) Rule visualization; (b) Trained membership functions for continuous/ordinal variables involved in the rules shown in (a).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6221020/10006388/9910017/yao4-3211765-large.gif
2023,10108979,FIGURE 1.,The example synthetic photos are in the top row. The bottom row shows a snapshot of the 3D models that correspond to the photos shown in the top row.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10108979/akinc1-3270438-large.gif
2023,10108979,FIGURE 2.,"Measurements selected, from top left to bottom right: 1) Alar base width, 2) Columella length, 3) Columella width, 4) Interalar distance, 5) Nasal bridge length, 6) Nasal tip projection, 7) Interalar angle, 8) Nasal tip angle, 9) Nasal tip projection: Goode, and 10) Nasal width-length ratio.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10108979/akinc2ab-3270438-large.gif
2023,10108979,FIGURE 3.,"The 22 marked facial feature points are shown on 3D models. The landmarks are Alar_Base_Junction - left (ac_l), Alar_Base_Junction - right (ac_r), Alar Flare - left (al_l), Alar Flare - right (al_r), Columellar Break Point (cb), Endocanthion - left (en_l), Endocanthion - right (en_r), Maxilloanteriorale - left (ma_l), Maxilloanteriorale - right (ma_r), Maxillofrontale - left (mf_l), Maxillofrontale - right (mf_r), Nasal Parenthesis - left (np_l), Nasal Parenthesis - right (np_r), Nasion/Radix (n), Pronasale/Tip (prn), Rhinion (r), Subnasale - left (sn_l), Subnasale - right (sn_r), Subnasale (sn), Supratip Break Point (s), Tip Defining Point - left (td_l), Tip Defining Point - right (td_r).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10108979/akinc3-3270438-large.gif
2023,10108979,FIGURE 4.,"Nose types used. From top left to bottom right in order: Crooked, Drooping Tip, Hooked, Hump, Large, Pinched, Saddle, Snub, and Wide.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10108979/akinc4ab-3270438-large.gif
2023,10108979,FIGURE 5.,A screenshot of this web-based software for scoring rhinoplasty outcomes.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10108979/akinc5-3270438-large.gif
2023,9966840,Fig. 1.,"Typical CT slices with bowel contours (the left two columns) and their 3D manually-labeled masks (the last column). The images in each row are from the same patient, arrows indicate gaseous holes within the bowel. (a) Fully-labeled data, with the entire bowel annotated. (b) Partially-labeled data, with the colon and sigmoid annotated. (c) Partially-labeled data, with only the jejunum-ileum annotated.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang1abc-3225667-large.gif
2023,9966840,Fig. 2.,"The pipeline of our BowelNet method for the entire bowel segmentation. In the first stage, BowelNet detects five bowel region proposals, namely: duodenum, jejunum-ileum, colon, sigmoid, and rectum (the figure only shows three out of these five regions). The second stage of BowelNet produces the fine segmentation of each detected bowel region from the first stage.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang2-3225667-large.gif
2023,9966840,Fig. 3.,"The architecture of the second stage of BowelNet for fine bowel segmentation. For better visualization, we use red arrows around black skeleton curves to represent the unit skeleton flux vectors.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang3-3225667-large.gif
2023,9966840,Fig. 4.,"The detailed architecture of the meta segmentor. Values inside the bracket refer to the convolutional kernel size, the number of input channels, and the number of output channels, respectively. © represents the channel-wise concatenation operation.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang4-3225667-large.gif
2023,9966840,Fig. 5.,Visual comparison of different methods on the entire bowel segmentation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang5-3225667-large.gif
2023,9966840,Fig. 6.,"Typical segmentation results with predicted bowel boundary in (c) and (e), and predicted bowel skeleton in (d) and (e).",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang6-3225667-large.gif
2023,9966840,Fig. 7.,Visual comparison of segmentations by the meta segmentor using different supervision signals. (a) Ground truth. (b) Average segmentation result of the two base segmentors with gray regions showing their disagreed segmentations. (c)~(g) Segmentation result of meta segmentor trained with different supervision signals.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang7abcdefg-3225667-large.gif
2023,9966840,Fig. 8.,An example of the jejunum-ileum segmentation by the meta segmentor. (a) Ground truth. (b) Segmentation result without adaptation to unlabeled testing data. (c) Segmentation result with adaptation to unlabeled testing data.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang8abc-3225667-large.gif
2023,9966840,Fig. 9.,An example of the reference and predicted skeletons of the jejunum-ileum. (a) Ground-truth mask. (b) Reference skeleton extracted by the traditional skeletonization algorithm [37] (note that a dilation operation is used for better visualization). (c) Predicted skeleton from the S segmentor.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/10091712/9966840/wang9abc-3225667-large.gif
2023,10044684,FIGURE 1.,Overview of the study.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki1-3245565-large.gif
2023,10044684,FIGURE 2.,ECG acquisition system (a) AgCl metal plate with lead cable (b) Sensors located on the steering wheel (c) ECG signal from channel 1 and heart rate from channel 2.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki2abc-3245565-large.gif
2023,10044684,FIGURE 3.,ECG acquisition algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki3-3245565-large.gif
2023,10044684,FIGURE 4.,Driver UI panel.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki4-3245565-large.gif
2023,10044684,FIGURE 5.,ECG data with four classes (a) Normal (b) AF (c) Other (d) Noisy.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki5abcd-3245565-large.gif
2023,10044684,FIGURE 6.,"Baseline fluctuation removal and high-frequency region noise cancellation (a) Original ECG signal from the driver (b) One-dimension median filtered ECG (c) One-dimension median filtered and low-pass filtered ECG (d) One-dimension median filtered, low-pass filtered, and moving averaged ECG.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki6abcd-3245565-large.gif
2023,10044684,FIGURE 7.,Wrong detection of P peaks by algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki7-3245565-large.gif
2023,10044684,FIGURE 8.,Comparison of P peak detection (a) P peak detection by a surgeon (b) P peak detection by algorithm (70%–95%) (c) P peak detection by algorithm (80%–95%).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki8abc-3245565-large.gif
2023,10044684,FIGURE 9.,Improvement of peak detection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki9-3245565-large.gif
2023,10044684,FIGURE 10.,Spectrogram and calculated threshold for each ECG class to determine the noise area (a) Smoothed spectrogram of a normal ECG (b) Smoothed spectrogram of an AF ECG (c) Smoothed spectrogram of a noisy ECG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki10abc-3245565-large.gif
2023,10044684,FIGURE 11.,Removal of noise region (a) Noise region (red mark) (b) ECG signal after noise removal.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki11ab-3245565-large.gif
2023,10044684,FIGURE 12.,R peak interval of AF ECG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki12-3245565-large.gif
2023,10044684,FIGURE 13.,Characteristics of normal and AF ECGs in Lorenz plot (a) Twelve areas of the Lorenz plot (b) Two-dimension histogram of normal ECG (c) Two-dimension histogram of AF ECG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki13abc-3245565-large.gif
2023,10044684,FIGURE 14.,Clustering using R peak shifts in normal and AF ECGs (a) Clustering of inter-beat intervals of normal ECG (b) Clustering of inter-beat intervals of AF ECG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki14ab-3245565-large.gif
2023,10044684,FIGURE 15.,Characteristics in ECG peak points (a) Clean ECG (b) Noisy ECG.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki15ab-3245565-large.gif
2023,10044684,FIGURE 16.,Two-stage classification structure using four machine learning models.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki16-3245565-large.gif
2023,10044684,FIGURE 17.,Machine learning models with features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki17-3245565-large.gif
2023,10044684,FIGURE 18.,Improvement of F1 score with selected features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki18-3245565-large.gif
2023,10044684,FIGURE 19.,ECG acquisition and classification in the vehicle.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki19-3245565-large.gif
2023,10044684,FIGURE 20.,Comparison of classification performance using all features and optimal feature subset (a) Confusion matrix with all features (b) Confusion matrix with the feature subset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki20ab-3245565-large.gif
2023,10044684,FIGURE 21.,Prediction time comparison between all features and subset features.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/10005208/10044684/hanki21-3245565-large.gif
2023,10004893,Fig. 1.,A molecule for CRO.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam1-3233473-large.gif
2023,10004893,Fig. 2.,On-wall ineffective collision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam2-3233473-large.gif
2023,10004893,Fig. 3.,Decomposition reaction.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam3-3233473-large.gif
2023,10004893,Fig. 4.,Inter-molecular ineffective collision.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam4-3233473-large.gif
2023,10004893,Fig. 5.,Synthesis.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam5-3233473-large.gif
2023,10004893,Fig. 6.,Use of repair function.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam6-3233473-large.gif
2023,10004893,Fig. 7.,Flowchart for CRO operator selection.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam7-3233473-large.gif
2023,10004893,Fig. 8.,Block diagram of the proposed method.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam8-3233473-large.gif
2023,10004893,Fig. 9.,Performance comparison of the proposed method with the other existing methods for the S. cerevisiae dataset.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8857/10144451/10004893/islam9-3233473-large.gif
2023,10141545,Fig. 1.,"Number of published articles combining ML, DA and UQ according to google scholar. “A + B” denotes the number of articles which include “A” in the title and “B” in the text.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-1-source-large.gif
2023,10141545,Fig. 2.,"Combination of ML, DA and UQ methods and challenges versus available data dimension and noise level.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-2-source-large.gif
2023,10141545,Fig. 3.,Progression of ML-based reduced-order-modelling.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-3-source-large.gif
2023,10141545,Fig. 4.,"Correction of a two-dimensional quasi-geostrophic model using a neural network. Top panels: True model error snapshots for a 1-day (left) or 2-day (right) integration. Bottom panels: Corresponding neural network predictions, based on the analysis increments. Figure reproduced from [143].",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-4-source-large.gif
2023,10141545,Fig. 5.,"Schematic illustration of the coordinate descent minimisation. DA steps are used to estimate the system state from sparse and noisy observations, and ML steps are used to estimate the parameters of the statistical correction based on the estimated states. The process can be iterated for increased accuracy in the estimation. Here,
F
i
,
F
a
,
F
∗
denote “initial-”, “analysis-”, and “optimal correction”, respectively.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-5-source-large.gif
2023,10141545,Fig. 6.,Workflow of training and applying RNN for observation covariance estimation.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-6-source-large.gif
2023,10141545,Fig. 7.,Sketch of end-to-end neural architectures for DA: Some architectures mimic the forecasting and analysis steps of sequential DA at each time step (see (16)) (left panel) whereas others implement iterative gradient descents for a variational DA criterion (right panel).,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-7-source-large.gif
2023,10141545,Fig. 8.,Workflow of online LA with surrogate modelling and different encoding-decoding strategies.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6570654/10141540/10141545/JAS100603-fig-8-source-large.gif
2023,10121783,FIGURE 1.,Overview of the different FL scenarios.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782661/10008219/10121783/guerr1-3274394-large.gif
2023,10121783,FIGURE 2.,Overview of the operations executed by a node in the GFL algorithm.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782661/10008219/10121783/guerr2-3274394-large.gif
2023,10121783,FIGURE 3.,Distribution of samples across the four first clients for both EMNIST and EMNISTp federated datasets.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782661/10008219/10121783/guerr3-3274394-large.gif
2023,10121783,FIGURE 4.,Training and validation accuracy on EMNIST and EMNISTp.,https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782661/10008219/10121783/guerr4-3274394-large.gif
2023,10121783,FIGURE 5.,"Blockchain delay as a function of the number of miners (
N
m
) and the block interval (
BI
). The fork probability associated with each
N
m
is shown in red.",https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/8782661/10008219/10121783/guerr5-3274394-large.gif
